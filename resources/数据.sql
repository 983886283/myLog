/*
 Navicat Premium Data Transfer

 Source Server         : localhost
 Source Server Type    : MySQL
 Source Server Version : 80012
 Source Host           : localhost:3306
 Source Schema         : blogdb

 Target Server Type    : MySQL
 Target Server Version : 80012
 File Encoding         : 65001

 Date: 24/09/2018 21:37:13
*/

SET NAMES utf8mb4;
SET FOREIGN_KEY_CHECKS = 0;

-- ----------------------------
-- Table structure for ipconfig
-- ----------------------------
DROP TABLE IF EXISTS `ipconfig`;
CREATE TABLE `ipconfig` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `ip` varchar(20) NOT NULL DEFAULT '',
  `num` int(20) NOT NULL DEFAULT '1',
  `state` int(1) NOT NULL DEFAULT '1',
  `createTime` date NOT NULL,
  `remark` varchar(100) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=1374 DEFAULT CHARSET=utf8;

-- ----------------------------
-- Records of ipconfig
-- ----------------------------
BEGIN;
INSERT INTO `ipconfig` VALUES (24, '36.110.113.210', 11, 1, '2017-07-08', '本机');
INSERT INTO `ipconfig` VALUES (25, '111.206.221.47', 1, 1, '2017-07-08', NULL);
INSERT INTO `ipconfig` VALUES (26, '123.122.22.34', 18, 1, '2017-07-08', '家中本机');
INSERT INTO `ipconfig` VALUES (27, '111.206.221.29', 1, 1, '2017-07-09', NULL);
INSERT INTO `ipconfig` VALUES (28, '111.206.221.44', 1, 1, '2017-07-09', NULL);
INSERT INTO `ipconfig` VALUES (30, '123.119.242.12', 1, 1, '2017-07-09', NULL);
INSERT INTO `ipconfig` VALUES (31, '111.206.221.26', 1, 1, '2017-07-09', NULL);
INSERT INTO `ipconfig` VALUES (32, '111.206.221.22', 1, 1, '2017-07-10', NULL);
INSERT INTO `ipconfig` VALUES (33, '211.144.1.74', 24, 1, '2017-07-10', NULL);
INSERT INTO `ipconfig` VALUES (34, '111.206.221.99', 1, 1, '2017-07-10', NULL);
INSERT INTO `ipconfig` VALUES (36, '111.206.221.109', 1, 1, '2017-07-10', NULL);
INSERT INTO `ipconfig` VALUES (38, '111.206.221.50', 1, 1, '2017-07-11', NULL);
INSERT INTO `ipconfig` VALUES (39, '111.206.221.81', 1, 1, '2017-07-11', NULL);
INSERT INTO `ipconfig` VALUES (41, '211.144.1.74', 2, 1, '2017-07-11', NULL);
INSERT INTO `ipconfig` VALUES (42, '111.206.221.84', 1, 1, '2017-07-11', NULL);
INSERT INTO `ipconfig` VALUES (43, '117.136.0.32', 1, 1, '2017-07-11', NULL);
INSERT INTO `ipconfig` VALUES (44, '123.119.251.229', 1, 1, '2017-07-11', NULL);
INSERT INTO `ipconfig` VALUES (45, '111.206.221.115', 1, 1, '2017-07-12', NULL);
INSERT INTO `ipconfig` VALUES (46, '111.206.221.38', 1, 1, '2017-07-12', NULL);
INSERT INTO `ipconfig` VALUES (47, '211.144.1.74', 21, 1, '2017-07-12', NULL);
INSERT INTO `ipconfig` VALUES (48, '111.206.221.94', 1, 1, '2017-07-12', NULL);
INSERT INTO `ipconfig` VALUES (49, '111.206.221.73', 1, 1, '2017-07-13', NULL);
INSERT INTO `ipconfig` VALUES (51, '211.144.1.74', 2, 1, '2017-07-13', NULL);
INSERT INTO `ipconfig` VALUES (52, '36.110.113.210', 2, 1, '2017-07-13', NULL);
INSERT INTO `ipconfig` VALUES (53, '111.206.221.27', 1, 1, '2017-07-13', NULL);
INSERT INTO `ipconfig` VALUES (54, '66.249.75.20', 1, 1, '2017-07-13', NULL);
INSERT INTO `ipconfig` VALUES (55, '123.119.251.229', 1, 1, '2017-07-13', NULL);
INSERT INTO `ipconfig` VALUES (56, '111.206.221.37', 1, 1, '2017-07-14', NULL);
INSERT INTO `ipconfig` VALUES (57, '1.62.187.128', 1, 1, '2017-07-14', NULL);
INSERT INTO `ipconfig` VALUES (58, '101.226.79.182', 1, 1, '2017-07-14', NULL);
INSERT INTO `ipconfig` VALUES (59, '36.110.113.210', 1, 1, '2017-07-14', NULL);
INSERT INTO `ipconfig` VALUES (60, '111.206.221.104', 1, 1, '2017-07-14', NULL);
INSERT INTO `ipconfig` VALUES (61, '111.206.221.101', 1, 1, '2017-07-14', NULL);
INSERT INTO `ipconfig` VALUES (62, '111.206.221.88', 1, 1, '2017-07-15', NULL);
INSERT INTO `ipconfig` VALUES (63, '111.206.221.37', 1, 1, '2017-07-15', NULL);
INSERT INTO `ipconfig` VALUES (64, '123.119.251.229', 2, 1, '2017-07-15', NULL);
INSERT INTO `ipconfig` VALUES (65, '111.206.221.87', 1, 1, '2017-07-15', NULL);
INSERT INTO `ipconfig` VALUES (66, '111.206.221.47', 1, 1, '2017-07-16', NULL);
INSERT INTO `ipconfig` VALUES (67, '111.206.221.95', 1, 1, '2017-07-16', NULL);
INSERT INTO `ipconfig` VALUES (68, '111.206.221.76', 1, 1, '2017-07-16', NULL);
INSERT INTO `ipconfig` VALUES (69, '111.206.221.24', 1, 1, '2017-07-17', NULL);
INSERT INTO `ipconfig` VALUES (70, '111.206.221.38', 1, 1, '2017-07-17', NULL);
INSERT INTO `ipconfig` VALUES (71, '211.144.1.74', 3, 1, '2017-07-17', NULL);
INSERT INTO `ipconfig` VALUES (72, '111.206.221.47', 1, 1, '2017-07-17', NULL);
INSERT INTO `ipconfig` VALUES (73, '111.206.222.153', 1, 1, '2017-07-17', NULL);
INSERT INTO `ipconfig` VALUES (74, '114.252.24.77', 1, 1, '2017-07-17', NULL);
INSERT INTO `ipconfig` VALUES (75, '66.249.70.18', 1, 1, '2017-07-18', NULL);
INSERT INTO `ipconfig` VALUES (76, '111.206.221.42', 1, 1, '2017-07-18', NULL);
INSERT INTO `ipconfig` VALUES (77, '66.249.64.156', 2, 1, '2017-07-18', NULL);
INSERT INTO `ipconfig` VALUES (78, '211.144.1.74', 1, 1, '2017-07-18', NULL);
INSERT INTO `ipconfig` VALUES (79, '111.206.221.37', 1, 1, '2017-07-18', NULL);
INSERT INTO `ipconfig` VALUES (80, '66.249.70.28', 1, 1, '2017-07-18', NULL);
INSERT INTO `ipconfig` VALUES (81, '111.206.221.98', 1, 1, '2017-07-18', NULL);
INSERT INTO `ipconfig` VALUES (82, '111.206.221.109', 1, 1, '2017-07-19', NULL);
INSERT INTO `ipconfig` VALUES (83, '111.206.221.90', 1, 1, '2017-07-19', NULL);
INSERT INTO `ipconfig` VALUES (84, '211.144.1.74', 42, 1, '2017-07-19', NULL);
INSERT INTO `ipconfig` VALUES (85, '36.110.113.210', 11, 1, '2017-07-19', NULL);
INSERT INTO `ipconfig` VALUES (86, '111.206.221.33', 1, 1, '2017-07-19', NULL);
INSERT INTO `ipconfig` VALUES (87, '111.206.221.99', 1, 1, '2017-07-20', NULL);
INSERT INTO `ipconfig` VALUES (88, '111.206.221.48', 1, 1, '2017-07-21', NULL);
INSERT INTO `ipconfig` VALUES (89, '111.206.221.29', 1, 1, '2017-07-21', NULL);
INSERT INTO `ipconfig` VALUES (90, '36.110.113.210', 3, 1, '2017-07-21', NULL);
INSERT INTO `ipconfig` VALUES (91, '111.206.221.42', 1, 1, '2017-07-22', NULL);
INSERT INTO `ipconfig` VALUES (92, '111.206.221.74', 1, 1, '2017-07-22', NULL);
INSERT INTO `ipconfig` VALUES (93, '211.144.1.74', 9, 1, '2017-07-22', NULL);
INSERT INTO `ipconfig` VALUES (94, '111.206.221.47', 1, 1, '2017-07-22', NULL);
INSERT INTO `ipconfig` VALUES (95, '36.110.113.210', 3, 1, '2017-07-22', NULL);
INSERT INTO `ipconfig` VALUES (96, '220.181.108.113', 1, 1, '2017-07-22', NULL);
INSERT INTO `ipconfig` VALUES (97, '111.206.221.39', 1, 1, '2017-07-23', NULL);
INSERT INTO `ipconfig` VALUES (98, '220.181.108.152', 1, 1, '2017-07-23', NULL);
INSERT INTO `ipconfig` VALUES (99, '111.206.221.70', 1, 1, '2017-07-23', NULL);
INSERT INTO `ipconfig` VALUES (100, '114.252.5.144', 2, 1, '2017-07-23', NULL);
INSERT INTO `ipconfig` VALUES (101, '180.97.35.219', 1, 1, '2017-07-23', NULL);
INSERT INTO `ipconfig` VALUES (102, '111.206.221.34', 1, 1, '2017-07-23', NULL);
INSERT INTO `ipconfig` VALUES (103, '111.206.221.49', 1, 1, '2017-07-24', NULL);
INSERT INTO `ipconfig` VALUES (104, '220.181.108.176', 1, 1, '2017-07-24', NULL);
INSERT INTO `ipconfig` VALUES (105, '111.206.221.95', 1, 1, '2017-07-24', NULL);
INSERT INTO `ipconfig` VALUES (106, '211.144.1.74', 5, 1, '2017-07-24', NULL);
INSERT INTO `ipconfig` VALUES (107, '36.110.113.210', 2, 1, '2017-07-24', NULL);
INSERT INTO `ipconfig` VALUES (108, '111.206.221.44', 1, 1, '2017-07-24', NULL);
INSERT INTO `ipconfig` VALUES (109, '111.206.221.84', 1, 1, '2017-07-25', NULL);
INSERT INTO `ipconfig` VALUES (110, '36.110.113.210', 18, 1, '2017-07-25', NULL);
INSERT INTO `ipconfig` VALUES (111, '111.206.221.66', 1, 1, '2017-07-25', NULL);
INSERT INTO `ipconfig` VALUES (112, '220.181.108.108', 1, 1, '2017-07-25', NULL);
INSERT INTO `ipconfig` VALUES (113, '211.144.1.74', 13, 1, '2017-07-25', NULL);
INSERT INTO `ipconfig` VALUES (114, '111.206.221.108', 1, 1, '2017-07-25', NULL);
INSERT INTO `ipconfig` VALUES (115, '111.206.221.111', 1, 1, '2017-07-26', NULL);
INSERT INTO `ipconfig` VALUES (116, '220.181.108.114', 1, 1, '2017-07-26', NULL);
INSERT INTO `ipconfig` VALUES (117, '111.206.221.68', 1, 1, '2017-07-26', NULL);
INSERT INTO `ipconfig` VALUES (118, '114.252.5.144', 19, 1, '2017-07-26', NULL);
INSERT INTO `ipconfig` VALUES (119, '111.206.221.78', 1, 1, '2017-07-26', NULL);
INSERT INTO `ipconfig` VALUES (120, '114.242.249.95', 2, 1, '2017-07-26', NULL);
INSERT INTO `ipconfig` VALUES (121, '101.226.33.238', 1, 1, '2017-07-26', NULL);
INSERT INTO `ipconfig` VALUES (122, '111.206.221.41', 2, 1, '2017-07-27', NULL);
INSERT INTO `ipconfig` VALUES (123, '223.71.231.36', 3, 1, '2017-07-27', NULL);
INSERT INTO `ipconfig` VALUES (124, '117.136.38.56', 1, 1, '2017-07-27', NULL);
INSERT INTO `ipconfig` VALUES (125, '111.206.221.48', 1, 1, '2017-07-27', NULL);
INSERT INTO `ipconfig` VALUES (126, '123.122.14.189', 8, 1, '2017-07-27', NULL);
INSERT INTO `ipconfig` VALUES (127, '111.206.221.69', 1, 1, '2017-07-28', NULL);
INSERT INTO `ipconfig` VALUES (128, '66.249.69.90', 1, 1, '2017-07-28', NULL);
INSERT INTO `ipconfig` VALUES (129, '66.249.69.92', 3, 1, '2017-07-28', NULL);
INSERT INTO `ipconfig` VALUES (130, '111.206.221.66', 1, 1, '2017-07-28', NULL);
INSERT INTO `ipconfig` VALUES (131, '111.206.221.104', 1, 1, '2017-07-28', NULL);
INSERT INTO `ipconfig` VALUES (132, '211.144.1.74', 1, 1, '2017-07-28', NULL);
INSERT INTO `ipconfig` VALUES (133, '123.122.14.189', 1, 1, '2017-07-28', NULL);
INSERT INTO `ipconfig` VALUES (134, '111.206.221.91', 1, 1, '2017-07-29', NULL);
INSERT INTO `ipconfig` VALUES (135, '111.206.221.87', 1, 1, '2017-07-29', NULL);
INSERT INTO `ipconfig` VALUES (136, '111.206.221.100', 1, 1, '2017-07-29', NULL);
INSERT INTO `ipconfig` VALUES (137, '111.206.221.106', 1, 1, '2017-07-30', NULL);
INSERT INTO `ipconfig` VALUES (138, '123.122.14.189', 1, 1, '2017-07-30', NULL);
INSERT INTO `ipconfig` VALUES (139, '111.206.221.87', 1, 1, '2017-07-30', NULL);
INSERT INTO `ipconfig` VALUES (140, '111.206.221.94', 1, 1, '2017-07-30', NULL);
INSERT INTO `ipconfig` VALUES (141, '111.206.221.47', 1, 1, '2017-07-31', NULL);
INSERT INTO `ipconfig` VALUES (142, '111.206.221.22', 1, 1, '2017-07-31', NULL);
INSERT INTO `ipconfig` VALUES (143, '211.144.1.74', 2, 1, '2017-07-31', NULL);
INSERT INTO `ipconfig` VALUES (144, '111.206.221.25', 1, 1, '2017-07-31', NULL);
INSERT INTO `ipconfig` VALUES (145, '111.206.221.67', 1, 1, '2017-08-01', NULL);
INSERT INTO `ipconfig` VALUES (146, '111.206.221.103', 1, 1, '2017-08-01', NULL);
INSERT INTO `ipconfig` VALUES (147, '111.206.221.108', 1, 1, '2017-08-01', NULL);
INSERT INTO `ipconfig` VALUES (148, '111.206.221.74', 1, 1, '2017-08-02', NULL);
INSERT INTO `ipconfig` VALUES (149, '111.206.221.46', 1, 1, '2017-08-02', NULL);
INSERT INTO `ipconfig` VALUES (150, '111.206.221.82', 1, 1, '2017-08-02', NULL);
INSERT INTO `ipconfig` VALUES (151, '111.206.221.34', 1, 1, '2017-08-03', NULL);
INSERT INTO `ipconfig` VALUES (152, '111.206.221.95', 1, 1, '2017-08-03', NULL);
INSERT INTO `ipconfig` VALUES (153, '211.144.1.74', 1, 1, '2017-08-03', NULL);
INSERT INTO `ipconfig` VALUES (154, '111.206.221.32', 1, 1, '2017-08-03', NULL);
INSERT INTO `ipconfig` VALUES (155, '36.110.113.210', 2, 1, '2017-08-03', NULL);
INSERT INTO `ipconfig` VALUES (156, '111.206.221.48', 1, 1, '2017-08-04', NULL);
INSERT INTO `ipconfig` VALUES (157, '111.206.221.38', 1, 1, '2017-08-04', NULL);
INSERT INTO `ipconfig` VALUES (158, '36.110.113.210', 8, 1, '2017-08-04', NULL);
INSERT INTO `ipconfig` VALUES (159, '111.206.221.36', 1, 1, '2017-08-05', NULL);
INSERT INTO `ipconfig` VALUES (160, '220.181.108.170', 1, 1, '2017-08-05', NULL);
INSERT INTO `ipconfig` VALUES (161, '111.206.221.79', 1, 1, '2017-08-05', NULL);
INSERT INTO `ipconfig` VALUES (162, '111.206.221.29', 1, 1, '2017-08-05', NULL);
INSERT INTO `ipconfig` VALUES (163, '66.249.70.26', 2, 1, '2017-08-05', NULL);
INSERT INTO `ipconfig` VALUES (164, '123.149.31.53', 1, 1, '2017-08-05', NULL);
INSERT INTO `ipconfig` VALUES (165, '220.181.108.146', 1, 1, '2017-08-06', NULL);
INSERT INTO `ipconfig` VALUES (166, '111.206.221.46', 1, 1, '2017-08-06', NULL);
INSERT INTO `ipconfig` VALUES (167, '114.252.20.222', 1, 1, '2017-08-06', NULL);
INSERT INTO `ipconfig` VALUES (168, '111.206.221.114', 1, 1, '2017-08-06', NULL);
INSERT INTO `ipconfig` VALUES (169, '220.181.108.105', 1, 1, '2017-08-07', NULL);
INSERT INTO `ipconfig` VALUES (170, '111.206.221.35', 1, 1, '2017-08-07', NULL);
INSERT INTO `ipconfig` VALUES (171, '111.206.221.110', 1, 1, '2017-08-07', NULL);
INSERT INTO `ipconfig` VALUES (172, '111.206.221.112', 1, 1, '2017-08-07', NULL);
INSERT INTO `ipconfig` VALUES (173, '111.206.221.99', 1, 1, '2017-08-08', NULL);
INSERT INTO `ipconfig` VALUES (174, '111.206.221.47', 1, 1, '2017-08-08', NULL);
INSERT INTO `ipconfig` VALUES (175, '111.206.221.34', 1, 1, '2017-08-08', NULL);
INSERT INTO `ipconfig` VALUES (176, '36.110.113.210', 1, 1, '2017-08-08', NULL);
INSERT INTO `ipconfig` VALUES (177, '111.206.221.97', 1, 1, '2017-08-09', NULL);
INSERT INTO `ipconfig` VALUES (178, '111.206.222.80', 1, 1, '2017-08-09', NULL);
INSERT INTO `ipconfig` VALUES (179, '111.206.221.111', 1, 1, '2017-08-09', NULL);
INSERT INTO `ipconfig` VALUES (180, '121.5.176.58', 4, 1, '2017-08-09', NULL);
INSERT INTO `ipconfig` VALUES (181, '111.206.221.92', 1, 1, '2017-08-09', NULL);
INSERT INTO `ipconfig` VALUES (182, '111.206.221.70', 1, 1, '2017-08-10', NULL);
INSERT INTO `ipconfig` VALUES (183, '111.206.221.96', 1, 1, '2017-08-10', NULL);
INSERT INTO `ipconfig` VALUES (184, '111.206.221.38', 1, 1, '2017-08-10', NULL);
INSERT INTO `ipconfig` VALUES (185, '106.108.73.213', 1, 1, '2017-08-10', NULL);
INSERT INTO `ipconfig` VALUES (186, '27.26.56.49', 1, 1, '2017-08-10', NULL);
INSERT INTO `ipconfig` VALUES (187, '111.206.222.154', 1, 1, '2017-08-11', NULL);
INSERT INTO `ipconfig` VALUES (188, '36.110.113.210', 3, 1, '2017-08-11', NULL);
INSERT INTO `ipconfig` VALUES (189, '111.206.222.220', 1, 1, '2017-08-11', NULL);
INSERT INTO `ipconfig` VALUES (190, '111.206.221.77', 1, 1, '2017-08-12', NULL);
INSERT INTO `ipconfig` VALUES (191, '111.206.221.88', 1, 1, '2017-08-12', NULL);
INSERT INTO `ipconfig` VALUES (192, '211.144.1.74', 4, 1, '2017-08-12', NULL);
INSERT INTO `ipconfig` VALUES (193, '36.110.113.210', 1, 1, '2017-08-12', NULL);
INSERT INTO `ipconfig` VALUES (194, '111.206.221.50', 1, 1, '2017-08-12', NULL);
INSERT INTO `ipconfig` VALUES (195, '123.122.20.29', 8, 1, '2017-08-12', NULL);
INSERT INTO `ipconfig` VALUES (196, '111.206.222.201', 1, 1, '2017-08-13', NULL);
INSERT INTO `ipconfig` VALUES (197, '111.206.221.72', 1, 1, '2017-08-13', NULL);
INSERT INTO `ipconfig` VALUES (198, '123.122.20.29', 4, 1, '2017-08-13', NULL);
INSERT INTO `ipconfig` VALUES (199, '111.206.221.66', 1, 1, '2017-08-13', NULL);
INSERT INTO `ipconfig` VALUES (200, '111.206.221.100', 1, 1, '2017-08-13', NULL);
INSERT INTO `ipconfig` VALUES (201, '111.206.221.89', 1, 1, '2017-08-14', NULL);
INSERT INTO `ipconfig` VALUES (202, '111.206.221.38', 1, 1, '2017-08-14', NULL);
INSERT INTO `ipconfig` VALUES (203, '36.110.113.210', 4, 1, '2017-08-14', NULL);
INSERT INTO `ipconfig` VALUES (204, '111.206.222.43', 1, 1, '2017-08-14', NULL);
INSERT INTO `ipconfig` VALUES (205, '111.206.221.23', 1, 1, '2017-08-14', NULL);
INSERT INTO `ipconfig` VALUES (206, '123.122.20.29', 2, 1, '2017-08-14', NULL);
INSERT INTO `ipconfig` VALUES (207, '111.206.221.37', 1, 1, '2017-08-15', NULL);
INSERT INTO `ipconfig` VALUES (208, '111.206.221.32', 1, 1, '2017-08-15', NULL);
INSERT INTO `ipconfig` VALUES (209, '111.206.222.85', 1, 1, '2017-08-15', NULL);
INSERT INTO `ipconfig` VALUES (210, '111.206.221.34', 1, 1, '2017-08-15', NULL);
INSERT INTO `ipconfig` VALUES (211, '111.206.221.40', 1, 1, '2017-08-16', NULL);
INSERT INTO `ipconfig` VALUES (212, '111.206.221.114', 1, 1, '2017-08-16', NULL);
INSERT INTO `ipconfig` VALUES (213, '111.206.222.78', 1, 1, '2017-08-16', NULL);
INSERT INTO `ipconfig` VALUES (214, '43.246.229.178', 1, 1, '2017-08-16', NULL);
INSERT INTO `ipconfig` VALUES (215, '111.206.221.46', 1, 1, '2017-08-16', NULL);
INSERT INTO `ipconfig` VALUES (216, '36.110.113.210', 1, 1, '2017-08-16', NULL);
INSERT INTO `ipconfig` VALUES (217, '111.206.221.67', 1, 1, '2017-08-17', NULL);
INSERT INTO `ipconfig` VALUES (218, '66.249.75.20', 1, 1, '2017-08-17', NULL);
INSERT INTO `ipconfig` VALUES (219, '111.206.221.30', 1, 1, '2017-08-17', NULL);
INSERT INTO `ipconfig` VALUES (220, '111.206.222.67', 1, 1, '2017-08-17', NULL);
INSERT INTO `ipconfig` VALUES (221, '211.144.1.74', 11, 1, '2017-08-17', NULL);
INSERT INTO `ipconfig` VALUES (222, '111.206.221.73', 1, 1, '2017-08-17', NULL);
INSERT INTO `ipconfig` VALUES (223, '111.206.221.76', 1, 1, '2017-08-18', NULL);
INSERT INTO `ipconfig` VALUES (224, '220.181.108.123', 1, 1, '2017-08-18', NULL);
INSERT INTO `ipconfig` VALUES (225, '36.110.113.210', 3, 1, '2017-08-18', NULL);
INSERT INTO `ipconfig` VALUES (226, '220.181.108.180', 1, 1, '2017-08-18', NULL);
INSERT INTO `ipconfig` VALUES (227, '111.206.221.26', 1, 1, '2017-08-18', NULL);
INSERT INTO `ipconfig` VALUES (228, '211.144.1.74', 2, 1, '2017-08-18', NULL);
INSERT INTO `ipconfig` VALUES (229, '111.206.221.79', 1, 1, '2017-08-19', NULL);
INSERT INTO `ipconfig` VALUES (230, '220.181.108.175', 1, 1, '2017-08-19', NULL);
INSERT INTO `ipconfig` VALUES (231, '111.206.221.44', 1, 1, '2017-08-19', NULL);
INSERT INTO `ipconfig` VALUES (232, '111.206.222.63', 1, 1, '2017-08-19', NULL);
INSERT INTO `ipconfig` VALUES (233, '114.252.22.250', 11, 1, '2017-08-19', NULL);
INSERT INTO `ipconfig` VALUES (234, '111.206.221.32', 2, 1, '2017-08-20', NULL);
INSERT INTO `ipconfig` VALUES (235, '220.181.108.181', 1, 1, '2017-08-20', NULL);
INSERT INTO `ipconfig` VALUES (236, '111.206.221.100', 1, 1, '2017-08-20', NULL);
INSERT INTO `ipconfig` VALUES (237, '114.252.22.250', 15, 1, '2017-08-20', NULL);
INSERT INTO `ipconfig` VALUES (238, '111.206.221.29', 1, 1, '2017-08-21', NULL);
INSERT INTO `ipconfig` VALUES (239, '111.206.222.159', 1, 1, '2017-08-21', NULL);
INSERT INTO `ipconfig` VALUES (240, '111.206.221.36', 1, 1, '2017-08-21', NULL);
INSERT INTO `ipconfig` VALUES (241, '36.110.113.210', 3, 1, '2017-08-21', NULL);
INSERT INTO `ipconfig` VALUES (242, '220.181.108.149', 1, 1, '2017-08-21', NULL);
INSERT INTO `ipconfig` VALUES (243, '111.206.221.37', 1, 1, '2017-08-21', NULL);
INSERT INTO `ipconfig` VALUES (244, '114.252.0.2', 1, 1, '2017-08-21', NULL);
INSERT INTO `ipconfig` VALUES (245, '220.181.108.111', 1, 1, '2017-08-22', NULL);
INSERT INTO `ipconfig` VALUES (246, '111.206.221.104', 1, 1, '2017-08-22', NULL);
INSERT INTO `ipconfig` VALUES (247, '111.206.222.83', 1, 1, '2017-08-22', NULL);
INSERT INTO `ipconfig` VALUES (248, '111.206.221.32', 1, 1, '2017-08-22', NULL);
INSERT INTO `ipconfig` VALUES (249, '36.110.113.210', 5, 1, '2017-08-22', NULL);
INSERT INTO `ipconfig` VALUES (250, '111.206.221.29', 1, 1, '2017-08-22', NULL);
INSERT INTO `ipconfig` VALUES (251, '123.125.71.100', 1, 1, '2017-08-22', NULL);
INSERT INTO `ipconfig` VALUES (252, '111.206.221.36', 1, 1, '2017-08-23', NULL);
INSERT INTO `ipconfig` VALUES (253, '220.181.108.174', 1, 1, '2017-08-23', NULL);
INSERT INTO `ipconfig` VALUES (254, '111.206.221.46', 1, 1, '2017-08-23', NULL);
INSERT INTO `ipconfig` VALUES (255, '111.206.222.174', 1, 1, '2017-08-23', NULL);
INSERT INTO `ipconfig` VALUES (256, '111.206.221.32', 1, 1, '2017-08-23', NULL);
INSERT INTO `ipconfig` VALUES (257, '111.206.221.111', 1, 1, '2017-08-24', NULL);
INSERT INTO `ipconfig` VALUES (258, '220.181.108.141', 1, 1, '2017-08-24', NULL);
INSERT INTO `ipconfig` VALUES (259, '111.206.222.213', 1, 1, '2017-08-24', NULL);
INSERT INTO `ipconfig` VALUES (260, '111.206.221.107', 1, 1, '2017-08-24', NULL);
INSERT INTO `ipconfig` VALUES (261, '111.206.221.47', 1, 1, '2017-08-24', NULL);
INSERT INTO `ipconfig` VALUES (262, '111.206.221.96', 1, 1, '2017-08-25', NULL);
INSERT INTO `ipconfig` VALUES (263, '111.206.221.77', 1, 1, '2017-08-25', NULL);
INSERT INTO `ipconfig` VALUES (264, '111.206.221.114', 1, 1, '2017-08-25', NULL);
INSERT INTO `ipconfig` VALUES (265, '66.249.73.216', 2, 1, '2017-08-26', NULL);
INSERT INTO `ipconfig` VALUES (266, '111.206.221.22', 1, 1, '2017-08-26', NULL);
INSERT INTO `ipconfig` VALUES (267, '111.206.221.94', 1, 1, '2017-08-26', NULL);
INSERT INTO `ipconfig` VALUES (268, '111.206.221.75', 1, 1, '2017-08-26', NULL);
INSERT INTO `ipconfig` VALUES (269, '111.206.221.45', 1, 1, '2017-08-26', NULL);
INSERT INTO `ipconfig` VALUES (270, '124.14.83.2', 2, 1, '2017-08-26', NULL);
INSERT INTO `ipconfig` VALUES (271, '111.206.221.26', 1, 1, '2017-08-27', NULL);
INSERT INTO `ipconfig` VALUES (272, '111.206.221.37', 1, 1, '2017-08-27', NULL);
INSERT INTO `ipconfig` VALUES (273, '123.119.246.42', 5, 1, '2017-08-27', NULL);
INSERT INTO `ipconfig` VALUES (274, '111.206.221.38', 1, 1, '2017-08-27', NULL);
INSERT INTO `ipconfig` VALUES (275, '111.206.221.83', 1, 1, '2017-08-28', NULL);
INSERT INTO `ipconfig` VALUES (276, '66.249.79.142', 1, 1, '2017-08-28', NULL);
INSERT INTO `ipconfig` VALUES (277, '111.206.221.27', 1, 1, '2017-08-28', NULL);
INSERT INTO `ipconfig` VALUES (278, '111.206.221.44', 1, 1, '2017-08-28', NULL);
INSERT INTO `ipconfig` VALUES (279, '111.206.221.82', 1, 1, '2017-08-29', NULL);
INSERT INTO `ipconfig` VALUES (280, '111.206.221.110', 1, 1, '2017-08-29', NULL);
INSERT INTO `ipconfig` VALUES (281, '220.181.108.111', 1, 1, '2017-08-29', NULL);
INSERT INTO `ipconfig` VALUES (282, '111.206.221.36', 1, 1, '2017-08-29', NULL);
INSERT INTO `ipconfig` VALUES (283, '111.206.221.84', 1, 1, '2017-08-30', NULL);
INSERT INTO `ipconfig` VALUES (284, '111.206.221.34', 1, 1, '2017-08-30', NULL);
INSERT INTO `ipconfig` VALUES (285, '220.181.108.119', 1, 1, '2017-08-30', NULL);
INSERT INTO `ipconfig` VALUES (286, '111.206.221.108', 1, 1, '2017-08-30', NULL);
INSERT INTO `ipconfig` VALUES (287, '111.206.221.32', 1, 1, '2017-08-31', NULL);
INSERT INTO `ipconfig` VALUES (288, '220.181.108.151', 1, 1, '2017-08-31', NULL);
INSERT INTO `ipconfig` VALUES (289, '36.110.113.210', 4, 1, '2017-08-31', NULL);
INSERT INTO `ipconfig` VALUES (290, '111.206.221.38', 1, 1, '2017-08-31', NULL);
INSERT INTO `ipconfig` VALUES (291, '111.206.221.48', 1, 1, '2017-08-31', NULL);
INSERT INTO `ipconfig` VALUES (292, '111.206.221.39', 2, 1, '2017-09-01', NULL);
INSERT INTO `ipconfig` VALUES (293, '211.144.1.74', 3, 1, '2017-09-01', NULL);
INSERT INTO `ipconfig` VALUES (294, '36.110.113.210', 2, 1, '2017-09-01', NULL);
INSERT INTO `ipconfig` VALUES (295, '220.181.108.97', 2, 1, '2017-09-01', NULL);
INSERT INTO `ipconfig` VALUES (296, '59.172.198.123', 1, 1, '2017-09-01', NULL);
INSERT INTO `ipconfig` VALUES (297, '111.206.221.36', 1, 1, '2017-09-01', NULL);
INSERT INTO `ipconfig` VALUES (298, '111.206.221.89', 1, 1, '2017-09-02', NULL);
INSERT INTO `ipconfig` VALUES (299, '220.181.108.96', 1, 1, '2017-09-02', NULL);
INSERT INTO `ipconfig` VALUES (300, '111.206.221.91', 1, 1, '2017-09-02', NULL);
INSERT INTO `ipconfig` VALUES (301, '111.206.221.36', 1, 1, '2017-09-02', NULL);
INSERT INTO `ipconfig` VALUES (302, '123.122.6.159', 11, 1, '2017-09-02', NULL);
INSERT INTO `ipconfig` VALUES (303, '111.206.221.25', 1, 1, '2017-09-03', NULL);
INSERT INTO `ipconfig` VALUES (304, '220.181.108.103', 1, 1, '2017-09-03', NULL);
INSERT INTO `ipconfig` VALUES (305, '123.122.6.159', 4, 1, '2017-09-03', NULL);
INSERT INTO `ipconfig` VALUES (306, '111.206.221.108', 1, 1, '2017-09-03', NULL);
INSERT INTO `ipconfig` VALUES (307, '124.202.171.70', 1, 1, '2017-09-03', NULL);
INSERT INTO `ipconfig` VALUES (308, '111.206.221.43', 1, 1, '2017-09-03', NULL);
INSERT INTO `ipconfig` VALUES (309, '111.206.221.110', 1, 1, '2017-09-03', NULL);
INSERT INTO `ipconfig` VALUES (310, '111.206.221.43', 1, 1, '2017-09-04', NULL);
INSERT INTO `ipconfig` VALUES (311, '220.181.108.101', 1, 1, '2017-09-04', NULL);
INSERT INTO `ipconfig` VALUES (312, '211.144.1.74', 5, 1, '2017-09-04', NULL);
INSERT INTO `ipconfig` VALUES (313, '111.206.221.89', 1, 1, '2017-09-04', NULL);
INSERT INTO `ipconfig` VALUES (314, '111.206.221.93', 1, 1, '2017-09-04', NULL);
INSERT INTO `ipconfig` VALUES (315, '111.206.221.77', 1, 1, '2017-09-05', NULL);
INSERT INTO `ipconfig` VALUES (316, '111.206.221.40', 1, 1, '2017-09-05', NULL);
INSERT INTO `ipconfig` VALUES (317, '220.181.108.156', 1, 1, '2017-09-05', NULL);
INSERT INTO `ipconfig` VALUES (318, '111.206.221.46', 1, 1, '2017-09-05', NULL);
INSERT INTO `ipconfig` VALUES (319, '111.206.221.69', 1, 1, '2017-09-06', NULL);
INSERT INTO `ipconfig` VALUES (320, '123.125.71.96', 1, 1, '2017-09-06', NULL);
INSERT INTO `ipconfig` VALUES (321, '111.206.221.35', 1, 1, '2017-09-06', NULL);
INSERT INTO `ipconfig` VALUES (322, '171.223.147.153', 1, 1, '2017-09-06', NULL);
INSERT INTO `ipconfig` VALUES (323, '111.206.221.111', 1, 1, '2017-09-06', NULL);
INSERT INTO `ipconfig` VALUES (324, '111.206.221.102', 1, 1, '2017-09-07', NULL);
INSERT INTO `ipconfig` VALUES (325, '203.208.60.197', 1, 1, '2017-09-07', NULL);
INSERT INTO `ipconfig` VALUES (326, '123.125.71.92', 1, 1, '2017-09-07', NULL);
INSERT INTO `ipconfig` VALUES (327, '111.206.221.31', 1, 1, '2017-09-07', NULL);
INSERT INTO `ipconfig` VALUES (328, '36.110.113.210', 2, 1, '2017-09-07', NULL);
INSERT INTO `ipconfig` VALUES (329, '111.206.221.75', 1, 1, '2017-09-07', NULL);
INSERT INTO `ipconfig` VALUES (330, '111.206.221.82', 1, 1, '2017-09-08', NULL);
INSERT INTO `ipconfig` VALUES (331, '220.181.108.111', 1, 1, '2017-09-08', NULL);
INSERT INTO `ipconfig` VALUES (332, '111.206.221.76', 1, 1, '2017-09-08', NULL);
INSERT INTO `ipconfig` VALUES (333, '36.110.113.210', 10, 1, '2017-09-08', NULL);
INSERT INTO `ipconfig` VALUES (334, '211.144.1.74', 4, 1, '2017-09-08', NULL);
INSERT INTO `ipconfig` VALUES (335, '111.206.221.68', 1, 1, '2017-09-08', NULL);
INSERT INTO `ipconfig` VALUES (336, '111.206.221.110', 1, 1, '2017-09-09', NULL);
INSERT INTO `ipconfig` VALUES (337, '220.181.108.106', 1, 1, '2017-09-09', NULL);
INSERT INTO `ipconfig` VALUES (338, '111.206.221.88', 1, 1, '2017-09-09', NULL);
INSERT INTO `ipconfig` VALUES (339, '111.206.221.106', 1, 1, '2017-09-09', NULL);
INSERT INTO `ipconfig` VALUES (340, '158.69.229.6', 1, 1, '2017-09-10', NULL);
INSERT INTO `ipconfig` VALUES (341, '111.206.221.31', 1, 1, '2017-09-10', NULL);
INSERT INTO `ipconfig` VALUES (342, '220.181.108.175', 1, 1, '2017-09-10', NULL);
INSERT INTO `ipconfig` VALUES (343, '111.206.221.51', 1, 1, '2017-09-10', NULL);
INSERT INTO `ipconfig` VALUES (344, '101.41.112.108', 1, 1, '2017-09-10', NULL);
INSERT INTO `ipconfig` VALUES (345, '111.206.221.41', 1, 1, '2017-09-10', NULL);
INSERT INTO `ipconfig` VALUES (346, '111.206.221.31', 1, 1, '2017-09-11', NULL);
INSERT INTO `ipconfig` VALUES (347, '111.206.221.80', 1, 1, '2017-09-11', NULL);
INSERT INTO `ipconfig` VALUES (348, '111.206.221.36', 1, 1, '2017-09-12', NULL);
INSERT INTO `ipconfig` VALUES (349, '220.181.108.113', 1, 1, '2017-09-12', NULL);
INSERT INTO `ipconfig` VALUES (350, '211.144.1.74', 1, 1, '2017-09-12', NULL);
INSERT INTO `ipconfig` VALUES (351, '111.206.221.26', 1, 1, '2017-09-12', NULL);
INSERT INTO `ipconfig` VALUES (352, '220.181.108.109', 1, 1, '2017-09-13', NULL);
INSERT INTO `ipconfig` VALUES (353, '111.206.221.33', 1, 1, '2017-09-13', NULL);
INSERT INTO `ipconfig` VALUES (354, '111.206.221.48', 1, 1, '2017-09-13', NULL);
INSERT INTO `ipconfig` VALUES (355, '111.206.221.43', 1, 1, '2017-09-14', NULL);
INSERT INTO `ipconfig` VALUES (356, '111.206.221.67', 1, 1, '2017-09-14', NULL);
INSERT INTO `ipconfig` VALUES (357, '220.181.108.105', 1, 1, '2017-09-14', NULL);
INSERT INTO `ipconfig` VALUES (358, '36.110.113.210', 2, 1, '2017-09-14', NULL);
INSERT INTO `ipconfig` VALUES (359, '111.206.221.86', 1, 1, '2017-09-14', NULL);
INSERT INTO `ipconfig` VALUES (360, '111.206.221.34', 1, 1, '2017-09-15', NULL);
INSERT INTO `ipconfig` VALUES (361, '220.181.108.106', 1, 1, '2017-09-15', NULL);
INSERT INTO `ipconfig` VALUES (362, '111.206.221.96', 1, 1, '2017-09-15', NULL);
INSERT INTO `ipconfig` VALUES (363, '220.181.108.185', 1, 1, '2017-09-15', NULL);
INSERT INTO `ipconfig` VALUES (364, '111.206.221.107', 1, 1, '2017-09-15', NULL);
INSERT INTO `ipconfig` VALUES (365, '111.206.221.23', 1, 1, '2017-09-16', NULL);
INSERT INTO `ipconfig` VALUES (366, '111.206.221.35', 1, 1, '2017-09-16', NULL);
INSERT INTO `ipconfig` VALUES (367, '220.181.108.106', 1, 1, '2017-09-16', NULL);
INSERT INTO `ipconfig` VALUES (368, '111.206.221.40', 1, 1, '2017-09-16', NULL);
INSERT INTO `ipconfig` VALUES (369, '111.206.221.105', 1, 1, '2017-09-17', NULL);
INSERT INTO `ipconfig` VALUES (370, '111.206.221.32', 1, 1, '2017-09-17', NULL);
INSERT INTO `ipconfig` VALUES (371, '66.249.73.217', 1, 1, '2017-09-17', NULL);
INSERT INTO `ipconfig` VALUES (372, '111.206.221.108', 1, 1, '2017-09-17', NULL);
INSERT INTO `ipconfig` VALUES (373, '111.206.221.45', 1, 1, '2017-09-17', NULL);
INSERT INTO `ipconfig` VALUES (374, '111.206.221.24', 1, 1, '2017-09-18', NULL);
INSERT INTO `ipconfig` VALUES (375, '111.206.221.44', 1, 1, '2017-09-18', NULL);
INSERT INTO `ipconfig` VALUES (376, '111.206.221.90', 1, 1, '2017-09-18', NULL);
INSERT INTO `ipconfig` VALUES (377, '111.206.221.67', 1, 1, '2017-09-19', NULL);
INSERT INTO `ipconfig` VALUES (378, '211.144.1.74', 2, 1, '2017-09-19', NULL);
INSERT INTO `ipconfig` VALUES (379, '111.206.221.29', 1, 1, '2017-09-19', NULL);
INSERT INTO `ipconfig` VALUES (380, '111.206.221.85', 1, 1, '2017-09-19', NULL);
INSERT INTO `ipconfig` VALUES (381, '111.206.221.26', 1, 1, '2017-09-20', NULL);
INSERT INTO `ipconfig` VALUES (382, '221.207.141.193', 1, 1, '2017-09-20', NULL);
INSERT INTO `ipconfig` VALUES (383, '111.206.221.35', 1, 1, '2017-09-20', NULL);
INSERT INTO `ipconfig` VALUES (384, '111.206.221.70', 1, 1, '2017-09-20', NULL);
INSERT INTO `ipconfig` VALUES (385, '117.173.98.145', 1, 1, '2017-09-20', NULL);
INSERT INTO `ipconfig` VALUES (386, '111.206.221.101', 1, 1, '2017-09-20', NULL);
INSERT INTO `ipconfig` VALUES (387, '111.206.221.27', 1, 1, '2017-09-21', NULL);
INSERT INTO `ipconfig` VALUES (388, '220.181.108.143', 1, 1, '2017-09-21', NULL);
INSERT INTO `ipconfig` VALUES (389, '211.144.1.74', 2, 1, '2017-09-21', NULL);
INSERT INTO `ipconfig` VALUES (390, '111.206.221.91', 1, 1, '2017-09-22', NULL);
INSERT INTO `ipconfig` VALUES (391, '111.206.221.112', 1, 1, '2017-09-22', NULL);
INSERT INTO `ipconfig` VALUES (392, '36.110.113.210', 2, 1, '2017-09-22', NULL);
INSERT INTO `ipconfig` VALUES (393, '111.206.221.33', 1, 1, '2017-09-22', NULL);
INSERT INTO `ipconfig` VALUES (394, '211.144.1.74', 1, 1, '2017-09-22', NULL);
INSERT INTO `ipconfig` VALUES (395, '111.206.221.35', 1, 1, '2017-09-23', NULL);
INSERT INTO `ipconfig` VALUES (396, '111.206.221.33', 1, 1, '2017-09-23', NULL);
INSERT INTO `ipconfig` VALUES (397, '114.252.29.141', 2, 1, '2017-09-23', NULL);
INSERT INTO `ipconfig` VALUES (398, '111.206.221.108', 1, 1, '2017-09-23', NULL);
INSERT INTO `ipconfig` VALUES (399, '111.206.221.69', 1, 1, '2017-09-24', NULL);
INSERT INTO `ipconfig` VALUES (400, '111.206.221.89', 1, 1, '2017-09-24', NULL);
INSERT INTO `ipconfig` VALUES (401, '111.206.221.75', 1, 1, '2017-09-24', NULL);
INSERT INTO `ipconfig` VALUES (402, '111.206.221.42', 1, 1, '2017-09-25', NULL);
INSERT INTO `ipconfig` VALUES (403, '111.206.221.23', 1, 1, '2017-09-25', NULL);
INSERT INTO `ipconfig` VALUES (404, '111.206.221.94', 1, 1, '2017-09-25', NULL);
INSERT INTO `ipconfig` VALUES (405, '111.206.221.93', 1, 1, '2017-09-26', NULL);
INSERT INTO `ipconfig` VALUES (406, '66.249.75.136', 1, 1, '2017-09-26', NULL);
INSERT INTO `ipconfig` VALUES (407, '111.206.221.101', 1, 1, '2017-09-26', NULL);
INSERT INTO `ipconfig` VALUES (408, '111.206.221.42', 1, 1, '2017-09-26', NULL);
INSERT INTO `ipconfig` VALUES (409, '66.249.75.138', 1, 1, '2017-09-26', NULL);
INSERT INTO `ipconfig` VALUES (410, '111.206.221.29', 1, 1, '2017-09-27', NULL);
INSERT INTO `ipconfig` VALUES (411, '111.206.221.106', 1, 1, '2017-09-27', NULL);
INSERT INTO `ipconfig` VALUES (412, '211.144.1.74', 2, 1, '2017-09-27', NULL);
INSERT INTO `ipconfig` VALUES (413, '111.206.221.36', 1, 1, '2017-09-27', NULL);
INSERT INTO `ipconfig` VALUES (414, '111.206.221.44', 1, 1, '2017-09-28', NULL);
INSERT INTO `ipconfig` VALUES (415, '111.206.221.31', 1, 1, '2017-09-28', NULL);
INSERT INTO `ipconfig` VALUES (416, '220.181.108.114', 1, 1, '2017-09-28', NULL);
INSERT INTO `ipconfig` VALUES (417, '111.206.221.27', 1, 1, '2017-09-28', NULL);
INSERT INTO `ipconfig` VALUES (418, '111.206.221.107', 1, 1, '2017-09-29', NULL);
INSERT INTO `ipconfig` VALUES (419, '220.181.108.114', 1, 1, '2017-09-29', NULL);
INSERT INTO `ipconfig` VALUES (420, '111.206.221.38', 1, 1, '2017-09-29', NULL);
INSERT INTO `ipconfig` VALUES (421, '111.206.221.101', 1, 1, '2017-09-29', NULL);
INSERT INTO `ipconfig` VALUES (422, '111.206.221.80', 1, 1, '2017-09-30', NULL);
INSERT INTO `ipconfig` VALUES (423, '220.181.108.103', 1, 1, '2017-09-30', NULL);
INSERT INTO `ipconfig` VALUES (424, '111.206.221.50', 1, 1, '2017-09-30', NULL);
INSERT INTO `ipconfig` VALUES (425, '111.206.221.83', 1, 1, '2017-09-30', NULL);
INSERT INTO `ipconfig` VALUES (426, '111.206.221.88', 1, 1, '2017-10-02', NULL);
INSERT INTO `ipconfig` VALUES (427, '111.206.221.66', 1, 1, '2017-10-02', NULL);
INSERT INTO `ipconfig` VALUES (428, '111.206.221.37', 1, 1, '2017-10-03', NULL);
INSERT INTO `ipconfig` VALUES (429, '111.206.221.23', 1, 1, '2017-10-03', NULL);
INSERT INTO `ipconfig` VALUES (430, '117.179.2.61', 1, 1, '2017-10-03', NULL);
INSERT INTO `ipconfig` VALUES (431, '66.249.65.124', 2, 1, '2017-10-03', NULL);
INSERT INTO `ipconfig` VALUES (432, '111.206.221.35', 1, 1, '2017-10-03', NULL);
INSERT INTO `ipconfig` VALUES (433, '123.125.71.92', 1, 1, '2017-10-04', NULL);
INSERT INTO `ipconfig` VALUES (434, '111.206.221.104', 1, 1, '2017-10-04', NULL);
INSERT INTO `ipconfig` VALUES (435, '66.249.73.217', 1, 1, '2017-10-04', NULL);
INSERT INTO `ipconfig` VALUES (436, '66.249.73.218', 1, 1, '2017-10-04', NULL);
INSERT INTO `ipconfig` VALUES (437, '220.181.108.104', 1, 1, '2017-10-05', NULL);
INSERT INTO `ipconfig` VALUES (438, '111.206.221.79', 1, 1, '2017-10-05', NULL);
INSERT INTO `ipconfig` VALUES (439, '111.206.222.194', 1, 1, '2017-10-05', NULL);
INSERT INTO `ipconfig` VALUES (440, '111.206.221.50', 1, 1, '2017-10-06', NULL);
INSERT INTO `ipconfig` VALUES (441, '111.206.221.81', 1, 1, '2017-10-06', NULL);
INSERT INTO `ipconfig` VALUES (442, '111.206.221.35', 1, 1, '2017-10-06', NULL);
INSERT INTO `ipconfig` VALUES (443, '111.206.221.69', 1, 1, '2017-10-07', NULL);
INSERT INTO `ipconfig` VALUES (444, '111.206.221.38', 1, 1, '2017-10-07', NULL);
INSERT INTO `ipconfig` VALUES (445, '220.181.108.95', 1, 1, '2017-10-07', NULL);
INSERT INTO `ipconfig` VALUES (446, '111.206.221.82', 1, 1, '2017-10-07', NULL);
INSERT INTO `ipconfig` VALUES (447, '111.206.221.49', 1, 1, '2017-10-08', NULL);
INSERT INTO `ipconfig` VALUES (448, '111.206.221.110', 1, 1, '2017-10-08', NULL);
INSERT INTO `ipconfig` VALUES (449, '111.206.221.31', 1, 1, '2017-10-08', NULL);
INSERT INTO `ipconfig` VALUES (450, '111.206.221.115', 1, 1, '2017-10-09', NULL);
INSERT INTO `ipconfig` VALUES (451, '111.206.221.43', 2, 1, '2017-10-09', NULL);
INSERT INTO `ipconfig` VALUES (452, '123.125.71.97', 1, 1, '2017-10-09', NULL);
INSERT INTO `ipconfig` VALUES (453, '211.144.1.74', 2, 1, '2017-10-09', NULL);
INSERT INTO `ipconfig` VALUES (454, '111.206.221.44', 1, 1, '2017-10-09', NULL);
INSERT INTO `ipconfig` VALUES (455, '36.110.113.210', 2, 1, '2017-10-09', NULL);
INSERT INTO `ipconfig` VALUES (456, '111.206.221.79', 1, 1, '2017-10-10', NULL);
INSERT INTO `ipconfig` VALUES (457, '36.110.113.210', 1, 1, '2017-10-10', NULL);
INSERT INTO `ipconfig` VALUES (458, '111.206.221.98', 2, 1, '2017-10-10', NULL);
INSERT INTO `ipconfig` VALUES (459, '111.206.221.108', 1, 1, '2017-10-10', NULL);
INSERT INTO `ipconfig` VALUES (460, '111.206.221.110', 1, 1, '2017-10-11', NULL);
INSERT INTO `ipconfig` VALUES (461, '123.125.71.100', 1, 1, '2017-10-11', NULL);
INSERT INTO `ipconfig` VALUES (462, '111.206.221.108', 1, 1, '2017-10-11', NULL);
INSERT INTO `ipconfig` VALUES (463, '111.206.221.74', 1, 1, '2017-10-12', NULL);
INSERT INTO `ipconfig` VALUES (464, '111.206.221.86', 1, 1, '2017-10-12', NULL);
INSERT INTO `ipconfig` VALUES (465, '111.206.221.81', 1, 1, '2017-10-12', NULL);
INSERT INTO `ipconfig` VALUES (466, '111.206.221.25', 1, 1, '2017-10-12', NULL);
INSERT INTO `ipconfig` VALUES (467, '111.206.221.82', 1, 1, '2017-10-13', NULL);
INSERT INTO `ipconfig` VALUES (468, '111.206.221.30', 1, 1, '2017-10-13', NULL);
INSERT INTO `ipconfig` VALUES (469, '111.206.221.25', 1, 1, '2017-10-13', NULL);
INSERT INTO `ipconfig` VALUES (470, '111.206.221.42', 1, 1, '2017-10-14', NULL);
INSERT INTO `ipconfig` VALUES (471, '111.206.221.31', 1, 1, '2017-10-14', NULL);
INSERT INTO `ipconfig` VALUES (472, '203.208.60.196', 1, 1, '2017-10-15', NULL);
INSERT INTO `ipconfig` VALUES (473, '111.206.221.43', 1, 1, '2017-10-15', NULL);
INSERT INTO `ipconfig` VALUES (474, '111.206.221.68', 1, 1, '2017-10-15', NULL);
INSERT INTO `ipconfig` VALUES (475, '111.206.221.105', 1, 1, '2017-10-15', NULL);
INSERT INTO `ipconfig` VALUES (476, '111.206.221.37', 1, 1, '2017-10-15', NULL);
INSERT INTO `ipconfig` VALUES (477, '111.206.221.97', 1, 1, '2017-10-16', NULL);
INSERT INTO `ipconfig` VALUES (478, '111.206.221.86', 1, 1, '2017-10-16', NULL);
INSERT INTO `ipconfig` VALUES (479, '111.206.221.110', 1, 1, '2017-10-16', NULL);
INSERT INTO `ipconfig` VALUES (480, '111.206.221.35', 1, 1, '2017-10-16', NULL);
INSERT INTO `ipconfig` VALUES (481, '111.206.221.115', 2, 1, '2017-10-17', NULL);
INSERT INTO `ipconfig` VALUES (482, '211.144.1.74', 3, 1, '2017-10-17', NULL);
INSERT INTO `ipconfig` VALUES (483, '111.206.221.40', 1, 1, '2017-10-18', NULL);
INSERT INTO `ipconfig` VALUES (484, '111.206.221.39', 1, 1, '2017-10-18', NULL);
INSERT INTO `ipconfig` VALUES (485, '111.206.221.41', 1, 1, '2017-10-18', NULL);
INSERT INTO `ipconfig` VALUES (486, '36.110.113.210', 4, 1, '2017-10-18', NULL);
INSERT INTO `ipconfig` VALUES (487, '211.144.1.74', 3, 1, '2017-10-18', NULL);
INSERT INTO `ipconfig` VALUES (488, '111.206.221.115', 1, 1, '2017-10-18', NULL);
INSERT INTO `ipconfig` VALUES (489, '111.206.221.106', 1, 1, '2017-10-19', NULL);
INSERT INTO `ipconfig` VALUES (490, '111.206.221.94', 1, 1, '2017-10-19', NULL);
INSERT INTO `ipconfig` VALUES (491, '111.206.221.42', 1, 1, '2017-10-19', NULL);
INSERT INTO `ipconfig` VALUES (492, '211.144.1.74', 4, 1, '2017-10-19', NULL);
INSERT INTO `ipconfig` VALUES (493, '111.206.221.71', 1, 1, '2017-10-19', NULL);
INSERT INTO `ipconfig` VALUES (494, '111.206.221.50', 1, 1, '2017-10-20', NULL);
INSERT INTO `ipconfig` VALUES (495, '111.206.221.96', 1, 1, '2017-10-20', NULL);
INSERT INTO `ipconfig` VALUES (496, '111.206.221.94', 1, 1, '2017-10-20', NULL);
INSERT INTO `ipconfig` VALUES (497, '220.181.108.178', 1, 1, '2017-10-20', NULL);
INSERT INTO `ipconfig` VALUES (498, '111.206.221.111', 1, 1, '2017-10-20', NULL);
INSERT INTO `ipconfig` VALUES (499, '111.206.221.38', 1, 1, '2017-10-21', NULL);
INSERT INTO `ipconfig` VALUES (500, '111.206.221.89', 1, 1, '2017-10-21', NULL);
INSERT INTO `ipconfig` VALUES (501, '220.181.108.114', 1, 1, '2017-10-21', NULL);
INSERT INTO `ipconfig` VALUES (502, '111.206.221.39', 1, 1, '2017-10-21', NULL);
INSERT INTO `ipconfig` VALUES (503, '220.181.108.173', 1, 1, '2017-10-21', NULL);
INSERT INTO `ipconfig` VALUES (504, '111.206.221.26', 1, 1, '2017-10-21', NULL);
INSERT INTO `ipconfig` VALUES (505, '139.205.81.136', 1, 1, '2017-10-21', NULL);
INSERT INTO `ipconfig` VALUES (506, '111.206.221.40', 1, 1, '2017-10-22', NULL);
INSERT INTO `ipconfig` VALUES (507, '111.206.221.39', 1, 1, '2017-10-22', NULL);
INSERT INTO `ipconfig` VALUES (508, '123.125.71.97', 1, 1, '2017-10-22', NULL);
INSERT INTO `ipconfig` VALUES (509, '111.206.221.38', 1, 1, '2017-10-22', NULL);
INSERT INTO `ipconfig` VALUES (510, '111.206.221.77', 1, 1, '2017-10-22', NULL);
INSERT INTO `ipconfig` VALUES (511, '220.181.108.112', 1, 1, '2017-10-23', NULL);
INSERT INTO `ipconfig` VALUES (512, '111.206.221.70', 1, 1, '2017-10-23', NULL);
INSERT INTO `ipconfig` VALUES (513, '111.206.221.31', 1, 1, '2017-10-23', NULL);
INSERT INTO `ipconfig` VALUES (514, '220.181.108.96', 1, 1, '2017-10-24', NULL);
INSERT INTO `ipconfig` VALUES (515, '211.144.1.74', 2, 1, '2017-10-24', NULL);
INSERT INTO `ipconfig` VALUES (516, '66.249.75.136', 1, 1, '2017-10-25', NULL);
INSERT INTO `ipconfig` VALUES (517, '36.110.113.210', 2, 1, '2017-10-25', NULL);
INSERT INTO `ipconfig` VALUES (518, '220.181.108.176', 1, 1, '2017-10-26', NULL);
INSERT INTO `ipconfig` VALUES (519, '114.252.35.206', 7, 1, '2017-10-26', NULL);
INSERT INTO `ipconfig` VALUES (520, '220.181.108.107', 1, 1, '2017-10-27', NULL);
INSERT INTO `ipconfig` VALUES (521, '111.206.221.94', 1, 1, '2017-10-27', NULL);
INSERT INTO `ipconfig` VALUES (522, '211.144.1.74', 4, 1, '2017-10-27', NULL);
INSERT INTO `ipconfig` VALUES (523, '36.110.113.210', 4, 1, '2017-10-27', NULL);
INSERT INTO `ipconfig` VALUES (524, '117.136.0.152', 1, 1, '2017-10-27', NULL);
INSERT INTO `ipconfig` VALUES (525, '111.206.221.35', 1, 1, '2017-10-28', NULL);
INSERT INTO `ipconfig` VALUES (526, '111.197.237.207', 11, 1, '2017-10-28', NULL);
INSERT INTO `ipconfig` VALUES (527, '111.206.221.97', 1, 1, '2017-10-29', NULL);
INSERT INTO `ipconfig` VALUES (528, '111.197.237.207', 5, 1, '2017-10-29', NULL);
INSERT INTO `ipconfig` VALUES (529, '220.181.108.172', 1, 1, '2017-10-30', NULL);
INSERT INTO `ipconfig` VALUES (530, '111.206.221.87', 1, 1, '2017-10-31', NULL);
INSERT INTO `ipconfig` VALUES (531, '119.57.115.195', 1, 1, '2017-10-31', NULL);
INSERT INTO `ipconfig` VALUES (532, '111.206.221.85', 1, 1, '2017-11-01', NULL);
INSERT INTO `ipconfig` VALUES (533, '211.144.1.74', 7, 1, '2017-11-01', NULL);
INSERT INTO `ipconfig` VALUES (534, '203.208.60.193', 1, 1, '2017-11-02', NULL);
INSERT INTO `ipconfig` VALUES (535, '36.110.113.210', 8, 1, '2017-11-02', NULL);
INSERT INTO `ipconfig` VALUES (536, '111.206.221.98', 1, 1, '2017-11-02', NULL);
INSERT INTO `ipconfig` VALUES (537, '211.144.1.74', 7, 1, '2017-11-02', NULL);
INSERT INTO `ipconfig` VALUES (538, '114.253.36.48', 4, 1, '2017-11-02', NULL);
INSERT INTO `ipconfig` VALUES (539, '220.181.132.179', 1, 1, '2017-11-03', NULL);
INSERT INTO `ipconfig` VALUES (540, '36.110.113.210', 4, 1, '2017-11-03', NULL);
INSERT INTO `ipconfig` VALUES (541, '211.144.1.74', 2, 1, '2017-11-03', NULL);
INSERT INTO `ipconfig` VALUES (542, '111.206.221.47', 1, 1, '2017-11-03', NULL);
INSERT INTO `ipconfig` VALUES (543, '114.253.36.48', 3, 1, '2017-11-05', NULL);
INSERT INTO `ipconfig` VALUES (544, '114.253.36.48', 4, 1, '2017-11-06', NULL);
INSERT INTO `ipconfig` VALUES (545, '36.110.113.210', 3, 1, '2017-11-07', NULL);
INSERT INTO `ipconfig` VALUES (546, '114.253.36.48', 3, 1, '2017-11-07', NULL);
INSERT INTO `ipconfig` VALUES (547, '211.144.1.74', 3, 1, '2017-11-08', NULL);
INSERT INTO `ipconfig` VALUES (548, '36.110.113.210', 6, 1, '2017-11-08', NULL);
INSERT INTO `ipconfig` VALUES (549, '36.110.113.210', 3, 1, '2017-11-09', NULL);
INSERT INTO `ipconfig` VALUES (550, '211.144.1.74', 1, 1, '2017-11-10', NULL);
INSERT INTO `ipconfig` VALUES (551, '66.249.69.245', 1, 1, '2017-11-10', NULL);
INSERT INTO `ipconfig` VALUES (552, '111.197.237.147', 3, 1, '2017-11-11', NULL);
INSERT INTO `ipconfig` VALUES (553, '111.197.237.147', 1, 1, '2017-11-12', NULL);
INSERT INTO `ipconfig` VALUES (554, '114.252.51.90', 12, 1, '2017-11-12', NULL);
INSERT INTO `ipconfig` VALUES (555, '122.226.183.158', 1, 1, '2017-11-13', NULL);
INSERT INTO `ipconfig` VALUES (556, '220.181.108.150', 1, 1, '2017-11-13', NULL);
INSERT INTO `ipconfig` VALUES (557, '211.144.1.74', 2, 1, '2017-11-13', NULL);
INSERT INTO `ipconfig` VALUES (558, '111.206.221.78', 1, 1, '2017-11-14', NULL);
INSERT INTO `ipconfig` VALUES (559, '114.252.51.90', 1, 1, '2017-11-14', NULL);
INSERT INTO `ipconfig` VALUES (560, '220.181.108.172', 1, 1, '2017-11-15', NULL);
INSERT INTO `ipconfig` VALUES (561, '211.144.1.74', 1, 1, '2017-11-16', NULL);
INSERT INTO `ipconfig` VALUES (562, '36.110.113.210', 3, 1, '2017-11-16', NULL);
INSERT INTO `ipconfig` VALUES (563, '211.144.1.74', 1, 1, '2017-11-17', NULL);
INSERT INTO `ipconfig` VALUES (564, '36.110.113.210', 1, 1, '2017-11-17', NULL);
INSERT INTO `ipconfig` VALUES (565, '111.206.221.93', 1, 1, '2017-11-18', NULL);
INSERT INTO `ipconfig` VALUES (566, '114.246.100.138', 1, 1, '2017-11-18', NULL);
INSERT INTO `ipconfig` VALUES (567, '203.208.60.195', 1, 1, '2017-11-19', NULL);
INSERT INTO `ipconfig` VALUES (568, '111.206.221.101', 1, 1, '2017-11-19', NULL);
INSERT INTO `ipconfig` VALUES (569, '114.246.100.138', 21, 1, '2017-11-19', NULL);
INSERT INTO `ipconfig` VALUES (570, '203.208.60.196', 1, 1, '2017-11-20', NULL);
INSERT INTO `ipconfig` VALUES (571, '111.206.221.34', 1, 1, '2017-11-20', NULL);
INSERT INTO `ipconfig` VALUES (572, '211.144.1.74', 4, 1, '2017-11-20', NULL);
INSERT INTO `ipconfig` VALUES (573, '114.246.100.138', 5, 1, '2017-11-20', NULL);
INSERT INTO `ipconfig` VALUES (574, '220.181.108.147', 1, 1, '2017-11-21', NULL);
INSERT INTO `ipconfig` VALUES (575, '111.206.221.89', 1, 1, '2017-11-21', NULL);
INSERT INTO `ipconfig` VALUES (576, '211.144.1.74', 3, 1, '2017-11-21', NULL);
INSERT INTO `ipconfig` VALUES (577, '36.110.113.210', 1, 1, '2017-11-21', NULL);
INSERT INTO `ipconfig` VALUES (578, '114.246.100.138', 1, 1, '2017-11-21', NULL);
INSERT INTO `ipconfig` VALUES (579, '111.206.221.108', 1, 1, '2017-11-22', NULL);
INSERT INTO `ipconfig` VALUES (580, '36.110.113.210', 1, 1, '2017-11-22', NULL);
INSERT INTO `ipconfig` VALUES (581, '211.144.1.74', 1, 1, '2017-11-22', NULL);
INSERT INTO `ipconfig` VALUES (582, '114.246.100.138', 6, 1, '2017-11-22', NULL);
INSERT INTO `ipconfig` VALUES (583, '36.110.113.210', 6, 1, '2017-11-23', NULL);
INSERT INTO `ipconfig` VALUES (584, '111.206.221.23', 1, 1, '2017-11-23', NULL);
INSERT INTO `ipconfig` VALUES (585, '211.144.1.74', 4, 1, '2017-11-23', NULL);
INSERT INTO `ipconfig` VALUES (586, '114.246.100.138', 2, 1, '2017-11-23', NULL);
INSERT INTO `ipconfig` VALUES (587, '66.249.75.20', 1, 1, '2017-11-23', NULL);
INSERT INTO `ipconfig` VALUES (588, '111.206.221.34', 1, 1, '2017-11-24', NULL);
INSERT INTO `ipconfig` VALUES (589, '211.144.1.74', 2, 1, '2017-11-24', NULL);
INSERT INTO `ipconfig` VALUES (590, '222.131.159.207', 8, 1, '2017-11-24', NULL);
INSERT INTO `ipconfig` VALUES (591, '222.131.159.207', 14, 1, '2017-11-25', NULL);
INSERT INTO `ipconfig` VALUES (592, '222.131.159.207', 19, 1, '2017-11-26', NULL);
INSERT INTO `ipconfig` VALUES (593, '66.249.65.124', 1, 1, '2017-11-27', NULL);
INSERT INTO `ipconfig` VALUES (594, '222.131.159.207', 7, 1, '2017-11-27', NULL);
INSERT INTO `ipconfig` VALUES (595, '36.110.113.210', 2, 1, '2017-11-28', NULL);
INSERT INTO `ipconfig` VALUES (596, '211.144.1.74', 4, 1, '2017-11-28', NULL);
INSERT INTO `ipconfig` VALUES (597, '114.252.63.200', 4, 1, '2017-11-28', NULL);
INSERT INTO `ipconfig` VALUES (598, '211.144.1.74', 2, 1, '2017-11-29', NULL);
INSERT INTO `ipconfig` VALUES (599, '36.110.113.210', 1, 1, '2017-11-29', NULL);
INSERT INTO `ipconfig` VALUES (600, '111.206.221.67', 1, 1, '2017-11-29', NULL);
INSERT INTO `ipconfig` VALUES (601, '114.252.59.59', 14, 1, '2017-11-29', NULL);
INSERT INTO `ipconfig` VALUES (602, '111.206.221.68', 1, 1, '2017-11-30', NULL);
INSERT INTO `ipconfig` VALUES (603, '211.144.1.74', 3, 1, '2017-11-30', NULL);
INSERT INTO `ipconfig` VALUES (604, '36.110.113.210', 1, 1, '2017-11-30', NULL);
INSERT INTO `ipconfig` VALUES (605, '114.252.59.59', 1, 1, '2017-11-30', NULL);
INSERT INTO `ipconfig` VALUES (606, '111.206.221.83', 1, 1, '2017-12-01', NULL);
INSERT INTO `ipconfig` VALUES (607, '114.252.59.59', 1, 1, '2017-12-01', NULL);
INSERT INTO `ipconfig` VALUES (608, '203.208.60.130', 1, 1, '2017-12-02', NULL);
INSERT INTO `ipconfig` VALUES (609, '123.116.62.40', 1, 1, '2017-12-02', NULL);
INSERT INTO `ipconfig` VALUES (610, '111.206.221.29', 1, 1, '2017-12-02', NULL);
INSERT INTO `ipconfig` VALUES (611, '111.206.221.101', 1, 1, '2017-12-03', NULL);
INSERT INTO `ipconfig` VALUES (612, '111.206.221.26', 1, 1, '2017-12-04', NULL);
INSERT INTO `ipconfig` VALUES (613, '36.110.113.210', 2, 1, '2017-12-04', NULL);
INSERT INTO `ipconfig` VALUES (614, '111.206.221.25', 1, 1, '2017-12-06', NULL);
INSERT INTO `ipconfig` VALUES (615, '111.206.221.70', 1, 1, '2017-12-07', NULL);
INSERT INTO `ipconfig` VALUES (616, '66.249.64.220', 1, 1, '2017-12-08', NULL);
INSERT INTO `ipconfig` VALUES (617, '66.249.64.218', 1, 1, '2017-12-08', NULL);
INSERT INTO `ipconfig` VALUES (618, '111.206.221.113', 1, 1, '2017-12-09', NULL);
INSERT INTO `ipconfig` VALUES (619, '66.249.69.222', 1, 1, '2017-12-09', NULL);
INSERT INTO `ipconfig` VALUES (620, '123.118.172.35', 4, 1, '2017-12-09', NULL);
INSERT INTO `ipconfig` VALUES (621, '111.206.221.112', 1, 1, '2017-12-10', NULL);
INSERT INTO `ipconfig` VALUES (622, '123.118.172.35', 4, 1, '2017-12-10', NULL);
INSERT INTO `ipconfig` VALUES (623, '111.206.221.114', 1, 1, '2017-12-11', NULL);
INSERT INTO `ipconfig` VALUES (624, '123.118.172.35', 1, 1, '2017-12-11', NULL);
INSERT INTO `ipconfig` VALUES (625, '36.110.113.210', 5, 1, '2017-12-12', NULL);
INSERT INTO `ipconfig` VALUES (626, '211.144.1.74', 1, 1, '2017-12-12', NULL);
INSERT INTO `ipconfig` VALUES (627, '220.181.132.181', 1, 1, '2017-12-12', NULL);
INSERT INTO `ipconfig` VALUES (628, '106.120.162.107', 1, 1, '2017-12-12', NULL);
INSERT INTO `ipconfig` VALUES (629, '66.249.66.218', 1, 1, '2017-12-12', NULL);
INSERT INTO `ipconfig` VALUES (630, '111.206.221.71', 1, 1, '2017-12-12', NULL);
INSERT INTO `ipconfig` VALUES (631, '123.118.172.35', 1, 1, '2017-12-12', NULL);
INSERT INTO `ipconfig` VALUES (632, '111.206.221.84', 1, 1, '2017-12-13', NULL);
INSERT INTO `ipconfig` VALUES (633, '111.206.221.40', 1, 1, '2017-12-14', NULL);
INSERT INTO `ipconfig` VALUES (634, '111.206.221.36', 1, 1, '2017-12-14', NULL);
INSERT INTO `ipconfig` VALUES (635, '111.206.221.115', 1, 1, '2017-12-14', NULL);
INSERT INTO `ipconfig` VALUES (636, '211.144.1.74', 1, 1, '2017-12-14', NULL);
INSERT INTO `ipconfig` VALUES (637, '111.206.221.43', 1, 1, '2017-12-14', NULL);
INSERT INTO `ipconfig` VALUES (638, '222.129.236.108', 2, 1, '2017-12-14', NULL);
INSERT INTO `ipconfig` VALUES (639, '111.206.221.73', 1, 1, '2017-12-15', NULL);
INSERT INTO `ipconfig` VALUES (640, '211.144.1.74', 8, 1, '2017-12-15', NULL);
INSERT INTO `ipconfig` VALUES (641, '111.206.221.25', 1, 1, '2017-12-15', NULL);
INSERT INTO `ipconfig` VALUES (642, '111.206.221.38', 1, 1, '2017-12-15', NULL);
INSERT INTO `ipconfig` VALUES (643, '111.206.221.35', 1, 1, '2017-12-15', NULL);
INSERT INTO `ipconfig` VALUES (644, '203.208.60.130', 1, 1, '2017-12-15', NULL);
INSERT INTO `ipconfig` VALUES (645, '111.206.221.35', 1, 1, '2017-12-16', NULL);
INSERT INTO `ipconfig` VALUES (646, '111.206.221.28', 1, 1, '2017-12-16', NULL);
INSERT INTO `ipconfig` VALUES (647, '111.206.221.40', 1, 1, '2017-12-16', NULL);
INSERT INTO `ipconfig` VALUES (648, '111.206.221.88', 1, 1, '2017-12-16', NULL);
INSERT INTO `ipconfig` VALUES (649, '111.206.221.23', 1, 1, '2017-12-17', NULL);
INSERT INTO `ipconfig` VALUES (650, '111.206.221.107', 1, 1, '2017-12-17', NULL);
INSERT INTO `ipconfig` VALUES (651, '111.206.221.40', 1, 1, '2017-12-17', NULL);
INSERT INTO `ipconfig` VALUES (652, '111.206.221.28', 1, 1, '2017-12-17', NULL);
INSERT INTO `ipconfig` VALUES (653, '111.206.221.101', 1, 1, '2017-12-17', NULL);
INSERT INTO `ipconfig` VALUES (654, '114.253.32.211', 1, 1, '2017-12-17', NULL);
INSERT INTO `ipconfig` VALUES (655, '111.206.221.37', 1, 1, '2017-12-17', NULL);
INSERT INTO `ipconfig` VALUES (656, '111.206.221.112', 1, 1, '2017-12-18', NULL);
INSERT INTO `ipconfig` VALUES (657, '111.206.221.50', 1, 1, '2017-12-18', NULL);
INSERT INTO `ipconfig` VALUES (658, '111.206.221.105', 1, 1, '2017-12-18', NULL);
INSERT INTO `ipconfig` VALUES (659, '111.206.221.26', 1, 1, '2017-12-18', NULL);
INSERT INTO `ipconfig` VALUES (660, '36.110.113.210', 2, 1, '2017-12-18', NULL);
INSERT INTO `ipconfig` VALUES (661, '111.206.221.39', 1, 1, '2017-12-19', NULL);
INSERT INTO `ipconfig` VALUES (662, '111.206.221.100', 1, 1, '2017-12-19', NULL);
INSERT INTO `ipconfig` VALUES (663, '111.206.221.34', 1, 1, '2017-12-19', NULL);
INSERT INTO `ipconfig` VALUES (664, '111.206.221.22', 1, 1, '2017-12-19', NULL);
INSERT INTO `ipconfig` VALUES (665, '211.144.1.74', 5, 1, '2017-12-19', NULL);
INSERT INTO `ipconfig` VALUES (666, '111.206.221.30', 1, 1, '2017-12-20', NULL);
INSERT INTO `ipconfig` VALUES (667, '36.110.113.210', 6, 1, '2017-12-20', NULL);
INSERT INTO `ipconfig` VALUES (668, '111.206.221.115', 1, 1, '2017-12-20', NULL);
INSERT INTO `ipconfig` VALUES (669, '111.206.221.74', 1, 1, '2017-12-20', NULL);
INSERT INTO `ipconfig` VALUES (670, '211.144.1.74', 2, 1, '2017-12-20', NULL);
INSERT INTO `ipconfig` VALUES (671, '114.253.32.211', 1, 1, '2017-12-20', NULL);
INSERT INTO `ipconfig` VALUES (672, '111.206.221.49', 1, 1, '2017-12-21', NULL);
INSERT INTO `ipconfig` VALUES (673, '111.206.221.88', 1, 1, '2017-12-21', NULL);
INSERT INTO `ipconfig` VALUES (674, '36.110.113.210', 2, 1, '2017-12-21', NULL);
INSERT INTO `ipconfig` VALUES (675, '111.206.221.35', 1, 1, '2017-12-21', NULL);
INSERT INTO `ipconfig` VALUES (676, '111.206.221.106', 1, 1, '2017-12-22', NULL);
INSERT INTO `ipconfig` VALUES (677, '220.181.108.118', 1, 1, '2017-12-22', NULL);
INSERT INTO `ipconfig` VALUES (678, '66.249.64.218', 2, 1, '2017-12-22', NULL);
INSERT INTO `ipconfig` VALUES (679, '111.206.221.102', 1, 1, '2017-12-22', NULL);
INSERT INTO `ipconfig` VALUES (680, '111.206.221.48', 1, 1, '2017-12-22', NULL);
INSERT INTO `ipconfig` VALUES (681, '111.206.221.27', 1, 1, '2017-12-22', NULL);
INSERT INTO `ipconfig` VALUES (682, '66.249.64.218', 1, 1, '2017-12-23', NULL);
INSERT INTO `ipconfig` VALUES (683, '111.206.221.24', 1, 1, '2017-12-23', NULL);
INSERT INTO `ipconfig` VALUES (684, '111.206.221.48', 1, 1, '2017-12-23', NULL);
INSERT INTO `ipconfig` VALUES (685, '121.35.102.59', 1, 1, '2017-12-23', NULL);
INSERT INTO `ipconfig` VALUES (686, '111.206.221.27', 1, 1, '2017-12-23', NULL);
INSERT INTO `ipconfig` VALUES (687, '111.206.221.93', 1, 1, '2017-12-24', NULL);
INSERT INTO `ipconfig` VALUES (688, '111.206.221.97', 1, 1, '2017-12-24', NULL);
INSERT INTO `ipconfig` VALUES (689, '111.206.221.37', 1, 1, '2017-12-24', NULL);
INSERT INTO `ipconfig` VALUES (690, '111.206.221.48', 1, 1, '2017-12-25', NULL);
INSERT INTO `ipconfig` VALUES (691, '111.206.221.109', 1, 1, '2017-12-25', NULL);
INSERT INTO `ipconfig` VALUES (692, '36.110.113.210', 1, 1, '2017-12-25', NULL);
INSERT INTO `ipconfig` VALUES (693, '111.206.221.105', 1, 1, '2017-12-25', NULL);
INSERT INTO `ipconfig` VALUES (694, '111.206.221.106', 1, 1, '2017-12-26', NULL);
INSERT INTO `ipconfig` VALUES (695, '111.206.221.112', 1, 1, '2017-12-26', NULL);
INSERT INTO `ipconfig` VALUES (696, '111.206.221.37', 1, 1, '2017-12-26', NULL);
INSERT INTO `ipconfig` VALUES (697, '111.206.221.44', 1, 1, '2017-12-27', NULL);
INSERT INTO `ipconfig` VALUES (698, '111.206.221.26', 1, 1, '2017-12-27', NULL);
INSERT INTO `ipconfig` VALUES (699, '111.206.221.92', 1, 1, '2017-12-27', NULL);
INSERT INTO `ipconfig` VALUES (700, '111.206.221.29', 1, 1, '2017-12-27', NULL);
INSERT INTO `ipconfig` VALUES (701, '111.206.221.24', 1, 1, '2017-12-28', NULL);
INSERT INTO `ipconfig` VALUES (702, '203.208.60.129', 2, 1, '2017-12-28', NULL);
INSERT INTO `ipconfig` VALUES (703, '111.206.221.49', 1, 1, '2017-12-28', NULL);
INSERT INTO `ipconfig` VALUES (704, '111.206.221.77', 1, 1, '2017-12-28', NULL);
INSERT INTO `ipconfig` VALUES (705, '203.208.60.130', 1, 1, '2017-12-28', NULL);
INSERT INTO `ipconfig` VALUES (706, '111.206.221.69', 1, 1, '2017-12-29', NULL);
INSERT INTO `ipconfig` VALUES (707, '203.208.60.131', 1, 1, '2017-12-29', NULL);
INSERT INTO `ipconfig` VALUES (708, '111.206.221.11', 1, 1, '2017-12-29', NULL);
INSERT INTO `ipconfig` VALUES (709, '111.206.221.16', 1, 1, '2017-12-29', NULL);
INSERT INTO `ipconfig` VALUES (710, '111.206.221.21', 1, 1, '2017-12-30', NULL);
INSERT INTO `ipconfig` VALUES (711, '111.206.221.76', 1, 1, '2017-12-30', NULL);
INSERT INTO `ipconfig` VALUES (712, '111.206.221.96', 1, 1, '2017-12-30', NULL);
INSERT INTO `ipconfig` VALUES (713, '111.206.221.38', 1, 1, '2017-12-31', NULL);
INSERT INTO `ipconfig` VALUES (714, '111.206.221.105', 1, 1, '2017-12-31', NULL);
INSERT INTO `ipconfig` VALUES (715, '111.206.221.92', 1, 1, '2017-12-31', NULL);
INSERT INTO `ipconfig` VALUES (716, '111.206.221.2', 1, 1, '2017-12-31', NULL);
INSERT INTO `ipconfig` VALUES (717, '114.246.66.120', 1, 1, '2017-12-31', NULL);
INSERT INTO `ipconfig` VALUES (718, '111.206.221.26', 1, 1, '2018-01-01', NULL);
INSERT INTO `ipconfig` VALUES (719, '111.206.221.13', 1, 1, '2018-01-01', NULL);
INSERT INTO `ipconfig` VALUES (720, '111.206.221.49', 1, 1, '2018-01-01', NULL);
INSERT INTO `ipconfig` VALUES (721, '111.206.221.113', 1, 1, '2018-01-01', NULL);
INSERT INTO `ipconfig` VALUES (722, '111.206.221.90', 1, 1, '2018-01-02', NULL);
INSERT INTO `ipconfig` VALUES (723, '111.206.221.38', 1, 1, '2018-01-02', NULL);
INSERT INTO `ipconfig` VALUES (724, '111.206.221.114', 1, 1, '2018-01-02', NULL);
INSERT INTO `ipconfig` VALUES (725, '111.206.221.13', 1, 1, '2018-01-03', NULL);
INSERT INTO `ipconfig` VALUES (726, '211.144.1.74', 2, 1, '2018-01-03', NULL);
INSERT INTO `ipconfig` VALUES (727, '111.206.221.34', 1, 1, '2018-01-03', NULL);
INSERT INTO `ipconfig` VALUES (728, '203.208.60.130', 1, 1, '2018-01-03', NULL);
INSERT INTO `ipconfig` VALUES (729, '203.208.60.129', 1, 1, '2018-01-03', NULL);
INSERT INTO `ipconfig` VALUES (730, '111.206.221.95', 1, 1, '2018-01-04', NULL);
INSERT INTO `ipconfig` VALUES (731, '111.206.221.73', 1, 1, '2018-01-04', NULL);
INSERT INTO `ipconfig` VALUES (732, '111.206.221.108', 1, 1, '2018-01-04', NULL);
INSERT INTO `ipconfig` VALUES (733, '203.208.60.130', 2, 1, '2018-01-04', NULL);
INSERT INTO `ipconfig` VALUES (734, '203.208.60.132', 1, 1, '2018-01-04', NULL);
INSERT INTO `ipconfig` VALUES (735, '111.206.221.34', 1, 1, '2018-01-05', NULL);
INSERT INTO `ipconfig` VALUES (736, '111.206.221.103', 1, 1, '2018-01-05', NULL);
INSERT INTO `ipconfig` VALUES (737, '220.181.132.181', 1, 1, '2018-01-05', NULL);
INSERT INTO `ipconfig` VALUES (738, '106.120.162.96', 1, 1, '2018-01-05', NULL);
INSERT INTO `ipconfig` VALUES (739, '111.206.221.46', 1, 1, '2018-01-05', NULL);
INSERT INTO `ipconfig` VALUES (740, '111.206.221.19', 1, 1, '2018-01-06', NULL);
INSERT INTO `ipconfig` VALUES (741, '203.208.60.132', 1, 1, '2018-01-06', NULL);
INSERT INTO `ipconfig` VALUES (742, '111.206.221.46', 1, 1, '2018-01-06', NULL);
INSERT INTO `ipconfig` VALUES (743, '111.206.221.67', 1, 1, '2018-01-06', NULL);
INSERT INTO `ipconfig` VALUES (744, '203.208.60.133', 2, 1, '2018-01-06', NULL);
INSERT INTO `ipconfig` VALUES (745, '111.206.221.101', 1, 1, '2018-01-06', NULL);
INSERT INTO `ipconfig` VALUES (746, '144.52.32.215', 1, 1, '2018-01-06', NULL);
INSERT INTO `ipconfig` VALUES (747, '111.206.221.21', 1, 1, '2018-01-07', NULL);
INSERT INTO `ipconfig` VALUES (748, '203.208.60.133', 2, 1, '2018-01-07', NULL);
INSERT INTO `ipconfig` VALUES (749, '111.206.221.109', 1, 1, '2018-01-07', NULL);
INSERT INTO `ipconfig` VALUES (750, '111.206.221.84', 1, 1, '2018-01-07', NULL);
INSERT INTO `ipconfig` VALUES (751, '111.206.221.39', 1, 1, '2018-01-07', NULL);
INSERT INTO `ipconfig` VALUES (752, '111.206.221.44', 1, 1, '2018-01-08', NULL);
INSERT INTO `ipconfig` VALUES (753, '111.206.221.13', 1, 1, '2018-01-08', NULL);
INSERT INTO `ipconfig` VALUES (754, '111.206.221.107', 1, 1, '2018-01-09', NULL);
INSERT INTO `ipconfig` VALUES (755, '114.245.202.177', 1, 1, '2018-01-09', NULL);
INSERT INTO `ipconfig` VALUES (756, '111.206.221.85', 1, 1, '2018-01-09', NULL);
INSERT INTO `ipconfig` VALUES (757, '111.206.221.30', 1, 1, '2018-01-09', NULL);
INSERT INTO `ipconfig` VALUES (758, '111.206.221.78', 1, 1, '2018-01-09', NULL);
INSERT INTO `ipconfig` VALUES (759, '111.206.221.101', 1, 1, '2018-01-10', NULL);
INSERT INTO `ipconfig` VALUES (760, '220.181.108.121', 1, 1, '2018-01-10', NULL);
INSERT INTO `ipconfig` VALUES (761, '111.206.221.41', 1, 1, '2018-01-10', NULL);
INSERT INTO `ipconfig` VALUES (762, '111.206.221.105', 1, 1, '2018-01-10', NULL);
INSERT INTO `ipconfig` VALUES (763, '111.206.221.45', 1, 1, '2018-01-10', NULL);
INSERT INTO `ipconfig` VALUES (764, '111.206.221.32', 1, 1, '2018-01-11', NULL);
INSERT INTO `ipconfig` VALUES (765, '220.181.108.113', 1, 1, '2018-01-11', NULL);
INSERT INTO `ipconfig` VALUES (766, '111.206.221.22', 1, 1, '2018-01-11', NULL);
INSERT INTO `ipconfig` VALUES (767, '203.208.60.133', 1, 1, '2018-01-11', NULL);
INSERT INTO `ipconfig` VALUES (768, '111.206.221.27', 1, 1, '2018-01-11', NULL);
INSERT INTO `ipconfig` VALUES (769, '111.206.221.73', 1, 1, '2018-01-12', NULL);
INSERT INTO `ipconfig` VALUES (770, '203.208.60.129', 1, 1, '2018-01-12', NULL);
INSERT INTO `ipconfig` VALUES (771, '203.208.60.131', 1, 1, '2018-01-12', NULL);
INSERT INTO `ipconfig` VALUES (772, '111.206.221.105', 1, 1, '2018-01-12', NULL);
INSERT INTO `ipconfig` VALUES (773, '203.208.60.132', 1, 1, '2018-01-12', NULL);
INSERT INTO `ipconfig` VALUES (774, '111.206.221.104', 1, 1, '2018-01-12', NULL);
INSERT INTO `ipconfig` VALUES (775, '203.208.60.133', 1, 1, '2018-01-12', NULL);
INSERT INTO `ipconfig` VALUES (776, '111.206.221.29', 1, 1, '2018-01-12', NULL);
INSERT INTO `ipconfig` VALUES (777, '111.206.221.100', 1, 1, '2018-01-13', NULL);
INSERT INTO `ipconfig` VALUES (778, '111.206.221.90', 1, 1, '2018-01-13', NULL);
INSERT INTO `ipconfig` VALUES (779, '111.206.221.104', 1, 1, '2018-01-13', NULL);
INSERT INTO `ipconfig` VALUES (780, '111.206.221.74', 1, 1, '2018-01-13', NULL);
INSERT INTO `ipconfig` VALUES (781, '111.206.221.88', 1, 1, '2018-01-14', NULL);
INSERT INTO `ipconfig` VALUES (782, '111.206.221.3', 1, 1, '2018-01-14', NULL);
INSERT INTO `ipconfig` VALUES (783, '111.206.221.8', 1, 1, '2018-01-14', NULL);
INSERT INTO `ipconfig` VALUES (784, '111.206.221.95', 1, 1, '2018-01-14', NULL);
INSERT INTO `ipconfig` VALUES (785, '61.170.244.246', 1, 1, '2018-01-14', NULL);
INSERT INTO `ipconfig` VALUES (786, '111.206.221.67', 1, 1, '2018-01-15', NULL);
INSERT INTO `ipconfig` VALUES (787, '36.110.113.210', 2, 1, '2018-01-15', NULL);
INSERT INTO `ipconfig` VALUES (788, '111.206.221.51', 1, 1, '2018-01-15', NULL);
INSERT INTO `ipconfig` VALUES (789, '111.206.221.106', 1, 1, '2018-01-15', NULL);
INSERT INTO `ipconfig` VALUES (790, '111.206.221.97', 1, 1, '2018-01-16', NULL);
INSERT INTO `ipconfig` VALUES (791, '111.206.221.5', 1, 1, '2018-01-16', NULL);
INSERT INTO `ipconfig` VALUES (792, '111.206.221.68', 1, 1, '2018-01-16', NULL);
INSERT INTO `ipconfig` VALUES (793, '111.206.221.39', 1, 1, '2018-01-17', NULL);
INSERT INTO `ipconfig` VALUES (794, '111.206.221.90', 1, 1, '2018-01-17', NULL);
INSERT INTO `ipconfig` VALUES (795, '111.206.221.70', 1, 1, '2018-01-17', NULL);
INSERT INTO `ipconfig` VALUES (796, '111.206.221.41', 1, 1, '2018-01-17', NULL);
INSERT INTO `ipconfig` VALUES (797, '111.206.221.11', 1, 1, '2018-01-18', NULL);
INSERT INTO `ipconfig` VALUES (798, '111.206.221.39', 1, 1, '2018-01-18', NULL);
INSERT INTO `ipconfig` VALUES (799, '36.110.113.210', 1, 1, '2018-01-18', NULL);
INSERT INTO `ipconfig` VALUES (800, '111.206.221.24', 1, 1, '2018-01-19', NULL);
INSERT INTO `ipconfig` VALUES (801, '111.206.221.44', 1, 1, '2018-01-19', NULL);
INSERT INTO `ipconfig` VALUES (802, '111.206.221.115', 1, 1, '2018-01-19', NULL);
INSERT INTO `ipconfig` VALUES (803, '203.208.60.132', 1, 1, '2018-01-19', NULL);
INSERT INTO `ipconfig` VALUES (804, '203.208.60.129', 1, 1, '2018-01-20', NULL);
INSERT INTO `ipconfig` VALUES (805, '111.206.221.19', 1, 1, '2018-01-20', NULL);
INSERT INTO `ipconfig` VALUES (806, '111.206.221.90', 1, 1, '2018-01-20', NULL);
INSERT INTO `ipconfig` VALUES (807, '111.206.221.6', 1, 1, '2018-01-20', NULL);
INSERT INTO `ipconfig` VALUES (808, '111.206.221.72', 1, 1, '2018-01-21', NULL);
INSERT INTO `ipconfig` VALUES (809, '111.206.221.11', 1, 1, '2018-01-21', NULL);
INSERT INTO `ipconfig` VALUES (810, '111.206.221.68', 1, 1, '2018-01-21', NULL);
INSERT INTO `ipconfig` VALUES (811, '111.206.221.101', 1, 1, '2018-01-22', NULL);
INSERT INTO `ipconfig` VALUES (812, '111.206.221.103', 1, 1, '2018-01-22', NULL);
INSERT INTO `ipconfig` VALUES (813, '111.206.221.31', 1, 1, '2018-01-22', NULL);
INSERT INTO `ipconfig` VALUES (814, '36.110.113.210', 2, 1, '2018-01-22', NULL);
INSERT INTO `ipconfig` VALUES (815, '111.206.221.42', 1, 1, '2018-01-23', NULL);
INSERT INTO `ipconfig` VALUES (816, '111.206.221.83', 1, 1, '2018-01-23', NULL);
INSERT INTO `ipconfig` VALUES (817, '111.206.221.81', 1, 1, '2018-01-23', NULL);
INSERT INTO `ipconfig` VALUES (818, '211.144.1.74', 1, 1, '2018-01-23', NULL);
INSERT INTO `ipconfig` VALUES (819, '111.206.221.67', 1, 1, '2018-01-24', NULL);
INSERT INTO `ipconfig` VALUES (820, '111.206.198.107', 1, 1, '2018-01-24', NULL);
INSERT INTO `ipconfig` VALUES (821, '111.206.198.18', 1, 1, '2018-01-24', NULL);
INSERT INTO `ipconfig` VALUES (822, '36.110.113.210', 2, 1, '2018-01-24', NULL);
INSERT INTO `ipconfig` VALUES (823, '111.206.221.111', 1, 1, '2018-01-25', NULL);
INSERT INTO `ipconfig` VALUES (824, '111.206.221.20', 1, 1, '2018-01-25', NULL);
INSERT INTO `ipconfig` VALUES (825, '111.206.198.4', 1, 1, '2018-01-25', NULL);
INSERT INTO `ipconfig` VALUES (826, '111.206.221.85', 1, 1, '2018-01-26', NULL);
INSERT INTO `ipconfig` VALUES (827, '111.206.198.28', 1, 1, '2018-01-26', NULL);
INSERT INTO `ipconfig` VALUES (828, '211.144.1.74', 2, 1, '2018-01-26', NULL);
INSERT INTO `ipconfig` VALUES (829, '111.206.221.21', 1, 1, '2018-01-26', NULL);
INSERT INTO `ipconfig` VALUES (830, '66.249.73.154', 1, 1, '2018-01-26', NULL);
INSERT INTO `ipconfig` VALUES (831, '111.206.221.15', 1, 1, '2018-01-27', NULL);
INSERT INTO `ipconfig` VALUES (832, '111.206.198.120', 1, 1, '2018-01-27', NULL);
INSERT INTO `ipconfig` VALUES (833, '111.206.221.88', 1, 1, '2018-01-27', NULL);
INSERT INTO `ipconfig` VALUES (834, '66.249.75.218', 1, 1, '2018-01-27', NULL);
INSERT INTO `ipconfig` VALUES (835, '111.206.221.95', 1, 1, '2018-01-28', NULL);
INSERT INTO `ipconfig` VALUES (836, '111.206.221.39', 1, 1, '2018-01-28', NULL);
INSERT INTO `ipconfig` VALUES (837, '113.246.186.81', 1, 1, '2018-01-28', NULL);
INSERT INTO `ipconfig` VALUES (838, '220.181.108.155', 1, 1, '2018-01-28', NULL);
INSERT INTO `ipconfig` VALUES (839, '66.249.75.220', 1, 1, '2018-01-28', NULL);
INSERT INTO `ipconfig` VALUES (840, '66.249.75.218', 1, 1, '2018-01-28', NULL);
INSERT INTO `ipconfig` VALUES (841, '111.206.198.90', 1, 1, '2018-01-28', NULL);
INSERT INTO `ipconfig` VALUES (842, '220.181.108.145', 1, 1, '2018-01-29', NULL);
INSERT INTO `ipconfig` VALUES (843, '111.206.221.109', 1, 1, '2018-01-29', NULL);
INSERT INTO `ipconfig` VALUES (844, '111.206.198.14', 1, 1, '2018-01-29', NULL);
INSERT INTO `ipconfig` VALUES (845, '111.206.198.74', 1, 1, '2018-01-29', NULL);
INSERT INTO `ipconfig` VALUES (846, '111.206.198.70', 1, 1, '2018-01-30', NULL);
INSERT INTO `ipconfig` VALUES (847, '111.206.221.14', 1, 1, '2018-01-30', NULL);
INSERT INTO `ipconfig` VALUES (848, '111.206.221.48', 1, 1, '2018-01-30', NULL);
INSERT INTO `ipconfig` VALUES (849, '172.20.86.70', 1, 1, '2018-01-30', NULL);
INSERT INTO `ipconfig` VALUES (850, '220.181.108.140', 1, 1, '2018-01-31', NULL);
INSERT INTO `ipconfig` VALUES (851, '111.206.221.43', 1, 1, '2018-01-31', NULL);
INSERT INTO `ipconfig` VALUES (852, '111.206.198.100', 1, 1, '2018-01-31', NULL);
INSERT INTO `ipconfig` VALUES (853, '111.206.198.121', 1, 1, '2018-02-01', NULL);
INSERT INTO `ipconfig` VALUES (854, '111.206.198.14', 1, 1, '2018-02-01', NULL);
INSERT INTO `ipconfig` VALUES (855, '121.69.105.22', 2, 1, '2018-02-01', NULL);
INSERT INTO `ipconfig` VALUES (856, '111.206.221.2', 1, 1, '2018-02-02', NULL);
INSERT INTO `ipconfig` VALUES (857, '111.206.221.17', 1, 1, '2018-02-02', NULL);
INSERT INTO `ipconfig` VALUES (858, '111.206.198.55', 2, 1, '2018-02-03', NULL);
INSERT INTO `ipconfig` VALUES (859, '111.206.198.14', 1, 1, '2018-02-03', NULL);
INSERT INTO `ipconfig` VALUES (860, '222.129.233.174', 3, 1, '2018-02-03', NULL);
INSERT INTO `ipconfig` VALUES (861, '111.206.221.37', 1, 1, '2018-02-04', NULL);
INSERT INTO `ipconfig` VALUES (862, '111.206.198.51', 1, 1, '2018-02-04', NULL);
INSERT INTO `ipconfig` VALUES (863, '111.206.221.15', 1, 1, '2018-02-04', NULL);
INSERT INTO `ipconfig` VALUES (864, '203.208.60.131', 1, 1, '2018-02-05', NULL);
INSERT INTO `ipconfig` VALUES (865, '111.206.221.24', 1, 1, '2018-02-05', NULL);
INSERT INTO `ipconfig` VALUES (866, '203.208.60.132', 1, 1, '2018-02-05', NULL);
INSERT INTO `ipconfig` VALUES (867, '77.37.136.195', 1, 1, '2018-02-05', NULL);
INSERT INTO `ipconfig` VALUES (868, '121.69.105.22', 3, 1, '2018-02-05', NULL);
INSERT INTO `ipconfig` VALUES (869, '111.206.198.30', 1, 1, '2018-02-05', NULL);
INSERT INTO `ipconfig` VALUES (870, '111.206.198.11', 1, 1, '2018-02-06', NULL);
INSERT INTO `ipconfig` VALUES (871, '111.206.198.97', 1, 1, '2018-02-06', NULL);
INSERT INTO `ipconfig` VALUES (872, '58.48.126.38', 2, 1, '2018-02-06', NULL);
INSERT INTO `ipconfig` VALUES (873, '111.206.198.45', 1, 1, '2018-02-06', NULL);
INSERT INTO `ipconfig` VALUES (874, '111.206.198.116', 1, 1, '2018-02-07', NULL);
INSERT INTO `ipconfig` VALUES (875, '111.206.221.28', 1, 1, '2018-02-07', NULL);
INSERT INTO `ipconfig` VALUES (876, '111.206.221.99', 1, 1, '2018-02-08', NULL);
INSERT INTO `ipconfig` VALUES (877, '111.206.221.47', 1, 1, '2018-02-08', NULL);
INSERT INTO `ipconfig` VALUES (878, '111.206.221.41', 1, 1, '2018-02-09', NULL);
INSERT INTO `ipconfig` VALUES (879, '111.206.198.39', 1, 1, '2018-02-09', NULL);
INSERT INTO `ipconfig` VALUES (880, '121.69.105.22', 5, 1, '2018-02-09', NULL);
INSERT INTO `ipconfig` VALUES (881, '111.206.198.89', 1, 1, '2018-02-09', NULL);
INSERT INTO `ipconfig` VALUES (882, '111.206.198.45', 1, 1, '2018-02-10', NULL);
INSERT INTO `ipconfig` VALUES (883, '121.69.105.22', 32, 1, '2018-02-10', NULL);
INSERT INTO `ipconfig` VALUES (884, '111.206.198.109', 1, 1, '2018-02-10', NULL);
INSERT INTO `ipconfig` VALUES (885, '111.206.221.104', 1, 1, '2018-02-10', NULL);
INSERT INTO `ipconfig` VALUES (886, '111.206.198.98', 1, 1, '2018-02-10', NULL);
INSERT INTO `ipconfig` VALUES (887, '111.206.221.107', 1, 1, '2018-02-11', NULL);
INSERT INTO `ipconfig` VALUES (888, '111.206.198.32', 1, 1, '2018-02-11', NULL);
INSERT INTO `ipconfig` VALUES (889, '121.69.105.22', 11, 1, '2018-02-11', NULL);
INSERT INTO `ipconfig` VALUES (890, '111.206.221.78', 1, 1, '2018-02-11', NULL);
INSERT INTO `ipconfig` VALUES (891, '111.206.221.70', 1, 1, '2018-02-11', NULL);
INSERT INTO `ipconfig` VALUES (892, '111.206.198.94', 1, 1, '2018-02-12', NULL);
INSERT INTO `ipconfig` VALUES (893, '111.206.198.77', 1, 1, '2018-02-12', NULL);
INSERT INTO `ipconfig` VALUES (894, '66.249.75.20', 1, 1, '2018-02-12', NULL);
INSERT INTO `ipconfig` VALUES (895, '111.206.221.29', 1, 1, '2018-02-12', NULL);
INSERT INTO `ipconfig` VALUES (896, '111.206.221.46', 1, 1, '2018-02-12', NULL);
INSERT INTO `ipconfig` VALUES (897, '66.249.75.20', 1, 1, '2018-02-13', NULL);
INSERT INTO `ipconfig` VALUES (898, '111.206.198.39', 1, 1, '2018-02-13', NULL);
INSERT INTO `ipconfig` VALUES (899, '111.206.221.87', 1, 1, '2018-02-13', NULL);
INSERT INTO `ipconfig` VALUES (900, '111.206.198.73', 1, 1, '2018-02-13', NULL);
INSERT INTO `ipconfig` VALUES (901, '111.206.198.21', 1, 1, '2018-02-13', NULL);
INSERT INTO `ipconfig` VALUES (902, '111.206.221.40', 1, 1, '2018-02-13', NULL);
INSERT INTO `ipconfig` VALUES (903, '111.206.221.16', 1, 1, '2018-02-14', NULL);
INSERT INTO `ipconfig` VALUES (904, '111.206.221.25', 1, 1, '2018-02-14', NULL);
INSERT INTO `ipconfig` VALUES (905, '111.206.198.88', 1, 1, '2018-02-14', NULL);
INSERT INTO `ipconfig` VALUES (906, '111.206.221.107', 1, 1, '2018-02-14', NULL);
INSERT INTO `ipconfig` VALUES (907, '111.206.221.110', 1, 1, '2018-02-14', NULL);
INSERT INTO `ipconfig` VALUES (908, '111.206.198.102', 1, 1, '2018-02-15', NULL);
INSERT INTO `ipconfig` VALUES (909, '111.206.198.69', 1, 1, '2018-02-15', NULL);
INSERT INTO `ipconfig` VALUES (910, '111.206.221.101', 1, 1, '2018-02-15', NULL);
INSERT INTO `ipconfig` VALUES (911, '117.179.250.44', 1, 1, '2018-02-15', NULL);
INSERT INTO `ipconfig` VALUES (912, '111.206.198.45', 1, 1, '2018-02-16', NULL);
INSERT INTO `ipconfig` VALUES (913, '111.206.221.37', 1, 1, '2018-02-16', NULL);
INSERT INTO `ipconfig` VALUES (914, '93.80.146.242', 1, 1, '2018-02-16', NULL);
INSERT INTO `ipconfig` VALUES (915, '111.206.221.105', 1, 1, '2018-02-16', NULL);
INSERT INTO `ipconfig` VALUES (916, '111.206.198.76', 1, 1, '2018-02-17', NULL);
INSERT INTO `ipconfig` VALUES (917, '111.206.198.55', 1, 1, '2018-02-17', NULL);
INSERT INTO `ipconfig` VALUES (918, '111.206.221.101', 1, 1, '2018-02-17', NULL);
INSERT INTO `ipconfig` VALUES (919, '111.206.198.35', 1, 1, '2018-02-18', NULL);
INSERT INTO `ipconfig` VALUES (920, '111.206.221.97', 1, 1, '2018-02-18', NULL);
INSERT INTO `ipconfig` VALUES (921, '111.206.198.119', 1, 1, '2018-02-18', NULL);
INSERT INTO `ipconfig` VALUES (922, '111.206.221.77', 1, 1, '2018-02-18', NULL);
INSERT INTO `ipconfig` VALUES (923, '109.173.119.152', 1, 1, '2018-02-18', NULL);
INSERT INTO `ipconfig` VALUES (924, '111.206.198.94', 2, 1, '2018-02-19', NULL);
INSERT INTO `ipconfig` VALUES (925, '111.206.198.36', 1, 1, '2018-02-19', NULL);
INSERT INTO `ipconfig` VALUES (926, '111.206.221.24', 1, 1, '2018-02-20', NULL);
INSERT INTO `ipconfig` VALUES (927, '111.206.221.32', 1, 1, '2018-02-20', NULL);
INSERT INTO `ipconfig` VALUES (928, '203.208.60.129', 2, 1, '2018-02-20', NULL);
INSERT INTO `ipconfig` VALUES (929, '111.206.221.66', 1, 1, '2018-02-20', NULL);
INSERT INTO `ipconfig` VALUES (930, '203.208.60.132', 1, 1, '2018-02-20', NULL);
INSERT INTO `ipconfig` VALUES (931, '203.208.60.133', 1, 1, '2018-02-20', NULL);
INSERT INTO `ipconfig` VALUES (932, '111.206.198.11', 1, 1, '2018-02-20', NULL);
INSERT INTO `ipconfig` VALUES (933, '111.206.221.66', 1, 1, '2018-02-21', NULL);
INSERT INTO `ipconfig` VALUES (934, '220.181.108.112', 1, 1, '2018-02-21', NULL);
INSERT INTO `ipconfig` VALUES (935, '111.206.198.103', 2, 1, '2018-02-21', NULL);
INSERT INTO `ipconfig` VALUES (936, '111.206.198.15', 1, 1, '2018-02-21', NULL);
INSERT INTO `ipconfig` VALUES (937, '123.181.163.84', 4, 1, '2018-02-21', NULL);
INSERT INTO `ipconfig` VALUES (938, '111.206.198.6', 1, 1, '2018-02-22', NULL);
INSERT INTO `ipconfig` VALUES (939, '111.206.198.22', 1, 1, '2018-02-22', NULL);
INSERT INTO `ipconfig` VALUES (940, '114.246.66.58', 7, 1, '2018-02-22', NULL);
INSERT INTO `ipconfig` VALUES (941, '111.206.221.75', 1, 1, '2018-02-22', NULL);
INSERT INTO `ipconfig` VALUES (942, '111.206.198.101', 1, 1, '2018-02-23', NULL);
INSERT INTO `ipconfig` VALUES (943, '210.217.2.152', 1, 1, '2018-02-23', NULL);
INSERT INTO `ipconfig` VALUES (944, '111.206.198.123', 1, 1, '2018-02-23', NULL);
INSERT INTO `ipconfig` VALUES (945, '111.206.221.75', 1, 1, '2018-02-24', NULL);
INSERT INTO `ipconfig` VALUES (946, '121.69.105.22', 12, 1, '2018-02-24', NULL);
INSERT INTO `ipconfig` VALUES (947, '111.206.198.16', 1, 1, '2018-02-24', NULL);
INSERT INTO `ipconfig` VALUES (948, '111.206.198.121', 1, 1, '2018-02-24', NULL);
INSERT INTO `ipconfig` VALUES (949, '111.206.198.30', 1, 1, '2018-02-24', NULL);
INSERT INTO `ipconfig` VALUES (950, '111.206.221.40', 1, 1, '2018-02-25', NULL);
INSERT INTO `ipconfig` VALUES (951, '111.206.221.38', 1, 1, '2018-02-25', NULL);
INSERT INTO `ipconfig` VALUES (952, '111.206.221.75', 1, 1, '2018-02-25', NULL);
INSERT INTO `ipconfig` VALUES (953, '114.252.35.123', 2, 1, '2018-02-25', NULL);
INSERT INTO `ipconfig` VALUES (954, '111.206.221.43', 1, 1, '2018-02-25', NULL);
INSERT INTO `ipconfig` VALUES (955, '111.206.198.116', 1, 1, '2018-02-26', NULL);
INSERT INTO `ipconfig` VALUES (956, '111.206.221.33', 1, 1, '2018-02-26', NULL);
INSERT INTO `ipconfig` VALUES (957, '121.69.105.22', 9, 1, '2018-02-26', NULL);
INSERT INTO `ipconfig` VALUES (958, '111.206.198.99', 1, 1, '2018-02-26', NULL);
INSERT INTO `ipconfig` VALUES (959, '111.206.198.84', 1, 1, '2018-02-27', NULL);
INSERT INTO `ipconfig` VALUES (960, '111.206.221.4', 1, 1, '2018-02-27', NULL);
INSERT INTO `ipconfig` VALUES (961, '121.69.105.22', 10, 1, '2018-02-27', NULL);
INSERT INTO `ipconfig` VALUES (962, '66.249.73.217', 1, 1, '2018-02-27', NULL);
INSERT INTO `ipconfig` VALUES (963, '111.206.198.44', 1, 1, '2018-02-27', NULL);
INSERT INTO `ipconfig` VALUES (964, '111.206.221.12', 1, 1, '2018-02-28', NULL);
INSERT INTO `ipconfig` VALUES (965, '66.249.64.222', 1, 1, '2018-02-28', NULL);
INSERT INTO `ipconfig` VALUES (966, '111.206.198.89', 2, 1, '2018-02-28', NULL);
INSERT INTO `ipconfig` VALUES (967, '111.206.221.39', 1, 1, '2018-02-28', NULL);
INSERT INTO `ipconfig` VALUES (968, '121.69.105.22', 1, 1, '2018-02-28', NULL);
INSERT INTO `ipconfig` VALUES (969, '111.206.221.81', 1, 1, '2018-03-01', NULL);
INSERT INTO `ipconfig` VALUES (970, '111.206.221.25', 1, 1, '2018-03-01', NULL);
INSERT INTO `ipconfig` VALUES (971, '121.69.105.22', 2, 1, '2018-03-01', NULL);
INSERT INTO `ipconfig` VALUES (972, '111.206.198.27', 1, 1, '2018-03-01', NULL);
INSERT INTO `ipconfig` VALUES (973, '111.206.198.6', 1, 1, '2018-03-02', NULL);
INSERT INTO `ipconfig` VALUES (974, '111.206.221.113', 1, 1, '2018-03-02', NULL);
INSERT INTO `ipconfig` VALUES (975, '121.69.105.22', 4, 1, '2018-03-02', NULL);
INSERT INTO `ipconfig` VALUES (976, '111.206.221.80', 1, 1, '2018-03-03', NULL);
INSERT INTO `ipconfig` VALUES (977, '114.252.44.189', 73, 1, '2018-03-03', NULL);
INSERT INTO `ipconfig` VALUES (978, '111.206.221.8', 1, 1, '2018-03-03', NULL);
INSERT INTO `ipconfig` VALUES (979, '111.206.198.41', 1, 1, '2018-03-04', NULL);
INSERT INTO `ipconfig` VALUES (980, '114.252.44.189', 15, 1, '2018-03-04', NULL);
INSERT INTO `ipconfig` VALUES (981, '203.208.60.132', 1, 1, '2018-03-05', NULL);
INSERT INTO `ipconfig` VALUES (982, '121.69.105.22', 12, 1, '2018-03-05', NULL);
INSERT INTO `ipconfig` VALUES (983, '111.206.221.73', 1, 1, '2018-03-05', NULL);
INSERT INTO `ipconfig` VALUES (984, '111.206.198.108', 1, 1, '2018-03-05', NULL);
INSERT INTO `ipconfig` VALUES (985, '111.206.198.41', 1, 1, '2018-03-06', NULL);
INSERT INTO `ipconfig` VALUES (986, '111.206.198.39', 1, 1, '2018-03-06', NULL);
INSERT INTO `ipconfig` VALUES (987, '121.69.105.22', 18, 1, '2018-03-06', NULL);
INSERT INTO `ipconfig` VALUES (988, '203.208.60.130', 1, 1, '2018-03-06', NULL);
INSERT INTO `ipconfig` VALUES (989, '111.206.221.85', 1, 1, '2018-03-06', NULL);
INSERT INTO `ipconfig` VALUES (990, '111.206.198.45', 1, 1, '2018-03-06', NULL);
INSERT INTO `ipconfig` VALUES (991, '111.206.221.14', 1, 1, '2018-03-07', NULL);
INSERT INTO `ipconfig` VALUES (992, '111.206.198.72', 1, 1, '2018-03-07', NULL);
INSERT INTO `ipconfig` VALUES (993, '121.69.105.22', 2, 1, '2018-03-07', NULL);
INSERT INTO `ipconfig` VALUES (994, '111.206.221.38', 1, 1, '2018-03-07', NULL);
INSERT INTO `ipconfig` VALUES (995, '203.208.60.130', 1, 1, '2018-03-07', NULL);
INSERT INTO `ipconfig` VALUES (996, '203.208.60.130', 1, 1, '2018-03-08', NULL);
INSERT INTO `ipconfig` VALUES (997, '111.206.198.5', 1, 1, '2018-03-08', NULL);
INSERT INTO `ipconfig` VALUES (998, '121.69.105.22', 3, 1, '2018-03-08', NULL);
INSERT INTO `ipconfig` VALUES (999, '111.206.198.120', 1, 1, '2018-03-08', NULL);
INSERT INTO `ipconfig` VALUES (1000, '111.206.221.110', 1, 1, '2018-03-08', NULL);
INSERT INTO `ipconfig` VALUES (1001, '111.206.198.21', 1, 1, '2018-03-09', NULL);
INSERT INTO `ipconfig` VALUES (1002, '111.206.198.98', 1, 1, '2018-03-09', NULL);
INSERT INTO `ipconfig` VALUES (1003, '111.206.221.70', 1, 1, '2018-03-09', NULL);
INSERT INTO `ipconfig` VALUES (1004, '182.55.99.64', 1, 1, '2018-03-09', NULL);
INSERT INTO `ipconfig` VALUES (1005, '111.206.198.19', 1, 1, '2018-03-10', NULL);
INSERT INTO `ipconfig` VALUES (1006, '111.206.221.25', 1, 1, '2018-03-10', NULL);
INSERT INTO `ipconfig` VALUES (1007, '111.206.221.21', 1, 1, '2018-03-10', NULL);
INSERT INTO `ipconfig` VALUES (1008, '111.206.198.12', 1, 1, '2018-03-11', NULL);
INSERT INTO `ipconfig` VALUES (1009, '111.206.221.87', 1, 1, '2018-03-11', NULL);
INSERT INTO `ipconfig` VALUES (1010, '111.206.198.126', 1, 1, '2018-03-11', NULL);
INSERT INTO `ipconfig` VALUES (1011, '111.206.221.32', 1, 1, '2018-03-12', NULL);
INSERT INTO `ipconfig` VALUES (1012, '111.206.221.46', 1, 1, '2018-03-12', NULL);
INSERT INTO `ipconfig` VALUES (1013, '111.206.221.4', 1, 1, '2018-03-12', NULL);
INSERT INTO `ipconfig` VALUES (1014, '111.206.221.99', 1, 1, '2018-03-13', NULL);
INSERT INTO `ipconfig` VALUES (1015, '111.206.198.37', 1, 1, '2018-03-13', NULL);
INSERT INTO `ipconfig` VALUES (1016, '111.206.198.22', 1, 1, '2018-03-13', NULL);
INSERT INTO `ipconfig` VALUES (1017, '111.206.198.83', 1, 1, '2018-03-14', NULL);
INSERT INTO `ipconfig` VALUES (1018, '111.206.198.79', 1, 1, '2018-03-14', NULL);
INSERT INTO `ipconfig` VALUES (1019, '111.206.221.4', 1, 1, '2018-03-14', NULL);
INSERT INTO `ipconfig` VALUES (1020, '123.122.64.124', 1, 1, '2018-03-14', NULL);
INSERT INTO `ipconfig` VALUES (1021, '66.249.69.60', 1, 1, '2018-03-14', NULL);
INSERT INTO `ipconfig` VALUES (1022, '111.206.198.75', 1, 1, '2018-03-15', NULL);
INSERT INTO `ipconfig` VALUES (1023, '111.206.198.5', 1, 1, '2018-03-15', NULL);
INSERT INTO `ipconfig` VALUES (1024, '111.206.221.37', 1, 1, '2018-03-16', NULL);
INSERT INTO `ipconfig` VALUES (1025, '111.206.198.116', 1, 1, '2018-03-16', NULL);
INSERT INTO `ipconfig` VALUES (1026, '111.206.198.26', 1, 1, '2018-03-16', NULL);
INSERT INTO `ipconfig` VALUES (1027, '114.246.103.194', 8, 1, '2018-03-16', NULL);
INSERT INTO `ipconfig` VALUES (1028, '111.206.198.125', 1, 1, '2018-03-17', NULL);
INSERT INTO `ipconfig` VALUES (1029, '111.206.198.78', 1, 1, '2018-03-17', NULL);
INSERT INTO `ipconfig` VALUES (1030, '114.246.103.194', 4, 1, '2018-03-17', NULL);
INSERT INTO `ipconfig` VALUES (1031, '111.206.221.30', 1, 1, '2018-03-17', NULL);
INSERT INTO `ipconfig` VALUES (1032, '223.74.232.16', 1, 1, '2018-03-19', NULL);
INSERT INTO `ipconfig` VALUES (1033, '66.70.182.111', 1, 1, '2018-03-19', NULL);
INSERT INTO `ipconfig` VALUES (1034, '121.69.105.22', 1, 1, '2018-03-19', NULL);
INSERT INTO `ipconfig` VALUES (1035, '203.208.60.132', 1, 1, '2018-03-19', NULL);
INSERT INTO `ipconfig` VALUES (1036, '111.206.198.30', 1, 1, '2018-03-19', NULL);
INSERT INTO `ipconfig` VALUES (1037, '114.246.103.194', 1, 1, '2018-03-19', NULL);
INSERT INTO `ipconfig` VALUES (1038, '111.206.198.68', 1, 1, '2018-03-20', NULL);
INSERT INTO `ipconfig` VALUES (1039, '121.69.105.22', 6, 1, '2018-03-20', NULL);
INSERT INTO `ipconfig` VALUES (1040, '111.206.198.31', 1, 1, '2018-03-20', NULL);
INSERT INTO `ipconfig` VALUES (1041, '111.206.198.37', 1, 1, '2018-03-20', NULL);
INSERT INTO `ipconfig` VALUES (1042, '114.246.103.194', 3, 1, '2018-03-20', NULL);
INSERT INTO `ipconfig` VALUES (1043, '111.206.221.115', 1, 1, '2018-03-21', NULL);
INSERT INTO `ipconfig` VALUES (1044, '121.69.105.22', 3, 1, '2018-03-21', NULL);
INSERT INTO `ipconfig` VALUES (1045, '111.206.221.98', 1, 1, '2018-03-21', NULL);
INSERT INTO `ipconfig` VALUES (1046, '203.208.60.129', 1, 1, '2018-03-21', NULL);
INSERT INTO `ipconfig` VALUES (1047, '111.206.221.6', 1, 1, '2018-03-21', NULL);
INSERT INTO `ipconfig` VALUES (1048, '111.206.198.5', 1, 1, '2018-03-22', NULL);
INSERT INTO `ipconfig` VALUES (1049, '111.206.221.37', 1, 1, '2018-03-22', NULL);
INSERT INTO `ipconfig` VALUES (1050, '114.240.243.136', 1, 1, '2018-03-22', NULL);
INSERT INTO `ipconfig` VALUES (1051, '121.69.105.22', 2, 1, '2018-03-22', NULL);
INSERT INTO `ipconfig` VALUES (1052, '111.206.221.69', 1, 1, '2018-03-22', NULL);
INSERT INTO `ipconfig` VALUES (1053, '111.206.198.45', 1, 1, '2018-03-23', NULL);
INSERT INTO `ipconfig` VALUES (1054, '66.249.73.216', 3, 1, '2018-03-23', NULL);
INSERT INTO `ipconfig` VALUES (1055, '111.206.221.50', 1, 1, '2018-03-23', NULL);
INSERT INTO `ipconfig` VALUES (1056, '66.249.73.217', 2, 1, '2018-03-23', NULL);
INSERT INTO `ipconfig` VALUES (1057, '121.69.105.22', 7, 1, '2018-03-23', NULL);
INSERT INTO `ipconfig` VALUES (1058, '121.69.78.186', 1, 1, '2018-03-23', NULL);
INSERT INTO `ipconfig` VALUES (1059, '111.206.198.13', 1, 1, '2018-03-23', NULL);
INSERT INTO `ipconfig` VALUES (1060, '111.206.221.70', 1, 1, '2018-03-24', NULL);
INSERT INTO `ipconfig` VALUES (1061, '66.249.73.90', 1, 1, '2018-03-24', NULL);
INSERT INTO `ipconfig` VALUES (1062, '111.206.221.114', 1, 1, '2018-03-24', NULL);
INSERT INTO `ipconfig` VALUES (1063, '111.206.221.43', 1, 1, '2018-03-24', NULL);
INSERT INTO `ipconfig` VALUES (1064, '111.206.221.50', 1, 1, '2018-03-25', NULL);
INSERT INTO `ipconfig` VALUES (1065, '111.206.198.101', 1, 1, '2018-03-25', NULL);
INSERT INTO `ipconfig` VALUES (1066, '111.206.198.39', 1, 1, '2018-03-25', NULL);
INSERT INTO `ipconfig` VALUES (1067, '111.206.221.105', 1, 1, '2018-03-26', NULL);
INSERT INTO `ipconfig` VALUES (1068, '121.69.105.22', 11, 1, '2018-03-26', NULL);
INSERT INTO `ipconfig` VALUES (1069, '111.206.198.107', 1, 1, '2018-03-26', NULL);
INSERT INTO `ipconfig` VALUES (1070, '111.206.221.8', 1, 1, '2018-03-26', NULL);
INSERT INTO `ipconfig` VALUES (1071, '114.246.102.66', 1, 1, '2018-03-26', NULL);
INSERT INTO `ipconfig` VALUES (1072, '111.206.221.14', 1, 1, '2018-03-27', NULL);
INSERT INTO `ipconfig` VALUES (1073, '111.206.198.96', 1, 1, '2018-03-27', NULL);
INSERT INTO `ipconfig` VALUES (1074, '111.206.221.88', 1, 1, '2018-03-27', NULL);
INSERT INTO `ipconfig` VALUES (1075, '121.69.105.22', 11, 1, '2018-03-27', NULL);
INSERT INTO `ipconfig` VALUES (1076, '111.206.198.34', 1, 1, '2018-03-27', NULL);
INSERT INTO `ipconfig` VALUES (1077, '111.206.198.39', 1, 1, '2018-03-27', NULL);
INSERT INTO `ipconfig` VALUES (1078, '114.246.102.66', 2, 1, '2018-03-27', NULL);
INSERT INTO `ipconfig` VALUES (1079, '111.206.198.47', 1, 1, '2018-03-28', NULL);
INSERT INTO `ipconfig` VALUES (1080, '111.206.221.76', 1, 1, '2018-03-28', NULL);
INSERT INTO `ipconfig` VALUES (1081, '111.206.198.105', 1, 1, '2018-03-28', NULL);
INSERT INTO `ipconfig` VALUES (1082, '121.69.105.22', 1, 1, '2018-03-28', NULL);
INSERT INTO `ipconfig` VALUES (1083, '66.249.69.58', 2, 1, '2018-03-28', NULL);
INSERT INTO `ipconfig` VALUES (1084, '111.206.221.2', 1, 1, '2018-03-28', NULL);
INSERT INTO `ipconfig` VALUES (1085, '111.206.221.105', 1, 1, '2018-03-29', NULL);
INSERT INTO `ipconfig` VALUES (1086, '111.206.198.108', 1, 1, '2018-03-29', NULL);
INSERT INTO `ipconfig` VALUES (1087, '121.69.105.22', 3, 1, '2018-03-29', NULL);
INSERT INTO `ipconfig` VALUES (1088, '220.181.108.118', 1, 1, '2018-03-29', NULL);
INSERT INTO `ipconfig` VALUES (1089, '111.206.198.24', 1, 1, '2018-03-29', NULL);
INSERT INTO `ipconfig` VALUES (1090, '111.206.221.49', 1, 1, '2018-03-30', NULL);
INSERT INTO `ipconfig` VALUES (1091, '66.249.64.220', 1, 1, '2018-03-30', NULL);
INSERT INTO `ipconfig` VALUES (1092, '111.206.221.18', 1, 1, '2018-03-30', NULL);
INSERT INTO `ipconfig` VALUES (1093, '121.69.105.22', 8, 1, '2018-03-30', NULL);
INSERT INTO `ipconfig` VALUES (1094, '111.206.221.87', 1, 1, '2018-03-30', NULL);
INSERT INTO `ipconfig` VALUES (1095, '111.198.24.208', 1, 1, '2018-03-30', NULL);
INSERT INTO `ipconfig` VALUES (1096, '111.206.221.91', 1, 1, '2018-03-30', NULL);
INSERT INTO `ipconfig` VALUES (1097, '111.206.221.84', 1, 1, '2018-03-30', NULL);
INSERT INTO `ipconfig` VALUES (1098, '111.206.221.37', 1, 1, '2018-03-31', NULL);
INSERT INTO `ipconfig` VALUES (1099, '111.206.221.24', 1, 1, '2018-03-31', NULL);
INSERT INTO `ipconfig` VALUES (1100, '111.206.198.83', 1, 1, '2018-03-31', NULL);
INSERT INTO `ipconfig` VALUES (1101, '111.206.198.105', 1, 1, '2018-03-31', NULL);
INSERT INTO `ipconfig` VALUES (1102, '222.131.157.204', 3, 1, '2018-03-31', NULL);
INSERT INTO `ipconfig` VALUES (1103, '111.206.198.125', 1, 1, '2018-04-01', NULL);
INSERT INTO `ipconfig` VALUES (1104, '111.206.198.48', 1, 1, '2018-04-01', NULL);
INSERT INTO `ipconfig` VALUES (1105, '111.206.198.71', 1, 1, '2018-04-01', NULL);
INSERT INTO `ipconfig` VALUES (1106, '111.206.198.106', 1, 1, '2018-04-01', NULL);
INSERT INTO `ipconfig` VALUES (1107, '111.206.198.55', 1, 1, '2018-04-01', NULL);
INSERT INTO `ipconfig` VALUES (1108, '111.206.221.91', 1, 1, '2018-04-01', NULL);
INSERT INTO `ipconfig` VALUES (1109, '111.206.198.34', 1, 1, '2018-04-01', NULL);
INSERT INTO `ipconfig` VALUES (1110, '111.206.198.25', 1, 1, '2018-04-01', NULL);
INSERT INTO `ipconfig` VALUES (1111, '111.206.221.22', 1, 1, '2018-04-01', NULL);
INSERT INTO `ipconfig` VALUES (1112, '111.206.221.78', 1, 1, '2018-04-01', NULL);
INSERT INTO `ipconfig` VALUES (1113, '111.206.198.8', 1, 1, '2018-04-01', NULL);
INSERT INTO `ipconfig` VALUES (1114, '111.206.221.73', 1, 1, '2018-04-02', NULL);
INSERT INTO `ipconfig` VALUES (1115, '111.206.198.29', 1, 1, '2018-04-02', NULL);
INSERT INTO `ipconfig` VALUES (1116, '111.206.198.117', 1, 1, '2018-04-02', NULL);
INSERT INTO `ipconfig` VALUES (1117, '121.69.105.22', 3, 1, '2018-04-02', NULL);
INSERT INTO `ipconfig` VALUES (1118, '111.206.198.108', 1, 1, '2018-04-02', NULL);
INSERT INTO `ipconfig` VALUES (1119, '111.206.221.7', 1, 1, '2018-04-02', NULL);
INSERT INTO `ipconfig` VALUES (1120, '111.206.198.9', 1, 1, '2018-04-02', NULL);
INSERT INTO `ipconfig` VALUES (1121, '111.206.198.35', 1, 1, '2018-04-02', NULL);
INSERT INTO `ipconfig` VALUES (1122, '111.206.221.107', 1, 1, '2018-04-02', NULL);
INSERT INTO `ipconfig` VALUES (1123, '111.206.221.41', 1, 1, '2018-04-02', NULL);
INSERT INTO `ipconfig` VALUES (1124, '222.212.185.217', 1, 1, '2018-04-02', NULL);
INSERT INTO `ipconfig` VALUES (1125, '111.206.221.14', 1, 1, '2018-04-02', NULL);
INSERT INTO `ipconfig` VALUES (1126, '111.206.198.44', 1, 1, '2018-04-02', NULL);
INSERT INTO `ipconfig` VALUES (1127, '111.206.198.87', 1, 1, '2018-04-02', NULL);
INSERT INTO `ipconfig` VALUES (1128, '111.206.198.44', 1, 1, '2018-04-03', NULL);
INSERT INTO `ipconfig` VALUES (1129, '111.206.198.93', 1, 1, '2018-04-03', NULL);
INSERT INTO `ipconfig` VALUES (1130, '111.206.221.113', 1, 1, '2018-04-03', NULL);
INSERT INTO `ipconfig` VALUES (1131, '111.206.198.85', 1, 1, '2018-04-03', NULL);
INSERT INTO `ipconfig` VALUES (1132, '220.181.108.91', 1, 1, '2018-04-03', NULL);
INSERT INTO `ipconfig` VALUES (1133, '111.206.221.97', 1, 1, '2018-04-03', NULL);
INSERT INTO `ipconfig` VALUES (1134, '121.69.105.22', 2, 1, '2018-04-03', NULL);
INSERT INTO `ipconfig` VALUES (1135, '111.206.221.103', 1, 1, '2018-04-03', NULL);
INSERT INTO `ipconfig` VALUES (1136, '111.206.221.18', 1, 1, '2018-04-03', NULL);
INSERT INTO `ipconfig` VALUES (1137, '111.206.198.89', 1, 1, '2018-04-03', NULL);
INSERT INTO `ipconfig` VALUES (1138, '111.206.198.108', 1, 1, '2018-04-03', NULL);
INSERT INTO `ipconfig` VALUES (1139, '111.206.221.6', 1, 1, '2018-04-03', NULL);
INSERT INTO `ipconfig` VALUES (1140, '124.200.189.34', 2, 1, '2018-04-03', NULL);
INSERT INTO `ipconfig` VALUES (1141, '111.206.221.7', 1, 1, '2018-04-04', NULL);
INSERT INTO `ipconfig` VALUES (1142, '111.206.198.76', 1, 1, '2018-04-04', NULL);
INSERT INTO `ipconfig` VALUES (1143, '111.206.198.121', 1, 1, '2018-04-04', NULL);
INSERT INTO `ipconfig` VALUES (1144, '111.206.198.48', 1, 1, '2018-04-04', NULL);
INSERT INTO `ipconfig` VALUES (1145, '111.206.198.77', 1, 1, '2018-04-04', NULL);
INSERT INTO `ipconfig` VALUES (1146, '203.208.60.133', 1, 1, '2018-04-04', NULL);
INSERT INTO `ipconfig` VALUES (1147, '203.208.60.129', 1, 1, '2018-04-04', NULL);
INSERT INTO `ipconfig` VALUES (1148, '124.200.189.34', 1, 1, '2018-04-04', NULL);
INSERT INTO `ipconfig` VALUES (1149, '121.69.105.22', 1, 1, '2018-04-04', NULL);
INSERT INTO `ipconfig` VALUES (1150, '111.206.221.98', 1, 1, '2018-04-04', NULL);
INSERT INTO `ipconfig` VALUES (1151, '111.206.198.74', 1, 1, '2018-04-04', NULL);
INSERT INTO `ipconfig` VALUES (1152, '111.206.221.90', 1, 1, '2018-04-05', NULL);
INSERT INTO `ipconfig` VALUES (1153, '111.206.198.83', 1, 1, '2018-04-05', NULL);
INSERT INTO `ipconfig` VALUES (1154, '111.206.221.38', 1, 1, '2018-04-05', NULL);
INSERT INTO `ipconfig` VALUES (1155, '111.206.221.76', 1, 1, '2018-04-05', NULL);
INSERT INTO `ipconfig` VALUES (1156, '111.206.198.92', 1, 1, '2018-04-05', NULL);
INSERT INTO `ipconfig` VALUES (1157, '111.206.198.94', 1, 1, '2018-04-05', NULL);
INSERT INTO `ipconfig` VALUES (1158, '222.131.157.204', 1, 1, '2018-04-05', NULL);
INSERT INTO `ipconfig` VALUES (1159, '111.206.221.70', 1, 1, '2018-04-06', NULL);
INSERT INTO `ipconfig` VALUES (1160, '111.206.221.80', 1, 1, '2018-04-06', NULL);
INSERT INTO `ipconfig` VALUES (1161, '111.206.221.92', 1, 1, '2018-04-06', NULL);
INSERT INTO `ipconfig` VALUES (1162, '111.206.221.12', 1, 1, '2018-04-06', NULL);
INSERT INTO `ipconfig` VALUES (1163, '111.206.221.28', 1, 1, '2018-04-06', NULL);
INSERT INTO `ipconfig` VALUES (1164, '111.206.198.51', 1, 1, '2018-04-06', NULL);
INSERT INTO `ipconfig` VALUES (1165, '111.206.198.70', 1, 1, '2018-04-07', NULL);
INSERT INTO `ipconfig` VALUES (1166, '111.206.221.80', 1, 1, '2018-04-07', NULL);
INSERT INTO `ipconfig` VALUES (1167, '111.206.198.132', 1, 1, '2018-04-07', NULL);
INSERT INTO `ipconfig` VALUES (1168, '111.206.221.99', 1, 1, '2018-04-07', NULL);
INSERT INTO `ipconfig` VALUES (1169, '111.206.198.86', 1, 1, '2018-04-07', NULL);
INSERT INTO `ipconfig` VALUES (1170, '111.206.221.76', 1, 1, '2018-04-07', NULL);
INSERT INTO `ipconfig` VALUES (1171, '111.206.221.102', 1, 1, '2018-04-07', NULL);
INSERT INTO `ipconfig` VALUES (1172, '111.206.198.9', 1, 1, '2018-04-07', NULL);
INSERT INTO `ipconfig` VALUES (1173, '111.206.198.32', 1, 1, '2018-04-08', NULL);
INSERT INTO `ipconfig` VALUES (1174, '111.206.221.103', 1, 1, '2018-04-08', NULL);
INSERT INTO `ipconfig` VALUES (1175, '220.181.108.91', 1, 1, '2018-04-08', NULL);
INSERT INTO `ipconfig` VALUES (1176, '111.206.221.67', 1, 1, '2018-04-08', NULL);
INSERT INTO `ipconfig` VALUES (1177, '111.206.221.82', 1, 1, '2018-04-08', NULL);
INSERT INTO `ipconfig` VALUES (1178, '124.200.189.34', 1, 1, '2018-04-08', NULL);
INSERT INTO `ipconfig` VALUES (1179, '111.206.221.36', 1, 1, '2018-04-08', NULL);
INSERT INTO `ipconfig` VALUES (1180, '111.206.198.36', 1, 1, '2018-04-08', NULL);
INSERT INTO `ipconfig` VALUES (1181, '111.206.221.114', 1, 1, '2018-04-09', NULL);
INSERT INTO `ipconfig` VALUES (1182, '111.206.198.122', 1, 1, '2018-04-09', NULL);
INSERT INTO `ipconfig` VALUES (1183, '111.206.221.32', 1, 1, '2018-04-09', NULL);
INSERT INTO `ipconfig` VALUES (1184, '111.206.198.6', 1, 1, '2018-04-09', NULL);
INSERT INTO `ipconfig` VALUES (1185, '124.200.189.34', 3, 1, '2018-04-09', NULL);
INSERT INTO `ipconfig` VALUES (1186, '121.69.105.22', 2, 1, '2018-04-09', NULL);
INSERT INTO `ipconfig` VALUES (1187, '111.206.198.19', 1, 1, '2018-04-09', NULL);
INSERT INTO `ipconfig` VALUES (1188, '111.206.198.50', 1, 1, '2018-04-09', NULL);
INSERT INTO `ipconfig` VALUES (1189, '111.206.221.100', 1, 1, '2018-04-10', NULL);
INSERT INTO `ipconfig` VALUES (1190, '111.206.221.51', 1, 1, '2018-04-10', NULL);
INSERT INTO `ipconfig` VALUES (1191, '121.69.105.22', 2, 1, '2018-04-10', NULL);
INSERT INTO `ipconfig` VALUES (1192, '111.206.221.105', 1, 1, '2018-04-10', NULL);
INSERT INTO `ipconfig` VALUES (1193, '111.206.221.74', 1, 1, '2018-04-10', NULL);
INSERT INTO `ipconfig` VALUES (1194, '111.206.198.91', 1, 1, '2018-04-10', NULL);
INSERT INTO `ipconfig` VALUES (1195, '111.206.221.95', 1, 1, '2018-04-10', NULL);
INSERT INTO `ipconfig` VALUES (1196, '111.206.221.100', 1, 1, '2018-04-11', NULL);
INSERT INTO `ipconfig` VALUES (1197, '111.206.221.80', 1, 1, '2018-04-11', NULL);
INSERT INTO `ipconfig` VALUES (1198, '111.206.221.32', 1, 1, '2018-04-11', NULL);
INSERT INTO `ipconfig` VALUES (1199, '111.206.198.31', 1, 1, '2018-04-11', NULL);
INSERT INTO `ipconfig` VALUES (1200, '121.69.105.22', 2, 1, '2018-04-11', NULL);
INSERT INTO `ipconfig` VALUES (1201, '66.249.75.218', 1, 1, '2018-04-11', NULL);
INSERT INTO `ipconfig` VALUES (1202, '111.206.221.23', 1, 1, '2018-04-11', NULL);
INSERT INTO `ipconfig` VALUES (1203, '111.206.198.10', 1, 1, '2018-04-11', NULL);
INSERT INTO `ipconfig` VALUES (1204, '66.249.69.124', 1, 1, '2018-04-11', NULL);
INSERT INTO `ipconfig` VALUES (1205, '45.116.9.78', 1, 1, '2018-04-11', NULL);
INSERT INTO `ipconfig` VALUES (1206, '111.206.198.93', 1, 1, '2018-04-12', NULL);
INSERT INTO `ipconfig` VALUES (1207, '111.206.198.43', 1, 1, '2018-04-12', NULL);
INSERT INTO `ipconfig` VALUES (1208, '111.206.221.106', 1, 1, '2018-04-12', NULL);
INSERT INTO `ipconfig` VALUES (1209, '111.206.198.70', 1, 1, '2018-04-12', NULL);
INSERT INTO `ipconfig` VALUES (1210, '111.206.221.46', 1, 1, '2018-04-12', NULL);
INSERT INTO `ipconfig` VALUES (1211, '111.206.198.33', 1, 1, '2018-04-12', NULL);
INSERT INTO `ipconfig` VALUES (1212, '111.206.198.50', 1, 1, '2018-04-13', NULL);
INSERT INTO `ipconfig` VALUES (1213, '111.206.198.16', 1, 1, '2018-04-13', NULL);
INSERT INTO `ipconfig` VALUES (1214, '111.206.221.44', 1, 1, '2018-04-13', NULL);
INSERT INTO `ipconfig` VALUES (1215, '111.206.198.82', 1, 1, '2018-04-13', NULL);
INSERT INTO `ipconfig` VALUES (1216, '121.69.105.22', 2, 1, '2018-04-13', NULL);
INSERT INTO `ipconfig` VALUES (1217, '111.206.198.4', 1, 1, '2018-04-13', NULL);
INSERT INTO `ipconfig` VALUES (1218, '111.206.221.49', 1, 1, '2018-04-13', NULL);
INSERT INTO `ipconfig` VALUES (1219, '111.206.221.15', 1, 1, '2018-04-13', NULL);
INSERT INTO `ipconfig` VALUES (1220, '111.206.198.31', 1, 1, '2018-04-13', NULL);
INSERT INTO `ipconfig` VALUES (1221, '111.206.198.86', 1, 1, '2018-04-14', NULL);
INSERT INTO `ipconfig` VALUES (1222, '111.206.198.22', 1, 1, '2018-04-14', NULL);
INSERT INTO `ipconfig` VALUES (1223, '66.249.70.30', 1, 1, '2018-04-14', NULL);
INSERT INTO `ipconfig` VALUES (1224, '111.206.221.31', 1, 1, '2018-04-14', NULL);
INSERT INTO `ipconfig` VALUES (1225, '111.206.221.46', 1, 1, '2018-04-14', NULL);
INSERT INTO `ipconfig` VALUES (1226, '121.69.105.22', 2, 1, '2018-04-14', NULL);
INSERT INTO `ipconfig` VALUES (1227, '124.200.189.34', 1, 1, '2018-04-14', NULL);
INSERT INTO `ipconfig` VALUES (1228, '111.206.221.12', 1, 1, '2018-04-14', NULL);
INSERT INTO `ipconfig` VALUES (1229, '111.206.198.89', 1, 1, '2018-04-14', NULL);
INSERT INTO `ipconfig` VALUES (1230, '111.206.221.97', 1, 1, '2018-04-15', NULL);
INSERT INTO `ipconfig` VALUES (1231, '111.206.221.30', 1, 1, '2018-04-15', NULL);
INSERT INTO `ipconfig` VALUES (1232, '220.181.108.175', 1, 1, '2018-04-15', NULL);
INSERT INTO `ipconfig` VALUES (1233, '111.206.221.70', 1, 1, '2018-04-15', NULL);
INSERT INTO `ipconfig` VALUES (1234, '111.206.198.6', 1, 1, '2018-04-15', NULL);
INSERT INTO `ipconfig` VALUES (1235, '222.131.154.112', 1, 1, '2018-04-15', NULL);
INSERT INTO `ipconfig` VALUES (1236, '111.206.198.116', 1, 1, '2018-04-15', NULL);
INSERT INTO `ipconfig` VALUES (1237, '111.206.221.8', 1, 1, '2018-04-15', NULL);
INSERT INTO `ipconfig` VALUES (1238, '111.206.221.13', 1, 1, '2018-04-16', NULL);
INSERT INTO `ipconfig` VALUES (1239, '111.206.221.95', 1, 1, '2018-04-16', NULL);
INSERT INTO `ipconfig` VALUES (1240, '111.206.221.10', 1, 1, '2018-04-16', NULL);
INSERT INTO `ipconfig` VALUES (1241, '111.206.221.4', 1, 1, '2018-04-16', NULL);
INSERT INTO `ipconfig` VALUES (1242, '124.200.189.34', 3, 1, '2018-04-16', NULL);
INSERT INTO `ipconfig` VALUES (1243, '111.206.198.15', 1, 1, '2018-04-16', NULL);
INSERT INTO `ipconfig` VALUES (1244, '111.206.221.27', 1, 1, '2018-04-16', NULL);
INSERT INTO `ipconfig` VALUES (1245, '0:0:0:0:0:0:0:1', 11, 1, '2018-05-21', NULL);
INSERT INTO `ipconfig` VALUES (1246, '0:0:0:0:0:0:0:1', 1, 1, '2018-05-22', NULL);
INSERT INTO `ipconfig` VALUES (1247, '0:0:0:0:0:0:0:1', 10, 1, '2018-05-23', NULL);
INSERT INTO `ipconfig` VALUES (1248, '0:0:0:0:0:0:0:1', 3, 1, '2018-05-24', NULL);
INSERT INTO `ipconfig` VALUES (1249, '0:0:0:0:0:0:0:1', 50, 1, '2018-05-25', NULL);
INSERT INTO `ipconfig` VALUES (1250, '0:0:0:0:0:0:0:1', 8, 1, '2018-05-28', NULL);
INSERT INTO `ipconfig` VALUES (1251, '0:0:0:0:0:0:0:1', 2, 1, '2018-05-29', NULL);
INSERT INTO `ipconfig` VALUES (1252, '0:0:0:0:0:0:0:1', 11, 1, '2018-05-31', NULL);
INSERT INTO `ipconfig` VALUES (1253, '124.200.189.34', 1, 1, '2018-05-31', NULL);
INSERT INTO `ipconfig` VALUES (1254, '114.246.100.124', 13, 1, '2018-05-31', NULL);
INSERT INTO `ipconfig` VALUES (1255, '220.181.132.200', 1, 1, '2018-05-31', NULL);
INSERT INTO `ipconfig` VALUES (1256, '101.199.112.54', 1, 1, '2018-05-31', NULL);
INSERT INTO `ipconfig` VALUES (1257, '101.199.112.49', 1, 1, '2018-06-01', NULL);
INSERT INTO `ipconfig` VALUES (1258, '66.249.64.218', 2, 1, '2018-06-01', NULL);
INSERT INTO `ipconfig` VALUES (1259, '121.69.105.22', 12, 1, '2018-06-01', NULL);
INSERT INTO `ipconfig` VALUES (1260, '101.199.110.112', 1, 1, '2018-06-01', NULL);
INSERT INTO `ipconfig` VALUES (1261, '114.246.100.124', 9, 1, '2018-06-02', NULL);
INSERT INTO `ipconfig` VALUES (1262, '111.206.198.21', 1, 1, '2018-06-02', NULL);
INSERT INTO `ipconfig` VALUES (1263, '111.206.198.41', 1, 1, '2018-06-02', NULL);
INSERT INTO `ipconfig` VALUES (1264, '203.208.60.224', 1, 1, '2018-06-02', NULL);
INSERT INTO `ipconfig` VALUES (1265, '203.208.60.227', 1, 1, '2018-06-04', NULL);
INSERT INTO `ipconfig` VALUES (1266, '203.208.60.226', 1, 1, '2018-06-04', NULL);
INSERT INTO `ipconfig` VALUES (1267, '124.200.189.34', 4, 1, '2018-06-04', NULL);
INSERT INTO `ipconfig` VALUES (1268, '203.208.60.223', 1, 1, '2018-06-05', NULL);
INSERT INTO `ipconfig` VALUES (1269, '203.208.60.224', 2, 1, '2018-06-05', NULL);
INSERT INTO `ipconfig` VALUES (1270, '124.200.189.34', 2, 1, '2018-06-05', NULL);
INSERT INTO `ipconfig` VALUES (1271, '66.249.66.19', 1, 1, '2018-06-06', NULL);
INSERT INTO `ipconfig` VALUES (1272, '203.208.60.227', 1, 1, '2018-06-06', NULL);
INSERT INTO `ipconfig` VALUES (1273, '124.200.189.34', 9, 1, '2018-06-06', NULL);
INSERT INTO `ipconfig` VALUES (1274, '101.199.112.55', 1, 1, '2018-06-06', NULL);
INSERT INTO `ipconfig` VALUES (1275, '180.163.220.95', 1, 1, '2018-06-06', NULL);
INSERT INTO `ipconfig` VALUES (1276, '101.199.110.115', 1, 1, '2018-06-06', NULL);
INSERT INTO `ipconfig` VALUES (1277, '111.206.198.22', 1, 1, '2018-06-07', NULL);
INSERT INTO `ipconfig` VALUES (1278, '111.206.198.101', 1, 1, '2018-06-07', NULL);
INSERT INTO `ipconfig` VALUES (1279, '203.208.60.227', 4, 1, '2018-06-07', NULL);
INSERT INTO `ipconfig` VALUES (1280, '203.208.60.223', 6, 1, '2018-06-07', NULL);
INSERT INTO `ipconfig` VALUES (1281, '203.208.60.225', 4, 1, '2018-06-07', NULL);
INSERT INTO `ipconfig` VALUES (1282, '203.208.60.226', 1, 1, '2018-06-07', NULL);
INSERT INTO `ipconfig` VALUES (1283, '121.69.105.22', 2, 1, '2018-06-07', NULL);
INSERT INTO `ipconfig` VALUES (1284, '203.208.60.224', 2, 1, '2018-06-07', NULL);
INSERT INTO `ipconfig` VALUES (1285, '111.206.221.45', 1, 1, '2018-06-08', NULL);
INSERT INTO `ipconfig` VALUES (1286, '111.206.221.96', 1, 1, '2018-06-08', NULL);
INSERT INTO `ipconfig` VALUES (1287, '121.69.105.22', 6, 1, '2018-06-08', NULL);
INSERT INTO `ipconfig` VALUES (1288, '203.208.60.223', 1, 1, '2018-06-08', NULL);
INSERT INTO `ipconfig` VALUES (1289, '101.199.110.107', 1, 1, '2018-06-09', NULL);
INSERT INTO `ipconfig` VALUES (1290, '121.69.105.22', 2, 1, '2018-06-11', NULL);
INSERT INTO `ipconfig` VALUES (1291, '203.208.60.226', 2, 1, '2018-06-11', NULL);
INSERT INTO `ipconfig` VALUES (1292, '124.200.189.34', 4, 1, '2018-06-11', NULL);
INSERT INTO `ipconfig` VALUES (1293, '203.208.60.227', 1, 1, '2018-06-11', NULL);
INSERT INTO `ipconfig` VALUES (1294, '124.200.189.34', 5, 1, '2018-06-12', NULL);
INSERT INTO `ipconfig` VALUES (1295, '101.199.110.114', 1, 1, '2018-06-12', NULL);
INSERT INTO `ipconfig` VALUES (1296, '121.69.105.22', 1, 1, '2018-06-12', NULL);
INSERT INTO `ipconfig` VALUES (1297, '111.206.221.31', 1, 1, '2018-06-13', NULL);
INSERT INTO `ipconfig` VALUES (1298, '111.206.198.121', 1, 1, '2018-06-13', NULL);
INSERT INTO `ipconfig` VALUES (1299, '124.200.189.34', 7, 1, '2018-06-13', NULL);
INSERT INTO `ipconfig` VALUES (1300, '121.69.105.22', 2, 1, '2018-06-14', NULL);
INSERT INTO `ipconfig` VALUES (1301, '203.208.60.225', 1, 1, '2018-06-14', NULL);
INSERT INTO `ipconfig` VALUES (1302, '203.208.60.224', 1, 1, '2018-06-15', NULL);
INSERT INTO `ipconfig` VALUES (1303, '124.200.189.34', 3, 1, '2018-06-15', NULL);
INSERT INTO `ipconfig` VALUES (1304, '111.206.221.41', 1, 1, '2018-06-16', NULL);
INSERT INTO `ipconfig` VALUES (1305, '111.206.198.26', 1, 1, '2018-06-16', NULL);
INSERT INTO `ipconfig` VALUES (1306, '114.252.62.253', 2, 1, '2018-06-16', NULL);
INSERT INTO `ipconfig` VALUES (1307, '111.206.198.10', 1, 1, '2018-06-19', NULL);
INSERT INTO `ipconfig` VALUES (1308, '111.206.221.18', 1, 1, '2018-06-19', NULL);
INSERT INTO `ipconfig` VALUES (1309, '121.69.105.22', 2, 1, '2018-06-19', NULL);
INSERT INTO `ipconfig` VALUES (1310, '101.199.110.112', 1, 1, '2018-06-19', NULL);
INSERT INTO `ipconfig` VALUES (1311, '121.69.105.22', 6, 1, '2018-06-20', NULL);
INSERT INTO `ipconfig` VALUES (1312, '203.208.60.223', 1, 1, '2018-06-20', NULL);
INSERT INTO `ipconfig` VALUES (1313, '121.69.105.22', 3, 1, '2018-06-21', NULL);
INSERT INTO `ipconfig` VALUES (1314, '111.206.198.106', 1, 1, '2018-06-22', NULL);
INSERT INTO `ipconfig` VALUES (1315, '111.206.221.105', 1, 1, '2018-06-22', NULL);
INSERT INTO `ipconfig` VALUES (1316, '203.208.60.224', 1, 1, '2018-06-22', NULL);
INSERT INTO `ipconfig` VALUES (1317, '121.69.105.22', 1, 1, '2018-06-22', NULL);
INSERT INTO `ipconfig` VALUES (1318, '203.208.60.226', 1, 1, '2018-06-22', NULL);
INSERT INTO `ipconfig` VALUES (1319, '123.122.68.11', 1, 1, '2018-06-23', NULL);
INSERT INTO `ipconfig` VALUES (1320, '123.122.68.11', 17, 1, '2018-06-24', NULL);
INSERT INTO `ipconfig` VALUES (1321, '111.172.207.35', 1, 1, '2018-06-24', NULL);
INSERT INTO `ipconfig` VALUES (1322, '111.172.207.159', 2, 1, '2018-06-24', NULL);
INSERT INTO `ipconfig` VALUES (1323, '59.174.248.134', 1, 1, '2018-06-24', NULL);
INSERT INTO `ipconfig` VALUES (1324, '111.206.198.17', 1, 1, '2018-06-25', NULL);
INSERT INTO `ipconfig` VALUES (1325, '111.206.221.111', 1, 1, '2018-06-25', NULL);
INSERT INTO `ipconfig` VALUES (1326, '121.69.105.22', 1, 1, '2018-06-25', NULL);
INSERT INTO `ipconfig` VALUES (1327, '124.200.189.34', 7, 1, '2018-06-25', NULL);
INSERT INTO `ipconfig` VALUES (1328, '123.122.68.11', 1, 1, '2018-06-25', NULL);
INSERT INTO `ipconfig` VALUES (1329, '203.208.60.226', 1, 1, '2018-06-26', NULL);
INSERT INTO `ipconfig` VALUES (1330, '123.122.68.11', 4, 1, '2018-06-26', NULL);
INSERT INTO `ipconfig` VALUES (1331, '121.69.105.22', 4, 1, '2018-06-27', NULL);
INSERT INTO `ipconfig` VALUES (1332, '111.206.198.97', 1, 1, '2018-06-28', NULL);
INSERT INTO `ipconfig` VALUES (1333, '111.206.198.77', 1, 1, '2018-06-28', NULL);
INSERT INTO `ipconfig` VALUES (1334, '124.200.189.34', 1, 1, '2018-06-28', NULL);
INSERT INTO `ipconfig` VALUES (1335, '121.69.105.22', 6, 1, '2018-06-29', NULL);
INSERT INTO `ipconfig` VALUES (1336, '203.208.60.223', 1, 1, '2018-06-29', NULL);
INSERT INTO `ipconfig` VALUES (1337, '203.208.60.224', 2, 1, '2018-06-29', NULL);
INSERT INTO `ipconfig` VALUES (1338, '203.208.60.226', 1, 1, '2018-06-29', NULL);
INSERT INTO `ipconfig` VALUES (1339, '54.191.228.110', 1, 1, '2018-06-29', NULL);
INSERT INTO `ipconfig` VALUES (1340, '220.181.132.200', 1, 1, '2018-06-30', NULL);
INSERT INTO `ipconfig` VALUES (1341, '123.118.168.190', 5, 1, '2018-07-01', NULL);
INSERT INTO `ipconfig` VALUES (1342, '111.206.198.51', 1, 1, '2018-07-01', NULL);
INSERT INTO `ipconfig` VALUES (1343, '111.206.198.73', 1, 1, '2018-07-01', NULL);
INSERT INTO `ipconfig` VALUES (1344, '121.69.105.22', 2, 1, '2018-07-02', NULL);
INSERT INTO `ipconfig` VALUES (1345, '124.200.189.34', 9, 1, '2018-07-02', NULL);
INSERT INTO `ipconfig` VALUES (1346, '114.252.60.104', 1, 1, '2018-07-02', NULL);
INSERT INTO `ipconfig` VALUES (1347, '203.93.121.166', 1, 1, '2018-07-03', NULL);
INSERT INTO `ipconfig` VALUES (1348, '124.200.189.34', 5, 1, '2018-07-03', NULL);
INSERT INTO `ipconfig` VALUES (1349, '124.200.189.34', 5, 1, '2018-07-05', NULL);
INSERT INTO `ipconfig` VALUES (1350, '121.69.105.22', 1, 1, '2018-07-05', NULL);
INSERT INTO `ipconfig` VALUES (1351, '111.206.198.21', 1, 1, '2018-07-05', NULL);
INSERT INTO `ipconfig` VALUES (1352, '203.208.60.226', 1, 1, '2018-07-05', NULL);
INSERT INTO `ipconfig` VALUES (1353, '101.199.108.120', 1, 1, '2018-07-06', NULL);
INSERT INTO `ipconfig` VALUES (1354, '111.206.198.33', 1, 1, '2018-07-06', NULL);
INSERT INTO `ipconfig` VALUES (1355, '203.208.60.224', 1, 1, '2018-07-06', NULL);
INSERT INTO `ipconfig` VALUES (1356, '111.206.221.106', 1, 1, '2018-07-06', NULL);
INSERT INTO `ipconfig` VALUES (1357, '121.69.105.22', 2, 1, '2018-07-06', NULL);
INSERT INTO `ipconfig` VALUES (1358, '203.208.60.223', 2, 1, '2018-07-06', NULL);
INSERT INTO `ipconfig` VALUES (1359, '111.206.221.68', 1, 1, '2018-07-06', NULL);
INSERT INTO `ipconfig` VALUES (1360, '111.206.198.99', 1, 1, '2018-07-07', NULL);
INSERT INTO `ipconfig` VALUES (1361, '111.206.221.82', 1, 1, '2018-07-07', NULL);
INSERT INTO `ipconfig` VALUES (1362, '124.200.189.34', 1, 1, '2018-07-09', NULL);
INSERT INTO `ipconfig` VALUES (1363, '124.200.189.34', 2, 1, '2018-07-10', NULL);
INSERT INTO `ipconfig` VALUES (1364, '203.208.60.226', 3, 1, '2018-07-12', NULL);
INSERT INTO `ipconfig` VALUES (1365, '121.69.105.22', 2, 1, '2018-07-12', NULL);
INSERT INTO `ipconfig` VALUES (1366, '70.42.131.170', 1, 1, '2018-07-13', NULL);
INSERT INTO `ipconfig` VALUES (1367, '111.206.198.90', 1, 1, '2018-07-13', NULL);
INSERT INTO `ipconfig` VALUES (1368, '203.208.60.223', 1, 1, '2018-07-13', NULL);
INSERT INTO `ipconfig` VALUES (1369, '203.208.60.225', 1, 1, '2018-07-13', NULL);
INSERT INTO `ipconfig` VALUES (1370, '0:0:0:0:0:0:0:1', 2, 1, '2018-07-25', NULL);
INSERT INTO `ipconfig` VALUES (1371, '127.0.0.1', 2, 1, '2018-07-26', NULL);
INSERT INTO `ipconfig` VALUES (1372, '0:0:0:0:0:0:0:1', 4, 1, '2018-07-28', NULL);
INSERT INTO `ipconfig` VALUES (1373, '0:0:0:0:0:0:0:1', 1, 1, '2018-07-31', NULL);
COMMIT;

-- ----------------------------
-- Table structure for level_db
-- ----------------------------
DROP TABLE IF EXISTS `level_db`;
CREATE TABLE `level_db` (
  `LV_ID` int(2) NOT NULL COMMENT '主键',
  `LV_NAME` varchar(100) NOT NULL COMMENT '级别名称',
  `STATE` varchar(10) NOT NULL DEFAULT '1' COMMENT '级别状态',
  PRIMARY KEY (`LV_ID`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

-- ----------------------------
-- Records of level_db
-- ----------------------------
BEGIN;
INSERT INTO `level_db` VALUES (0, '管理员', '1');
INSERT INTO `level_db` VALUES (1, '管理员助理', '1');
COMMIT;

-- ----------------------------
-- Table structure for logcontent
-- ----------------------------
DROP TABLE IF EXISTS `logcontent`;
CREATE TABLE `logcontent` (
  `id` varchar(32) NOT NULL COMMENT '主键',
  `recommend` tinyint(1) NOT NULL DEFAULT '-1' COMMENT '1推荐-1不推荐',
  `typeId` varchar(32) NOT NULL COMMENT '类型',
  `title` varchar(50) NOT NULL COMMENT '标题',
  `tag` varchar(200) NOT NULL DEFAULT '',
  `summary` varchar(1000) NOT NULL COMMENT '摘要',
  `body` longtext NOT NULL COMMENT '正文',
  `state` tinyint(1) NOT NULL DEFAULT '1' COMMENT '状态',
  `userId` varchar(100) NOT NULL COMMENT '作者',
  `click` int(10) NOT NULL DEFAULT '0' COMMENT '点击量',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '时间',
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`id`),
  KEY `typeid` (`typeId`),
  KEY `userid` (`userId`),
  CONSTRAINT `typeid` FOREIGN KEY (`typeId`) REFERENCES `logtype` (`id`),
  CONSTRAINT `userid` FOREIGN KEY (`userId`) REFERENCES `loguser` (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

-- ----------------------------
-- Records of logcontent
-- ----------------------------
BEGIN;
INSERT INTO `logcontent` VALUES ('02968828189e475a8ec0ef541b3f9ca7', 1, 'e5e23ac742ca41a0a7e6f575fc710f4e', 'Linux命令记录', 'linux,命令', '自己用过的Linux命令', '<pre>\n<code class=\"language-bash\">$0 		文件名（包括路径）\n$1    ${10}	第几个参数\n$#		命令行参数总个数\n$*		显示所有参数(等同于$1$2$3)\n$@		显示所有参数(等同于$1 $2 $3)\nshift	将参数指向向前一位(shift$2就是$1)</code></pre>\n\n<p>#用于把名称和路径分开</p>\n\n<pre>\n<code>dirname $0        路径\nbasename $0        名称</code></pre>\n\n<p>#进程状态变量</p>\n\n<pre>\n<code>$?		0为成功非0失败\n$$		获取当前shell的进程号</code></pre>\n\n<p>#查看进程</p>\n\n<pre>\n<code>ps -ef|grep 运行文件名称</code></pre>\n\n<p>#字符串处理</p>\n\n<pre>\n<code>str=\"string\"\n${str}		结果string\n${str:2}		结果ring(第二个开始取不包括第2个)\n${str:2:2}		结果ri(第二个开始取不包括第2个,取2个)\n${str}|cut -c 3-4	结果ri(第二个开始取不包括第2个,取2个)\n\n${str#str}		结果ing（开头开始删除最短匹配的字符串）\n${str##str}		结果ing（开头开始删除最长匹配的字符串）\n\n${str%ing}		结果str（结尾开始删除最短匹配的字符串）\n${str%%ing}		结果str（结尾开始删除最长匹配的字符串）\n\n${str/ing/my}	结果strmy（替换）\n${str/#ing/my}	结果strmy（从开头开始替换）\n${str/%ing/my}	结果strmy（从结尾开始替换）\n\n${#str}				结果为变量的长度5(推荐)\n${str}|wc -m		结果为变量的长度6（算入$）\nexpr length \"$str\"	结果为变量的长度5</code></pre>\n\n<p>#匹配行数</p>\n\n<pre>\n<code>wc -l 文件名					结果为文件行数</code></pre>\n\n<p>#重命名</p>\n\n<pre>\n<code>rename:用字符串替换的方式批量改变文件/文件名。\n?  可替代单个字符\n*  可替代多个字符\n\nrename 被替换词 替换词 文件名\n\neg:\nrename es se test1    test 中es字符串替换为se\nrename es se test?    test+一位其他字符 中es字符串替换为se\nrename es se test??    test+两位其他字符 中es字符串替换为se\n...\nrename es se test*    test+多位（包括0位）其他字符 中es字符串替换为se</code></pre>\n\n<p>#批量创建图片</p>\n\n<pre>\n<code>name=cuijianpeng\nfor f in 1 2 3 4 5 6 7 8 9 10\ndo\ntouch ${name}${f}\".jpg\"\ndone</code></pre>\n\n<p>#result=${value-word}格式（变量功能可以解决空字符串的问题，确保变量有值）</p>\n\n<pre>\n<code>result=${oldgirl-UNSET}\necho $oldgirl        没有值\necho $result        值为UNSET\n如果oldgirl没有值result结果为UNSET，有值的话结果为值的内容</code></pre>\n\n<p>#unset&nbsp;变量名</p>\n\n<pre>\n<code>取消变量的值</code></pre>\n\n<p>#result=${value=word}格式（变量功能可以解决空字符串的问题，确保变量有值）</p>\n\n<pre>\n<code class=\"language-bash\">result=${oldgirl=UNSET}\necho $oldgirl        值为UNSET\necho $result        值为UNSET\n如果oldgirl没有值oldgirl和result结果均为UNSET，有值的话结果为值的内容</code></pre>\n\n<p><br />\n#${value?&quot;not defined&quot;}格式（如果变量未定义则退出程序）</p>\n\n<pre>\n<code>如果变量值存在，返回变量的值，如果value未定义则显示-bash：value：not defined并退出</code></pre>\n\n<p>#result=${value+1}格式（测试变量是否存在）<br />\n&nbsp;&nbsp; &nbsp;测试变量是否存在</p>\n\n<p><br />\n#以&quot; &quot;为分割从1数到3<br />\n&nbsp;&nbsp; &nbsp;seq -s &quot; &quot; 3</p>\n\n<p><br />\n#测试函数的执行速度<br />\n&nbsp;&nbsp; &nbsp;time for i in $(seq 11111);do count=${#string};done;</p>\n\n<p><br />\n#资料<br />\n&nbsp;&nbsp; &nbsp;man bash命令<br />\n&nbsp;&nbsp; &nbsp;搜索&nbsp;&nbsp; &nbsp;/word<br />\n&nbsp;&nbsp; &nbsp;www.cnblogs.com/chengmo/archive/2010/10/02/1841355.html<br />\n&nbsp;&nbsp; &nbsp;less /etc/init.d/functions<br />\n&nbsp;&nbsp; &nbsp;less /etc/rc.sysinit<br />\n&nbsp;&nbsp; &nbsp;less /etc/init.d/nfs<br />\n&nbsp;&nbsp; &nbsp;less /etc/init.d/portmap<br />\n&nbsp;&nbsp; &nbsp;less /etc/init.d/httpd<br />\n&nbsp;&nbsp; &nbsp;<br />\n&nbsp;&nbsp; &nbsp;<br />\n#变量的数值计算<br />\n&nbsp;&nbsp; &nbsp;(())&nbsp;&nbsp; &nbsp;let&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;expr&nbsp;&nbsp; &nbsp;bc&nbsp;&nbsp; &nbsp;$[]</p>\n\n<p>&nbsp;&nbsp; &nbsp;(())命令&nbsp;&nbsp; &nbsp;常用效率高，执行简单的整数计算<br />\n&nbsp;&nbsp; &nbsp;++&nbsp;&nbsp; &nbsp;--&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;自增自减<br />\n&nbsp;&nbsp; &nbsp;+&nbsp;&nbsp; &nbsp;-&nbsp;&nbsp; &nbsp;!&nbsp;&nbsp; &nbsp;~&nbsp;&nbsp; &nbsp;正号、负号;逻辑与位的取反<br />\n&nbsp;&nbsp; &nbsp;+&nbsp;&nbsp; &nbsp;-&nbsp;&nbsp; &nbsp;*&nbsp;&nbsp; &nbsp;/&nbsp;&nbsp; &nbsp;%&nbsp;&nbsp; &nbsp;加减乘除余<br />\n&nbsp;&nbsp; &nbsp;&lt;&nbsp;&nbsp; &nbsp;&lt;=&nbsp;&nbsp; &nbsp;&gt;&nbsp;&nbsp; &nbsp;&gt;=&nbsp;&nbsp; &nbsp;比较符号<br />\n&nbsp;&nbsp; &nbsp;==&nbsp;&nbsp; &nbsp;！=&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;相等与不相等<br />\n&nbsp;&nbsp; &nbsp;&lt;&lt;&nbsp;&nbsp; &nbsp;&gt;&gt;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;向左位移，向右位移<br />\n&nbsp;&nbsp; &nbsp;&amp;&nbsp;&nbsp; &nbsp;^&nbsp;&nbsp; &nbsp;|&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;位的AND/异或/或<br />\n&nbsp;&nbsp; &nbsp;&amp;&amp;&nbsp;&nbsp; &nbsp;||&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;逻辑的AND/OR<br />\n&nbsp;&nbsp; &nbsp;?:&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;条件表达式<br />\n&nbsp;&nbsp; &nbsp;=&nbsp;&nbsp; &nbsp;+=&nbsp;&nbsp; &nbsp;-=&nbsp;&nbsp; &nbsp;*=&nbsp;&nbsp; &nbsp;/=&nbsp;&nbsp; &nbsp;%=&nbsp;&nbsp; &nbsp;&amp;=&nbsp;&nbsp; &nbsp;^=&nbsp;&nbsp; &nbsp;&lt;&lt;=&nbsp;&nbsp; &nbsp;&gt;&gt;=&nbsp;&nbsp; &nbsp;|=&nbsp;&nbsp; &nbsp;赋值运算符</p>\n\n<p>&nbsp;&nbsp; &nbsp;举例：((a=1+2**3-4%3))&nbsp;&nbsp; &nbsp;相当于a=1+23-(4%3)<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;b=$((1+2**3-4%3))&nbsp;&nbsp; &nbsp;<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;echo $((1+2**3-4%3))</p>\n\n<p>&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;echo $((a+=1))<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;echo $((a++))<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;echo $((a--))<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;echo $((3&gt;2))&nbsp;&nbsp; &nbsp;返回值1为true/2为false</p>\n\n<p>&nbsp;&nbsp; &nbsp;<br />\n#let命令&nbsp;&nbsp; &nbsp;效率低于(())<br />\n&nbsp;&nbsp; &nbsp;举例：i=2<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;let i++;<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;echo $i<br />\n&nbsp;&nbsp; &nbsp;</p>\n\n<p>#expr命令&nbsp;&nbsp; &nbsp;一般用于整数，也可用于字符串<br />\n&nbsp;&nbsp; &nbsp;举例：&nbsp;&nbsp; &nbsp;expr 2 + 2<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;expr 2 - 2<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;expr 2 \\* 2&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;expr 2 / 2</p>\n\n<p><br />\n#bc命令&nbsp;&nbsp; &nbsp;可用于小数(可直接输入bc进入计算器默认保留计算数的最多位可通过scale=2；设置保留位数)<br />\n&nbsp;&nbsp; &nbsp;举例：i=1.1<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;echo 1.3+$1|bc<br />\n&nbsp;&nbsp; &nbsp;举例： seq -s &quot;+&quot; 100|bc<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;<br />\n&nbsp;&nbsp; &nbsp;$[]<br />\n&nbsp;&nbsp; &nbsp;举例：echo $[2+3]&nbsp;&nbsp; &nbsp;</p>\n\n<p><br />\n#read命令&nbsp;&nbsp; &nbsp;-p设置提示信息&nbsp;&nbsp; &nbsp;-t设置输入等待时间<br />\n&nbsp;&nbsp; &nbsp;举例：read -t 10 -p &quot;pls input two number:&quot; a1 a2</p>\n\n<p><br />\n#判断是不是整数&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;<br />\n&nbsp;&nbsp; &nbsp;举例：&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;read aa<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;(($aa+1))&amp;&gt;/dev/null<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;if [ $? -ne 0 ]<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;then<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;echo &quot;false&quot;<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;else<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;echo &quot;true&quot;<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;fi</p>\n\n<p><br />\n#条件测试<br />\n&nbsp;&nbsp; &nbsp;格式1：test&lt;测试表达式&gt;<br />\n&nbsp;&nbsp; &nbsp;格式2：[ &lt;测试表达式&gt; ]<br />\n&nbsp;&nbsp; &nbsp;格式3：[[ &lt;测试表达式&gt; ]]&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;<br />\n&nbsp;&nbsp; &nbsp;格式1等价格式2，格式3为扩展的test命令</p>\n\n<p>#举例：test -f sum.sh &amp;&amp; echo &quot;1&quot;||echo &quot;0&quot;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;<br />\n&nbsp;&nbsp; &nbsp;test ！ -f sum.sh &amp;&amp; echo &quot;1&quot;||echo &quot;0&quot;&nbsp;&nbsp; &nbsp;<br />\n&nbsp;&nbsp; &nbsp;[ -f sum.sh ]&amp;&amp; echo &quot;true&quot;||&quot;false&quot;<br />\n&nbsp;&nbsp; &nbsp;[ ！ -f sum.sh ]&amp;&amp; echo &quot;true&quot;||&quot;false&quot;&nbsp;&nbsp; &nbsp;<br />\n&nbsp;&nbsp; &nbsp;[ -f sum.sh -a -d myshell ]&amp;&amp; echo &quot;true&quot;||&quot;false&quot;&nbsp;&nbsp; &nbsp;<br />\n&nbsp;&nbsp; &nbsp;[[ -f sum.sh ]]&amp;&amp; echo &quot;true&quot;||&quot;false&quot;<br />\n&nbsp;&nbsp; &nbsp;[[ -f sum.sh &amp;&amp; -d myshell ]]&amp;&amp; echo &quot;true&quot;||&quot;false&quot;<br />\n&nbsp;&nbsp; &nbsp;<br />\n#常用文件测试操作符(文件路径为变量需要&quot;&quot;或者{}括起来)<br />\n&nbsp;&nbsp; &nbsp;-f 文件&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;若文件存在且为普通文件为真<br />\n&nbsp;&nbsp; &nbsp;-d 文件&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;若文件存在且为目录文件为真<br />\n&nbsp;&nbsp; &nbsp;-s 文件&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;若文件存在且不为空(文件大小非0)为真<br />\n&nbsp;&nbsp; &nbsp;-e 文件&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;若文件存在则真(区别-f)<br />\n&nbsp;&nbsp; &nbsp;-r 文件&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;若文件存在且可读则真<br />\n&nbsp;&nbsp; &nbsp;-w 文件&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;若文件存在且可写则真<br />\n&nbsp;&nbsp; &nbsp;-x 文件&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;若文件存在且可执行则真<br />\n&nbsp;&nbsp; &nbsp;-L 文件&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;若文件存在且为链接文件为真<br />\n&nbsp;&nbsp; &nbsp;f1 -nt f2 文件&nbsp;&nbsp; &nbsp;若文件f1比文件f2新则真<br />\n&nbsp;&nbsp; &nbsp;f1 -ot f2 文件&nbsp;&nbsp; &nbsp;若文件f1比文件f2旧则真<br />\n&nbsp;&nbsp; &nbsp;<br />\n#字符串测试操作符(注意：需要使用&quot;&quot;否则容易报错)<br />\n&nbsp;&nbsp; &nbsp;-z&quot;字符串&quot;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;若串长度为0则真<br />\n&nbsp;&nbsp; &nbsp;-n&quot;字符串&quot;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;若串长度不为0则真<br />\n&nbsp;&nbsp; &nbsp;&quot;串1&quot;=&quot;串2&quot;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;若串1等于串2则真<br />\n&nbsp;&nbsp; &nbsp;&quot;串1&quot;!=&quot;串2&quot;&nbsp;&nbsp; &nbsp;若串1不等于串2则真<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;<br />\n#整数二元比较操作符&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;<br />\n&nbsp;&nbsp; &nbsp;在[]中使用的比较符&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;在(())和[[]]中使用的比较符&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;说明&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;-eq&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;==&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;equals<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;-ne&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;!=&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;not equals&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;-gt&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&gt;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;greater than&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;-ge&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&gt;=&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;greater equals&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;-lt&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&lt;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;less than&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;-le&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&lt;=&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;less equals&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;<br />\n#逻辑操作符<br />\n&nbsp;&nbsp; &nbsp;在[]中使用的比较符&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;在[[]]中使用的比较符&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;说明&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;-a&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&amp;&amp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;与，两端都为真<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;-o&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;||&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;或，一个为真<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;！&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;！&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;非，相反</p>\n\n<p>#打印时单行使用echo，多行使用cat<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;echo &#39;aa&#39;<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;echo &#39;bb&#39;<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;echo &#39;cc&#39;<br />\n&nbsp;&nbsp; &nbsp;等同于<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;cat &lt;&lt;END<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;aa<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;bb<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;cc<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;END</p>\n\n<p>#grep&nbsp;&nbsp; &nbsp;文本搜索工具，它能使用正则表达式搜索文本，并把匹配的行打印出来<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;-c：只输出匹配行的计数。<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;-I：不区分大小写(只适用于单字符)。<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;-h：查询多文件时不显示文件名。<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;-l：查询多文件时只输出包含匹配字符的文件名。<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;-n：显示匹配行及行号。<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;-s：不显示不存在或无匹配文本的错误信息。<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;-v：取反,显示不包含匹配文本的所有行。<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;pattern正则表达式主要参数：<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;\\： 忽略正则表达式中特殊字符的原有含义。<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;^：匹配正则表达式的开始行。<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;$: 匹配正则表达式的结束行。<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;\\&lt;：从匹配正则表达 式的行开始。<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;\\&gt;：到匹配正则表达式的行结束。<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;[ ]：单个字符，如[A]即A符合要求 。<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;[ - ]：范围，如[A-Z]，即A、B、C一直到Z都符合要求 。<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;。：所有的单个字符。<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;* ：有字符，长度可以为0。</p>\n\n<p>#sleep&nbsp;&nbsp; &nbsp;睡眠（秒为单位）<br />\n#usleep 睡眠（微秒为单位）<br />\n#break&nbsp;&nbsp; &nbsp;跳出循环</p>\n\n<p>#远端执行任务最好后台执行进程<br />\n&nbsp;sh 文件名 &amp;&nbsp;&nbsp; &nbsp;执行时后台运行进程<br />\n&nbsp;fg&nbsp;&nbsp; &nbsp;任务编号（可省略）&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;将进程后台执行转为前台执行<br />\n&nbsp;&nbsp; &nbsp;<br />\n如果任务在前台启动<br />\n&nbsp;ctrl+Z&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;暂停当前任务<br />\n&nbsp;bg&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;切换任务到后台<br />\n&nbsp;jobs&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;查看后台执行的任务<br />\n&nbsp;&nbsp; &nbsp;<br />\n&nbsp;&nbsp; &nbsp;<br />\n#重定向&gt;及&gt;&gt;的区别(没有文件就创建文件)<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&gt;是覆盖，&gt;&gt;是追加</p>\n\n<p>#检查远程端口是否连通的方法<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;nmap 127.0.0.1 -p 80<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;wget -T 10 -q --spider http://127.0.0.1&gt;/dev/null echo $?&nbsp;&nbsp; &nbsp;(-T超时设置-q执行时不显示执行结果信息)<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;curl -I -s http://127.0.0.1 echo $?<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;telnet 127.0.0.1 80<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;nc -w 5 127.0.0.1 80&nbsp;&nbsp; &nbsp;(-w为超时时间)<br />\n&nbsp;&nbsp; &nbsp;<br />\n&nbsp;&nbsp; &nbsp;<br />\n#cat&nbsp;&nbsp; &nbsp;<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;1.一次显示整个文件：cat a.sh<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;2.从键盘创建一个文件：cat /dev/null &gt; a.sh<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;3.将几个文件合并为一个文件：$cat file1 file2 &gt; a.sh</p>\n\n<p>#head 默认是查看文件的前10行<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;head -n 15&nbsp;&nbsp; &nbsp;查看文件的前15行<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;head -c 15&nbsp;&nbsp; &nbsp;查看文件的前15字节<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;head -q&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;不显示结果</p>\n\n<p>#tail 默认是查看文件的后10行<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;tail -n 15&nbsp;&nbsp; &nbsp;查看文件的后15行<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;tail -c 15&nbsp;&nbsp; &nbsp;查看文件的后15字节<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;tail -q&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;不显示结果<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;tail -f&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;动态跟踪文件的后10行</p>\n\n<p>#cut 可以只显示某一列的内容<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;cut -d &quot; &quot; -f2&nbsp;&nbsp; &nbsp;按空格切割列，去第二列</p>\n\n<p>#echo 在显示器上显示一段文字，一般起到一个提示的作用。<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;参　　&nbsp;数：-n 不要在最后自动换行<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;-e 若字符串中出现以下字符，则特别加以处理，而不会将它当成一般<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;文字输出：<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; \\a 发出警告声；<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; \\b 删除前一个字符；<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; \\c 最后不加上换行符号；<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; \\f 换行但光标仍旧停留在原来的位置；<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; \\n 换行且光标移至行首；<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; \\r 光标移至行首，但不换行；<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; \\t 插入tab；<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; \\v 与\\f相同；<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; \\\\ 插入\\字符；<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; \\nnn 插入nnn（八进制）所代表的ASCII字符；<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&ndash;help 显示帮助<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&ndash;version 显示版本信息<br />\n&nbsp;&nbsp; &nbsp;<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;<br />\n#exit 退出当前shell<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;0表示成功<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;非0表示失败<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;2表示用法不当<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;127表示命令没有找到<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;126表示不是可执行的<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&gt;=128 信号产生 &nbsp; &nbsp;&nbsp;</p>\n\n<p>#case&nbsp;&nbsp; &nbsp;条件选择<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;case &quot;a&quot; in<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;1|10) echo &quot;1&quot;<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;;;<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;2) echo &quot;2&quot;<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;;;<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;3) echo &quot;3&quot;<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;;;<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;*) echo &quot;*&quot;<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;esac&nbsp;&nbsp; &nbsp;</p>\n\n<p>#给字符串加颜色<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;echo -e &quot;\\033[30m 黑色字 \\033[0m&quot;<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;echo -e &quot;\\033[31m 红色字 \\033[0m&quot;<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;echo -e &quot;\\033[32m 绿色字 \\033[0m&quot;<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;echo -e &quot;\\033[33m 黄色字 \\033[0m&quot;<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;echo -e &quot;\\033[34m 蓝色字 \\033[0m&quot;<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;echo -e &quot;\\033[35m 紫色字 \\033[0m&quot;<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;echo -e &quot;\\033[36m 天蓝字 \\033[0m&quot;<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;echo -e &quot;\\033[37m 白色字 \\033[0m&quot;</p>\n\n<p>#当型循环语法&nbsp;&nbsp; &nbsp;while条件句&nbsp;&nbsp; &nbsp;条件满足执行<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;while 条件<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;do<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;指令...<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;done</p>\n\n<p>#!/bin/sh<br />\nwhile true<br />\n&nbsp;&nbsp; &nbsp;do<br />\n&nbsp;&nbsp; &nbsp;uptime<br />\n&nbsp;&nbsp; &nbsp;sleep 2<br />\ndone<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;<br />\n#直到型循环语法&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;<br />\n&nbsp;&nbsp; &nbsp;until条件句（应用场合不多见，了解） 条件满足退出<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;until 条件<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;do<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;指令...<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;done<br />\n&nbsp;&nbsp; &nbsp;<br />\n#实战处理日志<br />\n&nbsp;&nbsp; &nbsp;将apache日志每行大小相加（案例仅供参考，未执行）<br />\n&nbsp;&nbsp; &nbsp;eg1：&nbsp;&nbsp; &nbsp;echo `awk &#39;{print $10}&#39; access.log |grep -v -|tr &quot;\\n&quot; &quot;+&quot;|sed &#39;s#117+##117#g&#39;`|bc<br />\n&nbsp;&nbsp; &nbsp;<br />\n&nbsp;&nbsp; &nbsp;eg2：&nbsp;&nbsp; &nbsp;sum=0<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;while read line<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;do<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;size=`echo $line|awk &#39;{print $10}&#39;`<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;[ &quot;$size&quot; == &quot;-&quot; ] &amp;&amp; continue<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;((sum+=$size))<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;done&lt;access.log<br />\n&nbsp;&nbsp; &nbsp;<br />\n&nbsp;&nbsp; &nbsp;eg3：&nbsp;&nbsp; &nbsp;exec&lt;access.log<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;sum=0<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;while read line<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;do<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;size=`echo $line|awk &#39;{print $10}&#39;`<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;[ &quot;$size&quot; == &quot;-&quot; ] &amp;&amp; continue<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;((sum+=$size))<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;done<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;<br />\n&nbsp;&nbsp; &nbsp;eg4：&nbsp;&nbsp; &nbsp;sum=0<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;cat access.log|while line<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;do<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;size=`echo $line|awk &#39;{print $10}&#39;`<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;[ &quot;$size&quot; == &quot;-&quot; ] &amp;&amp; continue<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;((sum+=$size))<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;done&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;<br />\n<br />\n&nbsp;&nbsp; &nbsp;解析：awk将access.log中每行第10个提取出来<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;grep将-去除<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;tr将换行\\n替换为+<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;sed将结尾的+去掉<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;echo将生成的字符串打印<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;bc执行运算</p>\n\n<p>查找文件</p>\n\n<p>find / -name&nbsp;文件名</p>\n\n<p>根据pid查看端口</p>\n\n<p>netstat -antup | grep pid</p>\n\n<p>查看tomcat内存和CPU</p>\n\n<p>top -p pid</p>', 1, '5f199b8885e24fc8b28672b872edb606', 19, '2018-02-23 00:00:00', '2018-09-16 20:20:54');
INSERT INTO `logcontent` VALUES ('042d3007a26446bab7f7982e12d97476', -1, '7338e53acd514defa1a17e47016f3f4a', 'Elasticsearch（十二）', '数据路由,路由算法,_id or custom routing value谜底,Document增删改内部原理,Document查询内部原理', '数据路由/路由算法/谜底/preference/search_type/primary shard数量不可变/Document增删改内部原理/Document查询内部原理', '<p><a name=\"数据路由\"></a>1、document路由到shard上是什么意思？</p>\n\n<p>一个index的数据会被分为多片，每片都在一个shard中，所以一个document，只能存在一个shard中，当客户端创建document时，es需要决定这个document放在这个index的哪个shard上，这个过程就被称为document routing，数据路由</p>\n\n<p>&nbsp;</p>\n\n<p><a name=\"路由算法\"></a>2、路由算法？</p>\n\n<p>shard = hash(routing) % number_of_primary_shards</p>\n\n<p>举个例子，一个index有3个primary shard，P0，P1，P2</p>\n\n<p>每次增删改查一个document的时候，都会带过来一个routing number，默认就是这个document的_id（可能是手动指定，也可能是自动生成）</p>\n\n<p>假设routing = 1</p>\n\n<p>会将这个routing值，传入一个hash函数中，产出一个routing值的hash值，假设hash(routing) = 21</p>\n\n<p>然后将hash函数产出的值对这个index的primary shard的数量求余数，21 % 3 = 0</p>\n\n<p>这个document就放在P0上。</p>\n\n<p>决定一个document在哪个shard上，最重要的一个值就是routing值，默认是_id，也可以手动指定，相同的routing值，每次过来，从hash函数中，产出的hash值一定是相同的</p>\n\n<p>无论hash值是什么数字，对number_of_primary_shards求余数，结果一定是在0~number_of_primary_shards-1之间这个范围内的。本例为0,1,2，不会超出。</p>\n\n<p>&nbsp;</p>\n\n<p><a name=\"_id or custom routing value\"></a>3、_id or custom routing value</p>\n\n<p>默认的routing就是_id</p>\n\n<p>也可以在发送请求的时候，手动指定一个routing value，比如说put /index/type/id?routing=user_id</p>\n\n<p>&nbsp;</p>\n\n<p>手动指定routing value是很有用的，可以保证说，某一类document一定被路由到一个shard上去，那么在后续进行应用级别的负载均衡，以及提升批量读取的性能的时候，是很有帮助的</p>\n\n<p>&nbsp;</p>\n\n<p><a name=\"谜底\"></a>4、primary shard一旦index建立，primary&nbsp;shard数量不可变的谜底</p>\n\n<p>举例：查找数据时，因为routing为_id，所以routing不变，假如routing为21，primary&nbsp;shard数为3，查找数据的时候为21%3=0，到p0去寻找数据，如果primary&nbsp;shard增加了一个primary&nbsp;shard数为4，查找数据的时候为21%4=1，到p1去寻找数据，寻找不到，间接导致数据丢失，所以primary shard一旦index建立，primary&nbsp;shard数量不可变,但是replica shard可以改变</p>\n\n<p>&nbsp;</p>\n\n<p><a name=\"preference\"></a>5、preference</p>\n\n<p>决定了哪些shard会被用来执行搜索操作</p>\n\n<p>_primary, _primary_first, _local, _only_node:xyz, _prefer_node:xyz, _shards:2,3</p>\n\n<p>bouncing results问题:</p>\n\n<p>两个document排序，field值相同；请求在不同的shard上，排序可能不同；每次页面上看到的搜索结果排序可能不一样。这就是bouncing result，也就是跳跃的结果。</p>\n\n<p>解决方案就是将preference设置为一个字符串，比如说user_id，让每个user每次搜索的时候，都使用同一个replica shard去执行，就不会看到bouncing results了</p>\n\n<p>&nbsp;</p>\n\n<p><a name=\"search_type\"></a>6、search_type</p>\n\n<p>default：query_then_fetch</p>\n\n<p>dfs_query_then_fetch，可以提升revelance sort精准度</p>\n\n<p>&nbsp;</p>\n\n<p><a name=\"Document增删改内部原理\"></a>7、Document增删改内部原理</p>\n\n<p>（1）客户端选择一个node发送请求过去，这个node就是coordinating node（协调节点）</p>\n\n<p>（2）coordinating node，对document进行路由，将请求转发给对应的node（有primary shard）</p>\n\n<p>（3）实际的node上的primary shard处理请求，然后将数据同步到replica node</p>\n\n<p>（4）coordinating node，如果发现primary node和所有replica node都搞定之后，就返回响应结果给客户端</p>\n\n<p>&nbsp;</p>\n\n<p><a name=\"Document查询内部原理\"></a>8、Document查询内部原理</p>\n\n<p>（1）客户端发送请求到任意一个node，成为coordinate node（协调节点）</p>\n\n<p>（2）coordinate node对document进行路由，将请求转发到对应的node，此时会使用round-robin随机轮询算法，在primary shard以及其所有replica中随机选择一个，让读请求负载均衡</p>\n\n<p>（3）接收请求的node返回document给coordinate node</p>\n\n<p>（4）coordinate node返回document给客户端</p>\n\n<p>（5）特殊情况：document如果还在建立索引过程中，可能只有primary shard有，任何一个replica shard都没有，此时可能会导致无法读取到document，但是document完成索引建立之后，primary shard和replica shard就都有了</p>\n\n<p>注释：对于读请求，请求不仅可以发送至primary shard上也可以发送至replica shard上；</p>\n\n<p>round-robin随机轮询算法：如果coordinate node接收到对一个Document的4次请求，就会使用算法将两次查询请求发送给primary shard，两次查询请求发送给replica shard，得到负载均衡的效果</p>\n', 2, '5f199b8885e24fc8b28672b872edb606', 8, '2017-07-10 18:04:29', '2018-09-06 16:12:58');
INSERT INTO `logcontent` VALUES ('0bf17b27f55947afbbbd82cf01d6d63e', 1, '4eca926aa543420baea2f03f506d042e', '高性能Mysql笔记三', '高性能Mysql（第三版）,创建高性能的索引', '高性能Mysql（第三版）--创建高性能的索引', '<h2>索引基础</h2>\n\n<h3>索引</h3>\n\n<pre>\n<code>MySQL中，索引是在存储引擎层而不是服务器层实现的。所以，并没有统一的索引标准：不同存储引擎的索引的工作方式并不一样，也不是所有的存储引擎都支持所有类型的索引。即使多个存储引擎支持同一种类型的索引，其底层的实现也可能不同。索引大大减少了服务器需要扫描的数据量。索引可以帮助服务器避免排序和临时表。索引可以将随机I/O变为顺序I/O。\n\n1、单行访问是很慢的，特别是在硬盘存储中。如果服务器从存储中读取一个数据块不应只是为了获取其中一行，最好读取的块中能包含尽可能多所需要的行。使用索引可以创建位置引用以提升效率。\n2、按顺序访问范围数据很快。有两个原因。第一，顺序I/O不需要多次磁盘寻道，所以比随机I/O要快很多。第二，如果服务器能够按需要顺序读取数据，便不需要额外的排序操作，聚合查询也无须再做排序和将行按组进行聚合计算了。\n3、索引覆盖查询是很快的。如果一个索引包含了查询需要的所有列，那么存储引擎就不需要再回表查找行。避免了大量的单行访问。</code></pre>\n\n<h3>使用索引扫描来做排序</h3>\n\n<pre>\n<code>MySQL有两种方式进行结果排序：文件排序对小数据集很快，而按索引顺序扫描可以满足大数据量的排序；如果EXPLAIN出来的type列的值为“index”，则说明MySQL使用了索引扫描来做排序\n\n只有当索引的列顺序和ORDER BY子句的顺序完全一致，并且所有列的排序方向（倒序或正序）都一致时，才能够使用索引来对结果做排序。\n如果查询需要关联多张表，则只有当ORDER BY子句引用的字段全部为第一个表时，才能使用索引做排序。\n需要满足索引的最左前缀的要求，或者WHERE子句或者JOIN子句中前导列为常量（精确匹配），才能使用索引做排序；\n\neg：KEY rental_date (rental_date,inventory_id,customer_id)\n可以使用索引排序\nWHERE rental_date = \'2005-05-25\' ORDER BY inventory_id, customer_id\nWHERE rental_date = \'2005-05-25\' ORDER BY inventory_id\n不能使用索引排序\nWHERE rental_date = \'2005-05-25\' ORDER BY inventory_id DESC, customer_id ASC（排序方向不一致）\nWHERE rental_date = \'2005-05-25\' ORDER BY inventory_id，staff_id（索引的列顺序和ORDER BY子句的顺序不一致）\nWHERE rental_date &gt; \'2005-05-25\' ORDER BY inventory_id, customer_id（rental_date范围非精确）\nWHERE rental_date = \'2005-05-25\' AND inventory_id IN(1,2) ORDER BY customer_id（inventory_id范围非精确）</code></pre>\n\n<h3>B-Tree索引</h3>\n\n<pre>\n<code>B-Tree索引，底层的存储引擎也可能使用不同的存储结构，例如，NDB集群存储引擎内部实际上使用T-Tree结构存储这种索引，而InnoDB则使用的是B+Tree；\n\nB-Tree通常意味着所有的值都是按顺序存储的，顺序为定义索引时列的顺序。B-Tree对索引列是顺序存储的，所以很适合查找范围数据（LIKE）。因为索引树中的节点是有序的，可以按照这种方式用于ORDER BY和GROUP BY操作。\n\nB-Tree索引使存储引擎不需要进行全表扫描来获取数据，而是从索引的根节点开始进行搜索。根节点的槽中存放了指向子节点的指针，存储引擎根据这些指针向下层查找。通过比较节点页的值和要查找的值可以找到合适的指针进入下层子节点，这些指针实际上定义了子节点页中值的上限和下限。最终存储引擎要么是找到对应的值，要么该记录不存在。叶子节点的指针指向的是被索引的数据，而不是其他的节点页。\n\neg：索引为key(last_name, first_name, birthday));\n\nB-Tree索引使用范围\n1、全值匹配：指的是和索引中的所有列进行匹配\n2、匹配最左前缀：即只使用索引的第一列\n3、匹配列前缀：eg:last_name前缀/last_name精确匹配+first_name前缀/last_name精确匹配+first_name精确匹配+birthday前缀\n4、匹配范围值：eg:last_name范围/last_name精确匹配+first_name范围/last_name精确匹配+first_name精确匹配+birthday范围\n5、只访问索引的查询（不理解）\n\nB-Tree索引的限制\n1、必须按照索引的最左列开始查找。例如上面例子中的索引无法用于查找first_name为Bill的人，因为这列不是最左数据列。\n2、不能跳过索引中的列。也就是说，前面所述的索引无法用于查找姓为Smith并且在某个特定birthday的人。如果不指定first_name，则MySQL只能使用索引的第一列。\n3、如果查询中有某个列的范围查询，则其右边所有列都无法使用索引优化查找。例如有查询WHERE last_name=\'Smith\' AND first_name LIKE \'J％\' AND birthday=\'1976-12-23\'，这个查询只能使用索引的前两列，因为这里LIKE是一个范围条件（但是服务器可以把其余列用于其他目的）。如果范围查询列值的数量有限，那么可以通过使用多个等于条件来代替范围条件。\n</code></pre>\n\n<h3>哈希索引</h3>\n\n<pre>\n<code>哈希索引（hash index）基于哈希表实现，只有精确匹配索引所有列的查询才有效。\n\n对于每一行数据，存储引擎都会对所有的索引列计算一个哈希码，哈希码是一个较小的值，并且不同键值的行计算出来的哈希码也不一样。哈希索引将所有的哈希码存储在索引中，同时在哈希表中保存指向每个数据行的指针。因为索引自身只需存储对应的哈希值，所以索引的结构十分紧凑，这也让哈希索引查找的速度非常快。\n\neg:索引为KEY USING HASH(first_name)\nSELECT last_name FROM testhash WHERE first_name=\'Peter\';\nMySQL先计算\'Peter\'的哈希值，并使用该值寻找对应的记录指针。因为f（\'Peter\'）=8784，所以MySQL在索引中查找8784，可以找到指向第3行的指针，比较第3行的值是否为\'Peter\'，确保就是要查找的行（哈希冲突很多）。\n\n哈希索引的限制\n1、哈希索引只包含哈希值和行指针，而不存储字段值，所以不能使用索引中的值来避免读取行。不过，访问内存中的行的速度很快，所以大部分情况下这一点对性能的影响并不明显。\n2、哈希索引数据并不是按照索引值顺序存储的，所以也就无法用于排序。\n3、哈希索引也不支持部分索引列匹配查找，因为哈希索引始终是使用索引列的全部内容来计算哈希值的。\n4、哈希索引只支持等值比较查询，包括=、IN()、&lt;=&gt;（注意&lt;&gt;和&lt;=&gt;是不同的操作）。\n5、哈希索引不支持任何范围查询，例如WHERE price&gt;100。\n6、访问哈希索引的数据非常快，除非有很多哈希冲突（不同的索引列值却有相同的哈希值）。当出现哈希冲突的时候，存储引擎必须遍历链表中所有的行指针，逐行进行比较，直到找到所有符合条件的行。\n7、如果哈希冲突很多的话，一些索引维护操作的代价也会很高。例如，如果在某个选择性很低（哈希冲突很多）的列上建立哈希索引，那么当从表中删除一行时，存储引擎需要遍历对应哈希值的链表中的每一行，找到并删除对应行的引用，冲突越多，代价越大。\n\n创建自定义哈希索引\n如果存储引擎不支持哈希索引，可以在B-Tree基础上创建一个伪哈希索引。依然使用B-Tree进行查找，但是它使用哈希值（非SHA1()或MD5()，其结果为长字符串而非数字,数据量大且哈希冲突多可以自己实现哈希函数或者使用FNV64()），而不是键值本身进行索引查找，只需要很小的索引就可以为超长的键创建索引。\n创建索引。你需要做的就是在查询的WHERE子句中手动指定使用哈希函数。\n如：原本sql:SELECT id FROM url WHERE url=\"http://www.mysql.com\";索引为url列\n若删除原来url列上的索引，而新增一个索引的url_crc列，使用CRC32做哈希，使用下面的方式查询：\nSELECT id FROM url WHERE url=\"http://www.mysql.com\" AND url_crc=CRC32(\"http://www.mysql.com\"); \n性能会非常高，因为MySQL优化器会使用这个选择性很高而体积很小的基于url_crc列的索引来完成查找。即使有多个记录有相同的索引值，查找仍然很快，只需要根据哈希值做快速的整数比较就能找到索引条目，然后一一比较返回对应的行。缺陷是需要维护哈希值，可以手动维护，也可以使用触发器实现。</code></pre>\n\n<h3>空间数据索引（R-Tree）</h3>\n\n<pre>\n<code>Mysql中不推荐</code></pre>\n\n<h3>全文索引</h3>\n\n<pre>\n<code>全文索引是一种特殊类型的索引，它查找的是文本中的关键词，而不是直接比较索引中的值。全文搜索和其他几类索引的匹配方式完全不一样。它有许多需要注意的细节，如停用词、词干和复数、布尔搜索等。全文索引更类似于搜索引擎做的事情，而不是简单的WHERE条件匹配。在相同的列上同时创建全文索引和基于值的B-Tree索引不会有冲突，全文索引适用于MATCH AGAINST操作，而不是普通的WHERE条件操作。</code></pre>\n\n<h3>其他索引类别</h3>\n\n<pre>\n<code>还有很多第三方的存储引擎使用不同类型的数据结构来存储索引</code></pre>\n\n<h2>高性能的索引策略</h2>\n\n<h3>独立的列</h3>\n\n<pre>\n<code>索引应为独立的列（不能是表达式的一部分或函数的参数）：如果查询中的列不是独立的，则MySQL就不会使用索引。例如下边查询无法使用索引：\nWHERE actor_id + 1 = 5;\nWHERE TO_DAYS(CURRENT_DATE)-TO_DAYS(date_col)&lt;=10;</code></pre>\n\n<h3>前缀索引</h3>\n\n<pre>\n<code>如果索引很长的字符列,会让索引变得大且慢，这时可以根据业务找到最适合的前缀长度,创建前缀索引。\n缺点：MySQL无法使用前缀索引做ORDER BY和GROUP BY，也无法使用前缀索引做覆盖扫描。\neg:ALTER TABLE city_demo ADD KEY (city(7));</code></pre>\n\n<h3>多列索引</h3>\n\n<pre>\n<code>Version(5.0↑)引入了一种”索引合并”的策略，查询能够同时使用这两个单列索引进行扫描，并将结果进行合并。这种算法有三个变种：OR条件的联合，AND条件的相交，组合前两种情况的联合及相交。索引合并策略有时候是一种优化的结果，也说明了表上的索引建得很糟糕。\n\n1、当出现服务器对多个索引做相交操作时（通常有多个AND条件），通常意味着需要一个多列索引，而非多个单列索引。\n2、当服务器需要对多个索引做联合操作时（通常有多个OR条件），通常需要耗费大量CPU和内存资源在算法的缓存、排序和合并操作上。特别是当其中有些索引的选择性不高，需要合并扫描返回的大量数据。\n3、优化器不会把这些计算到“查询成本”中，优化器只关心随机页面读取。这会使得查询的成本被“低估”，导致该执行计划还不如直接走全表扫描。这样做不但会消耗更多的CPU和内存资源，还可能会影响查询的并发性，但如果是单独运行这样的查询则往往会忽略对并发性的影响。</code></pre>\n\n<h3>选择合适的索引列顺序</h3>\n\n<pre>\n<code>一般将选择性最高的列放在索引最前列，可以最快地过滤出需要的行。\n性能不只依赖于所有索引列的选择性,也和查询条件的具体值分布有关,可能需要根据运行频率最高的查询调整索引列顺序。\n有时需要根据排序,分组和范围条件综合考虑。\n尽可能将需要做范围查询的列放到索引的后面。\n避免多个范围查询，或者把范围改为多个等值查询，如age between 1 and 3改为age in (1,2,3)。</code></pre>\n\n<h3>聚簇索引</h3>\n\n<pre>\n<code>聚簇索引并不是一种索引类型，而是一种数据存储方式。具体的细节依赖于其实现方式，但InnoDB的聚簇索引实际上在同一个结构中保存了B-Tree索引和数据行。当表有聚簇索引时，它的数据行实际上存放在索引的叶子页中。因为无法同时把数据行存放在两个不同的地方，所以一个表只能有一个聚簇索引，不过，覆盖索引可以模拟多个聚簇索引的情况。因为是存储引擎负责实现索引，因此不是所有的存储引擎都支持聚簇索引。\n\nInnoDB将通过主键聚集数据，如果没有定义主键，InnoDB会选择一个唯一的非空索引代替。如果没有这样的索引，InnoDB会隐式定义一个主键来作为聚簇索引。InnoDB只聚集在同一个页面中的记录。包含相邻键值的页面可能会相距甚远。\n\n聚簇索引的优点\n1、可以把相关数据保存在一起，这样只需要从磁盘读取少数的数据页就能获取全部数据。如果没有使用聚簇索引，则每行都可能导致一次磁盘I/O。\n2、数据访问更快。聚簇索引将索引和数据保存在同一个B-Tree中，因此从聚簇索引中获取数据通常比在非聚簇索引中查找要快。\n3、使用覆盖索引扫描的查询可以直接使用页节点中的主键值。\n\n聚簇索引的缺点\n1、聚簇数据最大限度地提高了I/O密集型应用的性能，但如果数据全部都放在内存中，聚簇索引与访问的顺序就没那么重要了。\n2、插入速度严重依赖于插入顺序。最好避免随机的（不连续且值的分布范围非常大）主键，主键按照顺序插入是加载数据到InnoDB表中速度最快的方式。但如果不是按照主键顺序加载数据，那么在加载完成后最好使用OPTIMIZE TABLE命令重新组织一下表。\n3、更新聚簇索引列的代价很高，因为会强制InnoDB将每个被更新的行移动到新的位置。\n4、基于聚簇索引的表在插入新行，或者主键被更新导致需要移动行的时候，可能面临“页分裂”的问题。当行的主键值要求必须将这一行插入到某个已满的页中时，存储引擎会将该页分裂成两个页面来容纳该行，这就是一次页分裂操作。页分裂会导致表占用更多的磁盘空间。\n5、聚簇索引可能导致全表扫描变慢，尤其是行比较稀疏，或者由于页分裂导致数据存储不连续的时候。\n6、二级索引（非聚簇索引）可能比想象的要更大，因为在二级索引的叶子节点包含了引用的主键列。\n7、二级索引访问需要两次索引查找，而不是一次。\n\n不连续且值的分布范围非常大的聚簇索引造成的影响\n1、写入的目标页可能已经刷到磁盘上并从缓存中移除，或者是还没有被加载到缓存中，InnoDB在插入之前需要先找到并从磁盘读取目标页到内存中。这将导致大量的随机I/O。\n2、因为写入是乱序的，InnoDB不得不频繁地做页分裂操作，以便为新的行分配空间。页分裂会导致移动大量数据，一次插入最少需要修改三个页而不是一个页。\n3、由于频繁的页分裂，页会变得稀疏并被不规则地填充，所以最终数据会有碎片。\n\n顺序的主键也可能造成更坏的结果\n对于高并发工作负载，在InnoDB中按主键顺序插入可能会造成明显的争用。主键的上界会成为“热点”。因为所有的插入都发生在这里，所以并发插入可能导致间隙锁竞争。另一个热点可能是AUTO_INCREMENT锁机制；如果遇到这个问题，则可能需要考虑重新设计表或者应用，或者更改innodb_autoinc_lock_mode配置</code></pre>\n\n<h3>覆盖索引</h3>\n\n<pre>\n<code>如果一个索引包含（或者说覆盖）所有需要查询的字段的值，我们就称之为“覆盖索引”。\n\n覆盖索引好处\n1、索引条目通常远小于数据行大小，所以如果只需要读取索引，那MySQL就会极大地减少数据访问量。这对缓存的负载非常重要，因为这种情况下响应时间大部分花费在数据拷贝上。覆盖索引对于I/O密集型的应用也有帮助，因为索引比数据更小，更容易全部放入内存中（这对于MyISAM尤其正确，因为MyISAM能压缩索引以变得更小）。\n2、因为索引是按照列值顺序存储的（至少在单个页内是如此），所以对于I/O密集型的范围查询会比随机从磁盘读取每一行数据的I/O要快得多。对于某些存储引擎，例如MyISAM和Percona XtraDB，甚至可以通过OPTIMIZE命令使得索引完全顺序排列。\n3、一些存储引擎如MyISAM在内存中只缓存索引，数据则依赖于操作系统来缓存，因此要访问数据需要一次系统调用。可能导致严重的性能问题，尤其是那些系统调用占了数据访问中的最大开销的场景。\n4、由于InnoDB的聚簇索引，覆盖索引对InnoDB表特别有用。InnoDB的二级索引在叶子节点中保存了行的主键值，所以如果二级主键能够覆盖查询，则可以避免对主键索引的二次查询。\n\n不是所有类型的索引都可以成为覆盖索引。哈希索引、空间索引和全文索引等都不存储索引列的值，所以只能使用B-Tree索引做覆盖索引。另外，不同的存储引擎实现覆盖索引的方式也不同，不是所有的引擎都支持覆盖索引\n\n分析：SELECT * FROM products WHERE actor=\'SEAN\' AND title like \'%APOLLO%\'无法使用覆盖索引的原因（key: actor）\n1、索引覆盖了WHERE条件中的字段，但无法覆盖整个查询涉及的字段（*），因此无法享受覆盖索引\n2、MySQL能在索引中做最左前缀匹配的LIKE比较，因为该操作可以转换为简单的比较操作，但是如果是通配符开头的LIKE查询，存储引擎就无法做比较匹配。这种情况下，MySQL服务器只能提取数据行的值而不是索引值来做比较。</code></pre>\n\n<h2>&nbsp;</h2>', 1, '5f199b8885e24fc8b28672b872edb606', 4, '2018-08-31 17:22:01', '2018-09-22 19:37:36');
INSERT INTO `logcontent` VALUES ('0dea88ccf6be4425a33c5833e23a4912', 1, 'd7caeba238f0466d87db109b2b9724da', '简单线性回归笔记', 'Simple,Linear,Regression', '初学', '<p><strong><a id=\"前提介绍\" name=\"前提介绍\"></a>前提介绍：</strong></p>\n\n<p><strong><a id=\"统计量：描述数据特征\" name=\"统计量：描述数据特征\"></a>统计量：描述数据特征:</strong></p>\n\n<p>一、集中趋势衡量</p>\n\n<p>均值（平均数，平均值）：<br />\n<img src=\"http://localhost:8080/myLog/images/linear_regression/simple/1.png\" style=\"height:60px; width:60px\" /><br />\n{6, 2, 9, 1, 2}的均值为(6 + 2 + 9 + 1 + 2) / 5 = 20 / 5 = 4</p>\n\n<p>中位数 （median）: 将数据中的各个数值按照大小顺序排列，居于中间位置的变量，当n为奇数时：取位置中间的变量，当n为偶数时，取中间两个变量的平均值：<br />\n{1, 2, 2, 6, 9 }的中位数为2</p>\n\n<p>众数 （mode）：数据中出现次数最多的数：<br />\n{1, 2, 2, 6, 9 }的众数为2</p>\n\n<p>二、离散程度衡量</p>\n\n<p>方差（variance):<br />\n<img src=\"http://localhost:8080/myLog/images/linear_regression/simple/2.png\" style=\"height:60px; width:104px\" /><br />\n{6, 2, 9, 1, 2}的方差为((6 - 4)<span style=\"font-size:8px\"><sup>2</sup></span> + (2 - 4)<span style=\"font-size:8px\"><sup>2</sup></span> + (9 - 4)<span style=\"font-size:8px\"><sup>2</sup></span> + (1 - 4)<span style=\"font-size:8px\"><sup>2</sup></span> + (2 - 4)<span style=\"font-size:8px\"><sup>2</sup></span>&nbsp;= 4 + 4 + 25 + 9 + 4)/(5 - 1) = 11.5</p>\n\n<p>标准差 (standard deviation):<br />\n<img src=\"http://localhost:8080/myLog/images/linear_regression/simple/3.png\" style=\"height:30px; width:61px\" /><br />\n{6, 2, 9, 1, 2}的标准差为sqrt(11.5) = 3.39</p>\n\n<p><strong>介绍：</strong></p>\n\n<p>回归(regression) Y变量为连续数值型(continuous numerical variable):如：房价，人数，降雨量<br />\n分类(Classification): Y变量为类别型(categorical variable):如：颜色类别，电脑品牌，有无信誉</p>\n\n<p><strong><a id=\"简单线性回归\" name=\"简单线性回归\"></a>简单线性回归(Simple Linear Regression):</strong></p>\n\n<p>很多做决定过程通常是根据两个或者多个变量之间的关系<br />\n回归分析(regression analysis)用来建立方程模拟两个或者多个变量之间如何关联<br />\n被预测的变量叫做：因变量(dependent variable), y, 输出(output)<br />\n被用来进行预测的变量叫做： 自变量(independent variable), x, 输入(input)</p>\n\n<p><strong><a id=\"简单线性回归介绍\" name=\"简单线性回归介绍\"></a>简单线性回归介绍:</strong></p>\n\n<p>简单线性回归包含一个自变量(x)和一个因变量(y)<br />\n以上两个变量的关系用一条直线来模拟<br />\n如果包含两个以上的自变量，则称作多元回归分析(multiple regression)</p>\n\n<p><strong><a id=\"简单线性回归模型\" name=\"简单线性回归模型\"></a>简单线性回归模型:</strong></p>\n\n<p>被用来描述因变量(y)和自变量(X)以及偏差(error)之间关系的方程叫做回归模型<br />\n简单线性回归的模型是:y = &beta;<span style=\"font-size:8px\"><sub>0</sub></span> +&nbsp;&beta;<sub><span style=\"font-size:8px\">1</span></sub>x +&nbsp;&epsilon;<br />\n其中&beta;<span style=\"font-size:8px\"><sub>0</sub></span>&nbsp;、&beta;<sub><span style=\"font-size:8px\">1</span></sub>是参数，&epsilon;是偏差</p>\n\n<p><strong><a id=\"简单线性回归方程\" name=\"简单线性回归方程\"></a>简单线性回归方程:</strong></p>\n\n<p>E(y) = &beta;<span style=\"font-size:8px\"><sub>0</sub></span> +&nbsp;&beta;<sub><span style=\"font-size:8px\">1</span></sub>x<br />\n这个方程对应的图像是一条直线，称作回归线，其中，&beta;0是回归线的截距，&beta;1是回归线的斜率，E(y)是在一个给定x值下y的期望值（均值）</p>\n\n<p>正向线性关系：<img src=\"http://localhost:8080/myLog/images/linear_regression/simple/5.jpg\" style=\"height:100px; width:141px\" />负向线性关系：<img src=\"http://localhost:8080/myLog/images/linear_regression/simple/6.jpg\" style=\"height:100px; width:141px\" />无关系：<img src=\"http://localhost:8080/myLog/images/linear_regression/simple/7.jpg\" style=\"height:100px; width:141px\" /></p>\n\n<p><strong><a id=\"估计的简单线性回归方程\" name=\"估计的简单线性回归方程\"></a>估计的简单线性回归方程:</strong></p>\n\n<p>ŷ=b<span style=\"font-size:8px\"><sub>0</sub></span>+b<span style=\"font-size:8px\"><sub>1</sub></span>x<br />\n这个方程叫做估计线性方程(estimated regression line)，其中，b0是估计线性方程的纵截距，b1是估计线性方程的斜率，ŷ是在自变量x等于一个给定值的时候，y的估计值</p>\n\n<p><strong><a id=\"关于偏差ε的假定\" name=\"关于偏差ε的假定\"></a>关于偏差&epsilon;的假定:</strong><br />\n是一个随机的变量，均值为0<br />\n&epsilon;的方差(variance)对于所有的自变量x是一样的<br />\n&epsilon;的值是独立的<br />\n&epsilon;满足正态分布</p>\n\n<p><strong><a id=\"简单线性回归模型举例\" name=\"简单线性回归模型举例\"></a>简单线性回归模型举例:</strong></p>\n\n<p>汽车卖家做电视广告数量与卖出的汽车数量：<br />\n<img src=\"http://localhost:8080/myLog/images/linear_regression/simple/8.png\" style=\"height:200px; width:156px\" /><br />\n简单线性回归模型的最佳回归线(使sum of squares最小)<br />\n<img src=\"http://localhost:8080/myLog/images/linear_regression/simple/9.png\" style=\"height:150px; width:270px\" /><br />\n即<sub><img src=\"http://localhost:8080/myLog/images/linear_regression/simple/10.png\" style=\"height:30px; width:108px\" /></sub>最小（ŷ表示x在线上的点）<br />\n<img src=\"http://localhost:8080/myLog/images/linear_regression/simple/11.png\" style=\"height:55px; width:140px\" /><br />\n分子 = (1-2)(14-20)+(3-2)(24-20)+(2-2)(18-20)+(1-2)(17-20)+(3-2)(27-20)&nbsp;= 6 + 4 + 0 + 3 + 7&nbsp;= 20<br />\n分母 = （1-2）<span style=\"font-size:8px\"><sup>2</sup></span> + (3-2)<sup><span style=\"font-size:8px\">2</span></sup> + (2-2)<sup><span style=\"font-size:8px\">2</span></sup> + (1-2)<sup><span style=\"font-size:8px\">2</span></sup> + (3-2)<sup><span style=\"font-size:8px\">2</span></sup>&nbsp;= 1 + 1 + 0 + 1 + 1 =&nbsp;4<br />\nb<sub>1</sub> = 20/4 &nbsp;=5<br />\n<img src=\"http://localhost:8080/myLog/images/linear_regression/simple/12.png\" style=\"height:25px; width:86px\" /><br />\nb<sub>0</sub> = 20 - 5*2 = 20 - 10 = 10<br />\n得出ŷ = 5x<span style=\"font-size:8px\"><sub>&nbsp;</sub></span>+ 10</p>', 1, '5f199b8885e24fc8b28672b872edb606', 11, '2017-11-21 00:00:00', '2018-08-31 22:21:23');
INSERT INTO `logcontent` VALUES ('12ed98e8852b491ea94ac5ac3f0d6928', -1, '7338e53acd514defa1a17e47016f3f4a', '二、数据输入和输出', '数据输入和输出', '数据输入和输出', '<h2>数据输入和输出</h2>\n\n<p>一个&nbsp;<em>对象</em>&nbsp;是基于特定语言的内存的数据结构。&nbsp;为了通过网络发送或者存储它，我们需要将它表示成某种标准的格式。&nbsp;JSON&nbsp;是一种以人可读的文本表示对象的方法。&nbsp;它已经变成 NoSQL 世界交换数据的事实标准。当一个对象被序列化成为 JSON，它被称为一个&nbsp;<em>JSON 文档</em>&nbsp;。</p>\n\n<p>Elastcisearch 是分布式的&nbsp;<em>文档</em>&nbsp;存储。它能存储和检索复杂的数据结构--序列化成为JSON文档--以&nbsp;<em>实时</em>&nbsp;的方式。 换句话说，一旦一个文档被存储在 Elasticsearch 中，它就是可以被集群中的任意节点检索到。</p>\n\n<p>当然，我们不仅要存储数据，我们一定还需要查询它，成批且快速的查询它们。 尽管现存的 NoSQL 解决方案允许我们以文档的形式存储对象，但是他们仍旧需要我们思考如何查询我们的数据，以及确定哪些字段需要被索引以加快数据检索。</p>\n\n<p>在 Elasticsearch 中，&nbsp;<em>每个字段的所有数据</em>&nbsp;都是&nbsp;<em>默认被索引的</em>&nbsp;。&nbsp;即每个字段都有为了快速检索设置的专用倒排索引。而且，不像其他多数的数据库，它能在&nbsp;<em>相同的查询中</em>&nbsp;使用所有这些倒排索引，并以惊人的速度返回结果。</p>\n\n<h2>什么是文档?</h2>\n\n<p>在大多数应用中，多数实体或对象可以被序列化为包含键值对的 JSON 对象。&nbsp;一个&nbsp;<em>键</em>&nbsp;可以是一个字段或字段的名称，一个&nbsp;<em>值</em>&nbsp;可以是一个字符串，一个数字，一个布尔值， 另一个对象，一些数组值，或一些其它特殊类型诸如表示日期的字符串，或代表一个地理位置的对象：</p>\n\n<pre>\n<code class=\"language-json\">{\n    \"name\":         \"John Smith\",\n    \"age\":          42,\n    \"confirmed\":    true,\n    \"join_date\":    \"2014-06-01\",\n    \"home\": {\n        \"lat\":      51.5,\n        \"lon\":      0.1\n    },\n    \"accounts\": [\n        {\n            \"type\": \"facebook\",\n            \"id\":   \"johnsmith\"\n        },\n        {\n            \"type\": \"twitter\",\n            \"id\":   \"johnsmith\"\n        }\n    ]\n}</code></pre>\n\n<p>通常情况下，我们使用的术语&nbsp;<em>对象</em>&nbsp;和&nbsp;<em>文档</em>&nbsp;是可以互相替换的。不过，有一个区别：&nbsp;一个对象仅仅是类似于 hash 、 hashmap 、字典或者关联数组的 JSON 对象，对象中也可以嵌套其他的对象。 对象可能包含了另外一些对象。在 Elasticsearch 中，术语&nbsp;<em>文档</em>&nbsp;有着特定的含义。它是指最顶层或者根对象, 这个根对象被序列化成 JSON 并存储到 Elasticsearch 中，指定了唯一 ID。</p>\n\n<p><img alt=\"警告\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/warning.png\" /></p>\n\n<p>字段的名字可以是任何合法的字符串，但不可以包含时间段。</p>\n\n<h2>文档元数据</h2>\n\n<p>一个文档不仅仅包含它的数据&nbsp;，也包含&nbsp;<em>元数据</em>&nbsp;&mdash;&mdash;&nbsp;<em>有关</em>&nbsp;文档的信息。&nbsp;三个必须的元数据元素如下：</p>\n\n<p><code>_index&nbsp;</code>文档在哪存放</p>\n\n<p><code>_type&nbsp;</code>文档表示的对象类别</p>\n\n<p><code>_id&nbsp;</code>文档唯一标识</p>\n\n<h3>_index</h3>\n\n<p>一个&nbsp;<em>索引</em>&nbsp;应该是因共同的特性被分组到一起的文档集合。 例如，你可能存储所有的产品在索引&nbsp;<code>products</code>中，而存储所有销售的交易到索引&nbsp;<code>sales</code>&nbsp;中。 虽然也允许存储不相关的数据到一个索引中，但这通常看作是一个反模式的做法。</p>\n\n<p><img alt=\"提示\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/tip.png\" /></p>\n\n<p>实际上，在 Elasticsearch 中，我们的数据是被存储和索引在&nbsp;<em>分片</em>&nbsp;中，而一个索引仅仅是逻辑上的命名空间， 这个命名空间由一个或者多个分片组合在一起。&nbsp;然而，这是一个内部细节，我们的应用程序根本不应该关心分片，对于应用程序而言，只需知道文档位于一个&nbsp;<em>索引</em>内。 Elasticsearch 会处理所有的细节。</p>\n\n<p>所有需要我们做的就是选择一个索引名，这个名字必须小写，不能以下划线开头，不能包含逗号。我们用<code>website</code>&nbsp;作为索引名举例。</p>\n\n<h3>_type</h3>\n\n<p>数据可能在索引中只是松散的组合在一起，但是通常明确定义一些数据中的子分区是很有用的。 例如，所有的产品都放在一个索引中，但是你有许多不同的产品类别，比如 &quot;electronics&quot; 、 &quot;kitchen&quot; 和 &quot;lawn-care&quot;。</p>\n\n<p>这些文档共享一种相同的（或非常相似）的模式：他们有一个标题、描述、产品代码和价格。他们只是正好属于&ldquo;产品&rdquo;下的一些子类。</p>\n\n<p>Elasticsearch 公开了一个称为&nbsp;<em>types</em>&nbsp;（类型）的特性，它允许您在索引中对数据进行逻辑分区。不同 types 的文档可能有不同的字段，但最好能够非常相似。&nbsp;</p>\n\n<p>一个&nbsp;<code>_type</code>&nbsp;命名可以是大写或者小写，但是不能以下划线或者句号开头，不应该包含逗号，&nbsp;并且长度限制为256个字符. 我们使用&nbsp;<code>blog</code>&nbsp;作为类型名举例。</p>\n\n<h3>_id</h3>\n\n<p><em>ID</em>&nbsp;是一个字符串，&nbsp;当它和&nbsp;<code>_index</code>&nbsp;以及&nbsp;<code>_type</code>&nbsp;组合就可以唯一确定 Elasticsearch 中的一个文档。 当你创建一个新的文档，要么提供自己的&nbsp;<code>_id</code>&nbsp;，要么让 Elasticsearch 帮你生成。</p>\n\n<h3>其他元数据</h3>\n\n<p>通过前面已经列出的元数据元素， 我们已经能存储文档到 Elasticsearch 中并通过 ID 检索它--换句话说，使用 Elasticsearch 作为文档的存储介质。</p>\n\n<h2>索引文档</h2>\n\n<p>通过使用&nbsp;<code>index</code>&nbsp;API ，文档可以被&nbsp;<em>索引</em>&nbsp;&mdash;&mdash; 存储和使文档可被搜索&nbsp;。 但是首先，我们要确定文档的位置。正如我们刚刚讨论的，一个文档的&nbsp;<code>_index</code>&nbsp;、&nbsp;<code>_type</code>&nbsp;和&nbsp;<code>_id</code>&nbsp;唯一标识一个文档。 我们可以提供自定义的&nbsp;<code>_id</code>&nbsp;值，或者让&nbsp;<code>index</code>&nbsp;API 自动生成。</p>\n\n<h3>使用自定义的 ID</h3>\n\n<p>如果你的文档有一个自然的&nbsp;标识符 （例如，一个&nbsp;<code>user_account</code>&nbsp;字段或其他标识文档的值），你应该使用如下方式的&nbsp;<code>index</code>&nbsp;API 并提供你自己&nbsp;<code>_id</code>&nbsp;：</p>\n\n<pre>\n<code class=\"language-json\">PUT /{index}/{type}/{id}\n{\n  \"field\": \"value\",\n  ...\n}</code></pre>\n\n<p>举个例子，如果我们的索引称为&nbsp;<code>website</code>&nbsp;，类型称为&nbsp;<code>blog</code>&nbsp;，并且选择&nbsp;<code>123</code>&nbsp;作为 ID ，那么索引请求应该是下面这样：</p>\n\n<pre>\n<code class=\"language-json\">PUT /website/blog/123\n{\n  \"title\": \"My first blog entry\",\n  \"text\":  \"Just trying this out...\",\n  \"date\":  \"2014/01/01\"\n}</code></pre>\n\n<p>Elasticsearch 响应体如下所示：</p>\n\n<pre>\n<code class=\"language-json\">{\n   \"_index\":    \"website\",\n   \"_type\":     \"blog\",\n   \"_id\":       \"123\",\n   \"_version\":  1,\n   \"created\":   true\n}</code></pre>\n\n<p>该响应表明文档已经成功创建，该索引包括&nbsp;<code>_index</code>&nbsp;、&nbsp;<code>_type</code>&nbsp;和&nbsp;<code>_id</code>&nbsp;元数据， 以及一个新元素：<code>_version</code>&nbsp;。</p>\n\n<p>在 Elasticsearch 中每个文档都有一个版本号。当每次对文档进行修改时（包括删除），&nbsp;<code>_version</code>&nbsp;的值会递增。</p>\n\n<h3>Autogenerating IDs</h3>\n\n<p>如果你的数据没有自然的 ID， Elasticsearch 可以帮我们自动生成 ID 。&nbsp;请求的结构调整为： 不再使用<code>PUT</code>&nbsp;谓词(&ldquo;使用这个 URL 存储这个文档&rdquo;)， 而是使用&nbsp;<code>POST</code>&nbsp;谓词(&ldquo;存储文档在这个 URL 命名空间下&rdquo;)。</p>\n\n<p>现在该 URL 只需包含&nbsp;<code>_index</code>&nbsp;和&nbsp;<code>_type</code>&nbsp;:</p>\n\n<pre>\n<code class=\"language-json\">POST /website/blog/\n{\n  \"title\": \"My second blog entry\",\n  \"text\":  \"Still trying this out...\",\n  \"date\":  \"2014/01/01\"\n}</code></pre>\n\n<p>除了&nbsp;<code>_id</code>&nbsp;是 Elasticsearch 自动生成的，响应的其他部分和前面的类似：</p>\n\n<pre>\n<code class=\"language-json\">{\n   \"_index\":    \"website\",\n   \"_type\":     \"blog\",\n   \"_id\":       \"AVFgSgVHUP18jI2wRx0w\",\n   \"_version\":  1,\n   \"created\":   true\n}</code></pre>\n\n<p>自动生成的 ID 是 URL-safe、 基于 Base64 编码且长度为20个字符的 GUID 字符串。 这些 GUID 字符串由可修改的 FlakeID 模式生成，这种模式允许多个节点并行生成唯一 ID ，且互相之间的冲突概率几乎为零。</p>\n\n<h2>取回一个文档</h2>\n\n<p>为了从 Elasticsearch 中检索出文档&nbsp;，我们仍然使用相同的&nbsp;<code>_index</code>&nbsp;,&nbsp;<code>_type</code>&nbsp;, 和&nbsp;<code>_id</code>&nbsp;，但是 HTTP 谓词&nbsp;更改为&nbsp;<code>GET</code>&nbsp;:</p>\n\n<pre>\nGET /website/blog/123?pretty</pre>\n\n<p>响应体包括目前已经熟悉了的元数据元素，再加上&nbsp;<code>_source</code>&nbsp;字段，这个字段包含我们索引数据时发送给 Elasticsearch 的原始 JSON 文档：</p>\n\n<pre>\n<code class=\"language-json\">{\n  \"_index\" :   \"website\",\n  \"_type\" :    \"blog\",\n  \"_id\" :      \"123\",\n  \"_version\" : 1,\n  \"found\" :    true,\n  \"_source\" :  {\n      \"title\": \"My first blog entry\",\n      \"text\":  \"Just trying this out...\",\n      \"date\":  \"2014/01/01\"\n  }\n}</code></pre>\n\n<p><img alt=\"注意\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/note.png\" /></p>\n\n<p>在请求的查询串参数中加上&nbsp;<code>pretty</code>&nbsp;参数，&nbsp;正如前面的例子中看到的，这将会调用 Elasticsearch 的&nbsp;<em>pretty-print</em>&nbsp;功能，该功能&nbsp;使得 JSON 响应体更加可读。但是，&nbsp;<code>_source</code>字段不能被格式化打印出来。相反，我们得到的&nbsp;<code>_source</code>&nbsp;字段中的 JSON 串，刚好是和我们传给它的一样。</p>\n\n<p><code>GET</code>&nbsp;请求的响应体包括&nbsp;<code>{&quot;found&quot;: true}</code>&nbsp;，这证实了文档已经被找到。&nbsp;如果我们请求一个不存在的文档，我们仍旧会得到一个 JSON 响应体，但是&nbsp;<code>found</code>&nbsp;将会是&nbsp;<code>false</code>&nbsp;。 此外， HTTP 响应码将会是&nbsp;<code>404 Not Found</code>&nbsp;，而不是&nbsp;<code>200 OK</code>&nbsp;。</p>\n\n<p>我们可以通过传递&nbsp;<code>-i</code>&nbsp;参数给&nbsp;<code>curl</code>&nbsp;命令，该参数&nbsp;能够显示响应的头部：</p>\n\n<pre>\ncurl -i -XGET http://localhost:9200/website/blog/124?pretty</pre>\n\n<p>显示响应头部的响应体现在类似这样：</p>\n\n<pre>\nHTTP/1.1 404 Not Found\nContent-Type: application/json; charset=UTF-8\nContent-Length: 83\n</pre>\n\n<pre>\n<code class=\"language-json\">{\n  \"_index\" : \"website\",\n  \"_type\" :  \"blog\",\n  \"_id\" :    \"124\",\n  \"found\" :  false\n}</code></pre>\n\n<h3>返回文档的一部分</h3>\n\n<p>默认情况下，&nbsp;<code>GET</code>&nbsp;请求&nbsp;会返回整个文档，这个文档正如存储在&nbsp;<code>_source</code>&nbsp;字段中的一样。但是也许你只对其中的&nbsp;<code>title</code>&nbsp;字段感兴趣。单个字段能用&nbsp;<code>_source</code>&nbsp;参数请求得到，多个字段也能使用逗号分隔的列表来指定。</p>\n\n<pre>\nGET /website/blog/123?_source=title,text</pre>\n\n<p>该&nbsp;<code>_source</code>&nbsp;字段现在包含的只是我们请求的那些字段，并且已经将&nbsp;<code>date</code>&nbsp;字段过滤掉了。</p>\n\n<pre>\n<code class=\"language-json\">{\n  \"_index\" :   \"website\",\n  \"_type\" :    \"blog\",\n  \"_id\" :      \"123\",\n  \"_version\" : 1,\n  \"found\" :   true,\n  \"_source\" : {\n      \"title\": \"My first blog entry\" ,\n      \"text\":  \"Just trying this out...\"\n  }\n}</code></pre>\n\n<p>或者，如果你只想得到&nbsp;<code>_source</code>&nbsp;字段，不需要任何元数据，你能使用&nbsp;<code>_source</code>&nbsp;端点：</p>\n\n<pre>\nGET /website/blog/123/_source</pre>\n\n<p>那么返回的的内容如下所示：</p>\n\n<pre>\n<code class=\"language-json\">{\n   \"title\": \"My first blog entry\",\n   \"text\":  \"Just trying this out...\",\n   \"date\":  \"2014/01/01\"\n}</code></pre>\n\n<h2>检查文档是否存在</h2>\n\n<p>如果只想检查一个文档是否存在&nbsp;--根本不想关心内容--那么用&nbsp;<code>HEAD</code>&nbsp;方法来代替&nbsp;<code>GET</code>&nbsp;方法。&nbsp;<code>HEAD</code>&nbsp;请求没有返回体，只返回一个 HTTP 请求报头：</p>\n\n<pre>\ncurl -i -XHEAD http://localhost:9200/website/blog/123</pre>\n\n<p>如果文档存在， Elasticsearch 将返回一个&nbsp;<code>200 ok</code>&nbsp;的状态码：</p>\n\n<pre>\nHTTP/1.1 200 OK\nContent-Type: text/plain; charset=UTF-8\nContent-Length: 0</pre>\n\n<p>若文档不存在， Elasticsearch 将返回一个&nbsp;<code>404 Not Found</code>&nbsp;的状态码：</p>\n\n<pre>\ncurl -i -XHEAD http://localhost:9200/website/blog/124</pre>\n\n<pre>\nHTTP/1.1 404 Not Found\nContent-Type: text/plain; charset=UTF-8\nContent-Length: 0</pre>\n\n<p>当然，一个文档仅仅是在检查的时候不存在，并不意味着一毫秒之后它也不存在：也许同时正好另一个进程就创建了该文档。</p>\n\n<h2>更新整个文档</h2>\n\n<p>在 Elasticsearch 中文档是&nbsp;<em>不可改变</em>&nbsp;的，不能修改它们。&nbsp;相反，如果想要更新现有的文档，需要&nbsp;<em>重建索引</em>或者进行替换。</p>\n\n<pre>\n<code class=\"language-json\">PUT /website/blog/123\n{\n  \"title\": \"My first blog entry\",\n  \"text\":  \"I am starting to get the hang of this...\",\n  \"date\":  \"2014/01/02\"\n}</code></pre>\n\n<p>在响应体中，我们能看到 Elasticsearch 已经增加了&nbsp;<code>_version</code>&nbsp;字段值：</p>\n\n<pre>\n<code class=\"language-json\">{\n  \"_index\" :   \"website\",\n  \"_type\" :    \"blog\",\n  \"_id\" :      \"123\",\n  \"_version\" : 2,\n  \"created\":   false \n}</code></pre>\n\n<table border=\"0\" summary=\"Callout list\">\n	<tbody>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/update-doc.html#CO10-1\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p><code>created</code>&nbsp;标志设置成&nbsp;<code>false</code>&nbsp;，是因为相同的索引、类型和 ID 的文档已经存在。</p>\n			</td>\n		</tr>\n	</tbody>\n</table>\n\n<p>在内部，Elasticsearch 已将旧文档标记为已删除，并增加一个全新的文档。&nbsp;尽管你不能再对旧版本的文档进行访问，但它并不会立即消失。当继续索引更多的数据，Elasticsearch 会在后台清理这些已删除文档。</p>\n\n<ol style=\"list-style-type:decimal\">\n	<li>从旧文档构建 JSON</li>\n	<li>更改该 JSON</li>\n	<li>删除旧文档</li>\n	<li>索引一个新文档</li>\n</ol>\n\n<p>唯一的区别在于,&nbsp;<code>update</code>&nbsp;API 仅仅通过一个客户端请求来实现这些步骤，而不需要单独的&nbsp;<code>get</code>&nbsp;和&nbsp;<code>index</code>&nbsp;请求。</p>\n\n<h2>创建新文档</h2>\n\n<p>当我们索引一个文档，&nbsp;怎么确认我们正在创建一个完全新的文档，而不是覆盖现有的呢？</p>\n\n<p>请记住，&nbsp;<code>_index</code>&nbsp;、&nbsp;<code>_type</code>&nbsp;和&nbsp;<code>_id</code>&nbsp;的组合可以唯一标识一个文档。所以，确保创建一个新文档的最简单办法是，使用索引请求的&nbsp;<code>POST</code>&nbsp;形式让 Elasticsearch 自动生成唯一&nbsp;<code>_id</code>&nbsp;:</p>\n\n<pre>\nPOST /website/blog/\n{ ... }</pre>\n\n<p>然而，如果已经有自己的&nbsp;<code>_id</code>&nbsp;，那么我们必须告诉 Elasticsearch ，只有在相同的&nbsp;<code>_index</code>&nbsp;、&nbsp;<code>_type</code>&nbsp;和<code>_id</code>&nbsp;不存在时才接受我们的索引请求。这里有两种方式，他们做的实际是相同的事情。使用哪种，取决于哪种使用起来更方便。</p>\n\n<p>第一种方法使用&nbsp;<code>op_type</code>&nbsp;查询&nbsp;-字符串参数：</p>\n\n<pre>\nPUT /website/blog/123?op_type=create\n{ ... }</pre>\n\n<p>第二种方法是在 URL 末端使用&nbsp;<code>/_create</code>&nbsp;:</p>\n\n<pre>\nPUT /website/blog/123/_create\n{ ... }</pre>\n\n<p>如果创建新文档的请求成功执行，Elasticsearch 会返回元数据和一个&nbsp;<code>201 Created</code>&nbsp;的 HTTP 响应码。</p>\n\n<p>另一方面，如果具有相同的&nbsp;<code>_index</code>&nbsp;、&nbsp;<code>_type</code>&nbsp;和&nbsp;<code>_id</code>&nbsp;的文档已经存在，Elasticsearch 将会返回&nbsp;<code>409 Conflict</code>&nbsp;响应码，以及如下的错误信息：</p>\n\n<pre>\n<code class=\"language-json\">{\n   \"error\": {\n      \"root_cause\": [\n         {\n            \"type\": \"document_already_exists_exception\",\n            \"reason\": \"[blog][123]: document already exists\",\n            \"shard\": \"0\",\n            \"index\": \"website\"\n         }\n      ],\n      \"type\": \"document_already_exists_exception\",\n      \"reason\": \"[blog][123]: document already exists\",\n      \"shard\": \"0\",\n      \"index\": \"website\"\n   },\n   \"status\": 409\n}</code></pre>\n\n<h2>删除文档</h2>\n\n<p>删除文档&nbsp;的语法和我们所知道的规则相同，只是&nbsp;使用&nbsp;<code>DELETE</code>&nbsp;方法：</p>\n\n<pre>\nDELETE /website/blog/123</pre>\n\n<p>如果找到该文档，Elasticsearch 将要返回一个&nbsp;<code>200 ok</code>&nbsp;的 HTTP 响应码，和一个类似以下结构的响应体。注意，字段&nbsp;<code>_version</code>&nbsp;值已经增加:</p>\n\n<pre>\n<code class=\"language-json\">{\n  \"found\" :    true,\n  \"_index\" :   \"website\",\n  \"_type\" :    \"blog\",\n  \"_id\" :      \"123\",\n  \"_version\" : 3\n}</code></pre>\n\n<p>如果文档没有&nbsp;找到，我们将得到&nbsp;<code>404 Not Found</code>&nbsp;的响应码和类似这样的响应体：</p>\n\n<pre>\n<code class=\"language-json\">{\n  \"found\" :    false,\n  \"_index\" :   \"website\",\n  \"_type\" :    \"blog\",\n  \"_id\" :      \"123\",\n  \"_version\" : 4\n}</code></pre>\n\n<p>即使文档不存在（&nbsp;<code>Found</code>&nbsp;是&nbsp;<code>false</code>&nbsp;），&nbsp;<code>_version</code>&nbsp;值仍然会增加。这是 Elasticsearch 内部记录本的一部分，用来确保这些改变在跨多节点时以正确的顺序执行。</p>\n\n<p><img alt=\"注意\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/note.png\" /></p>\n\n<p>删除文档不会立即将文档从磁盘中删除，只是将文档标记为已删除状态。随着你不断的索引更多的数据，Elasticsearch 将会在后台清理标记为已删除的文档。</p>\n\n<h2>处理冲突</h2>\n\n<p>当我们使用&nbsp;<code>index</code>&nbsp;API 更新文档&nbsp;，可以一次性读取原始文档，做我们的修改，然后重新索引&nbsp;<em>整个文档</em>&nbsp;。 最近的索引请求将获胜：无论最后哪一个文档被索引，都将被唯一存储在 Elasticsearch 中。如果其他人同时更改这个文档，他们的更改将丢失。</p>\n\n<p>很多时候这是没有问题的。也许我们的主数据存储是一个关系型数据库，我们只是将数据复制到 Elasticsearch 中并使其可被搜索。 也许两个人同时更改相同的文档的几率很小。或者对于我们的业务来说偶尔丢失更改并不是很严重的问题。</p>\n\n<p>但有时丢失了一个变更就是&nbsp;<em>非常严重的</em>&nbsp;。试想我们使用 Elasticsearch 存储我们网上商城商品库存的数量， 每次我们卖一个商品的时候，我们在 Elasticsearch 中将库存数量减少。</p>\n\n<p>有一天，管理层决定做一次促销。突然地，我们一秒要卖好几个商品。 假设有两个 web 程序并行运行，每一个都同时处理所有商品的销售，如图&nbsp;7 &ldquo;Consequence of no concurrency control&rdquo;&nbsp;所示。</p>\n\n<p><strong>图&nbsp;7.&nbsp;Consequence of no concurrency control</strong></p>\n\n<p><img alt=\"Consequence of no concurrency control\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/elas_0301.png\" style=\"width:270px\" /></p>\n\n<p>&nbsp;</p>\n\n<p><code>web_1</code>&nbsp;对&nbsp;<code>stock_count</code>&nbsp;所做的更改已经丢失，因为&nbsp;<code>web_2</code>&nbsp;不知道它的&nbsp;<code>stock_count</code>&nbsp;的拷贝已经过期。 结果我们会认为有超过商品的实际数量的库存，因为卖给顾客的库存商品并不存在，我们将让他们非常失望。</p>\n\n<p>变更越频繁，读数据和更新数据的间隙越长，也就越可能丢失变更。</p>\n\n<p>在数据库领域中，有两种方法通常被用来确保并发更新时变更不会丢失：</p>\n\n<p><em>悲观并发控制</em></p>\n\n<p>这种方法被关系型数据库广泛使用，它假定有变更冲突可能发生，因此阻塞访问资源以防止冲突。 一个典型的例子是读取一行数据之前先将其锁住，确保只有放置锁的线程能够对这行数据进行修改。</p>\n\n<p><em>乐观并发控制</em></p>\n\n<p>Elasticsearch 中使用的这种方法假定冲突是不可能发生的，并且不会阻塞正在尝试的操作。 然而，如果源数据在读写当中被修改，更新将会失败。应用程序接下来将决定该如何解决冲突。 例如，可以重试更新、使用新的数据、或者将相关情况报告给用户。</p>\n\n<h2>乐观并发控制</h2>\n\n<p>Elasticsearch 是分布式的。当文档创建、更新或删除时，&nbsp;新版本的文档必须复制到集群中的其他节点。Elasticsearch 也是异步和并发的，这意味着这些复制请求被并行发送，并且到达目的地时也许&nbsp;<em>顺序是乱的</em>。 Elasticsearch 需要一种方法确保文档的旧版本不会覆盖新的版本。</p>\n\n<p>当我们之前讨论&nbsp;<code>index</code>&nbsp;，&nbsp;<code>GET</code>&nbsp;和&nbsp;<code>delete</code>&nbsp;请求时，我们指出每个文档都有一个&nbsp;<code>_version</code>&nbsp;（版本）号，当文档被修改时版本号递增。 Elasticsearch 使用这个&nbsp;<code>_version</code>&nbsp;号来确保变更以正确顺序得到执行。如果旧版本的文档在新版本之后到达，它可以被简单的忽略。</p>\n\n<p>我们可以利用&nbsp;<code>_version</code>&nbsp;号来确保&nbsp;应用中相互冲突的变更不会导致数据丢失。我们通过指定想要修改文档的&nbsp;<code>version</code>&nbsp;号来达到这个目的。 如果该版本不是当前版本号，我们的请求将会失败。</p>\n\n<p>让我们创建一个新的博客文章：</p>\n\n<pre>\n<code class=\"language-json\">PUT /website/blog/1/_create\n{\n  \"title\": \"My first blog entry\",\n  \"text\":  \"Just trying this out...\"\n}</code></pre>\n\n<p>响应体告诉我们，这个新创建的文档&nbsp;<code>_version</code>&nbsp;版本号是&nbsp;<code>1</code>&nbsp;。现在假设我们想编辑这个文档：我们加载其数据到 web 表单中， 做一些修改，然后保存新的版本。</p>\n\n<p>首先我们检索文档:</p>\n\n<pre>\nGET /website/blog/1</pre>\n\n<p>响应体包含相同的&nbsp;<code>_version</code>&nbsp;版本号&nbsp;<code>1</code>&nbsp;：</p>\n\n<pre>\n<code class=\"language-json\">{\n  \"_index\" :   \"website\",\n  \"_type\" :    \"blog\",\n  \"_id\" :      \"1\",\n  \"_version\" : 1,\n  \"found\" :    true,\n  \"_source\" :  {\n      \"title\": \"My first blog entry\",\n      \"text\":  \"Just trying this out...\"\n  }\n}</code></pre>\n\n<p>现在，当我们尝试通过重建文档的索引来保存修改，我们指定&nbsp;<code>version</code>&nbsp;为我们的修改会被应用的版本：</p>\n\n<pre>\n<code class=\"language-json\">PUT /website/blog/1?version=1 \n{\n  \"title\": \"My first blog entry\",\n  \"text\":  \"Starting to get the hang of this...\"\n}</code></pre>\n\n<table border=\"0\" summary=\"Callout list\">\n	<tbody>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/optimistic-concurrency-control.html#CO11-1\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p>我们想这个在我们索引中的文档只有现在的&nbsp;<code>_version</code>&nbsp;为&nbsp;<code>1</code>&nbsp;时，本次更新才能成功。</p>\n			</td>\n		</tr>\n	</tbody>\n</table>\n\n<p>此请求成功，并且响应体告诉我们&nbsp;<code>_version</code>&nbsp;已经递增到&nbsp;<code>2</code>&nbsp;：</p>\n\n<pre>\n<code class=\"language-json\">{\n  \"_index\":   \"website\",\n  \"_type\":    \"blog\",\n  \"_id\":      \"1\",\n  \"_version\": 2\n  \"created\":  false\n}</code></pre>\n\n<p>然而，如果我们重新运行相同的索引请求，仍然指定&nbsp;<code>version=1</code>&nbsp;， Elasticsearch 返回&nbsp;<code>409 Conflict</code>HTTP 响应码，和一个如下所示的响应体：</p>\n\n<pre>\n<code class=\"language-json\">{\n   \"error\": {\n      \"root_cause\": [\n         {\n            \"type\": \"version_conflict_engine_exception\",\n            \"reason\": \"[blog][1]: version conflict, current [2], provided [1]\",\n            \"index\": \"website\",\n            \"shard\": \"3\"\n         }\n      ],\n      \"type\": \"version_conflict_engine_exception\",\n      \"reason\": \"[blog][1]: version conflict, current [2], provided [1]\",\n      \"index\": \"website\",\n      \"shard\": \"3\"\n   },\n   \"status\": 409\n}</code></pre>\n\n<p>这告诉我们在 Elasticsearch 中这个文档的当前&nbsp;<code>_version</code>&nbsp;号是&nbsp;<code>2</code>&nbsp;，但我们指定的更新版本号为&nbsp;<code>1</code>&nbsp;。</p>\n\n<p>我们现在怎么做取决于我们的应用需求。我们可以告诉用户说其他人已经修改了文档，并且在再次保存之前检查这些修改内容。 或者，在之前的商品&nbsp;<code>stock_count</code>&nbsp;场景，我们可以获取到最新的文档并尝试重新应用这些修改。</p>\n\n<p>所有文档的更新或删除 API，都可以接受&nbsp;<code>version</code>&nbsp;参数，这允许你在代码中使用乐观的并发控制，这是一种明智的做法。</p>\n\n<h3>通过外部系统使用版本控制</h3>\n\n<p>一个常见的设置是使用其它数据库作为主要的数据存储，使用 Elasticsearch 做数据检索，&nbsp;这意味着主数据库的所有更改发生时都需要被复制到 Elasticsearch ，如果多个进程负责这一数据同步，你可能遇到类似于之前描述的并发问题。</p>\n\n<p>如果你的主数据库已经有了版本号&thinsp;&mdash;&thinsp;或一个能作为版本号的字段值比如&nbsp;<code>timestamp</code>&thinsp;&mdash;&thinsp;那么你就可以在 Elasticsearch 中通过增加&nbsp;<code>version_type=external</code>&nbsp;到查询字符串的方式重用这些相同的版本号，&nbsp;版本号必须是大于零的整数， 且小于&nbsp;<code>9.2E+18</code>&thinsp;&mdash;&thinsp;一个 Java 中&nbsp;<code>long</code>&nbsp;类型的正值。</p>\n\n<p>外部版本号的处理方式和我们之前讨论的内部版本号的处理方式有些不同， Elasticsearch 不是检查当前<code>_version</code>&nbsp;和请求中指定的版本号是否相同， 而是检查当前&nbsp;<code>_version</code>&nbsp;是否&nbsp;<em>小于</em>&nbsp;指定的版本号。 如果请求成功，外部的版本号作为文档的新&nbsp;<code>_version</code>&nbsp;进行存储。</p>\n\n<p>外部版本号不仅在索引和删除请求是可以指定，而且在&nbsp;<em>创建</em>&nbsp;新文档时也可以指定。</p>\n\n<p>例如，要创建一个新的具有外部版本号&nbsp;<code>5</code>&nbsp;的博客文章，我们可以按以下方法进行：</p>\n\n<pre>\n<code class=\"language-json\">PUT /website/blog/2?version=5&amp;version_type=external\n{\n  \"title\": \"My first external blog entry\",\n  \"text\":  \"Starting to get the hang of this...\"\n}</code></pre>\n\n<p>在响应中，我们能看到当前的&nbsp;<code>_version</code>&nbsp;版本号是&nbsp;<code>5</code>&nbsp;：</p>\n\n<pre>\n<code class=\"language-json\">{\n  \"_index\":   \"website\",\n  \"_type\":    \"blog\",\n  \"_id\":      \"2\",\n  \"_version\": 5,\n  \"created\":  true\n}</code></pre>\n\n<p>现在我们更新这个文档，指定一个新的&nbsp;<code>version</code>&nbsp;号是&nbsp;<code>10</code>&nbsp;：</p>\n\n<pre>\n<code class=\"language-json\">PUT /website/blog/2?version=10&amp;version_type=external\n{\n  \"title\": \"My first external blog entry\",\n  \"text\":  \"This is a piece of cake...\"\n}</code></pre>\n\n<p>请求成功并将当前&nbsp;<code>_version</code>&nbsp;设为&nbsp;<code>10</code>&nbsp;：</p>\n\n<pre>\n<code class=\"language-json\">{\n  \"_index\":   \"website\",\n  \"_type\":    \"blog\",\n  \"_id\":      \"2\",\n  \"_version\": 10,\n  \"created\":  false\n}</code></pre>\n\n<p>如果你要重新运行此请求时，它将会失败，并返回像我们之前看到的同样的冲突错误， 因为指定的外部版本号不大于 Elasticsearch 的当前版本号。</p>\n\n<h2>文档的部分更新</h2>\n\n<p>我们也介绍过文档是不可变的：他们不能被修改，只能被替换。&nbsp;<code>update</code>&nbsp;API 必须遵循同样的规则。 从外部来看，我们在一个文档的某个位置进行部分更新。然而在内部，&nbsp;<code>update</code>&nbsp;API 简单使用与之前描述相同的<em>检索-修改-重建索引</em>&nbsp;的处理过程。 区别在于这个过程发生在分片内部，这样就避免了多次请求的网络开销。通过减少检索和重建索引步骤之间的时间，我们也减少了其他进程的变更带来冲突的可能性。</p>\n\n<p><code>update</code>&nbsp;请求最简单的一种形式是接收文档的一部分作为&nbsp;<code>doc</code>&nbsp;的参数， 它只是与现有的文档进行合并。对象被合并到一起，覆盖现有的字段，增加新的字段。 例如，我们增加字段&nbsp;<code>tags</code>&nbsp;和&nbsp;<code>views</code>&nbsp;到我们的博客文章，如下所示：</p>\n\n<pre>\n<code class=\"language-json\">POST /website/blog/1/_update\n{\n   \"doc\" : {\n      \"tags\" : [ \"testing\" ],\n      \"views\": 0\n   }\n}</code></pre>\n\n<p>如果请求成功，我们看到类似于&nbsp;<code>index</code>&nbsp;请求的响应：</p>\n\n<pre>\n<code class=\"language-json\">{\n   \"_index\" :   \"website\",\n   \"_id\" :      \"1\",\n   \"_type\" :    \"blog\",\n   \"_version\" : 3\n}</code></pre>\n\n<p>检索文档显示了更新后的&nbsp;<code>_source</code>&nbsp;字段：</p>\n\n<pre>\n<code class=\"language-json\">{\n   \"_index\":    \"website\",\n   \"_type\":     \"blog\",\n   \"_id\":       \"1\",\n   \"_version\":  3,\n   \"found\":     true,\n   \"_source\": {\n      \"title\":  \"My first blog entry\",\n      \"text\":   \"Starting to get the hang of this...\",\n      \"tags\": [ \"testing\" ], \n      \"views\":  0 \n   }\n}</code></pre>\n\n<table border=\"0\" summary=\"Callout list\">\n	<tbody>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/partial-updates.html#CO12-1\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" /></a>&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/partial-updates.html#CO12-2\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/2.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p>新的字段已被添加到&nbsp;<code>_source</code>&nbsp;中。</p>\n			</td>\n		</tr>\n	</tbody>\n</table>\n\n<h3>使用脚本部分更新文档</h3>\n\n<p>脚本可以在&nbsp;<code>update</code>&nbsp;API中用来改变&nbsp;<code>_source</code>&nbsp;的字段内容，&nbsp;它在更新脚本中称为&nbsp;<code>ctx._source</code>&nbsp;。 例如，我们可以使用脚本来增加博客文章中&nbsp;<code>views</code>&nbsp;的数量：</p>\n\n<pre>\n<code class=\"language-json\">POST /website/blog/1/_update\n{\n   \"script\" : \"ctx._source.views+=1\"\n}</code></pre>\n\n<p><strong>用 Groovy 脚本编程</strong></p>\n\n<p>对于那些&nbsp;API 不能满足需求的情况，Elasticsearch 允许你使用脚本编写自定义的逻辑。&nbsp;许多API都支持脚本的使用，包括搜索、排序、聚合和文档更新。 脚本可以作为请求的一部分被传递，从特殊的 .scripts 索引中检索，或者从磁盘加载脚本。</p>\n\n<p>默认的脚本语言&nbsp;是&nbsp;Groovy，一种快速表达的脚本语言，在语法上与 JavaScript 类似。 它在 Elasticsearch V1.3.0 版本首次引入并运行在&nbsp;<em>沙盒</em>&nbsp;中，然而 Groovy 脚本引擎存在漏洞， 允许攻击者通过构建 Groovy 脚本，在 Elasticsearch Java VM 运行时脱离沙盒并执行 shell 命令。</p>\n\n<p>因此，在版本 v1.3.8 、 1.4.3 和 V1.5.0 及更高的版本中，它已经被默认禁用。 此外，您可以通过设置集群中的所有节点的&nbsp;<code>config/elasticsearch.yml</code>&nbsp;文件来禁用动态 Groovy 脚本：</p>\n\n<pre>\nscript.groovy.sandbox.enabled: false</pre>\n\n<p>这将关闭 Groovy 沙盒，从而防止动态 Groovy 脚本作为请求的一部分被接受， 或者从特殊的<code>.scripts</code>&nbsp;索引中被检索。当然，你仍然可以使用存储在每个节点的&nbsp;<code>config/scripts/</code>&nbsp;目录下的 Groovy 脚本。</p>\n\n<p>如果你的架构和安全性不需要担心漏洞攻击，例如你的 Elasticsearch 终端仅暴露和提供给可信赖的应用， 当它是你的应用需要的特性时，你可以选择重新启用动态脚本。</p>\n\n<p>我们也可以通过使用脚本给&nbsp;<code>tags</code>&nbsp;数组添加一个新的标签。在这个例子中，我们指定新的标签作为参数，而不是硬编码到脚本内部。 这使得 Elasticsearch 可以重用这个脚本，而不是每次我们想添加标签时都要对新脚本重新编译：</p>\n\n<pre>\n<code class=\"language-json\">POST /website/blog/1/_update\n{\n   \"script\" : \"ctx._source.tags+=new_tag\",\n   \"params\" : {\n      \"new_tag\" : \"search\"\n   }\n}</code></pre>\n\n<p>获取文档并显示最后两次请求的效果：</p>\n\n<pre>\n<code class=\"language-json\">{\n   \"_index\":    \"website\",\n   \"_type\":     \"blog\",\n   \"_id\":       \"1\",\n   \"_version\":  5,\n   \"found\":     true,\n   \"_source\": {\n      \"title\":  \"My first blog entry\",\n      \"text\":   \"Starting to get the hang of this...\",\n      \"tags\":  [\"testing\", \"search\"], \n      \"views\":  1 \n   }\n}</code></pre>\n\n<table border=\"0\" summary=\"Callout list\">\n	<tbody>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/partial-updates.html#CO13-1\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p><code>search</code>&nbsp;标签已追加到&nbsp;<code>tags</code>&nbsp;数组中。</p>\n			</td>\n		</tr>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/partial-updates.html#CO13-2\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/2.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p><code>views</code>&nbsp;字段已递增。</p>\n			</td>\n		</tr>\n	</tbody>\n</table>\n\n<p>我们甚至可以选择通过设置&nbsp;<code>ctx.op</code>&nbsp;为&nbsp;<code>delete</code>&nbsp;来删除基于其内容的文档：</p>\n\n<pre>\n<code class=\"language-json\">POST /website/blog/1/_update\n{\n   \"script\" : \"ctx.op = ctx._source.views == count ? \'delete\' : \'none\'\",\n    \"params\" : {\n        \"count\": 1\n    }\n}</code></pre>\n\n<p>更新的文档可能尚不存在</p>\n\n<p>假设我们需要&nbsp;在 Elasticsearch 中存储一个页面访问量计数器。 每当有用户浏览网页，我们对该页面的计数器进行累加。但是，如果它是一个新网页，我们不能确定计数器已经存在。 如果我们尝试更新一个不存在的文档，那么更新操作将会失败。</p>\n\n<p>在这样的情况下，我们可以使用&nbsp;<code>upsert</code>&nbsp;参数，指定如果文档不存在就应该先创建它：</p>\n\n<pre>\n<code class=\"language-json\">POST /website/pageviews/1/_update\n{\n   \"script\" : \"ctx._source.views+=1\",\n   \"upsert\": {\n       \"views\": 1\n   }\n}</code></pre>\n\n<p>我们第一次运行这个请求时，&nbsp;<code>upsert</code>&nbsp;值作为新文档被索引，初始化&nbsp;<code>views</code>&nbsp;字段为&nbsp;<code>1</code>&nbsp;。 在后续的运行中，由于文档已经存在，&nbsp;<code>script</code>&nbsp;更新操作将替代&nbsp;<code>upsert</code>&nbsp;进行应用，对&nbsp;<code>views</code>&nbsp;计数器进行累加。</p>\n\n<h3>更新和冲突</h3>\n\n<p>在本节的介绍中，我们说明&nbsp;<em>检索</em>&nbsp;和&nbsp;<em>重建索引</em>&nbsp;步骤的间隔越小，变更冲突的机会越小。 但是它并不能完全消除冲突的可能性。 还是有可能在&nbsp;<code>update</code>&nbsp;设法重新索引之前，来自另一进程的请求修改了文档。</p>\n\n<p>为了避免数据丢失，&nbsp;<code>update</code>&nbsp;API 在&nbsp;<em>检索</em>&nbsp;步骤时检索得到文档当前的&nbsp;<code>_version</code>&nbsp;号，并传递版本号到&nbsp;<em>重建索引</em>&nbsp;步骤的&nbsp;<code>index</code>&nbsp;请求。 如果另一个进程修改了处于检索和重新索引步骤之间的文档，那么&nbsp;<code>_version</code>&nbsp;号将不匹配，更新请求将会失败。</p>\n\n<p>对于部分更新的很多使用场景，文档已经被改变也没有关系。 例如，如果两个进程都对页面访问量计数器进行递增操作，它们发生的先后顺序其实不太重要； 如果冲突发生了，我们唯一需要做的就是尝试再次更新。</p>\n\n<p>这可以通过&nbsp;设置参数&nbsp;<code>retry_on_conflict</code>&nbsp;来自动完成， 这个参数规定了失败之前&nbsp;<code>update</code>&nbsp;应该重试的次数，它的默认值为&nbsp;<code>0</code>&nbsp;。</p>\n\n<pre>\n<code class=\"language-json\">POST /website/pageviews/1/_update?retry_on_conflict=5 \n{\n   \"script\" : \"ctx._source.views+=1\",\n   \"upsert\": {\n       \"views\": 0\n   }\n}</code></pre>\n\n<table border=\"0\" summary=\"Callout list\">\n	<tbody>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/partial-updates.html#CO14-1\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p>失败之前重试该更新5次。</p>\n			</td>\n		</tr>\n	</tbody>\n</table>\n\n<p>在增量操作无关顺序的场景，例如递增计数器等这个方法十分有效，但是在其他情况下变更的顺序&nbsp;<em>是</em>&nbsp;非常重要的。 &nbsp;<code>update</code>&nbsp;API 默认采用&nbsp;<em>最终写入生效</em>&nbsp;的方案，但它也接受一个&nbsp;<code>version</code>&nbsp;参数来允许&nbsp;指定想要更新文档的版本。</p>\n\n<h2>取回多个文档</h2>\n\n<p>Elasticsearch 的速度已经很快了，但甚至能更快。&nbsp;将多个请求合并成一个，避免单独处理每个请求花费的网络延时和开销。 如果你需要从 Elasticsearch 检索很多文档，那么使用&nbsp;<em>multi-get</em>&nbsp;或者&nbsp;<code>mget</code>&nbsp;API&nbsp;来将这些检索请求放在一个请求中，将比逐个文档请求更快地检索到全部文档。</p>\n\n<p><code>mget</code>&nbsp;API 要求有一个&nbsp;<code>docs</code>&nbsp;数组作为参数，每个&nbsp;元素包含需要检索文档的元数据， 包括&nbsp;<code>_index</code>&nbsp;、&nbsp;<code>_type</code>和&nbsp;<code>_id</code>&nbsp;。如果你想检索一个或者多个特定的字段，那么你可以通过&nbsp;<code>_source</code>&nbsp;参数来指定这些字段的名字：</p>\n\n<pre>\n<code class=\"language-json\">GET /_mget\n{\n   \"docs\" : [\n      {\n         \"_index\" : \"website\",\n         \"_type\" :  \"blog\",\n         \"_id\" :    2\n      },\n      {\n         \"_index\" : \"website\",\n         \"_type\" :  \"pageviews\",\n         \"_id\" :    1,\n         \"_source\": \"views\"\n      }\n   ]\n}</code></pre>\n\n<p>该响应体也包含一个&nbsp;<code>docs</code>&nbsp;数组&nbsp;， 对于每一个在请求中指定的文档，这个数组中都包含有一个对应的响应，且顺序与请求中的顺序相同。 其中的每一个响应都和使用单个&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/get-doc.html\" title=\"取回一个文档\"><code>get</code>&nbsp;request</a>&nbsp;请求所得到的响应体相同：</p>\n\n<pre>\n<code class=\"language-json\">{\n   \"docs\" : [\n      {\n         \"_index\" :   \"website\",\n         \"_id\" :      \"2\",\n         \"_type\" :    \"blog\",\n         \"found\" :    true,\n         \"_source\" : {\n            \"text\" :  \"This is a piece of cake...\",\n            \"title\" : \"My first external blog entry\"\n         },\n         \"_version\" : 10\n      },\n      {\n         \"_index\" :   \"website\",\n         \"_id\" :      \"1\",\n         \"_type\" :    \"pageviews\",\n         \"found\" :    true,\n         \"_version\" : 2,\n         \"_source\" : {\n            \"views\" : 2\n         }\n      }\n   ]\n}</code></pre>\n\n<p>如果想检索的数据都在相同的&nbsp;<code>_index</code>&nbsp;中（甚至相同的&nbsp;<code>_type</code>&nbsp;中），则可以在 URL 中指定默认的&nbsp;<code>/_index</code>或者默认的&nbsp;<code>/_index/_type</code>&nbsp;。</p>\n\n<p>你仍然可以通过单独请求覆盖这些值：</p>\n\n<pre>\n<code class=\"language-json\">GET /website/blog/_mget\n{\n   \"docs\" : [\n      { \"_id\" : 2 },\n      { \"_type\" : \"pageviews\", \"_id\" :   1 }\n   ]\n}</code></pre>\n\n<p>事实上，如果所有文档的&nbsp;<code>_index</code>&nbsp;和&nbsp;<code>_type</code>&nbsp;都是相同的，你可以只传一个&nbsp;<code>ids</code>&nbsp;数组，而不是整个&nbsp;<code>docs</code>&nbsp;数组：</p>\n\n<pre>\n<code class=\"language-json\">GET /website/blog/_mget\n{\n   \"ids\" : [ \"2\", \"1\" ]\n}</code></pre>\n\n<p>注意，我们请求的第二个文档是不存在的。我们指定类型为&nbsp;<code>blog</code>&nbsp;，但是文档 ID&nbsp;<code>1</code>&nbsp;的类型是&nbsp;<code>pageviews</code>，这个不存在的情况将在响应体中被报告：</p>\n\n<pre>\n<code class=\"language-json\">{\n  \"docs\" : [\n    {\n      \"_index\" :   \"website\",\n      \"_type\" :    \"blog\",\n      \"_id\" :      \"2\",\n      \"_version\" : 10,\n      \"found\" :    true,\n      \"_source\" : {\n        \"title\":   \"My first external blog entry\",\n        \"text\":    \"This is a piece of cake...\"\n      }\n    },\n    {\n      \"_index\" :   \"website\",\n      \"_type\" :    \"blog\",\n      \"_id\" :      \"1\",\n      \"found\" :    false  \n    }\n  ]\n}</code></pre>\n\n<table border=\"0\" summary=\"Callout list\">\n	<tbody>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/_Retrieving_Multiple_Documents.html#CO15-1\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p>未找到该文档。</p>\n			</td>\n		</tr>\n	</tbody>\n</table>\n\n<p>事实上第二个文档未能找到并不妨碍第一个文档被检索到。每个文档都是单独检索和报告的。</p>\n\n<p><img alt=\"注意\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/note.png\" /></p>\n\n<p>即使有某个文档没有找到，上述请求的 HTTP 状态码仍然是&nbsp;<code>200</code>&nbsp;。事实上，即使请求&nbsp;<em>没有</em>找到任何文档，它的状态码依然是&nbsp;<code>200</code>&nbsp;--因为&nbsp;<code>mget</code>&nbsp;请求本身已经成功执行。 为了确定某个文档查找是成功或者失败，你需要检查&nbsp;<code>found</code>&nbsp;标记。</p>\n\n<h2>代价较小的批量操作</h2>\n\n<p>与&nbsp;<code>mget</code>&nbsp;可以使我们一次取回多个文档同样的方式，&nbsp;<code>bulk</code>&nbsp;API 允许在单个步骤中进行多次&nbsp;<code>create</code>&nbsp;、<code>index</code>&nbsp;、&nbsp;<code>update</code>&nbsp;或&nbsp;<code>delete</code>&nbsp;请求。 如果你需要索引一个数据流比如日志事件，它可以排队和索引数百或数千批次。</p>\n\n<p><code>bulk</code>&nbsp;与其他的请求体格式稍有不同，如下所示：</p>\n\n<pre>\n{ action: { metadata }}\\n\n{ request body        }\\n\n{ action: { metadata }}\\n\n{ request body        }\\n\n...</pre>\n\n<p>这种格式类似一个有效的单行 JSON 文档&nbsp;<em>流</em>&nbsp;，它通过换行符(<code>\\n</code>)连接到一起。注意两个要点：</p>\n\n<ul style=\"list-style-type:disc\">\n	<li>每行一定要以换行符(<code>\\n</code>)结尾，&nbsp;<em>包括最后一行</em>&nbsp;。这些换行符被用作一个标记，可以有效分隔行。</li>\n	<li>这些行不能包含未转义的换行符，因为他们将会对解析造成干扰。这意味着这个 JSON&nbsp;<em>不</em>&nbsp;能使用 pretty 参数打印。</li>\n</ul>\n\n<p><img alt=\"提示\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/tip.png\" /></p>\n\n<p><code>action/metadata</code>&nbsp;行指定&nbsp;<em>哪一个文档</em>&nbsp;做&nbsp;<em>什么操作</em>&nbsp;。</p>\n\n<p><code>action</code>&nbsp;必须是以下选项之一:</p>\n\n<p><code>create&nbsp;</code>如果文档不存在，那么就创建它。</p>\n\n<p><code>index&nbsp;</code>创建一个新文档或者替换一个现有的文档。</p>\n\n<p><code>update&nbsp;</code>部分更新一个文档。</p>\n\n<p><code>delete&nbsp;</code>删除一个文档。</p>\n\n<p><code>metadata</code>&nbsp;应该&nbsp;指定被索引、创建、更新或者删除的文档的&nbsp;<code>_index</code>&nbsp;、&nbsp;<code>_type</code>&nbsp;和&nbsp;<code>_id</code>&nbsp;。</p>\n\n<p>例如，一个&nbsp;<code>delete</code>&nbsp;请求看起来是这样的：</p>\n\n<pre>\n{ &quot;delete&quot;: { &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;blog&quot;, &quot;_id&quot;: &quot;123&quot; }}</pre>\n\n<p><code>request body</code>&nbsp;行由文档的&nbsp;<code>_source</code>&nbsp;本身组成--文档包含的字段和值。它是&nbsp;<code>index</code>&nbsp;和&nbsp;<code>create</code>&nbsp;操作所必需的，这是有道理的：你必须提供文档以索引。</p>\n\n<p>它也是&nbsp;<code>update</code>&nbsp;操作所必需的，并且应该包含你传递给&nbsp;<code>update</code>&nbsp;API 的相同请求体：&nbsp;<code>doc</code>&nbsp;、&nbsp;<code>upsert</code>&nbsp;、<code>script</code>&nbsp;等等。 删除操作不需要&nbsp;<code>request body</code>&nbsp;行。</p>\n\n<pre>\n{ &quot;create&quot;:  { &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;blog&quot;, &quot;_id&quot;: &quot;123&quot; }}\n{ &quot;title&quot;:    &quot;My first blog post&quot; }</pre>\n\n<p>如果不指定&nbsp;<code>_id</code>&nbsp;，将会自动生成一个 ID ：</p>\n\n<pre>\n{ &quot;index&quot;: { &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;blog&quot; }}\n{ &quot;title&quot;:    &quot;My second blog post&quot; }</pre>\n\n<p>为了把所有的操作组合在一起，一个完整的&nbsp;<code>bulk</code>&nbsp;请求&nbsp;有以下形式:</p>\n\n<pre>\nPOST /_bulk\n{ &quot;delete&quot;: { &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;blog&quot;, &quot;_id&quot;: &quot;123&quot; }} <img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" />\n{ &quot;create&quot;: { &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;blog&quot;, &quot;_id&quot;: &quot;123&quot; }}\n{ &quot;title&quot;:    &quot;My first blog post&quot; }\n{ &quot;index&quot;:  { &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;blog&quot; }}\n{ &quot;title&quot;:    &quot;My second blog post&quot; }\n{ &quot;update&quot;: { &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;blog&quot;, &quot;_id&quot;: &quot;123&quot;, &quot;_retry_on_conflict&quot; : 3} }\n{ &quot;doc&quot; : {&quot;title&quot; : &quot;My updated blog post&quot;} } <img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/2.png\" /></pre>\n\n<table border=\"0\" summary=\"Callout list\">\n	<tbody>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/bulk.html#CO16-1\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p>请注意&nbsp;<code>delete</code>&nbsp;动作不能有请求体,它后面跟着的是另外一个操作。</p>\n			</td>\n		</tr>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/bulk.html#CO16-2\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/2.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p>谨记最后一个换行符不要落下。</p>\n			</td>\n		</tr>\n	</tbody>\n</table>\n\n<p>这个 Elasticsearch 响应包含&nbsp;<code>items</code>&nbsp;数组，&nbsp;这个数组的内容是以请求的顺序列出来的每个请求的结果。</p>\n\n<pre>\n<code class=\"language-json\">{\n   \"took\": 4,\n   \"errors\": false, \n   \"items\": [\n      {  \"delete\": {\n            \"_index\":   \"website\",\n            \"_type\":    \"blog\",\n            \"_id\":      \"123\",\n            \"_version\": 2,\n            \"status\":   200,\n            \"found\":    true\n      }},\n      {  \"create\": {\n            \"_index\":   \"website\",\n            \"_type\":    \"blog\",\n            \"_id\":      \"123\",\n            \"_version\": 3,\n            \"status\":   201\n      }},\n      {  \"create\": {\n            \"_index\":   \"website\",\n            \"_type\":    \"blog\",\n            \"_id\":      \"EiwfApScQiiy7TIKFxRCTw\",\n            \"_version\": 1,\n            \"status\":   201\n      }},\n      {  \"update\": {\n            \"_index\":   \"website\",\n            \"_type\":    \"blog\",\n            \"_id\":      \"123\",\n            \"_version\": 4,\n            \"status\":   200\n      }}\n   ]\n}</code></pre>\n\n<table border=\"0\" summary=\"Callout list\">\n	<tbody>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/bulk.html#CO17-1\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p>所有的子请求都成功完成。</p>\n			</td>\n		</tr>\n	</tbody>\n</table>\n\n<p>每个子请求都是独立执行，因此某个子请求的失败不会对其他子请求的成功与否造成影响。 如果其中任何子请求失败，最顶层的&nbsp;<code>error</code>&nbsp;标志被设置为&nbsp;<code>true</code>&nbsp;，并且在相应的请求报告出错误明细：</p>\n\n<pre>\nPOST /_bulk\n{ &quot;create&quot;: { &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;blog&quot;, &quot;_id&quot;: &quot;123&quot; }}\n{ &quot;title&quot;:    &quot;Cannot create - it already exists&quot; }\n{ &quot;index&quot;:  { &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;blog&quot;, &quot;_id&quot;: &quot;123&quot; }}\n{ &quot;title&quot;:    &quot;But we can update it&quot; }</pre>\n\n<p>在响应中，我们看到&nbsp;<code>create</code>&nbsp;文档&nbsp;<code>123</code>&nbsp;失败，因为它已经存在。但是随后的&nbsp;<code>index</code>&nbsp;请求，也是对文档&nbsp;<code>123</code>操作，就成功了：</p>\n\n<pre>\n<code class=\"language-json\">{\n   \"took\": 3,\n   \"errors\": true, \n   \"items\": [\n      {  \"create\": {\n            \"_index\":   \"website\",\n            \"_type\":    \"blog\",\n            \"_id\":      \"123\",\n            \"status\":   409, \n            \"error\":    \"DocumentAlreadyExistsException \n                        [[website][4] [blog][123]:\n                        document already exists]\"\n      }},\n      {  \"index\": {\n            \"_index\":   \"website\",\n            \"_type\":    \"blog\",\n            \"_id\":      \"123\",\n            \"_version\": 5,\n            \"status\":   200 \n      }}\n   ]\n}</code></pre>\n\n<table border=\"0\" summary=\"Callout list\">\n	<tbody>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/bulk.html#CO18-1\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p>一个或者多个请求失败。</p>\n			</td>\n		</tr>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/bulk.html#CO18-2\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/2.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p>这个请求的HTTP状态码报告为&nbsp;<code>409 CONFLICT</code>&nbsp;。</p>\n			</td>\n		</tr>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/bulk.html#CO18-3\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/3.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p>解释为什么请求失败的错误信息。</p>\n			</td>\n		</tr>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/bulk.html#CO18-4\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/4.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p>第二个请求成功，返回 HTTP 状态码&nbsp;<code>200 OK</code>&nbsp;。</p>\n			</td>\n		</tr>\n	</tbody>\n</table>\n\n<p>这也意味着&nbsp;<code>bulk</code>&nbsp;请求不是原子的： 不能用它来实现事务控制。每个请求是单独处理的，因此一个请求的成功或失败不会影响其他的请求。</p>\n\n<h3>不要重复指定Index和Type</h3>\n\n<p>也许你正在批量索引日志数据到相同的&nbsp;<code>index</code>&nbsp;和&nbsp;<code>type</code>&nbsp;中。&nbsp;但为每一个文档指定相同的元数据是一种浪费。相反，可以像&nbsp;<code>mget</code>&nbsp;API 一样，在&nbsp;<code>bulk</code>&nbsp;请求的 URL 中接收默认的&nbsp;<code>/_index</code>&nbsp;或者&nbsp;<code>/_index/_type</code>&nbsp;：</p>\n\n<pre>\n<code class=\"language-json\">POST /website/_bulk\n{ \"index\": { \"_type\": \"log\" }}\n{ \"event\": \"User logged in\" }</code></pre>\n\n<p>你仍然可以覆盖元数据行中的&nbsp;<code>_index</code>&nbsp;和&nbsp;<code>_type</code>&nbsp;, 但是它将使用 URL 中的这些元数据值作为默认值：</p>\n\n<pre>\n<code class=\"language-json\">POST /website/log/_bulk\n{ \"index\": {}}\n{ \"event\": \"User logged in\" }\n{ \"index\": { \"_type\": \"blog\" }}\n{ \"title\": \"Overriding the default type\" }</code></pre>\n\n<p>多大是太大了？</p>\n\n<p>整个批量请求都需要由接收到请求的节点加载到内存中，因此该请求越大，其他请求所能获得的内存就越少。&nbsp;批量请求的大小有一个最佳值，大于这个值，性能将不再提升，甚至会下降。 但是最佳值不是一个固定的值。它完全取决于硬件、文档的大小和复杂度、索引和搜索的负载的整体情况。</p>\n\n<p>幸运的是，很容易找到这个&nbsp;<em>最佳点</em>&nbsp;：通过批量索引典型文档，并不断增加批量大小进行尝试。 当性能开始下降，那么你的批量大小就太大了。一个好的办法是开始时将 1,000 到 5,000 个文档作为一个批次, 如果你的文档非常大，那么就减少批量的文档个数。</p>\n\n<p>密切关注你的批量请求的物理大小往往非常有用，一千个 1KB 的文档是完全不同于一千个 1MB 文档所占的物理大小。 一个好的批量大小在开始处理后所占用的物理大小约为 5-15 MB。</p>\n\n<h2>&nbsp;</h2>\n', 1, '5f199b8885e24fc8b28672b872edb606', 7, '2018-06-06 18:31:23', '2018-09-06 10:34:29');
INSERT INTO `logcontent` VALUES ('1ace9d18f30c4b368d2475a1a5035e24', -1, 'd7caeba238f0466d87db109b2b9724da', '多元线性回归笔记', '多元线性回归', '初学', '<p><strong><a id=\"多元线性回归与简单线性回归区别\" name=\"多元线性回归与简单线性回归区别\"></a>多元线性回归(multiple regression)与简单线性回归区别(simple linear regression)</strong><br />\n多个自变量(x)</p>\n\n<p><strong><a id=\"多元回归模型\" name=\"多元回归模型\"></a>多元回归模型</strong><br />\ny = &beta;<sub>0&nbsp;</sub>＋ &beta;<sub>１</sub>x<sub>1&nbsp;</sub>+ &beta;<sub>2</sub>x<sub>2&nbsp;</sub>+ ... +&beta;<sub>p</sub>x<sub>p&nbsp;</sub>+ &epsilon;<br />\n其中：&beta;<sub>0</sub>，&beta;<sub>１</sub>，&beta;<sub>2</sub>... &beta;<sub>p</sub>是参数，&epsilon;是误差值</p>\n\n<p><strong><a id=\"多元回归方程\" name=\"多元回归方程\"></a>多元回归方程</strong><br />\nE(y)=&beta;<sub>0</sub>＋&beta;<sub>１</sub>x<sub>1</sub>+&beta;<sub>2</sub>x<sub>2</sub>+ ... +&beta;<sub>p</sub>x<sub>p</sub></p>\n\n<p><strong><a id=\"估计多元回归方程\" name=\"估计多元回归方程\"></a>估计多元回归方程</strong><br />\nŷ=b<sub>0</sub>＋b<sub>１</sub>x<sub>1</sub>+b<sub>2</sub>x<sub>2</sub>+ ... +b<sub>p</sub>x<sub>p</sub></p>\n\n<p><strong><a id=\"估计流程\" name=\"估计流程\"></a>估计流程 &nbsp;(与简单线性回归类似）</strong><br />\n估计方法（使sum of squares最小）<br />\n<img src=\"http://localhost:8080/myLog/images/linear_regression/simple/10.png\" style=\"height:25px; width:90px\" /></p>\n\n<p><strong><a id=\"多元回归模型举例\" name=\"多元回归模型举例\"></a>多元回归模型举例</strong></p>\n\n<table border=\"1\" cellspacing=\"0\" style=\"border-collapse:collapse; border:2pt double #000000; height:300px; width:400pt\">\n	<tbody>\n		<tr>\n			<td style=\"background-color:#aeaaaa; height:21.38pf; text-align:center; vertical-align:middle; width:58.50pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">运输车</span></span></span></td>\n			<td style=\"background-color:#aeaaaa; height:21.38pf; text-align:center; vertical-align:middle; width:111.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">运输里程(X1)</span></span></span></td>\n			<td style=\"background-color:#aeaaaa; height:21.38pf; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">运输次数(X2)</span></span></span></td>\n			<td style=\"background-color:#aeaaaa; height:21.38pf; text-align:center; vertical-align:middle; width:119.25pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">总运输时间(Y)</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:21.38pf; text-align:center; vertical-align:middle; width:58.50pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">1</span></span></span></td>\n			<td style=\"height:21.38pf; text-align:center; vertical-align:middle; width:111.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">100</span></span></span></td>\n			<td style=\"height:21.38pf; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">4</span></span></span></td>\n			<td style=\"height:21.38pf; text-align:center; vertical-align:middle; width:119.25pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">9.3</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:40.88pf; text-align:center; vertical-align:middle; white-space:normal; width:58.50pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">2<br />\n			3</span></span></span></td>\n			<td style=\"height:40.88pf; text-align:center; vertical-align:middle; white-space:normal; width:111.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">50<br />\n			100</span></span></span></td>\n			<td style=\"height:40.88pf; text-align:center; vertical-align:middle; white-space:normal; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">3<br />\n			4</span></span></span></td>\n			<td style=\"height:40.88pf; text-align:center; vertical-align:middle; white-space:normal; width:119.25pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">4.8<br />\n			8.9</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:40.88pf; text-align:center; vertical-align:middle; white-space:normal; width:58.50pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">4<br />\n			5</span></span></span></td>\n			<td style=\"height:40.88pf; text-align:center; vertical-align:middle; white-space:normal; width:111.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">100<br />\n			50</span></span></span></td>\n			<td style=\"height:40.88pf; text-align:center; vertical-align:middle; white-space:normal; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">2<br />\n			2</span></span></span></td>\n			<td style=\"height:40.88pf; text-align:center; vertical-align:middle; white-space:normal; width:119.25pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">6.5<br />\n			4.2</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:40.88pf; text-align:center; vertical-align:middle; white-space:normal; width:58.50pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">6<br />\n			7</span></span></span></td>\n			<td style=\"height:40.88pf; text-align:center; vertical-align:middle; white-space:normal; width:111.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">80<br />\n			75</span></span></span></td>\n			<td style=\"height:40.88pf; text-align:center; vertical-align:middle; white-space:normal; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">2<br />\n			3</span></span></span></td>\n			<td style=\"height:40.88pf; text-align:center; vertical-align:middle; white-space:normal; width:119.25pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">6.2<br />\n			7.4</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:40.88pf; text-align:center; vertical-align:middle; white-space:normal; width:58.50pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">8<br />\n			9</span></span></span></td>\n			<td style=\"height:40.88pf; text-align:center; vertical-align:middle; white-space:normal; width:111.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">65<br />\n			90</span></span></span></td>\n			<td style=\"height:40.88pf; text-align:center; vertical-align:middle; white-space:normal; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">4<br />\n			3</span></span></span></td>\n			<td style=\"height:40.88pf; text-align:center; vertical-align:middle; white-space:normal; width:119.25pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">6<br />\n			7.6</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:21.38pf; text-align:center; vertical-align:middle; width:58.50pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">10</span></span></span></td>\n			<td style=\"height:21.38pf; text-align:center; vertical-align:middle; width:111.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">90</span></span></span></td>\n			<td style=\"height:21.38pf; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">2</span></span></span></td>\n			<td style=\"height:21.38pf; text-align:center; vertical-align:middle; width:119.25pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">6.1</span></span></span></td>\n		</tr>\n	</tbody>\n</table>\n\n<p>过程省略：Y =&nbsp;b<sub>0&nbsp;</sub>+ b<sub>1&nbsp;</sub>* X<sub>1</sub> + b<sub>2</sub> * X<sub>2</sub> =&nbsp;-0.869 + 0.0611 * X<sub>1</sub>&nbsp;+ 0.923 * X<sub>2</sub></p>\n\n<p><strong><a id=\"自变量中有分类型变量, 如何处理\" name=\"自变量中有分类型变量, 如何处理\"></a>如果自变量中有分类型变量(categorical data) , 如何处理？</strong></p>\n\n<table border=\"1\" cellspacing=\"0\" style=\"border-collapse:collapse; border:0.5pt solid #000000; height:112px; width:400pt\">\n	<tbody>\n		<tr>\n			<td style=\"background-color:#aeaaaa; height:21.38pf; text-align:center; vertical-align:middle; width:56.25pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">英里数(X1)</span></span></span></td>\n			<td style=\"background-color:#aeaaaa; height:21.38pf; text-align:center; vertical-align:middle; width:41.25pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">次数(X2)</span></span></span></td>\n			<td style=\"background-color:#aeaaaa; height:21.38pf; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">车型(X3)</span></span></span></td>\n			<td style=\"background-color:#aeaaaa; height:21.38pf; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">时间(Y)</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:20.44pf; text-align:center; vertical-align:middle; width:56.25pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">100</span></span></span></td>\n			<td style=\"height:20.44pf; text-align:center; vertical-align:middle; width:41.25pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">4</span></span></span></td>\n			<td style=\"height:20.44pf; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">1</span></span></span></td>\n			<td style=\"height:20.44pf; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">9.3</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:56.25pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">50</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:41.25pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">3</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">0</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">4.8</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:56.25pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">100</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:41.25pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">4</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">1</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">8.9</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:56.25pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">100</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:41.25pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">2</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">2</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">6.5</span></span></span></td>\n		</tr>\n	</tbody>\n</table>\n\n<p>由于车型中的0、1、2并不是代表数字而是代表了车型，所以需要作出转化</p>\n\n<table border=\"1\" cellspacing=\"0\" style=\"border-collapse:collapse; border:0.5pt solid #000000; height:112px; width:400pt\">\n	<tbody>\n		<tr>\n			<td style=\"background-color:#aeaaaa; height:21.38pf; text-align:center; vertical-align:middle; width:56.25pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">英里数(X1)</span></span></span></td>\n			<td style=\"background-color:#aeaaaa; height:21.38pf; text-align:center; vertical-align:middle; width:41.25pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">次数(X2)</span></span></span></td>\n			<td style=\"background-color:#aeaaaa; height:21.38pf; text-align:center; vertical-align:middle; width:48.75pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">车型0(X3)</span></span></span></td>\n			<td style=\"background-color:#aeaaaa; height:21.38pf; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">车型1(X4)</span></span></span></td>\n			<td style=\"background-color:#aeaaaa; height:21.38pf; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">车型2(X5)</span></span></span></td>\n			<td style=\"background-color:#aeaaaa; height:21.38pf; text-align:center; vertical-align:middle; width:41.25pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">时间(Y)</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:20.44pf; text-align:center; vertical-align:middle; width:56.25pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">100</span></span></span></td>\n			<td style=\"height:20.44pf; text-align:center; vertical-align:middle; width:41.25pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">4</span></span></span></td>\n			<td style=\"height:20.44pf; text-align:center; vertical-align:middle; width:48.75pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">0</span></span></span></td>\n			<td style=\"height:20.44pf; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">1</span></span></span></td>\n			<td style=\"height:20.44pf; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">0</span></span></span></td>\n			<td style=\"height:20.44pf; text-align:center; vertical-align:middle; width:41.25pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">9.3</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:56.25pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">50</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:41.25pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">3</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:48.75pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">1</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">0</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">0</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:41.25pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">4.8</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:56.25pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">100</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:41.25pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">4</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:48.75pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">0</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">1</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">0</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:41.25pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">8.9</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:56.25pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">100</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:41.25pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">2</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:48.75pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">0</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">0</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">1</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:41.25pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">6.5</span></span></span></td>\n		</tr>\n	</tbody>\n</table>\n\n<p>过程省略：Y =&nbsp;b<sub>0&nbsp;</sub>+ b<sub>1&nbsp;</sub>* X<sub>1</sub> + b<sub>2</sub> * X<sub>2</sub> + b<sub>3</sub> * X<sub>3</sub> + b<sub>4</sub> * X<sub>4</sub> + b<sub>5</sub> * X<sub>5</sub>  =&nbsp;-0.797 + 0.06X<sub>1</sub> + 0.8667X<sub>2</sub> -0.0012X<sub>3</sub> + 0.4339X<sub>4</sub> -0.4327X<sub>5</sub></p>\n', 1, '5f199b8885e24fc8b28672b872edb606', 2, '2017-11-26 20:24:09', '2018-08-31 22:21:23');
INSERT INTO `logcontent` VALUES ('1b06d5304b8948c0937042212476770b', -1, '3ba9a3aed3ce48f682974ebcf3552ce2', 'Eclipse中导出Maven项目依赖的jar包', 'maven,jar,导出', '依赖jar包导出', '<p><a id=\"导出到默认目录\" name=\"导出到默认目录\"></a>一、导出到默认目录 target/dependency</p>\n\n<p>方法1、进入工程pom.xml 所在的目录下，执行如下命令：</p>\n\n<pre>\n<code>mvn dependency:copy-dependencies</code></pre>\n\n<p>方法2、在eclipse中，选择项目的pom.xml文件，点击右键菜单中的Run As》Maven build...,在弹出的Edit Configuration窗口中，选中Goals文本框输入dependency:copy-dependencies后，点击Run运行</p>\n\n<p><a id=\"导出到自定义目录\" name=\"导出到自定义目录\"></a>二、导出到自定义目录</p>\n\n<p>在maven项目下创建lib文件夹，输入以下命令：</p>\n\n<pre>\n<code>mvn dependency:copy-dependencies -DoutputDirectory=lib</code></pre>\n\n<p>maven项目所依赖的jar包都会复制到项目目录下的lib目录下</p>\n\n<p><a id=\"设置依赖级别导出\" name=\"设置依赖级别导出\"></a>三、设置依赖级别导出</p>\n\n<p>同时可以设置依赖级别，通常使用compile级别<br />\nmvn dependency:copy-dependencies -DoutputDirectory=lib -DincludeScope=compile</p>\n', 1, '5f199b8885e24fc8b28672b872edb606', 1, '2018-07-03 11:39:52', '2018-08-31 22:21:23');
INSERT INTO `logcontent` VALUES ('1b54888f5f1040eba8234634b5387c57', 1, 'c7343568da3a4c15a29f90d9c3eabb28', 'Java设计模式', '设计模式', '设计模式', '<h2><a id=\"设计模式的六大原则\" name=\"设计模式的六大原则\"></a>设计模式的六大原则</h2>\n\n<pre>\n<code>开闭原则：实现热插拔，提高扩展性。\n里氏代换原则：实现抽象的规范，实现子父类互相替换；\n依赖倒转原则：针对接口编程，实现开闭原则的基础；\n接口隔离原则：降低耦合度，接口单独设计，互相隔离；\n迪米特法则，又称不知道原则：功能模块尽量独立；\n合成复用原则：尽量使用聚合，组合，而不是继承；</code></pre>\n\n<h2><a id=\"创建型模式：\" name=\"创建型模式：\"></a>创建型模式：</h2>\n\n<ul>\n	<li>工厂模式（Factory Pattern）</li>\n	<li>抽象工厂模式（Abstract Factory Pattern）</li>\n	<li>单例模式（Singleton Pattern）</li>\n	<li>建造者模式（Builder Pattern）</li>\n	<li>原型模式（Prototype Pattern）</li>\n</ul>\n\n<h3><a id=\"工厂模式\" name=\"工厂模式\"></a>工厂模式</h3>\n\n<p>根据不同路径new不同对象</p>\n\n<p>数据库访问（通过不同DBname返回不同DB对象）</p>\n\n<pre>\n<code>mongoClient.getDB(DBName)</code></pre>\n\n<h3><a id=\"抽象工厂模式^\" name=\"抽象工厂模式^\"></a>抽象工厂模式</h3>\n\n<p>围绕一个超级工厂创建其他工厂。该超级工厂又称为其他工厂的工厂(注意：产品族难扩展，产品等级易扩展)</p>\n\n<pre>\n<code> </code></pre>\n\n<h3><a id=\"单例模式\" name=\"单例模式\"></a>单例模式</h3>\n\n<p>保证一个类只有一个实例（好多种方式--饿汉，懒汉，双重锁，静态内部类，枚举类）</p>\n\n<p>简单版（枚举，不支持继承可以实现接口）：</p>\n\n<pre>\n<code>enum aa{\n	INSTANCE;\n}</code></pre>\n\n<p>复杂版（饿汉）：</p>\n\n<pre>\n<code>class aa{\n	private static final aa a = new aa();\n	private aa(){};\n	public static aa getInstance(){\n		return a;\n	}\n}</code></pre>\n\n<h3><a id=\"建造者模式\" name=\"建造者模式\"></a>建造者模式</h3>\n\n<p>向复杂对象中添加多个单体对象的问题，复杂对象存在非常多的变化。单体对象则为确定的不变的对象。通过建造者模式，将变与不变分开。建造者只需要关注建造方法，单一职责。</p>\n\n<p>StringBuilder(复杂对象)，append（str）中str为单体对象</p>\n\n<pre>\n<code>StringBuilder sb = new StringBuilder ();</code></pre>\n\n<h3><a id=\"原型模式\" name=\"原型模式\"></a>原型模式</h3>\n\n<p>通过克隆已有的对象来进行创建，而不需要通过new。适合于构造对象成本较大的场景或者对性能要求较高的场景。实现<code>Cloneable</code>接口，定义<code>clone</code>方法（注意<strong>：</strong>1. 原型模式属于浅拷贝，只能拷贝基础变量，可根据需求重写clone打破；2. clone方法是直接进行内存拷贝，不会调用对象的构造方法。即使对象的构造方法是私有的，也可进行clone）</p>\n\n<pre>\n<code>public class RoleInRailGun implements Cloneable{\n    public Object clone() {\n        Object clone = null;\n        try {\n            clone = super.clone();\n        } catch (CloneNotSupportedException e) {\n            e.printStackTrace();\n        }\n        return clone;\n    }\n}\n</code></pre>\n\n<h2><a id=\"结构型模式:\" name=\"结构型模式:\"></a>结构型模式:</h2>\n\n<ul>\n	<li>适配器模式（Adapter Pattern）</li>\n	<li>桥接模式（Bridge Pattern）</li>\n	<li>过滤器模式（Filter、Criteria Pattern）</li>\n	<li>组合模式（Composite Pattern）</li>\n	<li>装饰器模式（Decorator Pattern）</li>\n	<li>外观模式（Facade Pattern）</li>\n	<li>享元模式（Flyweight Pattern）</li>\n	<li>代理模式（Proxy Pattern）</li>\n</ul>\n\n<h3><a id=\"适配器模式\" name=\"适配器模式\"></a>适配器模式</h3>\n\n<p>实现两个原本不相关的接口的连通性，实现方法，主要有继承和依赖（推荐依赖）</p>\n\n<p>每个数据库jdbc都不一样，通过Connection接口将所有jdbc适配</p>\n\n<pre>\n<code>Class.forName(\"com.mysql.jdbc.Driver\");\nConnection conn = DriverManager.getConnection(\"jdbc:mysql://localhost:3306/dbname\",\"uname\", \"pw\");</code></pre>\n\n<h3><a id=\"桥接模式^\" name=\"桥接模式^\"></a>桥接模式</h3>\n\n<h3><a id=\"过滤器模式^\" name=\"过滤器模式^\"></a>过滤器模式</h3>\n\n<h3><a id=\"组合模式^\" name=\"组合模式^\"></a>组合模式</h3>\n\n<h3><a id=\"装饰器模式\" name=\"装饰器模式\"></a>装饰器模式</h3>\n\n<p>允许向一个现有的对象添加新的功能，同时又不改变其结构。创建一个装饰类，用来包装原有的类，并在保持类方法签名完整性的前提下，提供了额外的功能。</p>\n\n<p>通过java.util.Collections.synchronizedList()将List转换为线程安全的集合，再不改变List类的情况下提供功能</p>\n\n<pre>\n<code>List&lt;String&gt; l = new ArrayList&lt;&gt;();\njava.util.Collections.synchronizedList(l);\n\n内部实现为\nfinal Collection&lt;E&gt; c;\nfinal Object mutex;\nSynchronizedCollection(Collection&lt;E&gt; c) {//构造方法\n     this.c = Objects.requireNonNull(c);\n     mutex = this;\n}\npublic boolean add(E e) {\n     synchronized (mutex) {return c.add(e);}\n}</code></pre>\n\n<h3><a id=\"外观模式\" name=\"外观模式\"></a>外观模式</h3>\n\n<p>隐藏系统的复杂性，并向客户端提供一个可以访问系统的接口。它向现有的系统添加一个接口，来隐藏系统的复杂性。</p>\n\n<p>分词器在启动时需要加载各种词典，但用户完全不需要知晓，只要使用就可以</p>\n\n<pre>\n<code>String str = \"测试文字\";\nResult result=NlpAnalysis.parse(str);</code></pre>\n\n<h3><a id=\"享元模式\" name=\"享元模式\"></a>享元模式</h3>\n\n<p>Java中基本类型，如果常量池中有则直接使用，没有就创建一个新的常量</p>\n\n<h3><a id=\"代理模式\" name=\"代理模式\"></a>代理模式</h3>\n\n<p>为对象提供一种代理以控制对该对象的访问（如在使用前进行验证，使用后进行动作持久化等。。。）</p>\n\n<p>Spring的aop</p>\n\n<p>静态代理(被代理的类一定要实现接口啊)</p>\n\n<pre>\n<code>public class test {\n	public static void main(String[] args) throws Exception {\n		new AA_proxy(new AA()).syso();;\n	}\n} \ninterface A{\n	public void syso();\n}\nclass AA implements A{\n	public void syso() {\n		System.out.println(\"执行AA类的syso\");\n	}\n}\nclass AA_proxy implements A{\n	A a;\n	public AA_proxy(A a){\n		this.a = a;\n	}\n	public void syso() {\n		System.out.println(\"我在前面插一脚\");\n		a.syso();\n		System.out.println(\"我在后面插一脚\");\n	}\n}</code></pre>\n\n<p>动态代理</p>\n\n<pre>\n<code>public class test {\n	public static void main(String[] args) throws Exception {\n		((A)new ProxyFactory().getProxyInstance(new AA())).syso();\n	}\n}\ninterface A{\n	public void syso();\n}\nclass AA implements A{\n	public void syso() {\n		System.out.println(\"执行AA类的syso\");\n	}\n}\nclass ProxyFactory implements InvocationHandler{\n    private Object target;\n    public Object getProxyInstance(Object target){\n        this.target = target;\n        Class&lt;?&gt; clazz = target.getClass();\n        Object obj = Proxy.newProxyInstance(clazz.getClassLoader(), clazz.getInterfaces(), this);\n        return obj;\n    }\n    @Override\n    public Object invoke(Object obj, Method method, Object[] args) throws Throwable {\n        System.out.println(\"我在前面插一脚\");\n        method.invoke(target, args);\n        System.out.println(\"我在后面插一脚\");\n        return null;\n    }\n}</code></pre>\n\n<pre>\n<code>public class test {\n	public static void main(String[] args) throws Exception {\n		((A)(new ProxyFactory(new AA()).getProxyInstance())).syso();\n	}\n}\ninterface A{\n	public void syso();\n}\nclass AA implements A{\n	public void syso() {\n		System.out.println(\"执行AA类的syso\");\n	}\n}\nclass ProxyFactory {\n	private Object target;\n	public ProxyFactory(Object target) {\n		this.target = target;\n	}\n	public Object getProxyInstance() {\n		ClassLoader classLoader = target.getClass().getClassLoader();// 被代理类的类加载器\n		Class&lt;?&gt;[] interfaces = target.getClass().getInterfaces();// 被代理类的接口\n		InvocationHandler invocationHandler = new InvocationHandler() {// 代理类执行，执行某个类的某个方法，并传入方法入参，获得返回值\n			@Override\n			public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {\n				System.out.println(\"我在前面插一脚\");\n				Object returnValue = method.invoke(target, args);\n				System.out.println(\"我在后面插一脚\");\n				return returnValue;\n			}\n		};\n		// 返回代理类\n		Object proxy = Proxy.newProxyInstance(classLoader, interfaces, invocationHandler);\n		return proxy;\n	}\n}</code></pre>\n\n<h2><a id=\"行为型模式:\" name=\"行为型模式:\"></a>行为型模式:</h2>\n\n<ul>\n	<li>责任链模式（Chain of Responsibility Pattern）</li>\n	<li>命令模式（Command Pattern）</li>\n	<li>解释器模式（Interpreter Pattern）</li>\n	<li>迭代器模式（Iterator Pattern）</li>\n	<li>中介者模式（Mediator Pattern）</li>\n	<li>备忘录模式（Memento Pattern）</li>\n	<li>观察者模式（Observer Pattern）</li>\n	<li>状态模式（State Pattern）</li>\n	<li>空对象模式（Null Object Pattern）</li>\n	<li>策略模式（Strategy Pattern）</li>\n	<li>模板模式（Template Pattern）</li>\n	<li>访问者模式（Visitor Pattern）</li>\n</ul>\n\n<h3><a id=\"责任链模式\" name=\"责任链模式\"></a>责任链模式</h3>\n\n<h3><a id=\"命令模式\" name=\"命令模式\"></a>命令模式</h3>\n\n<h3><a id=\"解释器模式\" name=\"解释器模式\"></a>解释器模式</h3>\n\n<h3><a id=\"迭代器模式\" name=\"迭代器模式\"></a>迭代器模式</h3>\n\n<h3><a id=\"中介者模式\" name=\"中介者模式\"></a>中介者模式</h3>\n\n<h3><a id=\"备忘录模式\" name=\"备忘录模式\"></a>备忘录模式</h3>\n\n<h3><a id=\"观察者模式\" name=\"观察者模式\"></a>观察者模式</h3>\n\n<h3><a id=\"状态模式\" name=\"状态模式\"></a>状态模式</h3>\n\n<h3><a id=\"空对象模式\" name=\"空对象模式\"></a>空对象模式</h3>\n\n<h3><a id=\"策略模式\" name=\"策略模式\"></a>策略模式</h3>\n\n<h3><a id=\"模板模式\" name=\"模板模式\"></a>模板模式</h3>\n\n<h3><a id=\"访问者模式\" name=\"访问者模式\"></a>访问者模式</h3>', 1, '5f199b8885e24fc8b28672b872edb606', 69, '2018-09-09 14:41:30', '2018-09-17 07:58:23');
INSERT INTO `logcontent` VALUES ('1d055aef90564de08b12c428aba01b3d', -1, '7338e53acd514defa1a17e47016f3f4a', '五、映射和分析', '映射,分析', '映射和分析', '<h2>映射和分析</h2>\n\n<p>当摆弄索引里面的数据时，我们发现一些奇怪的事情。一些事情看起来被打乱了：在我们的索引中有12条推文，其中只有一条包含日期&nbsp;<code>2014-09-15</code>&nbsp;，但是看一看下面查询命中的&nbsp;<code>总数</code>&nbsp;（total）：</p>\n\n<pre>\nGET /_search?q=2014              # 12 results\nGET /_search?q=2014-09-15        # 12 results !\nGET /_search?q=date:2014-09-15   # 1  result\nGET /_search?q=date:2014         # 0  results !&nbsp;</pre>\n\n<p>为什么在&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/search-lite.html#all-field-intro\" title=\"_all 字段\"><code>_all</code></a>&nbsp;字段查询日期返回所有推文，而在&nbsp;<code>date</code>&nbsp;字段只查询年份却没有返回结果？为什么我们在<code>_all</code>&nbsp;字段和&nbsp;<code>date</code>&nbsp;字段的查询结果有差别？</p>\n\n<p>推测起来，这是因为数据在&nbsp;<code>_all</code>&nbsp;字段与&nbsp;<code>date</code>&nbsp;字段的索引方式不同。所以，通过请求&nbsp;<code>gb</code>&nbsp;索引中&nbsp;<code>tweet</code>类型的_映射_（或模式定义），让我们看一看 Elasticsearch 是如何解释我们文档结构的：</p>\n\n<pre>\nGET /gb/_mapping/tweet\n</pre>\n\n<p>这将得到如下结果：</p>\n\n<pre>\n<code class=\"language-json\">{\n   \"gb\": {\n      \"mappings\": {\n         \"tweet\": {\n            \"properties\": {\n               \"date\": {\n                  \"type\": \"date\",\n                  \"format\": \"strict_date_optional_time||epoch_millis\"\n               },\n               \"name\": {\n                  \"type\": \"string\"\n               },\n               \"tweet\": {\n                  \"type\": \"string\"\n               },\n               \"user_id\": {\n                  \"type\": \"long\"\n               }\n            }\n         }\n      }\n   }\n}</code></pre>\n\n<p>基于对字段类型的猜测， Elasticsearch 动态为我们产生了一个映射。这个响应告诉我们&nbsp;<code>date</code>&nbsp;字段被认为是&nbsp;<code>date</code>&nbsp;类型的。由于&nbsp;<code>_all</code>&nbsp;是默认字段，所以没有提及它。但是我们知道&nbsp;<code>_all</code>&nbsp;字段是&nbsp;<code>string</code>&nbsp;类型的。</p>\n\n<p>所以&nbsp;<code>date</code>&nbsp;字段和&nbsp;<code>string</code>&nbsp;字段&nbsp;索引方式不同，因此搜索结果也不一样。这完全不令人吃惊。你可能会认为&nbsp;核心数据类型 strings、numbers、Booleans 和 dates 的索引方式有稍许不同。没错，他们确实稍有不同。</p>\n\n<p>但是，到目前为止，最大的差异在于&nbsp;代表&nbsp;<em>精确值</em>&nbsp;（它包括&nbsp;<code>string</code>&nbsp;字段）的字段和代表&nbsp;<em>全文</em>&nbsp;的字段。这个区别非常重要&mdash;&mdash;它将搜索引擎和所有其他数据库区别开来。</p>\n\n<h2>精确值 VS 全文</h2>\n\n<p>Elasticsearch 中的数据可以概括的分为两类：精确值和全文。</p>\n\n<p><em>精确值</em>&nbsp;如它们听起来那样精确。例如日期或者用户 ID，但字符串也可以表示精确值，例如用户名或邮箱地址。对于精确值来讲，<code>Foo</code>&nbsp;和&nbsp;<code>foo</code>&nbsp;是不同的，<code>2014</code>&nbsp;和&nbsp;<code>2014-09-15</code>&nbsp;也是不同的。</p>\n\n<p>另一方面，<em>全文</em>&nbsp;是指文本数据（通常以人类容易识别的语言书写），例如一个推文的内容或一封邮件的内容。</p>\n\n<p><img alt=\"注意\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/note.png\" /></p>\n\n<p>全文通常是指非结构化的数据，但这里有一个误解：自然语言是高度结构化的。问题在于自然语言的规则是复杂的，导致计算机难以正确解析。例如，考虑这条语句：</p>\n\n<pre>\nMay is fun but June bores me.</pre>\n\n<p>它指的是月份还是人？</p>\n\n<p>精确值很容易查询。结果是二进制的：要么匹配查询，要么不匹配。这种查询很容易用 SQL 表示：</p>\n\n<pre>\n<code class=\"language-sql\">WHERE name    = \"John Smith\"\n  AND user_id = 2\n  AND date    &gt; \"2014-09-15\"</code></pre>\n\n<p>查询全文数据要微妙的多。我们问的不只是&ldquo;这个文档匹配查询吗&rdquo;，而是&ldquo;该文档匹配查询的程度有多大？&rdquo;换句话说，该文档与给定查询的相关性如何？</p>\n\n<p>我们很少对全文类型的域做精确匹配。相反，我们希望在文本类型的域中搜索。不仅如此，我们还希望搜索能够理解我们的&nbsp;<em>意图</em>&nbsp;：</p>\n\n<ul style=\"list-style-type:disc\">\n	<li>搜索&nbsp;<code>UK</code>&nbsp;，会返回包含&nbsp;<code>United Kindom</code>&nbsp;的文档。</li>\n	<li>搜索&nbsp;<code>jump</code>&nbsp;，会匹配&nbsp;<code>jumped</code>&nbsp;，&nbsp;<code>jumps</code>&nbsp;，&nbsp;<code>jumping</code>&nbsp;，甚至是&nbsp;<code>leap</code>&nbsp;。</li>\n	<li>搜索&nbsp;<code>johnny walker</code>&nbsp;会匹配&nbsp;<code>Johnnie Walker</code>&nbsp;，&nbsp;<code>johnnie depp</code>&nbsp;应该匹配&nbsp;<code>Johnny Depp</code>&nbsp;。</li>\n	<li><code>fox news hunting</code>&nbsp;应该返回福克斯新闻（ Foxs News ）中关于狩猎的故事，同时，&nbsp;<code>fox hunting news</code>&nbsp;应该返回关于猎狐的故事。</li>\n</ul>\n\n<p>为了促进这类在全文域中的查询，Elasticsearch 首先&nbsp;<em>分析</em>&nbsp;文档，之后根据结果创建&nbsp;<em>倒排索引</em>&nbsp;。在接下来的两节，我们会讨论倒排索引和分析过程。</p>\n\n<h2>倒排索引</h2>\n\n<p>Elasticsearch 使用一种称为&nbsp;<em>倒排索引</em>&nbsp;的结构，它适用于快速的全文搜索。一个倒排索引由文档中所有不重复词的列表构成，对于其中每个词，有一个包含它的文档列表。</p>\n\n<p>例如，假设我们有两个文档，每个文档的&nbsp;<code>content</code>&nbsp;域包含如下内容：</p>\n\n<ol style=\"list-style-type:decimal\">\n	<li>The quick brown fox jumped over the lazy dog</li>\n	<li>Quick brown foxes leap over lazy dogs in summer</li>\n</ol>\n\n<p>为了创建倒排索引，我们首先将每个文档的&nbsp;<code>content</code>&nbsp;域拆分成单独的&nbsp;词（我们称它为&nbsp;<code>词条</code>&nbsp;或&nbsp;<code>tokens</code>），创建一个包含所有不重复词条的排序列表，然后列出每个词条出现在哪个文档。结果如下所示：</p>\n\n<pre>\nTerm      Doc_1  Doc_2\n-------------------------\nQuick   |       |  X\nThe     |   X   |\nbrown   |   X   |  X\ndog     |   X   |\ndogs    |       |  X\nfox     |   X   |\nfoxes   |       |  X\nin      |       |  X\njumped  |   X   |\nlazy    |   X   |  X\nleap    |       |  X\nover    |   X   |  X\nquick   |   X   |\nsummer  |       |  X\nthe     |   X   |\n------------------------</pre>\n\n<p>现在，如果我们想搜索&nbsp;<code>quick brown</code>&nbsp;，我们只需要查找包含每个词条的文档：</p>\n\n<pre>\nTerm      Doc_1  Doc_2\n-------------------------\nbrown   |   X   |  X\nquick   |   X   |\n------------------------\nTotal   |   2   |  1</pre>\n\n<p>两个文档都匹配，但是第一个文档比第二个匹配度更高。如果我们使用仅计算匹配词条数量的简单&nbsp;<em>相似性算法</em>&nbsp;，那么，我们可以说，对于我们查询的相关性来讲，第一个文档比第二个文档更佳。</p>\n\n<p>但是，我们目前的倒排索引有一些问题：</p>\n\n<ul style=\"list-style-type:disc\">\n	<li><code>Quick</code>&nbsp;和&nbsp;<code>quick</code>&nbsp;以独立的词条出现，然而用户可能认为它们是相同的词。</li>\n	<li><code>fox</code>&nbsp;和&nbsp;<code>foxes</code>&nbsp;非常相似, 就像&nbsp;<code>dog</code>&nbsp;和&nbsp;<code>dogs</code>&nbsp;；他们有相同的词根。</li>\n	<li><code>jumped</code>&nbsp;和&nbsp;<code>leap</code>, 尽管没有相同的词根，但他们的意思很相近。他们是同义词。</li>\n</ul>\n\n<p>使用前面的索引搜索&nbsp;<code>+Quick +fox</code>&nbsp;不会得到任何匹配文档。（记住，<code>+</code>&nbsp;前缀表明这个词必须存在。）只有同时出现&nbsp;<code>Quick</code>&nbsp;和&nbsp;<code>fox</code>&nbsp;的文档才满足这个查询条件，但是第一个文档包含&nbsp;<code>quick fox</code>&nbsp;，第二个文档包含<code>Quick foxes</code>&nbsp;。</p>\n\n<p>我们的用户可以合理的期望两个文档与查询匹配。我们可以做的更好。</p>\n\n<p>如果我们将词条规范为标准模式，那么我们可以找到与用户搜索的词条不完全一致，但具有足够相关性的文档。例如：</p>\n\n<ul style=\"list-style-type:disc\">\n	<li><code>Quick</code>&nbsp;可以小写化为&nbsp;<code>quick</code>&nbsp;。</li>\n	<li><code>foxes</code>&nbsp;可以&nbsp;<em>词干提取</em>&nbsp;--变为词根的格式-- 为&nbsp;<code>fox</code>&nbsp;。类似的，&nbsp;<code>dogs</code>&nbsp;可以为提取为&nbsp;<code>dog</code>&nbsp;。</li>\n	<li><code>jumped</code>&nbsp;和&nbsp;<code>leap</code>&nbsp;是同义词，可以索引为相同的单词&nbsp;<code>jump</code>&nbsp;。</li>\n</ul>\n\n<p>现在索引看上去像这样：</p>\n\n<pre>\nTerm      Doc_1  Doc_2\n-------------------------\nbrown   |   X   |  X\ndog     |   X   |  X\nfox     |   X   |  X\nin      |       |  X\njump    |   X   |  X\nlazy    |   X   |  X\nover    |   X   |  X\nquick   |   X   |  X\nsummer  |       |  X\nthe     |   X   |  X\n------------------------</pre>\n\n<p>这还远远不够。我们搜索&nbsp;<code>+Quick +fox</code>&nbsp;<em>仍然</em>&nbsp;会失败，因为在我们的索引中，已经没有&nbsp;<code>Quick</code>&nbsp;了。但是，如果我们对搜索的字符串使用与&nbsp;<code>content</code>&nbsp;域相同的标准化规则，会变成查询&nbsp;<code>+quick +fox</code>&nbsp;，这样两个文档都会匹配！</p>\n\n<p><img alt=\"注意\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/note.png\" /></p>\n\n<p>这非常重要。你只能搜索在索引中出现的词条，所以索引文本和查询字符串必须标准化为相同的格式。</p>\n\n<p>分词和标准化的过程称为&nbsp;<em>分析</em>&nbsp;， 我们会在下个章节讨论。</p>\n\n<h2>分析与分析器</h2>\n\n<p><em>分析</em>&nbsp;包含下面的过程：</p>\n\n<ul style=\"list-style-type:disc\">\n	<li>首先，将一块文本分成适合于倒排索引的独立的&nbsp;<em>词条</em>&nbsp;，</li>\n	<li>之后，将这些词条统一化为标准格式以提高它们的&ldquo;可搜索性&rdquo;，或者&nbsp;<em>recall</em></li>\n</ul>\n\n<p>分析器执行上面的工作。&nbsp;<em>分析器</em>&nbsp;实际上是将三个功能封装到了一个包里：</p>\n\n<p>字符过滤器</p>\n\n<p>首先，字符串按顺序通过每个&nbsp;<em>字符过滤器</em>&nbsp;。他们的任务是在分词前整理字符串。一个字符过滤器可以用来去掉HTML，或者将&nbsp;<code>&amp;</code>&nbsp;转化成 `and`。</p>\n\n<p>分词器</p>\n\n<p>其次，字符串被&nbsp;<em>分词器</em>&nbsp;分为单个的词条。一个简单的分词器遇到空格和标点的时候，可能会将文本拆分成词条。</p>\n\n<p>Token 过滤器</p>\n\n<p>最后，词条按顺序通过每个&nbsp;<em>token 过滤器</em>&nbsp;。这个过程可能会改变词条（例如，小写化&nbsp;<code>Quick</code>&nbsp;），删除词条（例如， 像&nbsp;<code>a`， `and`， `the</code>&nbsp;等无用词），或者增加词条（例如，像&nbsp;<code>jump</code>&nbsp;和&nbsp;<code>leap</code>&nbsp;这种同义词）。</p>\n\n<p>Elasticsearch提供了开箱即用的字符过滤器、分词器和token 过滤器。 这些可以组合起来形成自定义的分析器以用于不同的目的。</p>\n\n<h3>内置分析器</h3>\n\n<p>但是， Elasticsearch还附带了可以直接使用的预包装的分析器。&nbsp;接下来我们会列出最重要的分析器。为了证明它们的差异，我们看看每个分析器会从下面的字符串得到哪些词条：</p>\n\n<pre>\n&quot;Set the shape to semi-transparent by calling set_trans(5)&quot;</pre>\n\n<p>标准分析器</p>\n\n<p>标准分析器是Elasticsearch默认使用的分析器。它是分析各种语言文本最常用的选择。它根据<a href=\"http://www.unicode.org/reports/tr29/\" target=\"_top\">Unicode 联盟</a>&nbsp;定义的&nbsp;<em>单词边界</em>&nbsp;划分文本。删除绝大部分标点。最后，将词条小写。它会产生</p>\n\n<pre>\nset, the, shape, to, semi, transparent, by, calling, set_trans, 5</pre>\n\n<p>简单分析器</p>\n\n<p>简单分析器在任何不是字母的地方分隔文本，将词条小写。它会产生</p>\n\n<pre>\nset, the, shape, to, semi, transparent, by, calling, set, trans</pre>\n\n<p>空格分析器</p>\n\n<p>空格分析器在空格的地方划分文本。它会产生</p>\n\n<pre>\nSet, the, shape, to, semi-transparent, by, calling, set_trans(5)</pre>\n\n<p>语言分析器</p>\n\n<p>特定语言分析器可用于&nbsp;<a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/5.6/analysis-lang-analyzer.html\" target=\"_top\">很多语言</a>。它们可以考虑指定语言的特点。例如，&nbsp;<code>英语</code>&nbsp;分析器附带了一组英语无用词（常用单词，例如&nbsp;<code>and</code>&nbsp;或者&nbsp;<code>the</code>&nbsp;，它们对相关性没有多少影响），它们会被删除。 由于理解英语语法的规则，这个分词器可以提取英语单词的&nbsp;<em>词干</em>&nbsp;。</p>\n\n<p><code>英语</code>&nbsp;分词器会产生下面的词条：</p>\n\n<pre>\nset, shape, semi, transpar, call, set_tran, 5</pre>\n\n<p>注意看&nbsp;<code>transparent`、 `calling</code>&nbsp;和&nbsp;<code>set_trans</code>&nbsp;已经变为词根格式。</p>\n\n<h3>什么时候使用分析器</h3>\n\n<p>当我们&nbsp;<em>索引</em>&nbsp;一个文档，它的全文域被分析成词条以用来创建倒排索引。&nbsp;但是，当我们在全文域&nbsp;<em>搜索</em>&nbsp;的时候，我们需要将查询字符串通过&nbsp;<em>相同的分析过程</em>&nbsp;，以保证我们搜索的词条格式与索引中的词条格式一致。</p>\n\n<p>全文查询，理解每个域是如何定义的，因此它们可以做&nbsp;正确的事：</p>\n\n<ul style=\"list-style-type:disc\">\n	<li>当你查询一个&nbsp;<em>全文</em>&nbsp;域时， 会对查询字符串应用相同的分析器，以产生正确的搜索词条列表。</li>\n	<li>当你查询一个&nbsp;<em>精确值</em>&nbsp;域时，不会分析查询字符串，&nbsp;而是搜索你指定的精确值。</li>\n</ul>\n\n<p>现在你可以理解在刚开始的查询为什么返回那样的结果：</p>\n\n<ul style=\"list-style-type:disc\">\n	<li><code>date</code>&nbsp;域包含一个精确值：单独的词条 `2014-09-15`。</li>\n	<li><code>_all</code>&nbsp;域是一个全文域，所以分词进程将日期转化为三个词条： `2014`， `09`， 和 `15`。</li>\n</ul>\n\n<p>当我们在&nbsp;<code>_all</code>&nbsp;域查询&nbsp;<code>2014`，它匹配所有的12条推文，因为它们都含有 `2014</code>&nbsp;：</p>\n\n<pre>\nGET /_search?q=2014              # 12 results</pre>\n\n<p>当我们在&nbsp;<code>_all</code>&nbsp;域查询&nbsp;<code>2014-09-15`，它首先分析查询字符串，产生匹配 `2014`， `09`， 或 `15</code>&nbsp;中&nbsp;<em>任意</em>&nbsp;词条的查询。这也会匹配所有12条推文，因为它们都含有&nbsp;<code>2014</code>&nbsp;：</p>\n\n<pre>\nGET /_search?q=2014-09-15        # 12 results !</pre>\n\n<p>当我们在&nbsp;<code>date</code>&nbsp;域查询 `2014-09-15`，它寻找&nbsp;<em>精确</em>&nbsp;日期，只找到一个推文：</p>\n\n<pre>\nGET /_search?q=date:2014-09-15   # 1  result</pre>\n\n<p>当我们在&nbsp;<code>date</code>&nbsp;域查询 `2014`，它找不到任何文档，因为没有文档含有这个精确日志：</p>\n\n<pre>\nGET /_search?q=date:2014         # 0  results !</pre>\n\n<p>测试分析器</p>\n\n<p>有些时候很难理解分词的过程和实际被存储到索引中的词条，特别是你刚接触&nbsp;Elasticsearch。为了理解发生了什么，你可以使用&nbsp;<code>analyze</code>&nbsp;API 来看文本是如何被分析的。在消息体里，指定分析器和要分析的文本：</p>\n\n<pre>\n<code>GET /_analyze\n{\n  \"analyzer\": \"standard\",\n  \"text\": \"Text to analyze\"\n}</code></pre>\n\n<p>结果中每个元素代表一个单独的词条：</p>\n\n<pre>\n<code>{\n   \"tokens\": [\n      {\n         \"token\":        \"text\",\n         \"start_offset\": 0,\n         \"end_offset\":   4,\n         \"type\":         \"&lt;ALPHANUM&gt;\",\n         \"position\":     1\n      },\n      {\n         \"token\":        \"to\",\n         \"start_offset\": 5,\n         \"end_offset\":   7,\n         \"type\":         \"&lt;ALPHANUM&gt;\",\n         \"position\":     2\n      },\n      {\n         \"token\":        \"analyze\",\n         \"start_offset\": 8,\n         \"end_offset\":   15,\n         \"type\":         \"&lt;ALPHANUM&gt;\",\n         \"position\":     3\n      }\n   ]\n}</code></pre>\n\n<p><code>token</code>&nbsp;是实际存储到索引中的词条。&nbsp;<code>position</code>&nbsp;指明词条在原始文本中出现的位置。&nbsp;<code>start_offset</code>&nbsp;和<code>end_offset</code>&nbsp;指明字符在原始字符串中的位置。</p>\n\n<p><img alt=\"提示\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/tip.png\" /></p>\n\n<p>每个分析器的&nbsp;<code>type</code>&nbsp;值都不一样，可以忽略它们。它们在Elasticsearch中的唯一作用在于<a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/5.6/analysis-keep-types-tokenfilter.html\" target=\"_top\"><code>keep_types</code>&nbsp;token 过滤器</a>。</p>\n\n<p><code>analyze</code>&nbsp;API 是一个有用的工具，它有助于我们理解Elasticsearch索引内部发生了什么，随着深入，我们会进一步讨论它。</p>\n\n<h3>指定分析器</h3>\n\n<p>当Elasticsearch在你的文档中检测到一个新的字符串域&nbsp;，它会自动设置其为一个全文&nbsp;<code>字符串</code>&nbsp;域，使用&nbsp;<code>标准</code>&nbsp;分析器对它进行分析。</p>\n\n<p>你不希望总是这样。可能你想使用一个不同的分析器，适用于你的数据使用的语言。有时候你想要一个字符串域就是一个字符串域--不使用分析，直接索引你传入的精确值，例如用户ID或者一个内部的状态域或标签。</p>\n\n<p>要做到这一点，我们必须手动指定这些域的映射。</p>\n\n<h2>映射</h2>\n\n<p>为了能够将时间域视为时间，数字域视为数字，字符串域视为全文或精确值字符串， Elasticsearch 需要知道每个域中数据的类型。这个信息包含在映射中。</p>\n\n<p>如&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/data-in-data-out.html\" title=\"数据输入和输出\"><em>数据输入和输出</em></a>&nbsp;中解释的，&nbsp;索引中每个文档都有&nbsp;<em>类型</em>&nbsp;。每种类型都有它自己的&nbsp;<em>映射</em>&nbsp;，或者&nbsp;<em>模式定义</em>。映射定义了类型中的域，每个域的数据类型，以及Elasticsearch如何处理这些域。映射也用于配置与类型有关的元数据。</p>\n\n<p>我们会在&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/mapping.html\" title=\"类型和映射\">类型和映射</a>&nbsp;详细讨论映射。本节，我们只讨论足够让你入门的内容。</p>\n\n<h3>核心简单域类型</h3>\n\n<p>Elasticsearch 支持&nbsp;如下简单域类型：</p>\n\n<ul style=\"list-style-type:disc\">\n	<li>字符串:&nbsp;<code>string</code></li>\n	<li>整数 :&nbsp;<code>byte</code>,&nbsp;<code>short</code>,&nbsp;<code>integer</code>,&nbsp;<code>long</code></li>\n	<li>浮点数:&nbsp;<code>float</code>,&nbsp;<code>double</code></li>\n	<li>布尔型:&nbsp;<code>boolean</code></li>\n	<li>日期:&nbsp;<code>date</code></li>\n</ul>\n\n<p>当你索引一个包含新域的文档--之前未曾出现-- Elasticsearch&nbsp;会使用&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/dynamic-mapping.html\" title=\"动态映射\"><em>动态映射</em></a>&nbsp;，通过JSON中基本数据类型，尝试猜测域类型，使用如下规则：</p>\n\n<table border=\"0\" cellpadding=\"4px\">\n	<tbody>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><strong>JSON type</strong></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p><strong>域 type</strong></p>\n			</td>\n		</tr>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p>布尔型:&nbsp;<code>true</code>&nbsp;或者&nbsp;<code>false</code></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p><code>boolean</code></p>\n			</td>\n		</tr>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p>整数:&nbsp;<code>123</code></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p><code>long</code></p>\n			</td>\n		</tr>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p>浮点数:&nbsp;<code>123.45</code></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p><code>double</code></p>\n			</td>\n		</tr>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p>字符串，有效日期:&nbsp;<code>2014-09-15</code></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p><code>date</code></p>\n			</td>\n		</tr>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p>字符串:&nbsp;<code>foo bar</code></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p><code>string</code></p>\n			</td>\n		</tr>\n	</tbody>\n</table>\n\n<p><img alt=\"注意\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/note.png\" /></p>\n\n<p>这意味着如果你通过引号(&nbsp;<code>&quot;123&quot;</code>&nbsp;)索引一个数字，它会被映射为&nbsp;<code>string</code>&nbsp;类型，而不是&nbsp;<code>long</code>。但是，如果这个域已经映射为&nbsp;<code>long</code>&nbsp;，那么 Elasticsearch 会尝试将这个字符串转化为 long ，如果无法转化，则抛出一个异常。</p>\n\n<h3>查看映射</h3>\n\n<p>通过&nbsp;<code>/_mapping</code>&nbsp;，我们可以查看 Elasticsearch 在一个或多个索引中的一个或多个类型的映射&nbsp;。在&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/mapping-analysis.html\" title=\"映射和分析\">开始章节</a>&nbsp;，我们已经取得索引&nbsp;<code>gb</code>&nbsp;中类型&nbsp;<code>tweet</code>&nbsp;的映射：</p>\n\n<pre>\nGET /gb/_mapping/tweet</pre>\n\n<p>Elasticsearch 根据我们索引的文档，为域(称为&nbsp;<em>属性</em>&nbsp;)动态生成的映射。</p>\n\n<pre>\n<code class=\"language-json\">{\n   \"gb\": {\n      \"mappings\": {\n         \"tweet\": {\n            \"properties\": {\n               \"date\": {\n                  \"type\": \"date\",\n                  \"format\": \"strict_date_optional_time||epoch_millis\"\n               },\n               \"name\": {\n                  \"type\": \"string\"\n               },\n               \"tweet\": {\n                  \"type\": \"string\"\n               },\n               \"user_id\": {\n                  \"type\": \"long\"\n               }\n            }\n         }\n      }\n   }\n}</code></pre>\n\n<p><img alt=\"提示\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/tip.png\" /></p>\n\n<p>错误的映射，例如&nbsp;将&nbsp;<code>age</code>&nbsp;域映射为&nbsp;<code>string</code>&nbsp;类型，而不是&nbsp;<code>integer</code>&nbsp;，会导致查询出现令人困惑的结果。</p>\n\n<p>检查一下！而不是假设你的映射是正确的。</p>\n\n<h3>自定义域映射</h3>\n\n<p>尽管在很多情况下基本域数据类型&nbsp;已经够用，但你经常需要为单独域自定义映射&nbsp;，特别是字符串域。自定义映射允许你执行下面的操作：</p>\n\n<ul style=\"list-style-type:disc\">\n	<li>全文字符串域和精确值字符串域的区别</li>\n	<li>使用特定语言分析器</li>\n	<li>优化域以适应部分匹配</li>\n	<li>指定自定义数据格式</li>\n	<li>还有更多</li>\n</ul>\n\n<p>域最重要的属性是&nbsp;<code>type</code>&nbsp;。对于不是&nbsp;<code>string</code>&nbsp;的域，你一般只需要设置&nbsp;<code>type</code>&nbsp;：</p>\n\n<pre>\n<code class=\"language-json\">{\n    \"number_of_clicks\": {\n        \"type\": \"integer\"\n    }\n}</code></pre>\n\n<p>默认，&nbsp;<code>string</code>&nbsp;类型域会被认为包含全文。就是说，它们的值在索引前，会通过&nbsp;一个分析器，针对于这个域的查询在搜索前也会经过一个分析器。</p>\n\n<p><code>string</code>&nbsp;域映射的两个最重要&nbsp;属性是&nbsp;<code>index</code>&nbsp;和&nbsp;<code>analyzer</code>&nbsp;。</p>\n\n<h4>index</h4>\n\n<p><code>index</code>&nbsp;属性控制怎样索引字符串。它可以是下面三个值：</p>\n\n<p><code>analyzed</code></p>\n\n<p>首先分析字符串，然后索引它。换句话说，以全文索引这个域。</p>\n\n<p><code>not_analyzed</code></p>\n\n<p>索引这个域，所以它能够被搜索，但索引的是精确值。不会对它进行分析。</p>\n\n<p><code>no</code></p>\n\n<p>不索引这个域。这个域不会被搜索到。</p>\n\n<p><code>string</code>&nbsp;域&nbsp;<code>index</code>&nbsp;属性默认是&nbsp;<code>analyzed</code>&nbsp;。如果我们想映射这个字段为一个精确值，我们需要设置它为<code>not_analyzed</code>&nbsp;：</p>\n\n<pre>\n<code class=\"language-json\">{\n    \"tag\": {\n        \"type\":     \"string\",\n        \"index\":    \"not_analyzed\"\n    }\n}</code></pre>\n\n<p><img alt=\"注意\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/note.png\" /></p>\n\n<p>其他简单类型（例如&nbsp;<code>long</code>&nbsp;，&nbsp;<code>double</code>&nbsp;，&nbsp;<code>date</code>&nbsp;等）也接受&nbsp;<code>index</code>&nbsp;参数，但有意义的值只有<code>no</code>&nbsp;和&nbsp;<code>not_analyzed</code>&nbsp;， 因为它们永远不会被分析。</p>\n\n<h4>analyzer</h4>\n\n<p>对于&nbsp;<code>analyzed</code>&nbsp;字符串域，用&nbsp;<code>analyzer</code>&nbsp;属性指定在搜索和索引时使用的分析器。默认， Elasticsearch 使用&nbsp;<code>standard</code>&nbsp;分析器，&nbsp;但你可以指定一个内置的分析器替代它，例如&nbsp;<code>whitespace</code>&nbsp;、&nbsp;<code>simple</code>&nbsp;和 `english`：</p>\n\n<pre>\n<code class=\"language-json\">{\n    \"tweet\": {\n        \"type\":     \"string\",\n        \"analyzer\": \"english\"\n    }\n}</code></pre>\n\n<p>更新映射</p>\n\n<p>当你首次&nbsp;创建一个索引的时候，可以指定类型的映射。你也可以使用&nbsp;<code>/_mapping</code>&nbsp;为新类型（或者为存在的类型更新映射）增加映射。</p>\n\n<p><img alt=\"注意\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/note.png\" /></p>\n\n<p>尽管你可以&nbsp;<em>增加_ 一个存在的映射，你不能 _修改</em>&nbsp;存在的域映射。如果一个域的映射已经存在，那么该域的数据可能已经被索引。如果你意图修改这个域的映射，索引的数据可能会出错，不能被正常的搜索。</p>\n\n<p>我们可以更新一个映射来添加一个新域，但不能将一个存在的域从&nbsp;<code>analyzed</code>&nbsp;改为&nbsp;<code>not_analyzed</code>&nbsp;。</p>\n\n<p>为了描述指定映射的两种方式，我们先删除&nbsp;<code>gd</code>&nbsp;索引：</p>\n\n<pre>\nDELETE /gb</pre>\n\n<p>然后创建一个新索引，指定&nbsp;<code>tweet</code>&nbsp;域使用&nbsp;<code>english</code>&nbsp;分析器：</p>\n\n<pre>\nPUT /gb <img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" />\n{\n  &quot;mappings&quot;: {\n    &quot;tweet&quot; : {\n      &quot;properties&quot; : {\n        &quot;tweet&quot; : {\n          &quot;type&quot; :    &quot;string&quot;,\n          &quot;analyzer&quot;: &quot;english&quot;\n        },\n        &quot;date&quot; : {\n          &quot;type&quot; :   &quot;date&quot;\n        },\n        &quot;name&quot; : {\n          &quot;type&quot; :   &quot;string&quot;\n        },\n        &quot;user_id&quot; : {\n          &quot;type&quot; :   &quot;long&quot;\n        }\n      }\n    }\n  }\n}</pre>\n\n<table border=\"0\" summary=\"Callout list\">\n	<tbody>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/mapping-intro.html#CO19-1\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p>通过消息体中指定的&nbsp;<code>mappings</code>&nbsp;创建了索引。</p>\n			</td>\n		</tr>\n	</tbody>\n</table>\n\n<p>稍后，我们决定在&nbsp;<code>tweet</code>&nbsp;映射增加一个新的名为&nbsp;<code>tag</code>&nbsp;的&nbsp;<code>not_analyzed</code>&nbsp;的文本域，使用&nbsp;<code>_mapping</code>&nbsp;：</p>\n\n<pre>\n<code>PUT /gb/_mapping/tweet\n{\n  \"properties\" : {\n    \"tag\" : {\n      \"type\" :    \"string\",\n      \"index\":    \"not_analyzed\"\n    }\n  }\n}</code></pre>\n\n<p>注意，我们不需要再次列出所有已存在的域，因为无论如何我们都无法改变它们。新域已经被合并到存在的映射中。</p>\n\n<h3>测试映射</h3>\n\n<p>你可以使用&nbsp;<code>analyze</code>&nbsp;API&nbsp;测试字符串域的映射。比较下面两个请求的输出：</p>\n\n<pre>\n<code>GET /gb/_analyze\n{\n  \"field\": \"tweet\",\n  \"text\": \"Black-cats\" \n}</code></pre>\n\n<pre>\n<code>GET /gb/_analyze\n{\n  \"field\": \"tag\",\n  \"text\": \"Black-cats\" \n}</code></pre>\n\n<table border=\"0\" summary=\"Callout list\">\n	<tbody>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/mapping-intro.html#CO20-1\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" /></a>&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/mapping-intro.html#CO20-2\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/2.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p>消息体里面传输我们想要分析的文本。</p>\n			</td>\n		</tr>\n	</tbody>\n</table>\n\n<p><code>tweet</code>&nbsp;域产生两个词条&nbsp;<code>black</code>&nbsp;和&nbsp;<code>cat</code>&nbsp;，&nbsp;<code>tag</code>&nbsp;域产生单独的词条&nbsp;<code>Black-cats</code>&nbsp;。换句话说，我们的映射正常工作。</p>\n\n<h2>复杂核心域类型</h2>\n\n<p>除了我们提到的简单标量数据类型，&nbsp;JSON 还有&nbsp;<code>null</code>&nbsp;值，数组，和对象，这些 Elasticsearch 都是支持的。</p>\n\n<h3>多值域</h3>\n\n<p>很有可能，我们希望&nbsp;<code>tag</code>&nbsp;域&nbsp;包含多个标签。我们可以以数组的形式索引标签：</p>\n\n<pre>\n{ &quot;tag&quot;: [ &quot;search&quot;, &quot;nosql&quot; ]}</pre>\n\n<p>对于数组，没有特殊的映射需求。任何域都可以包含0、1或者多个值，就像全文域分析得到多个词条。</p>\n\n<p>这暗示&nbsp;<em>数组中所有的值必须是相同数据类型的</em>&nbsp;。你不能将日期和字符串混在一起。如果你通过索引数组来创建新的域，Elasticsearch 会用数组中第一个值的数据类型作为这个域的&nbsp;<code>类型</code>&nbsp;。</p>\n\n<p><img alt=\"注意\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/note.png\" /></p>\n\n<p>当你从 Elasticsearch 得到一个文档，每个数组的顺序和你当初索引文档时一样。你得到的<code>_source</code>&nbsp;域，包含与你索引的一模一样的 JSON 文档。</p>\n\n<p>但是，数组是以多值域&nbsp;<em>索引的</em>&mdash;可以搜索，但是无序的。&nbsp;在搜索的时候，你不能指定 &ldquo;第一个&rdquo; 或者 &ldquo;最后一个&rdquo;。 更确切的说，把数组想象成&nbsp;<em>装在袋子里的值</em>&nbsp;。</p>\n\n<h3>空域</h3>\n\n<p>当然，数组可以为空。&nbsp;这相当于存在零值。 事实上，在 Lucene 中是不能存储&nbsp;<code>null</code>&nbsp;值的，所以我们认为存在&nbsp;<code>null</code>&nbsp;值的域为空域。</p>\n\n<p>下面三种域被认为是空的，它们将不会被索引：</p>\n\n<pre>\n&quot;null_value&quot;:               null,\n&quot;empty_array&quot;:              [],\n&quot;array_with_null_value&quot;:    [ null ]</pre>\n\n<h3>多层级对象</h3>\n\n<p>我们讨论的最后一个 JSON 原生数据类是&nbsp;<em>对象</em>&nbsp;-- 在其他语言中称为哈希，哈希 map，字典或者关联数组。</p>\n\n<p><em>内部对象</em>&nbsp;经常用于&nbsp;嵌入一个实体或对象到其它对象中。例如，与其在&nbsp;<code>tweet</code>&nbsp;文档中包含&nbsp;<code>user_name</code>&nbsp;和<code>user_id</code>&nbsp;域，我们也可以这样写：</p>\n\n<pre>\n<code class=\"language-json\">{\n    \"tweet\":            \"Elasticsearch is very flexible\",\n    \"user\": {\n        \"id\":           \"@johnsmith\",\n        \"gender\":       \"male\",\n        \"age\":          26,\n        \"name\": {\n            \"full\":     \"John Smith\",\n            \"first\":    \"John\",\n            \"last\":     \"Smith\"\n        }\n    }\n}</code></pre>\n\n<h3>内部对象的映射</h3>\n\n<p>Elasticsearch 会动态&nbsp;监测新的对象域并映射它们为&nbsp;<code>对象</code>&nbsp;，在&nbsp;<code>properties</code>&nbsp;属性下列出内部域：</p>\n\n<pre>\n<code class=\"language-json\">{\n  \"gb\": {\n    \"tweet\": { \n      \"properties\": {\n        \"tweet\":            { \"type\": \"string\" },\n        \"user\": { \n          \"type\":             \"object\",\n          \"properties\": {\n            \"id\":           { \"type\": \"string\" },\n            \"gender\":       { \"type\": \"string\" },\n            \"age\":          { \"type\": \"long\"   },\n            \"name\":   { \n              \"type\":         \"object\",\n              \"properties\": {\n                \"full\":     { \"type\": \"string\" },\n                \"first\":    { \"type\": \"string\" },\n                \"last\":     { \"type\": \"string\" }\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}</code></pre>\n\n<table border=\"0\" summary=\"Callout list\">\n	<tbody>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/complex-core-fields.html#CO21-1\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p>根对象</p>\n			</td>\n		</tr>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/complex-core-fields.html#CO21-2\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/2.png\" /></a>&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/complex-core-fields.html#CO21-3\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/3.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p>内部对象</p>\n			</td>\n		</tr>\n	</tbody>\n</table>\n\n<p><code>user</code>&nbsp;和&nbsp;<code>name</code>&nbsp;域的映射结构与&nbsp;<code>tweet</code>&nbsp;类型的相同。事实上，&nbsp;<code>type</code>&nbsp;映射只是一种特殊的&nbsp;<code>对象</code>&nbsp;映射，我们称之为&nbsp;<em>根对象</em>&nbsp;。除了它有一些文档元数据的特殊顶级域，例如&nbsp;<code>_source</code>&nbsp;和&nbsp;<code>_all</code>&nbsp;域，它和其他对象一样。</p>\n\n<h3>内部对象是如何索引的</h3>\n\n<p>Lucene 不理解内部对象。&nbsp;Lucene 文档是由一组键值对列表组成的。为了能让 Elasticsearch 有效地索引内部类，它把我们的文档转化成这样：</p>\n\n<pre>\n<code>{\n    \"tweet\":            [elasticsearch, flexible, very],\n    \"user.id\":          [@johnsmith],\n    \"user.gender\":      [male],\n    \"user.age\":         [26],\n    \"user.name.full\":   [john, smith],\n    \"user.name.first\":  [john],\n    \"user.name.last\":   [smith]\n}</code></pre>\n\n<p><em>内部域</em>&nbsp;可以通过名称引用（例如，&nbsp;<code>first</code>&nbsp;）。为了区分同名的两个域，我们可以使用全&nbsp;<em>路径</em>&nbsp;（例如，<code>user.name.first</code>&nbsp;） 或&nbsp;<code>type</code>&nbsp;名加路径（&nbsp;<code>tweet.user.name.first</code>&nbsp;）。</p>\n\n<p><img alt=\"注意\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/note.png\" /></p>\n\n<p>在前面简单扁平的文档中，没有&nbsp;<code>user</code>&nbsp;和&nbsp;<code>user.name</code>&nbsp;域。Lucene 索引只有标量和简单值，没有复杂数据结构。</p>\n\n<h3>内部对象数组</h3>\n\n<p>最后，考虑包含&nbsp;内部对象的数组是如何被索引的。 假设我们有个&nbsp;<code>followers</code>&nbsp;数组：</p>\n\n<pre>\n<code>{\n    \"followers\": [\n        { \"age\": 35, \"name\": \"Mary White\"},\n        { \"age\": 26, \"name\": \"Alex Jones\"},\n        { \"age\": 19, \"name\": \"Lisa Smith\"}\n    ]\n}</code></pre>\n\n<p>这个文档会像我们之前描述的那样被扁平化处理，结果如下所示：</p>\n\n<pre>\n<code>{\n    \"followers.age\":    [19, 26, 35],\n    \"followers.name\":   [alex, jones, lisa, smith, mary, white]\n}</code></pre>\n\n<p><code>{age: 35}</code>&nbsp;和&nbsp;<code>{name: Mary White}</code>&nbsp;之间的相关性已经丢失了，因为每个多值域只是一包无序的值，而不是有序数组。这足以让我们问，&ldquo;有一个26岁的追随者？&rdquo;</p>\n\n<p>但是我们不能得到一个准确的答案：&ldquo;是否有一个26岁&nbsp;<em>名字叫 Alex Jones</em>&nbsp;的追随者？&rdquo;</p>\n', 1, '5f199b8885e24fc8b28672b872edb606', 1, '2018-06-14 10:47:51', '2018-09-06 12:37:15');
INSERT INTO `logcontent` VALUES ('209565db36a34643bd4305e46e43b7e4', -1, '716b0d3ea6e04d03a9deb097be1e2cf1', '咖啡', '咖啡,coffee', '《有道云笔记》', '<p><strong><a id=\"浓缩咖啡/意大利咖啡/浓咖啡/Espresso\" name=\"浓缩咖啡/意大利咖啡/浓咖啡/Espresso\"></a>一、浓缩咖啡/意大利咖啡/浓咖啡/Espresso</strong></p>\n\n<pre>\n<code class=\"language-makefile\">受压的蒸气直接通过咖啡粉，得到的液体就叫浓缩咖啡。它属于纯粹的咖啡，也是其他咖啡饮料的基底。</code></pre>\n\n<p><strong><a id=\"美式咖啡/清咖啡/Americano\" name=\"美式咖啡/清咖啡/Americano\"></a>二、美式咖啡/清咖啡/Americano</strong></p>\n\n<pre>\n<code class=\"language-makefile\">1份浓缩咖啡+2份水</code></pre>\n\n<p><strong><a id=\"白咖啡/Flat White\" name=\"白咖啡/Flat White\"></a>三、白咖啡/Flat White</strong></p>\n\n<pre>\n<code class=\"language-makefile\">1份浓缩咖啡+1.5份热牛奶</code></pre>\n\n<p><strong><a id=\"玛琪雅朵/玛奇朵/Macchiato\" name=\"玛琪雅朵/玛奇朵/Macchiato\"></a>四、玛琪雅朵/玛奇朵/Macchiato</strong></p>\n\n<pre>\n<code class=\"language-makefile\">1份浓缩咖啡+0.5份奶泡（牛奶中的脂肪）</code></pre>\n\n<p><strong><a id=\"康宝蓝/康巴纳/Con Panna\" name=\"康宝蓝/康巴纳/Con Panna\"></a>五、康宝蓝/康巴纳/Con Panna</strong></p>\n\n<pre>\n<code class=\"language-makefile\">1份浓缩咖啡+0.5份鲜奶油</code></pre>\n\n<p><strong><a id=\"拿铁/Latte\" name=\"拿铁/Latte\"></a>六、拿铁/Latte</strong></p>\n\n<pre>\n<code class=\"language-makefile\">1份浓缩咖啡+1.5份热牛奶+0.5份奶泡</code></pre>\n\n<p><strong><a id=\"布列夫/半拿铁/Breve\" name=\"布列夫/半拿铁/Breve\"></a>七、布列夫/半拿铁/Breve</strong></p>\n\n<pre>\n<code class=\"language-makefile\">1份浓缩咖啡+0.75份热牛奶+0.75份鲜奶油+0.5份奶泡</code></pre>\n\n<p><strong><a id=\"卡布奇诺/Cappuccino\" name=\"卡布奇诺/Cappuccino\"></a>八、卡布奇诺/Cappuccino</strong></p>\n\n<pre>\n<code class=\"language-makefile\">1份浓缩咖啡+0.5份热牛奶+1.5份奶泡</code></pre>\n\n<p><strong><a id=\"摩卡/Mocha\" name=\"摩卡/Mocha\"></a>九、摩卡/Mocha</strong></p>\n\n<pre>\n<code class=\"language-makefile\">1份浓缩咖啡+1份热牛奶+0.5份巧克力酱+0.5份鲜奶油</code></pre>\n\n<pre>\n<code>      再详细介绍一下Espresso，这是一个意大利单词，是以7公克深度烘焙的综合咖啡豆，研磨成极细的咖啡粉，经过9个大气压与摄氏90度的高温蒸气，在20秒的短时间内急速萃取30毫升的浓烈咖啡液体。\n\n一杯成功的 “Espresso” 最重要的是看表面是否漂浮着一层厚厚的呈棕红色的油亮泡沫：“Cream”。\n\n       Espresso素有“咖啡之魂”的美称。据说 最早起源于十五世纪初的埃及开罗，但真正使其发扬光大的则是意大利，现在“Espresso”几乎已经成为意大利的一种代表象征。 Espresso很小杯，通常只有30毫升左右，味道很苦，表面浮着一层厚厚的油脂，会与一杯清水同上，可以选择加糖。咖啡馆里选择Espresso的人要么是想品尝到咖啡最本真的滋味；要么纯粹是因为困极了急需提神。有些咖啡馆的Esprssso可以选择“Single单份”或者“Double双份”，双份可不是两杯的意思，而是指用双倍的咖啡粉萃取，会更苦。\n\n       现在Espresso已不单单是“浓缩咖啡”，它更是一种综合咖啡，是一种烘焙方法，是一种咖啡煮法，是一种料理，也是一种生活……\n\n~综合咖啡\n\n       Espresso可让人穷其一生研究它的配方。一般而言，日晒法的咖啡豆较有醇味，而水洗法的豆子较有甜味；1～2年的新豆有活泼的酸质与口感，而陈年豆则沉稳浓稠。至于配方如何，就如编写音乐协奏曲，只有靠老师傅的长期经验与自我实验，难怪，咖啡师傅在意大利享有崇高的地位。\n\n~烘焙方法\n\n       Espresso是一种烘焙方法，在店里可以买到的“Espresso”，一定是重烘焙的咖啡豆，适合煮成浓缩咖啡。Espresso经常采用较深的烘焙，将脂质赶到细胞孔的出口，这时烘焙温度已超过200℃，差几秒钟就可能毁掉整锅豆子。不能不说，Espresso烘焙度的掌握是一门艺术。\n\n~烹煮方法\n\n       Espresso是一种利用科技来烹煮咖啡的方法，它必须符合以下条件：咖啡粉的分量在5公克～8公克之间，水温在85℃～95℃之间，水压在7～9个大气压，过滤时间则不能低于25秒，也不能高于35秒。如此煮制的一杯咖啡，才是最香浓的Espresso。\n\n~料理创意\n\n       由于Espresso的味道浓厚，加入牛奶或其他饮料也不会被稀释，所以可做成各种花式咖啡，俨然已成为一种有创意的料理。如，加牛奶、肉桂粉可制成卡布其诺；加牛奶和巧克力酱，可制成咖啡摩卡；若只加入奶泡，则成了一杯玛琪雅朵；加鲜奶油，又成了香甜的康宝蓝。\n\n~一种生活\n\n       在意大利，Espresso是当地人每日生活必备。早晨，先在家中喝一杯拿铁（由牛奶与Espresso制成）；之后，到店里要一杯Espresso，在吧台前面几小口将它喝掉，而等待的时间，则可以与人攀谈，连咖啡师傅也在这个时候加入进来，使咖啡馆成了一个小小的联谊天地。\n\n      看似简单不华的一小杯Espresso拥有着深厚的底蕴，其制作工艺严谨精密，需要咖啡师长期的经验与实验。因为它醇厚的味道，才可加入不同材料不被稀释，做成各种为人熟知喜爱的花式咖啡。</code></pre>\n\n<p>&nbsp;</p>\n', 1, '5f199b8885e24fc8b28672b872edb606', 7, '2017-09-02 22:22:33', '2018-08-31 22:21:23');
INSERT INTO `logcontent` VALUES ('23db65a7e84f4cf7b7d38ce564f0ccf6', 1, '4eca926aa543420baea2f03f506d042e', '高性能Mysql笔记五', '高性能Mysql（第三版）', '高性能Mysql（第三版）--Mysql高级特性', '<h2>分区表</h2>\n\n<pre>\n<code>分区表是一种粗粒度的、简易的索引策略，适用于大数据量的过滤场景。最适合的场景是，在没有合适的索引时，对其中几个分区进行全表扫描，或者是只有一个分区和索引是热点，而且这个分区和索引能够都在内存中；限制单表分区数不要超过150个，并且注意某些导致无法做分区过滤的细节，分区表对于单条记录的查询并没有什么优势，需要注意这类查询的性能。\n\n分区的场景\n1、表非常大以至于无法全部都放在内存中，或者只在表的最后部分有热点数据，其他均是历史数据\n2、分区表的数据更容易维护。例如，可以通过清除整个分区的方式来批量删除大量数据。还可以对一个独立分区进行优化、检查、修复等操作\n3、分区表的数据可以分布在不同的物理设备上，从而高效地利用多个硬件设备\n4、可以使用分区表来避免某些特殊的瓶颈，例如InnoDB的单个索引的互斥访问、ext3文件系统的inode锁竞争等\n5、如果需要，还可以备份和恢复独立的分区，这在非常大的数据集的场景下效果非常好\n\n分区表限制\n1、一个表最多只能有1024个分区。\n2、分区表达式必须是整数、返回整数的表达式或者直接使用列（5.5新特性）来进行分区。\n3、如果分区字段中有主键或者唯一索引的列，那么所有主键列和唯一索引列都必须包含进来。\n4、分区表中无法使用外键约束。\n5、所有分区都必须使用相同的存储引擎。\n6、分区函数中可以使用的函数和表达式也有一些限制。\n7、某些存储引擎不支持分区。\n8、对于MyISAM的分区表，不能再使用LOAD INDEX INTO CACHE操作。\n9、对于MyISAM表，使用分区表时需要打开更多的文件描述符。虽然看起来是一个表，其实背后有很多独立的分区，每一个分区对于存储引擎来说都是一个独立的表。这样即使分区表只占用一个表缓存条目，文件描述符还是需要多个。因此，即使已经配置了合适的表缓存，以确保不会超过操作系统的单个进程可以打开的文件描述符的个数，但对于分区表而言，还是会出现超过文件描述符限制的问题。\n\n分区表上的操作逻辑\nSELECT查询\n当查询一个分区表的时候，分区层先打开并锁住所有的底层表，优化器判断是否可以过滤部分分区，然后再调用对应的存储引擎接口访问各个分区的数据，查询时只能根据列而不能是表达式来过滤分区。\nINSERT操作\n当写入一条记录时，分区层先打开并锁住所有的底层表，然后确定哪个分区接收这条记录，再将记录写入对应底层表\nDELETE操作\n当删除一条记录时，分区层先打开并锁住所有的底层表，然后确定数据对应的分区，最后对相应底层表进行删除操作\nUPDATE操作\n当更新一条记录时，分区层先打开并锁住所有的底层表，MySQL先确定需要更新的记录在哪个分区，然后取出数据并更新，再判断更新后的数据应该放在哪个分区，最后对底层表进行写入操作，并对原数据所在的底层表进行删除操作\n（注：“先打开并锁住所有的底层表”并不是说分区表在处理过程中是锁住全表的。如果存储引擎能够自己实现行级锁，如InnoDB，则会在分区层释放对应表锁）\n\n分区表的类型\nRNAGE分区：每个分区存储落在某个范围的记录，分区表达式可以是列，也可以是包含列的表达式,常用\neg:按年进行分区\nPARTITION BY RANGE(c_data) (\nPARTITION p01 VALUES LESS THAN(\'2010-01-01\'),\nPARTITION p02 VALUES LESS THAN(\'2015-01-01\'),\nPARTITION pother VALUES LESS THAN MAXVALUE);\nLIST分区：基于列值匹配一个离散值集合中的某个值来进行选择，分区表达式可以是列，也可以是包含列的表达式\neg:按方位进行分区\nPARTITION BY LIST COLUMNS (category) (\nPARTITION p01 VALUES IN ( \'tomato\', \'potato\'),\nPARTITION p02 VALUES IN ( \'banana\', \'apple\')\n);\nHASH分区:基于给定的分区个数，将数据分配到不同的分区，分区表达式可以是列，也可以是包含列的表达式，分布情况不可控,PARTITIONS为分区个数\neg:按年进行分区\nPARTITION BY HASH(YEAR(c_data))\nPARTITIONS 6;\nLINEAR HASH：与HASH分区的区别在于，LINEAR HASH功能使用一个线性的2的幂运算法则，而HASH分区使用的是求哈希函数值的模数\neg:按年进行分区\nPARTITION BY LINEAR HASH(YEAR(c_data))\nPARTITIONS 6;\nKSY分区：类似于按HASH分区，区别在于KEY分区只支持计算一列或多列\n\n分区表容易出现的问题\n1、如果分区表达式的值为NULL或其他非法值，记录会存放到第一个分区，导致第一分区非常庞大，而查询除了对应的分区外还要额外查询第一分区（防止漏数据），代价会非常大。解决：创建一个“无用的第一分区”\n2、定义的索引列和分区列不匹配，会导致查询无法进行分区过滤。即因为每个分区都有其独立的索引，因此扫描字段的索引就需要扫描每一个分区内对应的索引。如果每个分区内对应索引的非叶子节点都在内存中，扫描的速度还可以接受，但如果能跳过某些分区索引会更好，应该避免建立和分区列不匹配的索引或者查询中包含了可以过滤分区的条件\n3、范围分区对于选择分区的成本很高，因为服务器需要扫描所有的范围条件来确定分区，随着分区数的增长，成本会越来越高（写入大量数据，每写入一行数据到范围分区的表时，都需要扫描分区定义列表来找到合适的目标分区）。可以通过限制分区的数量来缓解此问题，一般建议100个左右的分区，其他的分区类型，比如键分区和哈希分区，则没有这样的问题。\n4、打开并锁住所有底层表的成本可能很高，当查询访问分区表的时候，MySQL需要打开并锁住所有的底层表，这是分区表的另一个开销。这个操作在分区过滤之前发生，所以无法通过分区过滤降低此开销，并且该开销也和分区类型无关，会影响所有的查询。这一点对一些本身操作非常快的查询，比如主键查询，会带来明显的额外开销。可以用批量操作的方式来降低单个操作的开销，例如使用批量插入或者LOAD DATA INFILE、一次删除多行数据等等。当然同时还是需要限制分区的个数\n5、维护分区的成本可能很高，某些分区维护操作的速度会非常快，例如新增或者删除分区。而有些操作，例如重组分区或者类似ALTER语句的操作：这类操作需要复制数据。重组分区的原理与ALTER类似，先创建一个临时的分区，然后将数据复制到其中，最后再删除原分区</code></pre>\n\n<h2>视图</h2>\n\n<pre>\n<code>视图本身是一个虚拟表，不存放任何数据。在使用SQL语句访问视图的时候，它返回的数据是MySQL从其他表中生成的。视图和表是在同一个命名空间，MySQL在很多地方对于视图和表是同样对待的。不过视图和表也有不同，例如，不能对视图创建触发器，也不能使用DROP TABLE命令删除视图。\n\n对好几个表的复杂查询，使用视图有时候会大大简化问题。当视图使用临时表时，无法将WHERE条件下推到各个具体的表，也不能使用任何索引，需要特别注意这类查询的性能。如果为了便利，使用视图是很合适的。\n\n算法\n合并算法: 将存放的视图sql和用户发起的查询sql合并后执行，推荐\n临时表算法: 由存放的视图sql先创建临时表后根据用户的查询sql查询返回\n\n可更新视图\n可以通过更新视图更新相关表, 所有临时表算法实现的视图都无法更新\n\n视图对性能的影响\n一般情况视图不能提升性能,在某些情况下可以帮助提升性能,需要做比较详细的测试\n视图还可以实现基于列的权限控制不用真正创建列权限\n\n视图的限制\n不保存视图定义的原始sql语句\n查看视图创建的语句,可以通过使用视图的.frm文件的最后一行获取一些信息</code></pre>\n\n<h2>外键约束</h2>\n\n<pre>\n<code>外键限制会将约束放到MySQL中，这对于必须维护外键的场景，性能会更高。不过这也会带来额外的复杂性和额外的索引消耗，还会增加多表之间的交互，会导致系统中更多的锁和竞争。外键可以被看作是一个确保系统完整性的额外的特性，但是如果设计的是一个高性能的系统，那么外键就显得很臃肿了。很多人在更在意系统的性能的时候都不会使用外键，而是通过应用程序来维护\n\nInnoDB强制外键使用索引。如果外键列的选择性很低，则会导致一个非常大且选择性很低的索引。\n查询需要额外访问一些表,需要额外的锁容易导致额外的锁等待甚至死锁\n如果外键单纯做约束,通常在应用程序里实现会更好</code></pre>\n\n<h2>在MySQL内部存储代码</h2>\n\n<pre>\n<code>存储代码的优点\n1、在服务器内部执行，离数据近，还可以节省带宽和网络延迟\n2、代码重用。可以方便地统一业务规则，保证行为一致性，简化代码的维护和版本更新\n3、帮助提升安全，比如提供更细粒度的权限控制。例如银行用于转移资金的存储过程：这个存储过程可以在一个事务中完成资金转移和记录用于审计的日志。应用程序也可以通过存储过程的接口访问那些没有权限的表\n4、服务器端可以缓存存储过程的执行计划，大大降低反复调用存储过程的消耗\n5、因为是在服务器端部署的，所以备份、维护都可以在服务器端完成。所以存储程序的维护工作很简单。没什么外部依赖，如任何Perl包和其他不想在服务器上部署的外部软件\n6、它可以对应用开发和数据库开发人员之间进行更好地分工\n\n存储代码的缺点\n1、MySQL本身并未提供好用的开发和调试工具，较之应用程序的代码，存储代码效率要稍微差些，也很难实现太复杂的逻辑\n3、提高部署难度，需要额外部署存储程序\n4、安全隐患，因为存储程序部署在服务器内，如果将非标准的加密功能放在存储程序中，那么若数据库被攻破，数据同时也会泄漏\n5、存储过程会给数据库服务器增加额外的压力，而数据库服务器的扩展性相比应用服务器要差很多\n6、MySQL无法控制存储程序的资源消耗，所以在存储过程中的一个小错误，可能直接把服务器拖死\n7、存储代码在MySQL中的实现也有很多限制——执行计划缓存是连接级别的，游标的物化和临时表相同，在MySQL 5.5版本之前，异常处理也非常困难，等等。简而言之，较之T-SQL或者PL/SQL，MySQL的存储代码功能还非常非常弱\n8、调试MySQL的存储过程是一件很困难的事情。如果慢日志只是给出CALL XYZ(\'A\')，通常很难定位到底是什么导致的问题，这时不得不看看存储过程中的SQL语句是如何编写的。（这在Percona Server中可以通过参数控制。）\n9、它和基于语句的二进制日志复制合作得并不好。在基于语句的复制中，使用存储代码通常有很多的陷阱，除非你在这方面的经验非常丰富或者非常有耐心排查这类问题，否则需要谨慎使用\n\n存储过程和函数\n存储过程和函数的限制\n1、优化器无法使用关键字DETERMINISTIC来优化单个查询中多次调用存储函数的情况\n2、优化器无法评估存储函数的执行成本\n3、每个连接都有独立的存储过程的执行计划缓存。如果有多个连接需要调用同一个存储过程，将会浪费缓存空间来反复缓存同样的执行计划。（如果使用的是连接池或者是持久化连接，那么执行计划缓存可能会有更长的生命周期。）\n4、存储程序和复制是一组诡异组合。如果可以，最好不要复制对存储程序的调用。直接复制由存储程序改变的数据则会更好。MySQL 5.1引入的行复制能够改善这个问题。如果在MySQL 5.0中开启了二进制日志，那么要么在所有的存储过程中都增加DETERMINISTIC限制或者设置MySQL的选项log_bin_trust_function_creators。\n\n触发器\n触发器可以让你在执行INSERT、UPDATE或者DELETE的时候，指定执行前触发还是在执行后触发一些特定的操作。\n触发器注意\n1、对每一个表的每一个事件，最多只能定义一个触发器（换句话说，不能在AFTER INSERT上定义两个触发器）。\n2、MySQL只支持“基于行的触发”，也就是说，触发器始终是针对一条记录的，而不是针对整个SQL语句的。如果变更的数据集非常大的话，效率会很低。\n3、触发器可以掩盖服务器背后的工作（sql的触发器引起的大量操作）\n4、触发器的问题很难排查，如果某个性能问题和触发器相关，会很难分析和定位\n5、触发器可能导致死锁和锁等待\n6、如果触发器失败，那么原来的SQL语句也会失败而导致服务器抛出的错误代码\n7、操作MyISAM表的时候，如果遇到错误，没有办法做回滚操作，InnoDB表可以回滚\n\n事件\n1、类似于Linux的定时任务，不过是完全在MySQL内部实现的,可以在表INFORMATION_SCHEMA.EVENTS中看到各个事件状态。\n2、事件在一个独立事件调度线程中被初始化，这个线程和处理连接的线程没有任何关系。它不接收任何参数，也没有任何的返回值\n3、你可以指定事件本身是否被复制。有时需要被复制，有时则不需要\n4、如果一个定时事件执行需要很长的时间，那么有可能会出现前面一个事件还未执行完成，下一个时间点的事件又开始了。MySQL本身不会防止这种并发，所以需要用户自己编写这种情况下的防并发代码。你可以使用函数GET_LOCK()来确保当前总是只有一个事件在被执行</code></pre>\n\n<h2>游标</h2>\n\n<pre>\n<code>MySQL在服务器端提供只读的、单向的游标，而且只能在存储过程或者更底层的客户端API中使用。因为MySQL游标中指向的对象都是存储在临时表中而不是实际查询到的数据，所以MySQL游标总是只读的。它可以逐行指向查询结果，然后让程序做进一步的处理。在一个存储过程中，可以有多个游标，也可以在循环中“嵌套”地使用游标</code></pre>\n\n<h2>绑定变量</h2>\n\n<pre>\n<code>当查询语句的解析和执行计划生成消耗了主要的时间，那么绑定变量可以在一定程度上解决问题。因为只需要解析一次，对于大量重复类型的查询语句，性能会有很大的提高。另外，执行计划的缓存和传输使用的二进制协议，这都使得绑定变量的方式比普通SQL语句执行的方式要更快。\n\n创建一个绑定变量sql时客户端向服务器发送了一个sql语句原型，可以使用问号作为sql的占位,在使用sql接口执行时赋予变量值（INSERT INTO tbl(col1, col2, col3) VALUES (?, ?, ?);）\n服务器端解析并存储这个sql语句的部分执行计划返回客户端一个sql语句处理句柄\n通过向服务器端发送各个问号的取值和这个SQL的句柄来执行一个具体的查询\n\n优势\n1、在服务器端只需要解析一次SQL语句。\n2、在服务器端某些优化器的工作只需要执行一次，因为它会缓存一部分的执行计划。\n3、以二进制的方式只发送参数和句柄，比起每次都发送ASCII码文本效率更高，一个二进制的日期字段只需要三个字节，但如果是ASCII码则需要十个字节。不过最大的节省还是来自于BLOB和TEXT字段，绑定变量的形式可以分块传输，而无须一次性传输。二进制协议在客户端也可能节省很多内存，减少了网络开销，另外，还节省了将数据从存储原始格式转换成文本格式的开销。\n4、仅仅是参数——而不是整个查询语句——需要发送到服务器端，所以网络开销会更小。\n5、MySQL在存储参数的时候，直接将其存放到缓存中，不再需要在内存中多次复制。\n6、绑定变量相对也更安全。无须在应用程序中处理转义，一则更简单了，二则也大大减少了SQL注入和攻击的风险\n\n限制和注意事项\n1、绑定变量是会话级别的，所以连接之间不能共用绑定变量句柄。同样地，一旦连接断开，则原来的句柄也不能再使用了。（连接池和持久化连接可以在一定程度上缓解这个问题。）\n2、并不是所有的时候使用绑定变量都能获得更好的性能。如果只是执行一次SQL，那么使用绑定变量方式无疑比直接执行多了一次额外的准备阶段消耗，而且还需要一次额外的网络开销。（要正确地使用绑定变量，还需要在使用完成后，释放相关的资源。）\n3、当前版本下，还不能在存储函数中使用绑定变量（但是存储过程中可以使用）。\n4、如果总是忘记释放绑定变量资源，则在服务器端很容易发生资源“泄漏”。绑定变量 SQL总数的限制是一个全局限制，所以某一个地方的错误可能会对所有其他的线程都产生影响。\n5、有些操作，如BEGIN，无法在绑定变量中完成。</code></pre>\n\n<h2>用户自定义函数</h2>\n\n<pre>\n<code>用户自定义函数（UDF）\n存储过程只能使用SQL来编写，而UDF没有这个限制，你可以使用支持C语言调用约定的任何编程语言来实现\nUDF必须事先编译好并动态链接到服务器上，这种平台相关性使得UDF在很多方面都很强大。UDF速度非常快，而且可以访问大量操作系统的功能，还可以使用大量库函数\n和使用SQL语言编写存储程序不同，UDF无法读写数据表——至少，无法在调用UDF的线程中使用当前事务处理的上下文来读写数据表。这意味着，它更适合用作计算或者与外面的世界交互</code></pre>\n\n<h2>插件</h2>\n\n<pre>\n<code>存储过程插件\n后台插件: 如Percona Server中包含的Handler-Socket\nINFORMATION_SCHEMA插件\n全文解析插件: 可以对文档进行分词处理\n审计插件: 可以用作记录事件日志\n认证插件: 扩展认证功能</code></pre>\n\n<h2>字符集和校对</h2>\n\n<pre>\n<code>字符集：一种从二进制编码到某类字符符号的映射\n校对：一组用于某个字符集的排序规则，格式：以其相关的字符集名开始，中间包括一个语言名，并且以_ci（大小写不敏感）、_cs（大小写敏感）或_bin（二元）结束\n每种字符集都可能有多种校对规则，并且都有一个默认的校对规则。每个校对规则都只针对某个特定的字符集\n\n创建数据库的时候，默认字符集为服务器上的character_set_server的字符集。创建表的时候，默认字符集为数据库的字符集。创建列的时候，默认字符集为表的字符集。但真正存放数据的是列，只有当列没有指定字符集的时候，列数据才会使用表的字符集进行存储\n\n服务器和客户端通信\n当服务器和客户端通信的时候，它们可能使用不同的字符集。这时，服务器端将进行必要的翻译转换工作：\n1、服务器端总是假设客户端是按照character_set_client设置的字符来传输数据和SQL语句的。\n2、当服务器收到客户端的SQL语句时，它先将其转换成字符集character_set_connection。它还使用这个设置来决定如何将数据转换成字符串。\n3、当服务器端返回数据或者错误信息给客户端时，它会将其转换成character_set_result。\n根据需要，可以使用SET NAMES或者SET CHARACTER SET语句来改变上面的设置。不过在服务器上使用这个命令只能改变服务器端的设置。客户端程序和客户端的API也需要使用正确的字符集才能避免在通信时出现问题\n\n选择字符集和校对规则\n字符集：一个数据库中使用统一字符集，否则容易造成字符集之间的不兼容问题。\n校对规则：_ci（以大小写不敏感来比较大小）、_cs（以大小写敏感来比较大小）或_bin（以字符串编码的二进制值来比较大小）</code></pre>\n\n<h2>全文索引</h2>\n\n<h2>分布式（XA）事务</h2>\n\n<pre>\n<code>很少有人用MySQL的XA事务特性。除非你真正明白参数innodb_support_xa的意义，否则不要修改这个参数的值，并不是只有显式使用XA事务时才需要设置这个参数。InnoDB和二进制日志也是需要使用XA事务来做协调的，从而确保在系统崩溃的时候，数据能够一致地恢复。</code></pre>\n\n<h2>查询缓存</h2>\n\n<pre>\n<code>完全相同的查询在重复执行的时候，查询缓存可以立即返回结果，而无须在数据库中重新执行一次。根据我们的经验，在高并发压力环境中查询缓存会导致系统性能的下降，甚至僵死。如果你一定要使用查询缓存，那么不要设置太大内存，而且只有在明确收益的时候才使用。那该如何判断是否应该使用查询缓存呢？建议使用Percona Server，观察更细致的日志，并做一些简单的计算。还可以查看缓存命中率（并不总是有用）、“INSERTS和SELECT比率”（这个参数也并不直观）、或者“命中和写入比率”（这个参考意义较大）。查询缓存是一个非常方便的缓存，对应用程序完全透明，无须任何额外的编码，但是，如果希望有更高的缓存效率，我们建议使用memcached或者其他类似的解决方案\n\nMySQL查询缓存保存查询返回的完整结果。当查询命中该缓存，MySQL会立刻返回结果，跳过了解析、优化和执行阶段。\n\n当查询语句中有一些不确定的数据时，则不会被缓存。例如函数NOW()或者CURRENT_USER等\n当查询语句中有用户自定义函数、存储函数、用户变量、临时表、mysql库中的系统表，或者任何包含列级别权限的表，都不会被缓存\n\n查询缓存系统会跟踪查询中涉及的每个表，如果这些表发生变化，那么和这个表相关的所有的缓存数据都将失效\n\nMySQL如何判断缓存命中\n缓存存放在一个引用表中，通过一个哈希值引用，这个哈希值包括了如下因素，即查询本身、当前要查询的数据库、客户端协议的版本等一些其他可能会影响返回结果的信息。当判断缓存是否命中时，MySQL不会解析、“正规化”或者参数化查询语句，而是直接使用SQL语句和客户端发送过来的其他原始信息。任何字符上的不同，例如空格、注释等，任何的不同都会导致缓存的不命中。\n\n打开查询缓存的额外消耗\n1、读查询在开始之前必须先检查是否命中缓存。\n2、如果这个读查询可以被缓存，那么当完成执行后，MySQL若发现查询缓存中没有这个查询，会将其结果存入查询缓存\n3、当向某个表写入数据的时候，MySQL必须将对应表的所有缓存都设置失效。如果查询缓存非常大或者碎片很多，这个操作就可能会带来很大系统消耗\n\n缓存未命中的原因\n1、查询语句无法被缓存，可能是因为查询中包含一个不确定的函数，或者查询结果太大而无法缓存。\n2、MySQL从未处理这个查询，所以结果也从不曾被缓存过。\n3、虽然之前缓存了查询结果，但是由于查询缓存的内存用完了，MySQL需要将某些缓存“逐出”，或者由于数据表被修改导致缓存失效。\n\n查询大多已缓存但仍大量未命中的原因\n1、查询缓存还没有完成预热。也就是说，MySQL还没有机会将查询结果都缓存起来。\n2、查询语句之前从未执行过。如果你的应用程序不会重复执行一条查询语句，那么即使完成预热仍然会有很多缓存未命中。\n3、缓存失效操作太多了。\n\n配置和维护查询缓存\nquery_cache_type：是否打开查询缓存（OFF、ON或DEMAND）。DEMAND表示查询语句明确写明SQL_CACHE的语句才放入查询缓存。这个变量可以是会话级别也可以是全局级别\nquery_cache_size：查询缓存使用的总内存空间，单位是字节，值必须是1024的整数倍\nquery_cache_min_res_unit：在查询缓存中分配内存块时的最小单位\nquery_cache_limit：MySQL能够缓存的最大查询结果。如果查询结果大于这个值，则不会被缓存\nquery_cache_wlock_invalidate：如果某个数据表被其他的连接锁住，是否仍然从查询缓存中返回结果。默认是OFF，这可能在一定程序上会改变服务器的行为，因为这使得数据库可能返回其他线程锁住的数据。将参数设置成ON，则不会从缓存中读取这类数据，但是这可能会增加锁等待</code></pre>\n\n<p>&nbsp;</p>', 1, '5f199b8885e24fc8b28672b872edb606', 16, '2018-09-01 10:58:12', '2018-09-11 09:08:56');
INSERT INTO `logcontent` VALUES ('241b5fc5691740eb9b088f6bbb59719f', -1, '7338e53acd514defa1a17e47016f3f4a', 'Elasticsearch（十一）', 'mget批量查询,bulk批量增删改', 'mget批量查询/bulk批量增删改', '<p><a name=\"mget批量查询\"></a>1、mget批量查询</p>\n\n<p>&nbsp;</p>\n\n<p>优势：查询100条数据，就要发送100次网络请求，如果进行批量查询的话，查询100条数据，就只要发送1次网络请求，网络请求的性能开销缩减100倍</p>\n\n<p>&nbsp;</p>\n\n<p>（1）一条一条的查询</p>\n\n<pre>\n<code class=\"language-json\">GET e/p/1\nGET e/p/2</code></pre>\n\n<p>（2）mget批量查询</p>\n\n<pre>\n<code class=\"language-json\">GET /_mget\n{\n  \"docs\":[\n     {\n      \"_index\":\"e\",\n      \"_type\":\"p\",\n      \"_id\":1\n     },\n     {\n      \"_index\":\"e\",\n      \"_type\":\"p\",\n      \"_id\":2\n     }\n  ]\n}\n=====结果======\n{\n  \"docs\": [\n    {\n      \"_index\": \"e\",\n      \"_type\": \"p\",\n      \"_id\": \"1\",\n      \"_version\": 1,\n      \"found\": true,\n      \"_source\": {\n        \"name\": \"gaolujie yagao\",\n        \"desc\": \"gaoxiao meibai\",\n        \"price\": 30,\n        \"producer\": \"gaolujie producer\",\n        \"tags\": [\n          \"meibai\",\n          \"fangzhu\"\n        ]\n      }\n    },\n    {\n      \"_index\": \"e\",\n      \"_type\": \"p\",\n      \"_id\": \"2\",\n      \"_version\": 1,\n      \"found\": true,\n      \"_source\": {\n        \"name\": \"jiajieshi yagao\",\n        \"desc\": \"youxiao fangzhu\",\n        \"price\": 25,\n        \"producer\": \"jiajieshi producer\",\n        \"tags\": [\n          \"fangzhu\"\n        ]\n      }\n    }\n  ]\n}</code></pre>\n\n<p>（3）如果查询的document是index或者type相同的话，可以提取到上面去</p>\n\n<pre>\n<code class=\"language-json\">GET /p/_mget\n{\n  \"docs\":[\n     {\n      \"_index\":\"e\",\n      \"_id\":1\n     },\n     {\n      \"_index\":\"e\",\n      \"_id\":2\n     }\n  ]\n}\n单独提取index或者type或者全部都可以</code></pre>\n\n<p>（4）如果查询的数据都在同一个index下的同一个type下，最简单了</p>\n\n<pre>\n<code class=\"language-json\">GET /test_index/test_type/_mget\n{\n   \"ids\": [1, 2]\n}</code></pre>\n\n<p><a name=\"bulk批量增删改\"></a>2、bulk批量增删改</p>\n\n<pre>\n<code class=\"language-json\">语法（每个json串不能换行，只能放一行，同时一个json串和一个json串之间，必须有一个换行）\nPOST /_bulk\n{ \"delete\": { \"_index\": \"e\", \"_type\": \"p\", \"_id\": \"1\" }} \n{ \"create\": { \"_index\": \"e\", \"_type\": \"p\", \"_id\": \"1\" }}\n{ \"test_field\":    \"test12\" }\n{ \"index\":  { \"_index\": \"e\", \"_type\": \"p\", \"_id\": \"1\" }}\n{ \"test_field\":    \"replaced test2\" }\n{ \"update\": { \"_index\": \"e\", \"_type\": \"p\", \"_id\": \"1\", \"_retry_on_conflict\" : 3} }\n{ \"doc\" : {\"test_field2\" : \"bulk test1\"} }</code></pre>\n\n<pre>\n<code class=\"language-json\">一般每个操作要两个json串（除delete），语法如下：\n\n{\"action动作\": {\"metadata\"}}\n{\"data数据\"}\n\n举例，比如你现在要创建一个文档，放bulk里面，看起来会是这样子的：\n\n{\"index\": {\"_index\": \"e\", \"_type\", \"p\", \"_id\": \"1\"}}\n{\"test_field1\": \"test1\", \"test_field2\": \"test2\"}\n\n有哪些类型的action操作可以执行呢？\n（1）delete：删除一个文档，只要1个json串就可以了\n（2）create：PUT /index/type/id/_create，强制创建\n（3）index：普通的put操作，可以是创建文档，也可以是全量替换文档\n（4）update：执行的partial update操作</code></pre>\n\n<pre>\n<code class=\"language-json\">bulk操作中，任意一个操作失败，是不会影响其他的操作的，但是在返回结果里，会告诉你异常日志\n如果修改的document是index或者type相同的话，可以提取到上面去</code></pre>\n\n<pre>\n\n（1）bulk size最佳大小</pre>\n\n<p>bulk request会加载到内存里，如果太大的话，性能反而会下降，因此需要反复尝试一个最佳的bulk size。一般从1000~5000条数据开始，尝试逐渐增加。另外，如果看大小的话，最好是在5~15MB之间。</p>\n\n<p>（2）bulk api的奇特json格式与底层性能优化关系大揭秘</p>\n\n<p>bulk中的每个操作都可能要转发到不同的node的shard去执行</p>\n\n<p>如果采用比较良好的json数组格式，要按照下述流程去进行处理</p>\n\n<p>将json数组解析为JSONArray对象，这个时候，整个数据，就会在内存中出现一份一模一样的拷贝，一份数据是json文本，一份数据是JSONArray对象</p>\n\n<p>解析json数组里的每个json，对每个请求中的document进行路由</p>\n\n<p>为路由到同一个shard上的多个请求，创建一个请求数组</p>\n\n<p>将这个请求数组序列化</p>\n\n<p>将序列化后的请求数组发送到对应的节点上去</p>\n\n<p>&nbsp;</p>\n\n<p>耗费更多内存，更多的jvm gc开销</p>\n\n<p>我们之前提到过bulk size最佳大小的那个问题，一般建议说在几千条那样，然后大小在10MB左右，所以说，可怕的事情来了。假设说现在100个bulk请求发送到了一个节点上去，然后每个请求是10MB，100个请求，就是1000MB = 1GB，然后每个请求的json都copy一份为jsonarray对象，此时内存中的占用就会翻倍，就会占用2GB的内存，甚至还不止。因为弄成jsonarray之后，还可能会多搞一些其他的数据结构，2GB+的内存占用。</p>\n\n<p>&nbsp;</p>\n\n<p>占用更多的内存可能就会积压其他请求的内存使用量，比如说最重要的搜索请求，分析请求，等等，此时就可能会导致其他请求的性能急速下降</p>\n\n<p>另外的话，占用内存更多，就会导致java虚拟机的垃圾回收次数更多，跟频繁，每次要回收的垃圾对象更多，耗费的时间更多，导致es的java虚拟机停止工作线程的时间更多</p>\n\n<p>&nbsp;</p>\n\n<p>现在的奇特格式</p>\n\n<p>{&quot;action&quot;: {&quot;meta&quot;}}\\n</p>\n\n<p>{&quot;data&quot;}\\n</p>\n\n<p>{&quot;action&quot;: {&quot;meta&quot;}}\\n</p>\n\n<p>{&quot;data&quot;}\\n</p>\n\n<p>不用将其转换为json对象，不会出现内存中的相同数据的拷贝，直接按照换行符切割json</p>\n\n<p>对每两个一组的json，读取meta，进行document路由</p>\n\n<p>直接将对应的json发送到node上去</p>\n\n<p>&nbsp;</p>\n\n<p>最大的优势在于，不需要将json数组解析为一个JSONArray对象，形成一份大数据的拷贝，浪费内存空间，尽可能地保证性能</p>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n', 2, '5f199b8885e24fc8b28672b872edb606', 5, '2017-07-10 18:04:27', '2018-09-06 16:12:58');
INSERT INTO `logcontent` VALUES ('24b0efb7beb945e59f7875f63220a3b6', 1, 'de93ceeed0464710af6dbb22e535d0a2', 'tkinter图形化界面及打包笔记', 'tkinter', '对tkinter图形化界面及打包做一个简单的总结', '<p><a id=\"Python环境\" name=\"Python环境\"></a>Python环境</p>\n\n<pre>\n<code class=\"language-bash\">Windows版 3.5.4</code></pre>\n\n<p><a id=\"demo\" name=\"demo\"></a>demo，可执行</p>\n\n<pre>\n<code class=\"language-python\"># -*- coding:utf8 -*-\nfrom tkinter import *\nfrom tkinter import filedialog,messagebox\n\nfilePath = \'\'\ncheckAll = True\ndef openfile():\n    global filePath\n    filePath = filedialog.askopenfilename(title=\'打开文件\', filetypes=[(\'Excel\', \'*.xls *.xlsx\'), (\'All Files\', \'*\')])\n    strvar.set(filePath)\ndef cheAll():\n    global checkAll\n    if checkAll:\n        checkAll = False\n    else:\n        checkAll = True\n\n####提交####\ndef subm():\n    flag = messagebox.askokcancel(\'导出\', \'确认导出\')\n    if not flag:\n        return\n    t.delete(1.0,END)\n    baseUrl = e1.get()\n    detailTrue = var.get()\n    entryId = e2.get()\n    randomOrNot = var2.get()\n    randomNum = int(s2.get())\n    t.insert(END, \'基本链接:\'+str(baseUrl)+\'\\n\')\n    t.insert(END, \'是否详情:\'+str(detailTrue)+\'\\n\')\n    t.insert(END, \'详情主键Id:\'+str(entryId)+\'\\n\')\n    t.insert(END, \'是否随机:\'+str(randomOrNot)+\'\\n\')\n    t.insert(END, \'随机数:\'+str(randomNum)+\'\\n\')\n    t.insert(END, \'Excel上传路径:\'+str(filePath)+\'\\n\')\n    t.insert(END, \'是否勾选全部:\'+str(checkAll)+\'\\n\')\n    root.update()\n\n####界面区####\nif __name__ == \"__main__\":\n    root = Tk()\n    root.title(\"数据导出\")\n    root.geometry(\'600x700\')\n    Label(root, text=\'数据导出\', font=(\'宋体\', 15)).grid(row=0, column=0, columnspan=4, sticky=N)\n    Label(root, text=\'—————————————————————————————————————————————\').grid(row=1, column=0, columnspan=4, sticky=NW)\n    \n    L0 = Label(root)\n    l1 = Label(L0, text=\'基本链接: \', font=(\'宋体\', 10))\n    l1.pack(side=LEFT)\n    e1 = Entry(L0,width=70)\n    e1.pack(side=LEFT)\n    L0.grid(row=2, column=0, columnspan=4, sticky=NW,pady=7)\n    \n    L1 = Label(root)\n    l2 = Label(L1, text=\'是否需要详情:\', font=(\'宋体\', 10))\n    l2.pack(side=LEFT)\n    var = BooleanVar()\n    R1 = Radiobutton(L1, text=\"是\", variable=var, value=True)\n    R1.pack(side=LEFT)\n    R2 = Radiobutton(L1, text=\"否\", variable=var, value=False)\n    R2.select()\n    R2.pack(side=LEFT)\n    l3 = Label(L1)#空行\n    l3.pack(side=LEFT,padx=50)\n    l4 = Label(L1, text=\'是否随机取页:\', font=(\'宋体\', 10))\n    l4.pack(side=LEFT)\n    var2 = BooleanVar()\n    R3 = Radiobutton(L1, text=\"是\", variable=var2, value=True)\n    R3.pack(side=LEFT)\n    R4 = Radiobutton(L1, text=\"否\", variable=var2, value=False)\n    R4.select()\n    R4.pack(side=LEFT)\n    L1.grid(row=3, column=0, columnspan=4, sticky=NW,pady=7)\n    \n    L3 = Label(root)\n    l6 = Label(L3, text=\'详情主键Id:\', font=(\'宋体\', 10))\n    l6.pack(side=LEFT)\n    e2 = Entry(L3,width=10)\n    e2.pack(side=LEFT)\n    l5 = Label(L3)#空行\n    l5.pack(side=LEFT,padx=60)\n    l8 = Label(L3, text=\'随机页数:\', font=(\'宋体\', 10))\n    l8.pack(side=LEFT)\n    s2 = Spinbox(L3,from_ = 0,to = 1000,width=5)\n    s2.pack(side=LEFT)\n    c = Checkbutton(L3,text=\'全部\', command=cheAll)\n    c.select()\n    c.pack()\n    L3.grid(row=4, column=0, columnspan=4, sticky=NW,pady=7)\n    \n    L5 = Label(root)\n    l9 = Label(L5, text=\'上传公司Excel:\', font=(\'宋体\', 10))\n    l9.pack(side=LEFT)\n    strvar = StringVar()\n    strvar.set(\"Excel上传\")\n    btn = Button(L5, textvariable=strvar, command=openfile,width=50)\n    btn.pack(side=LEFT)\n    L5.grid(row=5, column=0, columnspan=3, sticky=NW,pady=7)\n    \n    buttontext = StringVar()\n    buttontext.set(\'导出Excel\')\n    button = Button(root, textvariable=buttontext, width=60, command=subm)\n    button.grid(row=6, column=0, columnspan=3, sticky=NW,pady=7,padx=80)\n    \n    t = Text(root)\n    t.grid(row=7, column=0, columnspan=3, sticky=NW,pady=7,padx=16)\n    root.mainloop()</code></pre>\n\n<p>额外参考：<a href=\"https://my.oschina.net/TyLucifer/blog/112961\">Python Tkinter参考资料之（通用控件属性）</a></p>\n\n<p><a id=\"打包exe文件\" name=\"打包exe文件\"></a>打包exe文件</p>\n\n<p>打包使用PyInstaller</p>\n\n<p>查看是否已安装</p>\n\n<pre>\n<code class=\"language-bash\">pip3 list</code></pre>\n\n<p>如果没有，可以pip安装，如果安装失败自行百度</p>\n\n<pre>\n<code class=\"language-bash\">pip3 install PyInstaller</code></pre>\n\n<p>开始打包（python单文件打包，比较基础，仅供参考）</p>\n\n<pre>\n<code class=\"language-bash\">进入指定py文件的目录，然后执行\npyinstaller -F -w pyExpor.py</code></pre>\n\n<p><a id=\"参数简单解释\" name=\"参数简单解释\"></a>参数简单解释</p>\n\n<table align=\"left\" border=\"2\" cellspacing=\"0\" style=\"border-collapse:collapse; border:none; width:500pt\">\n	<tbody>\n		<tr>\n			<td style=\"height:14.25pt; vertical-align:bottom; white-space:nowrap; width:450pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">-h, --help</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:bottom; white-space:nowrap; width:278pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">参数介绍</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">-v, --version</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">展示版本</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">-D, --onedir</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">生成一个包含可执行文件的文件夹（好多文件）</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">-F, --onefile</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">只生成一个可执行文件（只有一个文件）</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">--specpath DIR</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">存储生成的spec文件的文件夹(默认:当前目录)</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">-c, --console, --nowindowed</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">打开exe文件操作时后台显示cmd窗口打印输出</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">-w, --windowed, --noconsole</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">打开exe文件操作时隐藏cmd窗口</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">-i &lt;FILE.ico or FILE.exe,ID or FILE.icns&gt;, --icon &lt;FILE.ico or FILE.exe,ID or FILE.icns&gt;</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">指定ico图标文件</span></span></span></td>\n		</tr>\n	</tbody>\n</table>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>', 1, '5f199b8885e24fc8b28672b872edb606', 70, '2017-08-17 00:00:00', '2018-08-31 22:21:23');
INSERT INTO `logcontent` VALUES ('24bd79ceeeb5437d997f9ead04728dad', -1, '4eca926aa543420baea2f03f506d042e', 'Mysql语句笔记', '函数,触发器', 'mysql函数', '<p><a name=\"Mysql\"></a><a id=\"Mysql基本语句\" name=\"Mysql基本语句\"></a>Mysql基本语句：</p>\n\n<pre>\n<code class=\"language-sql\">连接：mysql -h主机地址 -u用户名 －p用户密码 （注:u与root可以不用加空格，其它也一样） \n断开：exit （回车） \n\n创建授权：grant select on 数据库.* to 用户名@登录主机 identified by \\\"密码\\\" \n修改密码：mysqladmin -u用户名 -p旧密码 password 新密码 \n删除授权: revoke select,insert,update,delete om *.* from test2@localhost; \n\n显示数据库：show databases; \n显示数据表：show tables; \n显示表结构：desc 表名; \n\n创建库：create database 库名; \n删除库：drop database 库名; \n使用库(选中库)：use 库名; \n\n创建表：create table 表名 (字段设定列表); \n删除表：drop table 表名; \n修改表名：alter table t1 rename t2 \n查询表：select * from 表名; \n清空表：delete from 表名; \n\n拷贝表结构,不拷贝表数据：CREATE TABLE newadmin LIKE admin\n拷贝数据：INSERT INTO newadmin SELECT * FROM admin;\n拷贝表结构和数据，不会有主键，索引（不推荐）：CREATE TABLE newadmin AS (SELECT * FROM admin)\n拷贝表和表数据推荐CREATE TABLE newadmin LIKE admin; INSERT INTO newadmin SELECT * FROM admin;\n\n备份表: mysqlbinmysqldump -h(ip) -uroot -p(password) databasename tablename &gt; tablename.sql \n恢复表: mysqlbinmysql -h(ip) -uroot -p(password) databasename tablename &lt; tablename.sql（操作前先把原来表删除） \n\n增加列：ALTER TABLE t2 ADD c INT UNSIGNED NOT NULL AUTO_INCREMENT,ADD INDEX (c); \n修改列：ALTER TABLE t2 CHANGE a b VARCHAR(10) NOT NULL;\n删除列：ALTER TABLE t2 DROP COLUMN c; \n\n备份数据库：mysql\\bin\\mysqldump -h(ip) -uroot -p(password) databasename &gt; database.sql \n恢复数据库：mysql\\bin\\mysql -h(ip) -uroot -p(password) databasename &lt; database.sql \n复制数据库：mysql\\bin\\mysqldump --all-databases &gt; all-databases.sql \n修复数据库：mysqlcheck -A -o -uroot -p54safer \n\n文本数据导入： load data local infile \\\"文件名\\\" into table 表名; \n数据导入导出：mysql\\bin\\mysqlimport database tables.txt</code></pre>\n\n<p><a id=\"Mysql自定义函数用法\" name=\"Mysql自定义函数用法\"></a><strong>Mysql自定义函数用法</strong></p>\n\n<p>自定义函数 (user-defined function UDF)：对MySQL功能的一个扩展</p>\n\n<p>创建UDF:</p>\n\n<pre>\n<code class=\"language-sql\">CREATE FUNCTION 函数名称(参数列表)\n　　RETURNS 返回值类型\n　　函数体\n\n注：函数体由合法的SQL语句构成；\n函数体可以是简单的select或insert语句；\n函数体如果为符合结构则使用begin…end；\n复合结构可以包括声明、循环、控制结构；</code></pre>\n\n<p>删除UDF:</p>\n\n<pre>\n<code class=\"language-sql\">DROP FUNCTION 函数名称</code></pre>\n\n<p>调用自定义函数语法:</p>\n\n<pre>\n<code class=\"language-sql\">SELECT function_name(parameter_value,...)</code></pre>\n\n<hr />\n<p>eg:创建简单的无参UDF</p>\n\n<pre>\n<code>CREATE FUNCTION simpleFun()RETURNS VARCHAR(20) RETURN \"Hello World!\";</code></pre>\n\n<p>调用该无参UDF</p>\n\n<pre>\n<code class=\"language-sql\">SELECT simpleFun()</code></pre>\n\n<p>删除该无参UDF</p>\n\n<pre>\n<code class=\"language-sql\">DROP FUNCTION simpleFun</code></pre>\n\n<p>&nbsp;</p>', 1, '5f199b8885e24fc8b28672b872edb606', 36, '2018-01-26 09:55:29', '2018-09-12 09:53:08');
INSERT INTO `logcontent` VALUES ('28a8700222294e4d98c45a05af1d94e6', -1, '7338e53acd514defa1a17e47016f3f4a', '七、排序与相关性', '排序与相关性', '排序与相关性', '<h2>排序与相关性</h2>\n\n<h2>排序</h2>\n\n<p>为了按照相关性来排序，需要将相关性表示为一个数值。在 Elasticsearch 中，&nbsp;<em>相关性得分</em>&nbsp;由一个浮点数进行表示，并在搜索结果中通过&nbsp;<code>_score</code>&nbsp;参数返回，&nbsp;默认排序是&nbsp;<code>_score</code>&nbsp;降序。</p>\n\n<p>有时，相关性评分对你来说并没有意义。例如，下面的查询返回所有&nbsp;<code>user_id</code>&nbsp;字段包含&nbsp;<code>1</code>&nbsp;的结果：</p>\n\n<pre>\nGET /_search\n{\n    &quot;query&quot; : {\n        &quot;bool&quot; : {\n            &quot;filter&quot; : {\n                &quot;term&quot; : {\n                    &quot;user_id&quot; : 1\n                }\n            }\n        }\n    }\n}</pre>\n\n<p>这里没有一个有意义的分数：因为我们使用的是 filter （过滤），这表明我们只希望获取匹配&nbsp;<code>user_id: 1</code>的文档，并没有试图确定这些文档的相关性。 实际上文档将按照随机顺序返回，并且每个文档都会评为零分。</p>\n\n<p><img alt=\"注意\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/note.png\" /></p>\n\n<p>如果评分为零对你造成了困扰，你可以使用&nbsp;<code>constant_score</code>&nbsp;查询进行替代：</p>\n\n<pre>\nGET /_search\n{\n    &quot;query&quot; : {\n        &quot;constant_score&quot; : {\n            &quot;filter&quot; : {\n                &quot;term&quot; : {\n                    &quot;user_id&quot; : 1\n                }\n            }\n        }\n    }\n}</pre>\n\n<p>这将让所有文档应用一个恒定分数（默认为&nbsp;<code>1</code>&nbsp;）。它将执行与前述查询相同的查询，并且所有的文档将像之前一样随机返回，这些文档只是有了一个分数而不是零分。</p>\n\n<h3>按照字段的值排序</h3>\n\n<p>在这个案例中，通过时间来对 tweets 进行排序是有意义的，最新的 tweets 排在最前。&nbsp;我们可以使用<code>sort</code>&nbsp;参数进行实现：</p>\n\n<pre>\n<code>GET /_search\n{\n    \"query\" : {\n        \"bool\" : {\n            \"filter\" : { \"term\" : { \"user_id\" : 1 }}\n        }\n    },\n    \"sort\": { \"date\": { \"order\": \"desc\" }}\n}</code></pre>\n\n<p>你会注意到结果中的两个不同点：</p>\n\n<pre>\n<code>\"hits\" : {\n    \"total\" :           6,\n    \"max_score\" :       null, \n    \"hits\" : [ {\n        \"_index\" :      \"us\",\n        \"_type\" :       \"tweet\",\n        \"_id\" :         \"14\",\n        \"_score\" :      null, \n        \"_source\" :     {\n             \"date\":    \"2014-09-24\",\n             ...\n        },\n        \"sort\" :        [ 1411516800000 ] \n    },\n    ...\n}</code></pre>\n\n<table border=\"0\" summary=\"Callout list\">\n	<tbody>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/_Sorting.html#CO27-1\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" /></a>&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/_Sorting.html#CO27-2\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/2.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p><code>_score</code>&nbsp;不被计算, 因为它并没有用于排序。</p>\n			</td>\n		</tr>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/_Sorting.html#CO27-3\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/3.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p><code>date</code>&nbsp;字段的值表示为自 epoch (January 1, 1970 00:00:00 UTC)以来的毫秒数，通过&nbsp;<code>sort</code>&nbsp;字段的值进行返回。</p>\n			</td>\n		</tr>\n	</tbody>\n</table>\n\n<p>首先我们在每个结果中有一个新的名为&nbsp;<code>sort</code>&nbsp;的元素，它包含了我们用于排序的值。 在这个案例中，我们按照&nbsp;<code>date</code>&nbsp;进行排序，在内部被索引为&nbsp;<em>自 epoch 以来的毫秒数</em>&nbsp;。 long 类型数&nbsp;<code>1411516800000</code>&nbsp;等价于日期字符串&nbsp;<code>2014-09-24 00:00:00 UTC</code>&nbsp;。</p>\n\n<p>其次&nbsp;<code>_score</code>&nbsp;和&nbsp;<code>max_score</code>&nbsp;字段都是&nbsp;<code>null</code>&nbsp;。&nbsp;计算&nbsp;<code>_score</code>&nbsp;的花销巨大，通常仅用于排序； 我们并不根据相关性排序，所以记录&nbsp;<code>_score</code>&nbsp;是没有意义的。如果无论如何你都要计算&nbsp;<code>_score</code>&nbsp;， 你可以将<code>track_scores</code>&nbsp;参数设置为&nbsp;<code>true</code>&nbsp;。</p>\n\n<p><img alt=\"提示\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/tip.png\" /></p>\n\n<p>一个简便方法是, 你可以&nbsp;指定一个字段用来排序：</p>\n\n<pre>\n    &quot;sort&quot;: &quot;number_of_children&quot;</pre>\n\n<p>字段将会默认升序排序&nbsp;，而按照&nbsp;<code>_score</code>&nbsp;的值进行降序排序。</p>\n\n<h3>多级排序</h3>\n\n<p>假定我们想要结合使用&nbsp;<code>date</code>&nbsp;和&nbsp;<code>_score</code>&nbsp;进行查询，并且匹配的结果首先按照日期排序，然后按照相关性排序：</p>\n\n<pre>\nGET /_search\n{\n    &quot;query&quot; : {\n        &quot;bool&quot; : {\n            &quot;must&quot;:   { &quot;match&quot;: { &quot;tweet&quot;: &quot;manage text search&quot; }},\n            &quot;filter&quot; : { &quot;term&quot; : { &quot;user_id&quot; : 2 }}\n        }\n    },\n    &quot;sort&quot;: [\n        { &quot;date&quot;:   { &quot;order&quot;: &quot;desc&quot; }},\n        { &quot;_score&quot;: { &quot;order&quot;: &quot;desc&quot; }}\n    ]\n}\n</pre>\n\n<p>排序条件的顺序是很重要的。结果首先按第一个条件排序，仅当结果集的第一个&nbsp;<code>sort</code>&nbsp;值完全相同时才会按照第二个条件进行排序，以此类推。</p>\n\n<p>多级排序并不一定包含&nbsp;<code>_score</code>&nbsp;。你可以根据一些不同的字段进行排序，&nbsp;如地理距离或是脚本计算的特定值。</p>\n\n<p><img alt=\"注意\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/note.png\" /></p>\n\n<p>Query-string 搜索&nbsp;也支持自定义排序，可以在查询字符串中使用&nbsp;<code>sort</code>&nbsp;参数：</p>\n\n<pre>\nGET /_search?sort=date:desc&amp;sort=_score&amp;q=search</pre>\n\n<h3>多值字段的排序</h3>\n\n<p>一种情形是字段有多个值的排序，&nbsp;需要记住这些值并没有固有的顺序；一个多值的字段仅仅是多个值的包装，这时应该选择哪个进行排序呢？</p>\n\n<p>对于数字或日期，你可以将多值字段减为单值，这可以通过使用&nbsp;<code>min</code>&nbsp;、&nbsp;<code>max</code>&nbsp;、&nbsp;<code>avg</code>&nbsp;或是&nbsp;<code>sum</code>&nbsp;<em>排序模式</em>&nbsp;。例如你可以按照每个&nbsp;<code>date</code>&nbsp;字段中的最早日期进行排序，通过以下方法：</p>\n\n<pre>\n&quot;sort&quot;: {\n    &quot;dates&quot;: {\n        &quot;order&quot;: &quot;asc&quot;,\n        &quot;mode&quot;:  &quot;min&quot;\n    }\n}</pre>\n\n<p>被解析的字符串字段也是多值字段，&nbsp;但是很少会按照你想要的方式进行排序。如果你想分析一个字符串，如&nbsp;<code>fine old art</code>&nbsp;， 这包含 3 项。我们很可能想要按第一项的字母排序，然后按第二项的字母排序，诸如此类，但是 Elasticsearch 在排序过程中没有这样的信息。</p>\n\n<p>你可以使用&nbsp;<code>min</code>&nbsp;和&nbsp;<code>max</code>&nbsp;排序模式（默认是&nbsp;<code>min</code>&nbsp;），但是这会导致排序以&nbsp;<code>art</code>&nbsp;或是&nbsp;<code>old</code>&nbsp;，任何一个都不是所希望的。</p>\n\n<p>为了以字符串字段进行排序，这个字段应仅包含一项： 整个&nbsp;<code>not_analyzed</code>&nbsp;字符串。&nbsp;但是我们仍需要<code>analyzed</code>&nbsp;字段，这样才能以全文进行查询</p>\n\n<p>一个简单的方法是用两种方式对同一个字符串进行索引，这将在文档中包括两个字段：&nbsp;<code>analyzed</code>&nbsp;用于搜索，&nbsp;<code>not_analyzed</code>&nbsp;用于排序</p>\n\n<p>但是保存相同的字符串两次在&nbsp;<code>_source</code>&nbsp;字段是浪费空间的。 我们真正想要做的是传递一个&nbsp;<em>单字段</em>&nbsp;但是却用两种方式索引它。所有的 _core_field 类型 (strings, numbers, Booleans, dates) 接收一个&nbsp;<code>fields</code>&nbsp;参数</p>\n\n<p>该参数允许你转化一个简单的映射如：</p>\n\n<pre>\n&quot;tweet&quot;: {\n    &quot;type&quot;:     &quot;string&quot;,\n    &quot;analyzer&quot;: &quot;english&quot;\n}</pre>\n\n<p>为一个多字段映射如：</p>\n\n<pre>\n&quot;tweet&quot;: { <img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" />\n    &quot;type&quot;:     &quot;string&quot;,\n    &quot;analyzer&quot;: &quot;english&quot;,\n    &quot;fields&quot;: {\n        &quot;raw&quot;: { <img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/2.png\" />\n            &quot;type&quot;:  &quot;string&quot;,\n            &quot;index&quot;: &quot;not_analyzed&quot;\n        }\n    }\n}\n</pre>\n\n<table border=\"0\" summary=\"Callout list\">\n	<tbody>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/multi-fields.html#CO28-1\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p><code>tweet</code>&nbsp;主字段与之前的一样: 是一个&nbsp;<code>analyzed</code>&nbsp;全文字段。</p>\n			</td>\n		</tr>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/multi-fields.html#CO28-2\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/2.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p>新的&nbsp;<code>tweet.raw</code>&nbsp;子字段是&nbsp;<code>not_analyzed</code>.</p>\n			</td>\n		</tr>\n	</tbody>\n</table>\n\n<p>现在，至少只要我们重新索引了我们的数据，使用&nbsp;<code>tweet</code>&nbsp;字段用于搜索，<code>tweet.raw</code>&nbsp;字段用于排序：</p>\n\n<pre>\nGET /_search\n{\n    &quot;query&quot;: {\n        &quot;match&quot;: {\n            &quot;tweet&quot;: &quot;elasticsearch&quot;\n        }\n    },\n    &quot;sort&quot;: &quot;tweet.raw&quot;\n}\n</pre>\n\n<p><img alt=\"警告\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/warning.png\" /></p>\n\n<p>以全文&nbsp;<code>analyzed</code>&nbsp;字段排序会消耗大量的内存。</p>\n\n<h2>什么是相关性?</h2>\n\n<p>我们曾经讲过，默认情况下，返回结果是按相关性倒序排列的。&nbsp;但是什么是相关性？ 相关性如何计算？</p>\n\n<p>每个文档都有相关性评分，用一个正浮点数字段&nbsp;<code>_score</code>&nbsp;来表示&nbsp;。&nbsp;<code>_score</code>&nbsp;的评分越高，相关性越高。</p>\n\n<p>查询语句会为每个文档生成一个&nbsp;<code>_score</code>&nbsp;字段。评分的计算方式取决于查询类型&nbsp;不同的查询语句用于不同的目的：&nbsp;<code>fuzzy</code>&nbsp;查询会计算与关键词的拼写相似程度，<code>terms</code>&nbsp;查询会计算 找到的内容与关键词组成部分匹配的百分比，但是通常我们说的&nbsp;<em>relevance</em>&nbsp;是我们用来计算全文本字段的值相对于全文本检索词相似程度的算法。</p>\n\n<p>Elasticsearch 的相似度算法&nbsp;被定义为检索词频率/反向文档频率，&nbsp;<em>TF/IDF</em>&nbsp;，包括以下内容：</p>\n\n<p>检索词频率</p>\n\n<p>检索词在该字段出现的频率？出现频率越高，相关性也越高。 字段中出现过 5 次要比只出现过 1 次的相关性高。</p>\n\n<p>反向文档频率</p>\n\n<p>每个检索词在索引中出现的频率？频率越高，相关性越低。检索词出现在多数文档中会比出现在少数文档中的权重更低。</p>\n\n<p>字段长度准则</p>\n\n<p>字段的长度是多少？长度越长，相关性越低。 检索词出现在一个短的 title 要比同样的词出现在一个长的 content 字段权重更大。</p>\n\n<p>单个查询可以联合使用 TF/IDF 和其他方式，比如短语查询中检索词的距离或模糊查询里的检索词相似度。</p>\n\n<p>相关性并不只是全文本检索的专利。也适用于 yes|no 的子句，匹配的子句越多，相关性评分越高。</p>\n\n<p>如果多条查询子句被合并为一条复合查询语句&nbsp;，比如 bool 查询，则每个查询子句计算得出的评分会被合并到总的相关性评分中。</p>\n\n<p><img alt=\"提示\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/tip.png\" /></p>\n\n<p>我们有一️整章着眼于相关性计算和如何让其配合你的需求&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/controlling-relevance.html\" title=\"控制相关度\"><em>控制相关度</em></a>。</p>\n\n<h3>理解评分标准</h3>\n\n<p>当调试一条复杂的查询语句时，&nbsp;想要理解&nbsp;<code>_score</code>&nbsp;究竟是如何计算是比较困难的。Elasticsearch 在 每个查询语句中都有一个 explain 参数，将&nbsp;<code>explain</code>&nbsp;设为&nbsp;<code>true</code>&nbsp;就可以得到更详细的信息。</p>\n\n<pre>\nGET /_search?explain <img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" />\n{\n   &quot;query&quot;   : { &quot;match&quot; : { &quot;tweet&quot; : &quot;honeymoon&quot; }}\n}</pre>\n\n<table border=\"0\" summary=\"Callout list\">\n	<tbody>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/relevance-intro.html#CO29-1\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p><code>explain</code>&nbsp;参数可以让返回结果添加一个&nbsp;<code>_score</code>&nbsp;评分的得来依据。</p>\n			</td>\n		</tr>\n	</tbody>\n</table>\n\n<p><img alt=\"注意\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/note.png\" /></p>\n\n<p>增加一个&nbsp;<code>explain</code>&nbsp;参数会为每个匹配到的文档产生一大堆额外内容，但是花时间去理解它是很有意义的。 如果现在看不明白也没关系&thinsp;&mdash;&thinsp;等你需要的时候再来回顾这一节就行。下面我们来一点点的了解这块知识点。</p>\n\n<p>首先，我们看一下普通查询返回的元数据：</p>\n\n<pre>\n{\n    &quot;_index&quot; :      &quot;us&quot;,\n    &quot;_type&quot; :       &quot;tweet&quot;,\n    &quot;_id&quot; :         &quot;12&quot;,\n    &quot;_score&quot; :      0.076713204,\n    &quot;_source&quot; :     { ... trimmed ... },</pre>\n\n<p>这里加入了该文档来自于哪个节点哪个分片上的信息，这对我们是比较有帮助的，因为词频率和 文档频率是在每个分片中计算出来的，而不是每个索引中：</p>\n\n<pre>\n    &quot;_shard&quot; :      1,\n    &quot;_node&quot; :       &quot;mzIVYCsqSWCG_M_ZffSs9Q&quot;,</pre>\n\n<p>然后它提供了&nbsp;<code>_explanation</code>&nbsp;。每个&nbsp;入口都包含一个&nbsp;<code>description</code>&nbsp;、&nbsp;<code>value</code>&nbsp;、&nbsp;<code>details</code>&nbsp;字段，它分别告诉你计算的类型、计算结果和任何我们需要的计算细节。</p>\n\n<pre>\n&quot;_explanation&quot;: { <img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" />\n   &quot;description&quot;: &quot;weight(tweet:honeymoon in 0)\n                  [PerFieldSimilarity], result of:&quot;,\n   &quot;value&quot;:       0.076713204,\n   &quot;details&quot;: [\n      {\n         &quot;description&quot;: &quot;fieldWeight in 0, product of:&quot;,\n         &quot;value&quot;:       0.076713204,\n         &quot;details&quot;: [\n            {  <img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/2.png\" />\n               &quot;description&quot;: &quot;tf(freq=1.0), with freq of:&quot;,\n               &quot;value&quot;:       1,\n               &quot;details&quot;: [\n                  {\n                     &quot;description&quot;: &quot;termFreq=1.0&quot;,\n                     &quot;value&quot;:       1\n                  }\n               ]\n            },\n            { <img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/3.png\" />\n               &quot;description&quot;: &quot;idf(docFreq=1, maxDocs=1)&quot;,\n               &quot;value&quot;:       0.30685282\n            },\n            { <img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/4.png\" />\n               &quot;description&quot;: &quot;fieldNorm(doc=0)&quot;,\n               &quot;value&quot;:        0.25,\n            }\n         ]\n      }\n   ]\n}</pre>\n\n<table border=\"0\" summary=\"Callout list\">\n	<tbody>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/relevance-intro.html#CO30-1\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p><code>honeymoon</code>&nbsp;相关性评分计算的总结</p>\n			</td>\n		</tr>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/relevance-intro.html#CO30-2\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/2.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p>检索词频率</p>\n			</td>\n		</tr>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/relevance-intro.html#CO30-3\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/3.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p>反向文档频率</p>\n			</td>\n		</tr>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/relevance-intro.html#CO30-4\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/4.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p>字段长度准则</p>\n			</td>\n		</tr>\n	</tbody>\n</table>\n\n<p><img alt=\"警告\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/warning.png\" /></p>\n\n<p>输出&nbsp;<code>explain</code>&nbsp;结果代价是十分昂贵的，它只能用作调试工具&nbsp;。千万不要用于生产环境。</p>\n\n<p>第一部分是关于计算的总结。告诉了我们&nbsp;<code>honeymoon</code>&nbsp;在&nbsp;<code>tweet</code>&nbsp;字段中的检索词频率/反向文档频率或TF/IDF， （这里的文档&nbsp;<code>0</code>&nbsp;是一个内部的 ID，跟我们没有关系，可以忽略。）</p>\n\n<p>然后它提供了权重是如何计算的细节：</p>\n\n<p>检索词频率:</p>\n\n<pre>\n检索词 `honeymoon` 在这个文档的 `tweet` 字段中的出现次数。</pre>\n\n<p>反向文档频率:</p>\n\n<pre>\n检索词 `honeymoon` 在索引上所有文档的 `tweet` 字段中出现的次数。</pre>\n\n<p>字段长度准则:</p>\n\n<pre>\n在这个文档中， `tweet` 字段内容的长度 -- 内容越长，值越小。</pre>\n\n<p>复杂的查询语句解释也非常复杂，但是包含的内容与上面例子大致相同。 通过这段信息我们可以了解搜索结果是如何产生的。</p>\n\n<p><img alt=\"提示\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/tip.png\" /></p>\n\n<p>JSON 形式的&nbsp;<code>explain</code>&nbsp;描述是难以阅读的， 但是转成 YAML 会好很多，只需要在参数中加上<code>format=yaml</code>&nbsp;。</p>\n\n<h3>理解文档是如何被匹配到的</h3>\n\n<p>当&nbsp;<code>explain</code>&nbsp;选项加到某一文档上时，&nbsp;<code>explain</code>&nbsp;api 会帮助你理解为何这个文档会被匹配，更重要的是，一个文档为何没有被匹配。</p>\n\n<p>请求路径为&nbsp;<code>/index/type/id/_explain</code>&nbsp;，如下所示：</p>\n\n<pre>\nGET /us/tweet/12/_explain\n{\n   &quot;query&quot; : {\n      &quot;bool&quot; : {\n         &quot;filter&quot; : { &quot;term&quot; :  { &quot;user_id&quot; : 2           }},\n         &quot;must&quot; :  { &quot;match&quot; : { &quot;tweet&quot; :   &quot;honeymoon&quot; }}\n      }\n   }\n}</pre>\n\n<p>不只是我们之前看到的充分解释&nbsp;，我们现在有了一个&nbsp;<code>description</code>&nbsp;元素，它将告诉我们：</p>\n\n<pre>\n&quot;failure to match filter: cache(user_id:[2 TO 2])&quot;</pre>\n\n<p>也就是说我们的&nbsp;<code>user_id</code>&nbsp;过滤子句使该文档不能匹配到。</p>\n\n<h2>Doc Values 介绍</h2>\n\n<p>本章的最后一个话题是关于&nbsp;<code>Elasticsearch</code>&nbsp;内部的一些运行情况。在这里我们先不介绍新的知识点，所以我们应该意识到，<code>Doc Values</code>&nbsp;是我们需要反复提到的一个重要话题。</p>\n\n<p>当你对一个字段进行排序时，<code>Elasticsearch</code>&nbsp;需要访问每个匹配到的文档得到相关的值。倒排索引的检索性能是非常快的，但是在字段值排序时却不是理想的结构。</p>\n\n<ul style=\"list-style-type:disc\">\n	<li>在搜索的时候，我们能通过搜索关键词快速得到结果集。</li>\n	<li>当排序的时候，我们需要倒排索引里面某个字段值的集合。换句话说，我们需要&nbsp;<code>转置</code>&nbsp;倒排索引。</li>\n</ul>\n\n<p><code>转置</code>&nbsp;结构在其他系统中经常被称作&nbsp;<code>列存储</code>&nbsp;。实质上，它将所有单字段的值存储在单数据列中，这使得对其进行操作是十分高效的，例如排序。</p>\n\n<p>在&nbsp;<code>Elasticsearch</code>&nbsp;中，<code>Doc Values</code>&nbsp;就是一种列式存储结构，默认情况下每个字段的&nbsp;<code>Doc Values</code>&nbsp;都是激活的，<code>Doc Values</code>&nbsp;是在索引时创建的，当字段索引时，<code>Elasticsearch</code>&nbsp;为了能够快速检索，会把字段的值加入倒排索引中，同时它也会存储该字段的 `Doc Values`。</p>\n\n<p><code>Elasticsearch</code>&nbsp;中的&nbsp;<code>Doc Values</code>&nbsp;常被应用到以下场景：</p>\n\n<ul style=\"list-style-type:disc\">\n	<li>对一个字段进行排序</li>\n	<li>对一个字段进行聚合</li>\n	<li>某些过滤，比如地理位置过滤</li>\n	<li>某些与字段相关的脚本计算</li>\n</ul>\n\n<p>因为文档值被序列化到磁盘，我们可以依靠操作系统的帮助来快速访问。当&nbsp;<code>working set</code>&nbsp;远小于节点的可用内存，系统会自动将所有的文档值保存在内存中，使得其读写十分高速； 当其远大于可用内存，操作系统会自动把&nbsp;<code>Doc Values</code>&nbsp;加载到系统的页缓存中，从而避免了&nbsp;<code>jvm</code>&nbsp;堆内存溢出异常。</p>\n\n<p>我们稍后会深入讨论 `Doc Values`。现在所有你需要知道的是排序发生在索引时建立的平行数据结构中。</p>\n', 1, '5f199b8885e24fc8b28672b872edb606', 3, '2018-06-27 15:15:08', '2018-09-14 11:06:28');
INSERT INTO `logcontent` VALUES ('2c8748b556bd4c27bd38f09cbd931ef0', 1, 'd7caeba238f0466d87db109b2b9724da', '神经网络NN算法笔记', 'nn,神经网络', '初学', '<p><strong><a id=\"背景:\" name=\"背景:\"></a>背景:</strong></p>\n\n<pre>\n<code>以人脑中的神经网络为启发，历史上出现过很多不同版本\n最著名的算法是1980年的 backpropagation</code></pre>\n\n<p><strong><a id=\"多层向前神经网络\" name=\"多层向前神经网络\"></a>多层向前神经网络(Multilayer Feed-Forward Neural Network)</strong></p>\n\n<p><img src=\"http://localhost:8080/myLog/images/nn/nn1.png\" style=\"height:248px; width:340px\" /></p>\n\n<pre>\n<code>Backpropagation被使用在多层向前神经网络上\n多层向前神经网络由以下部分组成：输入层(input layer), 隐藏层 (hidden layers), 输出层 (output layers)\n每层由单元(units)组成\n输入层(input layer)是由训练集的实例特征向量传入\n经过连接结点的权重(weight)传入下一层，一层的输出是下一层的输入\n隐藏层的个数可以是任意的，输入层有一层，输出层有一层\n每个单元(unit)也可以被称作神经结点，根据生物学来源定义\n上图为二层的神经网络（输入层不算）\n一层中加权的求和，然后根据非线性方程转化输出\n作为多层向前神经网络，理论上，如果有足够多的隐藏层(hidden layers) 和足够大的训练集, 可以模拟出任何方程</code></pre>\n\n<p><strong><a id=\"设计神经网络结构\" name=\"设计神经网络结构\"></a>设计神经网络结构</strong></p>\n\n<pre>\n<code>使用神经网络训练数据之前，必须确定神经网络的层数，以及每层单元的个数\n特征向量在被传入输入层时通常被先标准化(normalize）到0和1之间 （为了加速学习过程）\n离散型变量可以被编码成每一个输入单元对应一个特征值可能赋的值\n比如：特征值A可能取三个值（a0, a1, a2), 可以使用3个输入单元来代表A。\n如果A=a0, 那么代表a0的单元值就取1, 其他取0；\n如果A=a1, 那么代表a1de单元值就取1，其他取0，以此类推\n神经网络即可以用来做分类(classification）问题，也可以解决回归(regression)问题\n对于分类问题，如果是两类，可以用一个输出单元表示（0和1分别代表2类）\n如果多于两类，那么每一个类别用一个输出单元表示\n如果没有明确的规则来设计最好有多少个隐藏层，需要根据实验测试和误差，以及准确度来实验并改进</code></pre>\n\n<p><strong><a id=\"交叉验证方法\" name=\"交叉验证方法\"></a>交叉验证方法(Cross-Validation)</strong></p>\n\n<p><img src=\"http://localhost:8080/myLog/images/nn/nn2.jpg\" style=\"height:250px; width:350px\" /></p>\n\n<p><strong><a id=\"Backpropagation算法\" name=\"Backpropagation算法\"></a>Backpropagation算法</strong></p>\n\n<p>通过迭代性的来处理训练集中的实例<br />\n对比经过神经网络后输入层预测值(predicted value)与真实值(target value)之间<br />\n反方向（从输出层=&gt;隐藏层=&gt;输入层）来以最小化误差(error)来更新每个连接的权重(weight)<br />\n算法详细介绍<br />\n输入：D：数据集，l 学习率(learning rate)， 一个多层前向神经网络<br />\n输入：一个训练好的神经网络(a trained neural network)</p>\n\n<pre>\n<code>加权的求和，然后根据非线性方程转化输出</code></pre>\n\n<p><img src=\"http://localhost:8080/myLog/images/nn/nn3.png\" style=\"height:226px; width:376px\" /></p>\n\n<p><span style=\"font-size:20px\">加权求和：</span><img src=\"http://localhost:8080/myLog/images/nn/nn4.png\" style=\"height:29px; width:100px\" /><br />\n<span style=\"font-size:20px\">非线性方程转化：</span><img src=\"http://localhost:8080/myLog/images/nn/nn5.png\" style=\"height:42px; width:87px\" /></p>\n\n<p><span style=\"font-size:16px\">根据误差(error)反向传送：</span></p>\n\n<p>对于输入层：<br />\n<img src=\"http://localhost:8080/myLog/images/nn/nn6.png\" style=\"height:18px; width:150px\" /></p>\n\n<p><span style=\"font-size:14px\">对于隐藏层：</span><br />\n<img src=\"http://localhost:8080/myLog/images/nn/nn7.png\" style=\"height:31px; width:162px\" /></p>\n\n<p>权重更新：<br />\n<img src=\"http://localhost:8080/myLog/images/nn/nn8.png\" style=\"height:41px; width:95px\" /></p>\n\n<p>偏向更新：<br />\n<img src=\"http://localhost:8080/myLog/images/nn/nn9.png\" style=\"height:51px; width:91px\" /></p>\n\n<p><span style=\"font-size:16px\">终止条件:</span></p>\n\n<p>权重的更新低于某个阈值<br />\n预测的错误率低于某个阈值<br />\n达到预设一定的循环次数</p>\n\n<p><strong><a id=\"Backpropagation 算法举例\" name=\"Backpropagation 算法举例\"></a>Backpropagation 算法举例</strong></p>\n\n<p style=\"margin-left:0pt; margin-right:0pt; text-align:justify\"><img src=\"http://localhost:8080/myLog/images/nn/nn10.png\" style=\"height:200px; width:306px\" /></p>\n\n<table border=\"1\" cellspacing=\"0\" style=\"width:500px\">\n	<tbody>\n		<tr>\n			<td>X<sub><span style=\"font-size:10px\">1</span></sub></td>\n			<td>X<sub><span style=\"font-size:10px\">2</span></sub></td>\n			<td>X<sub><span style=\"font-size:10px\">3</span></sub></td>\n			<td>X<span style=\"font-size:10px\"><sub>14</sub></span></td>\n			<td>X<sub><span style=\"font-size:10px\">15</span></sub></td>\n			<td>X<span style=\"font-size:10px\"><sub>24</sub></span></td>\n			<td>X<sub><span style=\"font-size:10px\">25</span></sub></td>\n			<td>X<span style=\"font-size:10px\"><sub>34</sub></span></td>\n			<td>X<span style=\"font-size:10px\"><sub>35</sub></span></td>\n			<td>X<span style=\"font-size:10px\"><sub>46</sub></span></td>\n			<td>X<sub><span style=\"font-size:10px\">56</span></sub></td>\n			<td>&theta;<sub><span style=\"font-size:10px\">4</span></sub></td>\n			<td>&theta;<span style=\"font-size:10px\"><sub>5</sub></span></td>\n			<td>&theta;<sub><span style=\"font-size:10px\">6</span></sub></td>\n		</tr>\n		<tr>\n			<td>1</td>\n			<td>0</td>\n			<td>1</td>\n			<td>0.2</td>\n			<td>-0.3</td>\n			<td>0.4</td>\n			<td>0.1</td>\n			<td>-0.5</td>\n			<td>0.2</td>\n			<td>-0.3</td>\n			<td>-0.2</td>\n			<td>-0.4</td>\n			<td>0.2</td>\n			<td>0.1</td>\n		</tr>\n	</tbody>\n</table>\n\n<p><strong>加权求和及非线性方程转化</strong></p>\n\n<p>单元4&gt;&gt;</p>\n\n<ul>\n	<li>I<sub>j&nbsp;</sub>= x<sub>1&nbsp;</sub>* w<sub>14&nbsp;</sub>+ x<sub>2&nbsp;</sub>* w<sub>24&nbsp;</sub>+ x<sub>3&nbsp;</sub>* w<sub>34&nbsp;</sub>+ &theta;<sub>4</sub> = 1 * 0.2 + 0 * 0.4 + 1 * ( -0.5 ) + ( -0.4 ) = 0.2 + 0 - 0.5 - 0.4 = -0.7</li>\n	<li>O<sub>j</sub> = 1 / ( 1 + e<sup>0.7&nbsp;</sup>) = 0.332</li>\n</ul>\n\n<p>单元5&gt;&gt;</p>\n\n<ul>\n	<li>I<sub>j&nbsp;</sub>= x<sub>1&nbsp;</sub>* w<sub>15&nbsp;</sub>+ x<sub>2&nbsp;</sub>* w<sub>25&nbsp;</sub>+ x<sub>3&nbsp;</sub>* w<sub>35&nbsp;</sub>+ &theta;<sub>5</sub> = 1 * ( -0.3 ) + 0 * 0.1 + 1 * 0.2 + 0.2 = -0.3 + 0 + 0.2 + 0.2 = 0.1</li>\n	<li>O<sub>j</sub> = 1 / ( 1+e<sup>-0.1&nbsp;</sup>) = 0.525</li>\n</ul>\n\n<p>单元6&gt;&gt;</p>\n\n<ul>\n	<li>I<sub>j&nbsp;</sub>= x<sub>4&nbsp;</sub>* w<sub>46&nbsp;</sub>+ x<sub>5 </sub>* w<sub>56 </sub>+ &theta;<sub>6</sub> = 0.332 * ( -0.3 ) + 0.525 * ( -0.2 ) + 0.1 = -0.0996 - 0.105 + 0.1 = -0.105</li>\n	<li>O<sub>j</sub> = 1 / ( 1 + e<sup>0.105&nbsp;</sup>) = 0.474</li>\n</ul>\n\n<p><strong>误差（error）反向传送</strong></p>\n\n<p>单元6（输入层）&gt;&gt;</p>\n\n<ul>\n	<li>Err<sub>j</sub> = 0.474 * (&nbsp;1-0.474 ) * ( 1 - 0.474 ) = 0.1311</li>\n</ul>\n\n<p>单元5（隐藏层）&gt;&gt;</p>\n\n<ul>\n	<li>Err<sub>j</sub> = 0.525 * ( 1 - 0.525 ) * ( 0.1311 )&nbsp;* ( -0.2 ) = -0.0065</li>\n</ul>\n\n<p>单元4（隐藏层）&gt;&gt;</p>\n\n<ul>\n	<li>Err<sub>j</sub> = 0.332 * ( 1 - 0.332 ) * ( 0.1311 ) * ( -0.3 ) = -0.0087</li>\n</ul>\n\n<p>权重更新&gt;&gt;</p>\n\n<p>w<sub>46</sub> = -0.3 + ( 0.9 ) * ( 0.1311 ) * ( 0.332 ) = -0.261<br />\nw<sub>56</sub> = -0.2 + ( 0.9 ) * ( 0.1311 ) * ( 0.521 ) = -0.138<br />\nw<sub>15</sub> = -0.3 + ( 0.9 ) * ( -0.0065 ) * ( 1 ) = -0.306<br />\nw<sub>25</sub> = 0.1 + ( 0.9 ) * ( -0.0065 ) * ( 0 ) = 0.1<br />\nw<sub>35</sub> = 0.2 + ( 0.9 ) * ( -0.0065 ) * ( 1 ) = 0.194<br />\nw<sub>14</sub> = 0.2 + ( 0.9 ) * ( -0.0087 ) * ( 1 ) = 0.192<br />\nw<sub>24</sub> = 0.4 + ( 0.9 ) * ( -0.0087 ) * ( 0 ) = 0.4<br />\nw<sub>34</sub> = -0.5 + ( 0.9 ) * ( -0.0087 ) * ( 1 ) = -0.508<br />\n&theta;<sub>6</sub> = 0.1 + ( 0.9 ) ( 0.1311 ) = 0.218<br />\n&theta;<sub>5</sub> = 0.2 + ( 0.9 ) ( -0.0065 ) = 0.194<br />\n&theta;<sub>4</sub> = -0.4 + ( 0.9 ) ( -0.0087 ) = -0.408</p>\n\n<p><strong><a id=\"关于非线性转化方程\" name=\"关于非线性转化方程\"></a>关于非线性转化方程(non-linear transformation function)</strong>&nbsp;<br />\nsigmoid函数(S 曲线)用来作为activation function:<br />\n双曲函数(tanh)<br />\n逻辑函数(logistic function)</p>\n', 1, '5f199b8885e24fc8b28672b872edb606', 30, '2017-11-22 23:11:58', '2018-08-31 22:21:23');
INSERT INTO `logcontent` VALUES ('2c9143b1153647c5bbe864a5adfe955d', -1, '3ba9a3aed3ce48f682974ebcf3552ce2', 'JMeter函数助手添加自定义扩展函数', 'JMeter,扩展,函数', 'JMeter添加自定义扩展函数', '<p><a id=\"环境\" name=\"环境\"></a>环境</p>\n\n<pre>\n<code>JMeter Version:apache-jmeter-3.2\nJDK/JRE Version:java version \"1.8.0_131\"\nEclipse Version：Oxygen.3 Release (4.7.3)</code></pre>\n\n<p><a id=\"安装apache-jmeter-3.2\" name=\"安装apache-jmeter-3.2\"></a>安装apache-jmeter-3.2</p>\n\n<pre>\n<code>下载地址：http://archive.apache.org/dist/jmeter/binaries/apache-jmeter-3.2.zip\n解压即可使用\n假设安装目录为G:\\apache-jmeter-3.2</code></pre>\n\n<p><a id=\"自定义扩展函数\" name=\"自定义扩展函数\"></a>自定义扩展函数</p>\n\n<pre>\n<code>下载JMeter3.2源码\n下载地址：http://archive.apache.org/dist/jmeter/source/apache-jmeter-3.2_src.zip \n\n将源码导入Eclipse（需要特殊方式导入）\n1、新建Java项目，项目名随意（暂定项目名JMeter）（右键-New-Java Project）\n2、删除JMeter目录下bin和src两个文件夹\n3、修改.classpath文件\n使用apache-jmeter-3.2_src.zip解压文件夹中的eclipse.classpath中内容复盖.classpath中的内容\n4、导入源码（import-General-File System-Browse选择apache-jmeter-3.2_src.zip解压包，选中后确定）\n5、修改依赖jar\nJMeter项目上（右键-Properties-Java Build Path-Libraries）\n选中错误的jar并点击remove进行移除\n点击add External JARs选择G:\\apache-jmeter-3.2\\lib目录下的所有jar\n6、编辑自定义Java文件\n在src/functions/org/apache/jmeter/functions/路径下新建Java文件（类名随意）\n例子参考下文\n将Java文件对应的class文件复制到G:\\apache-jmeter-3.2\\lib\\ext目录下的ApacheJMeter_functions.jar对应目录中即可\n将Java文件所需的第三方jar复制到G:\\apache-jmeter-3.2\\lib\\目录下</code></pre>\n\n<p><a id=\"模板小例子\" name=\"模板小例子\"></a>模板栗子</p>\n\n<pre>\n<code>参考：https://www.cnblogs.com/qiaoyeye/p/7218770.html\n\npackage org.apache.jmeter.functions;\n\nimport java.io.FileInputStream;\nimport java.io.InputStream;\nimport java.util.Collection;\nimport java.util.LinkedList;\nimport java.util.List;\nimport org.apache.jmeter.engine.util.CompoundVariable;\nimport org.apache.jmeter.samplers.SampleResult;\nimport org.apache.jmeter.samplers.Sampler;\nimport org.apache.jmeter.threads.JMeterVariables;\nimport sun.misc.BASE64Encoder;\n\npublic class MyBase64 extends AbstractFunction {\n	private static final List&lt;String&gt; desc = new LinkedList&lt;&gt;();\n	private static final String KEY = \"__MyBase64\";\n	private Object[] values;\n	static {\n		desc.add(\"图片路径\");\n\n		desc.add(\"图片base64后存放变量\");\n	}\n	public List&lt;String&gt; getArgumentDesc() {\n		return desc;\n	}\n	public synchronized String execute(SampleResult arg0, Sampler arg1) throws InvalidVariableException {\n		JMeterVariables localJMeterVariables = getVariables();\n		String str1 = ((CompoundVariable) this.values[0]).execute();\n		String str2 = getImgBase64(str1);\n		if ((localJMeterVariables != null) &amp;&amp; (this.values.length &gt; 1)) {\n			String str3 = ((CompoundVariable) this.values[1]).execute().trim();\n			localJMeterVariables.put(str3, str2);\n		}\n		return str2;\n	}\n	public String getReferenceKey() {\n		return \"__MyBase64\";\n	}\n	public synchronized void setParameters(Collection&lt;CompoundVariable&gt; arg0) throws InvalidVariableException {\n		checkParameterCount(arg0, 1, 2);\n		this.values = arg0.toArray();\n	}\n	public String getImgBase64(String filePath) {\n		InputStream in = null;\n		byte[] data = null;\n		String result = null;\n		try {\n			in = new FileInputStream(filePath);\n			data = new byte[in.available()];\n			in.read(data);\n			in.close();\n			BASE64Encoder encoder = new BASE64Encoder();\n			result = encoder.encode(data);\n		} catch (Exception e) {\n			e.printStackTrace();\n		}\n		return result;\n	}\n}</code></pre>\n\n<p>&nbsp;</p>\n', 1, '5f199b8885e24fc8b28672b872edb606', 11, '2018-06-06 15:47:46', '2018-08-31 22:21:23');
INSERT INTO `logcontent` VALUES ('2e4d46224cbc42529c466aeaf3630b6a', -1, '7338e53acd514defa1a17e47016f3f4a', '八、执行分布式检索', '执行分布式检索', '执行分布式检索', '<h2>执行分布式检索</h2>\n\n<p>在继续之前，我们将绕道讨论一下在分布式环境中搜索是怎么执行的。&nbsp;这比我们讨论的基本的&nbsp;<em>增-删-改-查</em>&nbsp;(CRUD)请求要复杂一些。</p>\n\n<p><strong>内容提示</strong></p>\n\n<p>你可以根据兴趣阅读本章内容。你并不需要为了使用 Elasticsearch 而理解和记住所有的细节。</p>\n\n<p>这章的阅读目的只为初步了解下工作原理，以便将来需要时可以及时找到这些知识， 但是不要被细节所困扰。</p>\n\n<p>一个 CRUD 操作只对单个文档进行处理，文档的唯一性由&nbsp;<code>_index</code>,&nbsp;<code>_type</code>, 和&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/routing-value.html\" title=\"路由一个文档到一个分片中\"><code>routing</code>&nbsp;values</a>&nbsp;（通常默认是该文档的&nbsp;<code>_id</code>&nbsp;）的组合来确定。 这表示我们确切的知道集群中哪个分片含有此文档。</p>\n\n<p>搜索需要一种更加复杂的执行模型因为我们不知道查询会命中哪些文档: 这些文档有可能在集群的任何分片上。 一个搜索请求必须询问我们关注的索引（index or indices）的所有分片的某个副本来确定它们是否含有任何匹配的文档。</p>\n\n<p>但是找到所有的匹配文档仅仅完成事情的一半。 在&nbsp;<code>search</code>&nbsp;接口返回一个 ``page`` 结果之前，多分片中的结果必须组合成单个排序列表。 为此，搜索被执行成一个两阶段过程，我们称之为&nbsp;<em>query then fetch</em>&nbsp;。</p>\n\n<h2>查询阶段</h2>\n\n<p>在初始&nbsp;<em>查询阶段</em>&nbsp;时，&nbsp;查询会广播到索引中每一个分片拷贝（主分片或者副本分片）。 每个分片在本地执行搜索并构建一个匹配文档的 _优先队列_。</p>\n\n<p><strong>优先队列</strong></p>\n\n<p>一个&nbsp;<em>优先队列</em>&nbsp;仅仅是一个存有&nbsp;<em>top-n</em>&nbsp;匹配文档的有序列表。优先队列的大小取决于分页参数<code>from</code>&nbsp;和&nbsp;<code>size</code>&nbsp;。例如，如下搜索请求将需要足够大的优先队列来放入100条文档。</p>\n\n<pre>\nGET /_search\n{\n    &quot;from&quot;: 90,\n    &quot;size&quot;: 10\n}</pre>\n\n<p>这个查询阶段的过程如图&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/_query_phase.html#img-distrib-search\" title=\"图 14. 查询过程分布式搜索\">图&nbsp;14 &ldquo;查询过程分布式搜索&rdquo;</a>&nbsp;所示。</p>\n\n<p>&nbsp;</p>\n\n<p><strong>图&nbsp;14.&nbsp;查询过程分布式搜索</strong></p>\n\n<p><img alt=\"查询过程分布式搜索\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/elas_0901.png\" /></p>\n\n<p>&nbsp;</p>\n\n<p>查询阶段包含以下三个步骤:</p>\n\n<ol style=\"list-style-type:decimal\">\n	<li>客户端发送一个&nbsp;<code>search</code>&nbsp;请求到&nbsp;<code>Node 3</code>&nbsp;，&nbsp;<code>Node 3</code>&nbsp;会创建一个大小为&nbsp;<code>from + size</code>&nbsp;的空优先队列。</li>\n	<li><code>Node 3</code>&nbsp;将查询请求转发到索引的每个主分片或副本分片中。每个分片在本地执行查询并添加结果到大小为&nbsp;<code>from + size</code>&nbsp;的本地有序优先队列中。</li>\n	<li>每个分片返回各自优先队列中所有文档的 ID 和排序值给协调节点，也就是&nbsp;<code>Node 3</code>&nbsp;，它合并这些值到自己的优先队列中来产生一个全局排序后的结果列表。</li>\n</ol>\n\n<p>当一个搜索请求被发送到某个节点时，这个节点就变成了协调节点。&nbsp;这个节点的任务是广播查询请求到所有相关分片并将它们的响应整合成全局排序后的结果集合，这个结果集合会返回给客户端。</p>\n\n<p>第一步是广播请求到索引中每一个节点的分片拷贝。就像&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/distrib-read.html\" title=\"取回一个文档\">document&nbsp;<code>GET</code>&nbsp;requests</a>&nbsp;所描述的， 查询请求可以被某个主分片或某个副本分片处理，&nbsp;这就是为什么更多的副本（当结合更多的硬件）能够增加搜索吞吐率。 协调节点将在之后的请求中轮询所有的分片拷贝来分摊负载。</p>\n\n<p>每个分片在本地执行查询请求并且创建一个长度为&nbsp;<code>from + size</code>&nbsp;的优先队列&mdash;也就是说，每个分片创建的结果集足够大，均可以满足全局的搜索请求。 分片返回一个轻量级的结果列表到协调节点，它仅包含文档 ID 集合以及任何排序需要用到的值，例如&nbsp;<code>_score</code>&nbsp;。</p>\n\n<p>协调节点将这些分片级的结果合并到自己的有序优先队列里，它代表了全局排序结果集合。至此查询过程结束。</p>\n\n<p><img alt=\"注意\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/note.png\" /></p>\n\n<p>一个索引可以由一个或几个主分片组成，&nbsp;所以一个针对单个索引的搜索请求需要能够把来自多个分片的结果组合起来。 针对&nbsp;<em>multiple</em>&nbsp;或者&nbsp;<em>all</em>&nbsp;索引的搜索工作方式也是完全一致的--仅仅是包含了更多的分片而已。</p>\n\n<h2>取回阶段</h2>\n\n<p>查询阶段标识哪些文档满足&nbsp;搜索请求，但是我们仍然需要取回这些文档。这是取回阶段的任务, 正如&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/_fetch_phase.html#img-distrib-fetch\" title=\"图 15. 分布式搜索的取回阶段\">图&nbsp;15 &ldquo;分布式搜索的取回阶段&rdquo;</a>&nbsp;所展示的。</p>\n\n<p>&nbsp;</p>\n\n<p><strong>图&nbsp;15.&nbsp;分布式搜索的取回阶段</strong></p>\n\n<p><img alt=\"分布式搜索的取回阶段\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/elas_0902.png\" /></p>\n\n<p>&nbsp;</p>\n\n<p>分布式阶段由以下步骤构成：</p>\n\n<ol style=\"list-style-type:decimal\">\n	<li>协调节点辨别出哪些文档需要被取回并向相关的分片提交多个&nbsp;<code>GET</code>&nbsp;请求。</li>\n	<li>每个分片加载并&nbsp;<em>丰富</em>&nbsp;文档，如果有需要的话，接着返回文档给协调节点。</li>\n	<li>一旦所有的文档都被取回了，协调节点返回结果给客户端。</li>\n</ol>\n\n<p>协调节点首先决定哪些文档&nbsp;<em>确实</em>&nbsp;需要被取回。例如，如果我们的查询指定了&nbsp;<code>{ &quot;from&quot;: 90, &quot;size&quot;: 10 }</code>&nbsp;，最初的90个结果会被丢弃，只有从第91个开始的10个结果需要被取回。这些文档可能来自和最初搜索请求有关的一个、多个甚至全部分片。</p>\n\n<p>协调节点给持有相关文档的每个分片创建一个&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/distrib-multi-doc.html\" title=\"多文档模式\">multi-get request</a>&nbsp;，并发送请求给同样处理查询阶段的分片副本。</p>\n\n<p>分片加载文档体--&nbsp;<code>_source</code>&nbsp;字段--如果有需要，用元数据和&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/highlighting-intro.html\" title=\"高亮搜索\">search snippet highlighting</a>&nbsp;丰富结果文档。 一旦协调节点接收到所有的结果文档，它就组装这些结果为单个响应返回给客户端。</p>\n\n<p><strong>深分页（Deep Pagination）</strong></p>\n\n<p>先查后取的过程支持用&nbsp;<code>from</code>&nbsp;和&nbsp;<code>size</code>&nbsp;参数分页，但是这是&nbsp;<em>有限制的</em>&nbsp;。&nbsp;要记住需要传递信息给协调节点的每个分片必须先创建一个&nbsp;<code>from + size</code>&nbsp;长度的队列，协调节点需要根据<code>number_of_shards * (from + size)</code>&nbsp;排序文档，来找到被包含在&nbsp;<code>size</code>&nbsp;里的文档。</p>\n\n<p>取决于你的文档的大小，分片的数量和你使用的硬件，给 10,000 到 50,000 的结果文档深分页（ 1,000 到 5,000 页）是完全可行的。但是使用足够大的&nbsp;<code>from</code>&nbsp;值，排序过程可能会变得非常沉重，使用大量的CPU、内存和带宽。因为这个原因，我们强烈建议你不要使用深分页。</p>\n\n<p>实际上， &ldquo;深分页&rdquo; 很少符合人的行为。当2到3页过去以后，人会停止翻页，并且改变搜索标准。会不知疲倦地一页一页的获取网页直到你的服务崩溃的罪魁祸首一般是机器人或者web spider。</p>\n\n<p>如果你&nbsp;<em>确实</em>&nbsp;需要从你的集群取回大量的文档，你可以通过用&nbsp;<code>scroll</code>&nbsp;查询禁用排序使这个取回行为更有效率。</p>\n\n<h2>搜索选项</h2>\n\n<p>有几个&nbsp;查询参数可以影响搜索过程。</p>\n\n<h3>偏好</h3>\n\n<p>偏好这个参数&nbsp;<code>preference</code>&nbsp;允许&nbsp;用来控制由哪些分片或节点来处理搜索请求。 它接受像&nbsp;<code>_primary</code>,<code>_primary_first</code>,&nbsp;<code>_local</code>,&nbsp;<code>_only_node:xyz</code>,&nbsp;<code>_prefer_node:xyz</code>, 和&nbsp;<code>_shards:2,3</code>&nbsp;这样的值, 这些值在<a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/5.6/search-request-preference.html\" target=\"_top\">search&nbsp;<code>preference</code></a>&nbsp;文档页面被详细解释。</p>\n\n<p>但是最有用的值是某些随机字符串，它可以避免&nbsp;<em>bouncing results</em>&nbsp;问题。</p>\n\n<p>&nbsp;</p>\n\n<p><strong>Bouncing Results</strong></p>\n\n<p>想象一下有两个文档有同样值的时间戳字段，搜索结果用&nbsp;<code>timestamp</code>&nbsp;字段来排序。 由于搜索请求是在所有有效的分片副本间轮询的，那就有可能发生主分片处理请求时，这两个文档是一种顺序， 而副本分片处理请求时又是另一种顺序。</p>\n\n<p>这就是所谓的&nbsp;<em>bouncing results</em>&nbsp;问题: 每次用户刷新页面，搜索结果表现是不同的顺序。 让同一个用户始终使用同一个分片，这样可以避免这种问题， 可以设置&nbsp;<code>preference</code>&nbsp;参数为一个特定的任意值比如用户会话ID来解决。</p>\n\n<h3>超时问题</h3>\n\n<p>通常分片处理完它所有的数据后再把结果返回给协同节点，协同节点把收到的所有结果合并为最终结果。</p>\n\n<p>这意味着花费的时间是最慢分片的处理时间加结果合并的时间。如果有一个节点有问题，就会导致所有的响应缓慢。</p>\n\n<p>参数&nbsp;<code>timeout</code>&nbsp;告诉&nbsp;分片允许处理数据的最大时间。如果没有足够的时间处理所有数据，这个分片的结果可以是部分的，甚至是空数据。</p>\n\n<p>搜索的返回结果会用属性&nbsp;<code>timed_out</code>&nbsp;标明分片是否返回的是部分结果：</p>\n\n<pre>\n    ...\n    &quot;timed_out&quot;:     true,  <img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" />\n    ...</pre>\n\n<table border=\"0\" summary=\"Callout list\">\n	<tbody>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/_search_options.html#CO31-1\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p>这个搜索请求超时了。</p>\n			</td>\n		</tr>\n	</tbody>\n</table>\n\n<p><img alt=\"警告\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/warning.png\" /></p>\n\n<p>超时仍然是一个最有效的操作，知道这一点很重要； 很可能查询会超过设定的超时时间。这种行为有两个原因：</p>\n\n<ol style=\"list-style-type:decimal\">\n	<li>超时检查是基于每文档做的。 但是某些查询类型有大量的工作在文档评估之前需要完成。 这种 &quot;setup&quot; 阶段并不考虑超时设置，所以太长的建立时间会导致超过超时时间的整体延迟。</li>\n	<li>因为时间检查是基于每个文档的，一次长时间查询在单个文档上执行并且在下个文档被评估之前不会超时。 这也意味着差的脚本（比如带无限循环的脚本）将会永远执行下去。</li>\n</ol>\n\n<h3>路由</h3>\n\n<p>在&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/routing-value.html\" title=\"路由一个文档到一个分片中\">路由一个文档到一个分片中</a>&nbsp;中, 我们解释过如何定制参数&nbsp;<code>routing</code>&nbsp;，它能够在索引时提供来确保相关的文档，比如属于某个用户的文档被存储在某个分片上。 在搜索的时候，不用搜索索引的所有分片，而是通过指定几个&nbsp;<code>routing</code>&nbsp;值来限定只搜索几个相关的分片：</p>\n\n<pre>\nGET /_search?routing=user_1,user2</pre>\n\n<p>这个技术在设计大规模搜索系统时就会派上用场。</p>\n\n<h3>搜索类型</h3>\n\n<p>缺省的搜索类型是&nbsp;<code>query_then_fetch</code>&nbsp;。 在某些情况下，你可能想明确设置&nbsp;<code>search_type</code>&nbsp;为<code>dfs_query_then_fetch</code>&nbsp;来改善相关性精确度：</p>\n\n<pre>\nGET /_search?search_type=dfs_query_then_fetch</pre>\n\n<p>搜索类型&nbsp;<code>dfs_query_then_fetch</code>&nbsp;有预查询阶段，这个阶段可以从所有相关分片获取词频来计算全局词频。&nbsp;</p>\n\n<h2>游标查询&nbsp;<em>Scroll</em></h2>\n\n<p><code>scroll</code>&nbsp;查询&nbsp;可以用来对 Elasticsearch 有效地执行大批量的文档查询，而又不用付出深度分页那种代价。</p>\n\n<p>游标查询允许我们&nbsp;先做查询初始化，然后再批量地拉取结果。 这有点儿像传统数据库中的&nbsp;<em>cursor</em>&nbsp;。</p>\n\n<p>游标查询会取某个时间点的快照数据。 查询初始化之后索引上的任何变化会被它忽略。 它通过保存旧的数据文件来实现这个特性，结果就像保留初始化时的索引&nbsp;<em>视图</em>&nbsp;一样。</p>\n\n<p>深度分页的代价根源是结果集全局排序，如果去掉全局排序的特性的话查询结果的成本就会很低。 游标查询用字段&nbsp;<code>_doc</code>&nbsp;来排序。 这个指令让 Elasticsearch 仅仅从还有结果的分片返回下一批结果。</p>\n\n<p>启用游标查询可以通过在查询的时候设置参数&nbsp;<code>scroll</code>&nbsp;的值为我们期望的游标查询的过期时间。 游标查询的过期时间会在每次做查询的时候刷新，所以这个时间只需要足够处理当前批的结果就可以了，而不是处理查询结果的所有文档的所需时间。 这个过期时间的参数很重要，因为保持这个游标查询窗口需要消耗资源，所以我们期望如果不再需要维护这种资源就该早点儿释放掉。 设置这个超时能够让 Elasticsearch 在稍后空闲的时候自动释放这部分资源。</p>\n\n<pre>\nGET /old_index/_search?scroll=1m <img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" />\n{\n    &quot;query&quot;: { &quot;match_all&quot;: {}},\n    &quot;sort&quot; : [&quot;_doc&quot;], <img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/2.png\" />\n    &quot;size&quot;:  1000\n}</pre>\n\n<table border=\"0\" summary=\"Callout list\">\n	<tbody>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/scroll.html#CO32-1\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p>保持游标查询窗口一分钟。</p>\n			</td>\n		</tr>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/scroll.html#CO32-2\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/2.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p>关键字&nbsp;<code>_doc</code>&nbsp;是最有效的排序顺序。</p>\n			</td>\n		</tr>\n	</tbody>\n</table>\n\n<p>这个查询的返回结果包括一个字段&nbsp;<code>_scroll_id`， 它是一个base64编码的长字符串 (((&quot;scroll_id&quot;))) 。 现在我们能传递字段 `_scroll_id</code>&nbsp;到&nbsp;<code>_search/scroll</code>&nbsp;查询接口获取下一批结果：</p>\n\n<pre>\nGET /_search/scroll\n{\n    &quot;scroll&quot;: &quot;1m&quot;, <img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" />\n    &quot;scroll_id&quot; : &quot;cXVlcnlUaGVuRmV0Y2g7NTsxMDk5NDpkUmpiR2FjOFNhNnlCM1ZDMWpWYnRROzEwOTk1OmRSamJHYWM4U2E2eUIzVkMxalZidFE7MTA5OTM6ZFJqYkdhYzhTYTZ5QjNWQzFqVmJ0UTsxMTE5MDpBVUtwN2lxc1FLZV8yRGVjWlI2QUVBOzEwOTk2OmRSamJHYWM4U2E2eUIzVkMxalZidFE7MDs=&quot;\n}</pre>\n\n<table border=\"0\" summary=\"Callout list\">\n	<tbody>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/scroll.html#CO33-1\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p>注意再次设置游标查询过期时间为一分钟。</p>\n			</td>\n		</tr>\n	</tbody>\n</table>\n\n<p>这个游标查询返回的下一批结果。 尽管我们指定字段&nbsp;<code>size</code>&nbsp;的值为1000，我们有可能取到超过这个值数量的文档。&nbsp;当查询的时候， 字段&nbsp;<code>size</code>&nbsp;作用于单个分片，所以每个批次实际返回的文档数量最大为&nbsp;<code>size * number_of_primary_shards</code>&nbsp;。</p>\n\n<p><img alt=\"注意\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/note.png\" /></p>\n\n<p>注意游标查询每次返回一个新字段&nbsp;<code>_scroll_id`。每次我们做下一次游标查询， 我们必须把前一次查询返回的字段 `_scroll_id</code>&nbsp;传递进去。 当没有更多的结果返回的时候，我们就处理完所有匹配的文档了。</p>\n\n<p><img alt=\"提示\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/tip.png\" /></p>\n\n<p>提示：某些官方的 Elasticsearch 客户端比如&nbsp;<a href=\"http://elasticsearch-py.readthedocs.org/en/master/helpers.html#scan\" target=\"_top\">Python 客户端</a>&nbsp;和&nbsp;<a href=\"https://metacpan.org/pod/Search::Elasticsearch::Scroll\" target=\"_top\">Perl 客户端</a>&nbsp;提供了这个功能易用的封装。</p>\n', 1, '5f199b8885e24fc8b28672b872edb606', 0, '2018-06-27 15:26:35', '2018-08-31 22:21:23');
INSERT INTO `logcontent` VALUES ('2fb19e895893485b8ffc0aa899a0a828', -1, 'f29612168904487aadb4043df110361c', 'Nginx的Location区段', 'nginx,location', 'nginx的location区段', '<p>Nginx location 配置语法</p>\n\n<pre>\n<code class=\"language-nginx\">location [=|~|~*|^~|@] pattern{……}</code></pre>\n\n<pre>\n<code>1：没有修饰符 表示：必须以指定模式开始\nserver {\nserver_name localhost:8080/myLog;\nlocation /blog {}\n}\n\n可以匹配\nlocalhost:8080/myLog/blog\nlocalhost:8080/myLog/blog?id=1\nlocalhost:8080/myLog/blogs\nlocalhost:8080/myLog/blog/</code></pre>\n\n<pre>\n<code>2：= 表示：必须与指定的模式精确匹配，如：\nserver {\nserver_name localhost:8080/myLog;\nlocation = /blog {}\n}\n\n可以匹配\nlocalhost:8080/myLog/blog\nlocalhost:8080/myLog/blog?id=1\n\n错误匹配\nlocalhost:8080/myLog/blog/\nlocalhost:8080/myLog/blogs</code></pre>\n\n<pre>\n<code>3：~ 表示：指定的正则表达式(区分大小写)\nserver {\nserver_name localhost:8080/myLog;\nlocation ~ ^/blog$ {}\n}\n\n可以匹配\nlocalhost:8080/myLog/blog\nlocalhost:8080/myLog/blog?id=1\n\n错误匹配\nlocalhost:8080/myLog/BLOG\nlocalhost:8080/myLog/blog/\nlocalhost:8080/myLog/blogs</code></pre>\n\n<pre>\n<code>4：~* 表示：指定的正则表达式不区分大小写\nserver {\nserver_name localhost:8080/myLog;\nlocation ~* ^/blog$ {}\n}\n\n可以匹配\nlocalhost:8080/myLog/blog\nlocalhost:8080/myLog/blog?id=1\nlocalhost:8080/myLog/BLOG\n\n错误匹配\nlocalhost:8080/myLog/blog/\nlocalhost:8080/myLog/blogs</code></pre>\n\n<pre>\n<code>5：^~ 类似于无修饰符的行为,也是以指定模式开始,不同的是,如果模式匹配,那么就停止搜索其他模式了。</code></pre>\n\n<pre>\n<code>6：@ ：定义命名location区段,这些区段客户段不能访问,只可以由内部产生的请求来访问,如try_files或error_page等</code></pre>\n\n<p>a. &quot;普通location&quot;的匹配规则是&quot;最大前缀&quot;,因此&quot;普通location&quot;匹配与顺序无关;<br />\n而&quot;正则location&quot;的匹配规则是&quot;顺序匹配&quot;,且只要匹配到一个就停止。<br />\nb.&quot;普通location&quot;与&quot;正则 location&quot;之间的匹配顺序是,先匹配&quot;普通 location&quot;,可能再匹配&quot;正则 location&quot;。<br />\n匹配&quot;普通 location&quot;后,有时需要继续匹配&quot;正则 location&quot;，有时则不需要继续匹配&quot;正则 location&quot;。<br />\n&nbsp; &nbsp; 以下两种情况不需要继续匹配正则location：<br />\n&nbsp;&nbsp; &nbsp;（1）当普通location的前面指定了&ldquo;^~&rdquo;，一旦匹配，则不需要继续匹配。<br />\n&nbsp;&nbsp; &nbsp;（2）当普通location严格匹配，不是最大前缀匹配，则不再继续匹配正则<br />\n<strong>总结：&ldquo;正则 location 匹配让步普通 location 的严格精确匹配结果；但覆盖普通 location 的最大前缀匹配结果&rdquo;</strong></p>\n', 1, '5f199b8885e24fc8b28672b872edb606', 1, '2018-03-26 19:07:12', '2018-09-07 18:57:14');
INSERT INTO `logcontent` VALUES ('30352ee417ef4cfdbb540ca299d1922e', -1, '3ba9a3aed3ce48f682974ebcf3552ce2', 'Centos7虚拟机安装', 'linux,Centos', '记录操作', '<p><a id=\"一、下载CentOS\" name=\"一、下载CentOS\"></a>一、下载CentOS</p>\n\n<pre>\n<code>打开http://mirrors.aliyun.com/centos/7/isos/x86_64/\n下载\nCentOS-7-x86_64-Everything-1708.iso</code></pre>\n\n<p>下载vmware以及注册码略</p>\n\n<p>安装CentOS略</p>\n\n<p>启动CentOS7</p>\n\n<p><a id=\"无法联网，输入ifconfig出现ens33\" name=\"无法联网，输入ifconfig出现ens33\"></a>问题一：无法联网，输入ifconfig出现ens33，没有etho即IP之类</p>\n\n<p>（1）将里面的NAME和DEVICE项修改为eth0；ONBOOT项的no改为yes</p>\n\n<pre>\n<code class=\"language-bash\">vi /etc/sysconfig/network-scripts/ifcfg-ens33</code></pre>\n\n<p>（2）重命名网卡配置文件ifcfg-ens33为ifcfg-eth0</p>\n\n<pre>\n<code class=\"language-bash\">[root@localhost ~]# cd /etc/sysconfig/network-scripts/\n[root@localhost network-scripts]# mv ifcfg-ens33 ifcfg-eth0\n</code></pre>\n\n<p>ifcfg-eth0文件内容</p>\n\n<pre>\n<code class=\"language-bash\">[root@localhost etc]#  cat /etc/sysconfig/network-scripts/ifcfg-eth0 \nTYPE=Ethernet\nPROXY_METHOD=none\nBROWSER_ONLY=no\nBOOTPROTO=dhcp\nDEFROUTE=yes\nIPV4_FAILURE_FATAL=no\nIPV6INIT=yes\nIPV6_AUTOCONF=yes\nIPV6_DEFROUTE=yes\nIPV6_FAILURE_FATAL=no\nIPV6_ADDR_GEN_MODE=stable-privacy\nNAME=eth0\nUUID=844360e0-bdd1-43ee-bbb9-d3a50ab361e8\nDEVICE=eth0\nONBOOT=yes\n</code></pre>\n\n<p>编辑/etc/default/grub并加入&ldquo;net.ifnames=0 biosdevname=0 &rdquo;到GRUBCMDLINELINUX变量</p>\n\n<pre>\n<code class=\"language-bash\">[root@localhost etc]# cat /etc/default/grub \nGRUB_TIMEOUT=5\nGRUB_DISTRIBUTOR=\"$(sed \'s, release .*$,,g\' /etc/system-release)\"\nGRUB_DEFAULT=saved\nGRUB_DISABLE_SUBMENU=true\nGRUB_TERMINAL_OUTPUT=\"console\"\nGRUB_CMDLINE_LINUX=\"crashkernel=auto net.ifnames=0 biosdevname=0 rhgb quiet\"\nGRUB_DISABLE_RECOVERY=\"true\"\n</code></pre>\n\n<p>（3）运行命令grub2-mkconfig -o /boot/grub2/grub.cfg 来重新生成GRUB配置并更新内核参数。</p>\n\n<pre>\n<code class=\"language-bash\">[root@localhost network-scripts]# grub2-mkconfig -o /boot/grub2/grub.cfg  \nGenerating grub configuration file ...  \nFound linux image: /boot/vmlinuz-3.10.0-514.el7.x86_64  \nFound initrd image: /boot/initramfs-3.10.0-514.el7.x86_64.img  \nFound linux image: /boot/vmlinuz-0-rescue-b7f83ca165964a47b8b283907b126140  \nFound initrd image: /boot/initramfs-0-rescue-b7f83ca165964a47b8b283907b126140.img  \ndone</code></pre>\n\n<p>（4）重启系统</p>\n\n<pre>\n<code class=\"language-bash\">[root@localhost network-scripts]# reboot</code></pre>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n', 1, '5f199b8885e24fc8b28672b872edb606', 13, '2018-03-17 19:18:44', '2018-09-11 18:29:26');
INSERT INTO `logcontent` VALUES ('31766834d79b47eab6db8e6047a59866', 1, 'd7caeba238f0466d87db109b2b9724da', '机器学习基本概念笔记', '机器学习,深度学习(Deep Learning)', '机器学习简介', '<p><a id=\"机器学习\" name=\"机器学习\"></a>机器学习 （Machine Learning, ML)</p>\n\n<pre>\n<code>概念：多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。\n\n学科定位：人工智能(Artificial Intelligence, AI）的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域，它主要使用归纳、综合而不是演绎。\n\n定义：探究和开发一系列算法来如何使计算机不需要通过外部明显的指示，而可以自己通过数据来学习，建模，并且利用建好的模型和新的输入来进行预测的学科。\n\n学习：针对经验E (experience) 和一系列的任务 T (tasks) 和一定表现的衡量 P，如果随之经验E的积累，针对定义好的任务T可以提高表现P，就说计算机具有学习能力</code></pre>\n\n<p><a id=\"深度学习\" name=\"深度学习\"></a>深度学习(Deep Learning)</p>\n\n<pre>\n<code>概念：深度学习是基于机器学习延伸出来的一个新的领域，由以人大脑结构为启发的神经网络算法为起源加之模型结构深度的增加发展，并伴随大数据和计算能力的提高而产生的一系列新的算法。</code></pre>\n\n<pre>\n<code>基本概念：训练集，测试集，特征值，监督学习，非监督学习，半监督学习，分类，回归\n\n概念学习：指从有关某个布尔函数的输入输出训练样例中推断出该布尔函数\n\n╔════════════════════════════════════════════════════════════════╗\n\n例子：学习 “享受运动\"  这一概念：小明进行水上运动，是否享受运动取决于很多因素\n天气：晴，阴，雨 \n温度：暖，冷\n湿度：普通，大\n风力：强，弱\n水温：暖，冷\n预报：一样，变化\n↓  ↓  ↓  ↓\n享受运动：是，否\n\n概念定义在实例(instance)集合之上，这个集合表示为X。（X：所有可能的日子，每个日子的值由天气，温度，湿度，风力，水温，预报6个属性表示。\n\n待学习的概念或目标函数成为目标概念（target concept), 记做c。\n当享受运动时，c(x) = 1；当不享受运动时，c(x) = 0；也可叫做Y=c(x)\nx: 每一个实例\nX: 样例, 所有实例的集合\n学习目标：f: 输入X得到Y\n\n╚════════════════════════════════════════════════════════════════╝\n\n训练集(training set/data)/训练样例（training examples): 用来进行训练，也就是产生模型或者算法的数据集\n\n测试集(testing set/data)/测试样例 (testing examples)：用来专门进行测试已经学习好的模型或者算法的数据集\n\n特征向量(features/feature vector)：属性的集合，通常用一个向量来表示，附属于一个实例\n\n标记(label): c(x), 实例类别的标记\n\n正例(positive example)\n\n反例(negative example)\n╔════════════════════════════════════════════════════════════════╗\n\n例子：研究美国硅谷房价\n影响房价的两个重要因素：面积(平方米），学区（评分1-10）\n\n样例 面积 学区  房价\n 1  100   8   1000\n 2  120   9   1300\n\n╚════════════════════════════════════════════════════════════════╝\n\n分类 (classification): 目标标记为类别型数据(category)\n\n回归(regression): 目标标记为连续性数值 (continuous numeric value)\n\n╔════════════════════════════════════════════════════════════════╗\n\n例子：研究肿瘤良性，恶性于尺寸，颜色的关系\n特征值：肿瘤尺寸，颜色\n标记：良性/恶性\n有监督学习(supervised learning)： 训练集有类别标记(class label)\n无监督学习(unsupervised learning)： 无类别标记(class label)\n半监督学习（semi-supervised learning)：有类别标记的训练集 + 无标记的训练集\n\n╚════════════════════════════════════════════════════════════════╝\n\n机器学习步骤框架\n把数据拆分为训练集和测试集\n用训练集和训练集的特征向量来训练算法\n用学习来的算法运用在测试集上来评估算法 （如果涉及到调整参数（parameter tuning), 用验证集（validation set）\n100 天：训练集\n10天：测试集（不知道是否 ” 享受运动“， 知道6个属性，来预测每一天是否享受运动）\n\n训练集 =&gt; 提取特征向量 =&gt; 结合一定的算法（分类器：比如决策树，KNN）=&gt;得到结果</code></pre>\n\n<p>&nbsp;</p>\n', 1, '5f199b8885e24fc8b28672b872edb606', 10, '2017-10-27 14:23:33', '2018-08-31 22:21:23');
INSERT INTO `logcontent` VALUES ('3b0deb2b02154d018a55350900b67a22', -1, '716b0d3ea6e04d03a9deb097be1e2cf1', '中医笔记', '中医,阴阳,五行,养生,经络,气血,津液,顺四时,六淫,劳逸结合,情志,吃,虚实病机', '《黄帝内经》《阴阳一调百病消》', '<p><strong><a id=\"阴阳调和百病不生\" name=\"阴阳调和百病不生\"></a>一、阴阳调和百病不生</strong></p>\n\n<pre>\n<code class=\"language-makefile\">1、分辨阴阳失调\n\n阳不足:表现为功能性的衰弱，如少气、懒言、怕冷、疲倦、不耐劳动等。\n\n阴不足:表现为物质性的损失，如贫血、萎黄、消瘦等。\n\n2、阴阳失衡的四种主要症状\n\n阳虚：单纯阳虚、阴盛导致\n\n阴虚：单纯阴虚、阳盛导致\n\n阳盛：单纯阳盛、阴虚导致\n\n阴盛：单纯阴盛、阳虚导致\n\n3、诊断时辨明阴阳\n\n诊脉中数、浮、滑属阳脉，为阳证，迟、沉、涩属阴脉，为阴症，\n\n舌诊上红绛色表血热，属阳，淡或青色为血虚或血寒，属阴\n\n舌苔的变化，燥黄属阳，滑白属阴\n\n4、治疗时调和阴阳\n\n表症用汗法，里证用下法，寒症用温法，热症用凉法。</code></pre>\n\n<p><strong><a id=\"巧用五行养身体\" name=\"巧用五行养身体\"></a>二、巧用五行养身体</strong></p>\n\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 自然五行与人体五行一览表</p>\n\n<table border=\"1\" cellspacing=\"0\" style=\"border-collapse:collapse; border:.5pt solid #000000; height:361px; width:425.25pt\">\n	<tbody>\n		<tr>\n			<td colspan=\"7\" style=\"background-color:#b8cce4; height:14.25pt; text-align:center; vertical-align:middle; width:425.25pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">人体</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"background-color:#b8cce4; height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">五脏</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">肝</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">心</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">脾</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">肺</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">肾</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\">&nbsp;</td>\n		</tr>\n		<tr>\n			<td style=\"background-color:#b8cce4; height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">六腑</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">胆</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">小肠</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">胃</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">大肠</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">膀胱</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">三焦</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"background-color:#b8cce4; height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">五官</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">目</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">舌</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">口</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">鼻</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">耳</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\">&nbsp;</td>\n		</tr>\n		<tr>\n			<td style=\"background-color:#b8cce4; height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">形体</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">筋</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">脉</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">肉</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">皮毛</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">骨</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\">&nbsp;</td>\n		</tr>\n		<tr>\n			<td style=\"background-color:#b8cce4; height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">情志</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">怒</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">喜</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">思</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">悲</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">恐</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\">&nbsp;</td>\n		</tr>\n		<tr>\n			<td style=\"background-color:#b8cce4; height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">五声</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">呼</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">笑</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">歌</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">哭</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">呻</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\">&nbsp;</td>\n		</tr>\n		<tr>\n			<td style=\"background-color:#b8cce4; height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">五液</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">泪</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">汗</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">涎</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">涕</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">唾</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\">&nbsp;</td>\n		</tr>\n		<tr>\n			<td style=\"background-color:#b8cce4; height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">病所</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">头项</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">胸胁</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">脊</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">肩背</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">腰股</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\">&nbsp;</td>\n		</tr>\n		<tr>\n			<td style=\"background-color:#b8cce4; height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">病态</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">握</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">忧</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">哕</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">咳</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">慄</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\">&nbsp;</td>\n		</tr>\n		<tr>\n			<td colspan=\"7\" style=\"background-color:#b8cce4; height:14.25pt; text-align:center; vertical-align:middle; width:425.25pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">自然</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"background-color:#b8cce4; height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">五行</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">木</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">火</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">土</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">金</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">水</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\">&nbsp;</td>\n		</tr>\n		<tr>\n			<td style=\"background-color:#b8cce4; height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">五季</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">春</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">夏</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">长夏</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">秋</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">冬</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\">&nbsp;</td>\n		</tr>\n		<tr>\n			<td style=\"background-color:#b8cce4; height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">五方</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">东</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">南</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">中</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">西</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">北</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\">&nbsp;</td>\n		</tr>\n		<tr>\n			<td style=\"background-color:#b8cce4; height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">五气</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">风</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">热</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">湿</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">燥</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">寒</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\">&nbsp;</td>\n		</tr>\n		<tr>\n			<td style=\"background-color:#b8cce4; height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">五味</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">酸</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">苦</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">甘</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">辛</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">咸</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\">&nbsp;</td>\n		</tr>\n		<tr>\n			<td style=\"background-color:#b8cce4; height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">五音</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">角</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">徵</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">宫</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">商</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">羽</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\">&nbsp;</td>\n		</tr>\n		<tr>\n			<td style=\"background-color:#b8cce4; height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">五色</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">青</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">赤</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">黄</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">白</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">黑</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\">&nbsp;</td>\n		</tr>\n		<tr>\n			<td style=\"background-color:#b8cce4; height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">五化</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">生</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">长</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">化</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">收</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\"><span style=\"font-size:11.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">藏</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:60.75pt\">&nbsp;</td>\n		</tr>\n	</tbody>\n</table>\n\n<pre>\n<code class=\"language-makefile\">1、利用自然五行调理身体\n\n2、利用五行生克调理脏腑\n\n木生火，火生土，土生金，金生水，水生木；\n\n五行相生：水生木，肾属水，肝属木，肝虚可以用滋肾的方法养肝。\n\n五行相克：金克木，肺属金，肝属木，肝火旺时，可强化肺功能清肝火。\n\n五行相侮：金克木，但如果木太强或金太弱，金反而会被木所克。\n\n五行制化：生中有克，克中有生，五行之间相互作用。\n\n治疗原则:虚则补其母，实则泻其子。\n\n3、利用五行对应面色、口味发现脏腑病变\n\n望面色\n\n根据五行对应表，五脏各有主色（肝色青、心色赤等），内脏精气的华彩体现在面部，通过面部色泽变化确定病变的相应脏腑。\n\n辨口味\n\n口味的异常变化，能反映五脏的病理状态（口酸为肝火太旺，口苦为心火旺，口甘为脾胃湿热，口咸为肾虚所致），不同的脏腑疾病会出现不同的饮食嗜味（肝病嗜酸、心病嗜苦等）</code></pre>\n\n<p><strong><a id=\"养生必先调五脏\" name=\"养生必先调五脏\"></a>三、养生必先调五脏</strong></p>\n\n<p><strong><a id=\"打通经络病自消\" name=\"打通经络病自消\"></a>四、打通经络病自消<br />\n<a id=\"调气血\" name=\"调气血\"></a>五、调气血</strong></p>\n\n<pre>\n<code class=\"language-makefile\">气虚\n\n血虚\n\n如何调养气血\n脾胃为气血化生之源，为后天之本，所以平时的保健以健脾益胃为好；肺为体内外之气交换的场所，以有氧运动为好。津液可以化生为血，需常喝水。</code></pre>\n\n<p><strong><a id=\"养好津液最重要\" name=\"养好津液最重要\"></a>六、养好津液最重要</strong></p>\n\n<pre>\n<code class=\"language-makefile\">津液来源于饮食，通过脾胃小肠大肠消化吸收饮食中的水分和营养而生成，如胃液、肠液、唾液、关节液等，习惯上也包括代谢产物中的尿、汗、泪等。\n\n1、利用五液滋养五脏\n\n汗为心之液：心主血，血之液气化为汗，汗多耗血伤津，反之血亏则汗源不足，血虚之候慎用汗法\n\n涕为肺之液：肺正常时，涕润泽鼻窍而不外流；肺感风寒，鼻流清涕；肺感风热，鼻流浊涕；肺燥，鼻干涕少或无涕。\n\n涎为脾之液：脾胃不和易导致涎液分泌过多，发生口涎自出等现象。\n\n泪为肝之液：肝阴血不足，则泪液少，两目干涩；肝经湿热，导致迎风流泪等。\n\n唾为肾之液：唾为肾精所化，多唾易耗肾精，气功家常吞咽津唾，以养肾精。</code></pre>\n\n<p><strong><a id=\"顺四时养身体\" name=\"顺四时养身体\"></a>七、顺四时养身体</strong></p>\n\n<pre>\n<code class=\"language-makefile\">春季开发阳气\n\n起居：晚睡早起\n\n饮食：辛甘微温之品\n\n运动：轻柔舒缓的户外锻炼项目\n\n夏季保养阳气\n\n起居：晚睡早起，暑热盛时宜午睡\n\n饮食：平淡爽口，易于消化，忌贪凉\n\n运动：傍晚或清晨适度进行\n\n秋季收敛养阴\n\n起居：早睡早起，注意增减衣物\n\n饮食：防燥护阴\n\n运动：静功锻炼\n\n冬季敛阳护阴\n\n起居：早睡晚起，注意保暖\n\n饮食：宜热食，燥热或辛辣的食物不宜多吃\n\n运动：太阳出来后锻炼为宜\n\n养生八防：春防风防寒；夏防暑热及因暑至寒；长夏防湿；秋防燥；冬防寒防风；</code></pre>\n\n<p><strong><a id=\"远离六淫\" name=\"远离六淫\"></a>八、远离六淫</strong></p>\n\n<pre>\n<code class=\"language-makefile\">六淫：风、寒、暑、湿、燥、火\n\n一般春季多风病，夏季多暑病，长夏初秋多湿病，深秋多燥病，冬季多寒病；</code></pre>\n\n<p><strong><a id=\"劳逸结合不生病\" name=\"劳逸结合不生病\"></a>九、劳逸结合不生病</strong></p>\n\n<pre>\n<code class=\"language-makefile\">过劳之患\n\n过逸之患</code></pre>\n\n<p><strong><a id=\"调养情志\" name=\"调养情志\"></a>十、调养情志</strong></p>\n\n<pre>\n<code class=\"language-makefile\">大喜伤心，大怒伤肝，大惊伤脾，大忧伤肺，大恐伤肾；\n\n七情如何伤身：怒则气止，喜则气缓，悲忧气消，恐则气下，惊则气乱，思则气结</code></pre>\n\n<p><strong><a id=\"吃是关键\" name=\"吃是关键\"></a>十一、吃是关键</strong></p>\n\n<pre>\n<code class=\"language-makefile\">1、饮食应有度\n\n不宜极饥而食，食不可过饱\n\n不宜极渴而饮，饮不可过多\n\n人的膳食结构应谷、肉、果、菜齐全，谷为主，肉为副，蔬菜为充，水果为助，根据需要，兼则取之\n\n饮食宜寒温适中，五味适中，不可嗜好太过\n\n饮食洁净</code></pre>\n\n<p><strong><a id=\"虚实病机\" name=\"虚实病机\"></a>十二、虚实病机</strong></p>\n\n<pre>\n<code class=\"language-makefile\">虚实病机是人体正邪之争的临床表现\n\n实的病机：主要指邪气亢盛，致病邪气与机体抗病能力都比较强盛，斗争剧烈，反应明显，临床上出现病理性反应较剧烈的症候表现\n\n虚的病机：主要指正气不足，人体生理功能减退，抗病能力下降，因而正气不足以与邪气抗争，无剧烈病理反应，多为一系列虚弱不足症候表现\n\n虚中夹实:病理变化以正虚为主，又兼邪实的病理状态\n\n实中夹虚:邪实为主，兼正气虚损的病理状态\n\n虚实转化:由实转虚、由虚转实</code></pre>\n\n<p>&nbsp;</p>\n', 1, '5f199b8885e24fc8b28672b872edb606', 13, '2017-09-02 22:30:14', '2018-08-31 22:21:23');
INSERT INTO `logcontent` VALUES ('42cfb3032bfa4baa8707ae6bc1079406', -1, 'd7caeba238f0466d87db109b2b9724da', 'hierarchical clustering 层次聚类', 'hierarchical clustering', '初学', '<p><a id=\"步骤\" name=\"步骤\"></a>假设有N个待聚类的样本，对于层次聚类来说，步骤：<br />\n（初始化）把每个样本归为一类，计算每两个类之间的距离，也就是样本与样本之间的相似度；<br />\n寻找各个类之间最近的两个类，把他们归为一类（这样类的总数就少了一个）；<br />\n重新计算新生成的这个类与各个旧类之间的相似度；<br />\n重复2和3直到所有样本点都归为一类，结束</p>\n\n<p><img src=\"http://localhost:8080/myLog//images/hierarchicalClustering/1.png\" style=\"height:200px; width:240px\" /></p>\n\n<p><a id=\"判断两个类之间的相似度\" name=\"判断两个类之间的相似度\"></a>整个聚类过程其实是建立了一棵树，在建立的过程中，可以通过在第二步上设置一个阈值，当最近的两个类的距离大于这个阈值，则认为迭代可以终止。另外关键的一步就是第三步，如何判断两个类之间的相似度有不少种方法。这里介绍一下三种：</p>\n\n<ul>\n	<li>SingleLinkage：又叫做 nearest-neighbor ，就是取两个类中距离最近的两个样本的距离作为这两个集合的距离，也就是说，最近两个样本之间的距离越小，这两个类之间的相似度就越大。容易造成一种叫做 Chaining 的效果，两个 cluster 明明从&ldquo;大局&rdquo;上离得比较远，但是由于其中个别的点距离比较近就被合并了，并且这样合并之后 Chaining 效应会进一步扩大，最后会得到比较松散的 cluster 。</li>\n	<li>CompleteLinkage：这个则完全是 Single Linkage 的反面极端，取两个集合中距离最远的两个点的距离作为两个集合的距离。其效果也是刚好相反的，限制非常大，两个 cluster 即使已经很接近了，但是只要有不配合的点存在，就顽固到底，老死不相合并，也是不太好的办法。这两种相似度的定义方法的共同问题就是指考虑了某个有特点的数据，而没有考虑类内数据的整体特点。</li>\n	<li>Average-linkage：这种方法就是把两个集合中的点两两的距离全部放在一起求一个平均值，相对也能得到合适一点的结果。average-linkage的一个变种就是取两两距离的中值，与取均值相比更加能够解除个别偏离样本对结果的干扰。</li>\n</ul>\n', 1, '5f199b8885e24fc8b28672b872edb606', 4, '2017-11-29 11:40:42', '2018-08-31 22:21:23');
INSERT INTO `logcontent` VALUES ('46726fbae1f843feab167b9a805906a1', -1, '811e56b193664cf4b2802f8849a2f7d1', 'Web网站架构演变过程', '架构', '转自：http://www.cnblogs.com/xiaoMzjm/p/5223799.html\n架构篇', '<p><strong><a id=\"前言\" name=\"前言\"></a>前言</strong></p>\n\n<p>　　我们以javaweb为例，来搭建一个简单的电商系统，看看这个系统可以如何一步步演变。<br />\n&nbsp;<br />\n　　该系统具备的功能：</p>\n\n<pre>\n<code>用户模块：用户注册和管理\n商品模块：商品展示和管理\n交易模块：创建交易和管理</code></pre>\n\n<p><strong><a id=\"阶段一、单机构建网站\" name=\"阶段一、单机构建网站\"></a>阶段一、单机构建网站</strong></p>\n\n<p>　　网站的初期，我们经常会在单机上跑我们所有的程序和软件。此时我们使用一个容器，如tomcat、jetty、jboss，然后直接使用jsp/servlet技术，或者使用一些开源的框架如maven+spring+struct+hibernate、maven+spring+springmvc+mybatis；最后再选择一个数据库管理系统来存储数据，如mysql、sqlserver、oracle，然后通过JDBC进行数据库的连接和操作。<br />\n　　把以上的所有软件都装载同一台机器上，应用跑起来了，也算是一个小系统了。此时系统结果如下：</p>\n\n<p><img src=\"http://localhost:8080/myLog/images/architect/0.png\" style=\"height:239px; width:190px\" /></p>\n\n<p><strong><a id=\"阶段二、应用服务器与数据库分离\" name=\"阶段二、应用服务器与数据库分离\"></a>阶段二、应用服务器与数据库分离</strong></p>\n\n<p>　　随着网站的上线，访问量逐步上升，服务器的负载慢慢提高，在服务器还没有超载的时候，我们应该就要做好准备，提升网站的负载能力。假如我们代码层面已难以优化，在不提高单台机器的性能的情况下，增加机器是一个不错的方式，不仅可以有效地提高系统的负载能力，而且性价比高。<br />\n　　增加的机器用来做什么呢？此时我们可以把数据库，web服务器拆分开来，这样不仅提高了单台机器的负载能力，也提高了容灾能力。<br />\n　　应用服务器与数据库分开后的架构如下图所示：</p>\n\n<p><img src=\"http://localhost:8080/myLog/images/architect/1.png\" style=\"height:245px; width:292px\" /></p>\n\n<p><strong><a id=\"阶段三、应用服务器集群\" name=\"阶段三、应用服务器集群\"></a>阶段三、应用服务器集群</strong></p>\n\n<p>　　随着访问量继续增加，单台应用服务器已经无法满足需求了。在假设数据库服务器没有压力的情况下，我们可以把应用服务器从一台变成了两台甚至多台，把用户的请求分散到不同的服务器中，从而提高负载能力。多台应用服务器之间没有直接的交互，他们都是依赖数据库各自对外提供服务。著名的做故障切换的软件有keepalived，keepalived是一个类似于layer3、4、7交换机制的软件，他不是某个具体软件故障切换的专属品，而是可以适用于各种软件的一款产品。keepalived配合上ipvsadm又可以做负载均衡，可谓是神器。<br />\n　　我们以增加了一台应用服务器为例，增加后的系统结构图如下：</p>\n\n<p><img src=\"http://localhost:8080/myLog/images/architect/2.png\" style=\"height:275px; width:425px\" /></p>\n\n<p>系统演变到这里，将会出现下面四个问题：</p>\n\n<pre>\n<code>1、用户的请求由谁来转发到具体的应用服务器\n2、有什么转发的算法\n3、应用服务器如何返回用户的请求\n4、用户如果每次访问到的服务器不一样，那么如何维护session的一致性</code></pre>\n\n<p>　　解决问题的方案：</p>\n\n<p>　　1、第一个问题即是负载均衡的问题，一般有5种解决方案：</p>\n\n<pre>\n<code>（1）http重定向。HTTP重定向就是应用层的请求转发。用户的请求其实已经到了HTTP重定向负载均衡服务器，服务器根据算法要求用户重定向，用户收到重定向请求后，再次请求真正的集群\n优点：简单。\n缺点：性能较差。\n\n（2）DNS域名解析负载均衡。DNS域名解析负载均衡就是在用户请求DNS服务器，获取域名对应的IP地址时，DNS服务器直接给出负载均衡后的服务器IP。\n优点：交给DNS，不用我们去维护负载均衡服务器。\n缺点：当一个应用服务器挂了，不能及时通知DNS，而且DNS负载均衡的控制权在域名服务商那里，网站无法做更多的改善和更强大的管理。\n\n（3）反向代理服务器。在用户的请求到达反向代理服务器时（已经到达网站机房），由反向代理服务器根据算法转发到具体的服务器。常用的apache，nginx都可以充当反向代理服务器。\n优点：部署简单。\n缺点：代理服务器可能成为性能的瓶颈，特别是一次上传大文件。\n\n（4）IP层负载均衡。在请求到达负载均衡器后，负载均衡器通过修改请求的目的IP地址，从而实现请求的转发，做到负载均衡。\n优点：性能更好。\n缺点：负载均衡器的宽带成为瓶颈。\n\n（5）数据链路层负载均衡。在请求到达负载均衡器后，负载均衡器通过修改请求的mac地址，从而做到负载均衡，与IP负载均衡不一样的是，当请求访问完服务器之后，直接返回客户。而无需再经过负载均衡器。</code></pre>\n\n<p>　　2、第二个问题即是集群调度算法问题，常见的调度算法有10种。</p>\n\n<pre>\n<code>（1）rr 轮询调度算法。顾名思义，轮询分发请求。\n优点：实现简单\n缺点：不考虑每台服务器的处理能力\n\n（2）wrr 加权调度算法。我们给每个服务器设置权值weight，负载均衡调度器根据权值调度服务器，服务器被调用的次数跟权值成正比。\n优点：考虑了服务器处理能力的不同\n\n（3）sh 原地址散列：提取用户IP，根据散列函数得出一个key，再根据静态映射表，查处对应的value，即目标服务器IP。过目标机器超负荷，则返回空。\n\n（4）dh 目标地址散列：同上，只是现在提取的是目标地址的IP来做哈希。\n优点：以上两种算法的都能实现同一个用户访问同一个服务器。\n\n（5）lc 最少连接。优先把请求转发给连接数少的服务器。\n优点：使得集群中各个服务器的负载更加均匀。\n\n（6）wlc 加权最少连接。在lc的基础上，为每台服务器加上权值。算法为：（活动连接数*256+非活动连接数）÷权重 ，计算出来的值小的服务器优先被选择。\n优点：可以根据服务器的能力分配请求。\n\n（7）sed 最短期望延迟。其实sed跟wlc类似，区别是不考虑非活动连接数。算法为：（活动连接数+1)*256÷权重，同样计算出来的值小的服务器优先被选择。\n\n（8）nq 永不排队。改进的sed算法。我们想一下什么情况下才能“永不排队”，那就是服务器的连接数为0的时候，那么假如有服务器连接数为0，均衡器直接把请求转发给它，无需经过sed的计算。\n\n（9）LBLC 基于局部性的最少连接。均衡器根据请求的目的IP地址，找出该IP地址最近被使用的服务器，把请求转发之，若该服务器超载，最采用最少连接数算法。\n\n（10）LBLCR 带复制的基于局部性的最少连接。均衡器根据请求的目的IP地址，找出该IP地址最近使用的“服务器组”，注意，并不是具体某个服务器，然后采用最少连接数从该组中挑出具体的某台服务器出来，把请求转发之。若该服务器超载，那么根据最少连接数算法，在集群的非本服务器组的服务器中，找出一台服务器出来，加入本服务器组，然后把请求转发之。</code></pre>\n\n<p>　　3、第三个问题是集群模式问题，一般3种解决方案：</p>\n\n<pre>\n<code>1、NAT：负载均衡器接收用户的请求，转发给具体服务器，服务器处理完请求返回给均衡器，均衡器再重新返回给用户。\n\n2、DR：负载均衡器接收用户的请求，转发给具体服务器，服务器出来玩请求后直接返回给用户。需要系统支持IP Tunneling协议，难以跨平台。\n\n3、TUN：同上，但无需IP Tunneling协议，跨平台性好，大部分系统都可以支持。</code></pre>\n\n<p>　　4、第四个问题是session问题，一般有4种解决方案：</p>\n\n<pre>\n<code>1、Session Sticky。session sticky就是把同一个用户在某一个会话中的请求，都分配到固定的某一台服务器中，这样我们就不需要解决跨服务器的session问题了，常见的算法有ip_hash法，即上面提到的两种散列算法。\n优点：实现简单。\n缺点：应用服务器重启则session消失。\n\n2、Session Replication。session replication就是在集群中复制session，使得每个服务器都保存有全部用户的session数据。\n\n优点：减轻负载均衡服务器的压力，不需要要实现ip_hasp算法来转发请求。\n\n缺点：复制时宽带开销大，访问量大的话session占用内存大且浪费。\n\n3、Session数据集中存储：session数据集中存储就是利用数据库来存储session数据，实现了session和应用服务器的解耦。\n优点：相比session replication的方案，集群间对于宽带和内存的压力减少了很多。\n缺点：需要维护存储session的数据库。\n\n4、Cookie Base：cookie base就是把session存在cookie中，有浏览器来告诉应用服务器我的session是什么，同样实现了session和应用服务器的解耦。\n优点：实现简单，基本免维护。\n缺点：cookie长度限制，安全性低，宽带消耗。</code></pre>\n\n<p>　　值得一提的是：</p>\n\n<p>　　nginx目前支持的负载均衡算法有wrr、sh（支持一致性哈希）、fair（本人觉得可以归结为lc）。但nginx作为均衡器的话，还可以一同作为静态资源服务器。</p>\n\n<p>　　keepalived+ipvsadm比较强大，目前支持的算法有：rr、wrr、lc、wlc、lblc、sh、dh</p>\n\n<p>　　keepalived支持集群模式有：NAT、DR、TUN</p>\n\n<p>　　nginx本身并没有提供session同步的解决方案，而apache则提供了session共享的支持。</p>\n\n<p>　　好了，解决了以上的问题之后，系统的结构如下：</p>\n\n<p><img src=\"http://localhost:8080/myLog/images/architect/3.png\" style=\"height:468px; width:425px\" /></p>\n\n<p><br />\n<strong><a id=\"阶段四、数据库读写分离化\" name=\"阶段四、数据库读写分离化\"></a>阶段四、数据库读写分离化</strong></p>\n\n<p>　　上面我们总是假设数据库负载正常，但随着访问量的的提高，数据库的负载也在慢慢增大。那么可能有人马上就想到跟应用服务器一样，把数据库一分为二再负载均衡即可。但对于数据库来说，并没有那么简单。假如我们简单的把数据库一分为二，然后对于数据库的请求，分别负载到A机器和B机器，那么显而易见会造成两台数据库数据不统一的问题。那么对于这种情况，我们可以先考虑使用读写分离的方式。<br />\n　　读写分离后的数据库系统结构如下：</p>\n\n<p><img src=\"http://localhost:8080/myLog/images/architect/4.png\" style=\"height:483px; width:436px\" /></p>\n\n<p>这个结构变化后也会带来两个问题：<br />\n主从数据库之间数据同步问题<br />\n应用对于数据源的选择问题<br />\n解决问题方案：</p>\n\n<pre>\n<code>我们可以使用mysql自带的master+slave的方式实现主从复制。\n采用第三方数据库中间件，例如mycat。mycat是从cobar发展而来的，而cobar是阿里开源的数据库中间件，后来停止开发。mycat是国内比较好的mysql开源数据库分库分表中间件。</code></pre>\n\n<p><strong><a id=\"阶段五、用搜索引擎缓解读库的压力\" name=\"阶段五、用搜索引擎缓解读库的压力\"></a>阶段五、用搜索引擎缓解读库的压力</strong></p>\n\n<p>　　数据库做读库的话，常常对模糊搜索力不从心，即使做了读写分离，这个问题还未能解决。以我们所举的交易网站为例，发布的商品存储在数据库中，用户最常使用的功能就是查找商品，尤其是根据商品的标题来查找对应的商品。对于这种需求，一般我们都是通过like功能来实现的，但是这种方式的代价非常大。此时我们可以使用搜索引擎的倒排索引来完成。</p>\n\n<p>　　搜索引擎优点：它能够大大提高查询速度。<br />\n　　引入搜索引擎后也会带来以下的开销：带来大量的维护工作，我们需要自己实现索引的构建过程，设计全量/增加的构建方式来应对非实时与实时的查询需求。需要维护搜索引擎集群<br />\n&nbsp; &nbsp; &nbsp; &nbsp;搜索引擎并不能替代数据库，他解决了某些场景下的&ldquo;读&rdquo;的问题，是否引入搜索引擎，需要综合考虑整个系统的需求。引入搜索引擎后的系统结构如下：</p>\n\n<p><img src=\"http://localhost:8080/myLog/images/architect/5.png\" style=\"height:485px; width:449px\" /></p>\n\n<p><strong><a id=\"阶段六、用缓存缓解读库的压力\" name=\"阶段六、用缓存缓解读库的压力\"></a>阶段六、用缓存缓解读库的压力</strong></p>\n\n<pre>\n<code>1、后台应用层和数据库层的缓存\n　　随着访问量的增加，逐渐出现了许多用户访问同一部分内容的情况，对于这些比较热门的内容，没必要每次都从数据库读取。我们可以使用缓存技术，例如可以使用google的开源缓存技术guava或者使用memcacahe作为应用层的缓存，也可以使用redis作为数据库层的缓存。\n　　另外，在某些场景下，关系型数据库并不是很适合，例如我想做一个“每日输入密码错误次数限制”的功能，思路大概是在用户登录时，如果登录错误，则记录下该用户的IP和错误次数，那么这个数据要放在哪里呢？假如放在内存中，那么显然会占用太大的内容；假如放在关系型数据库中，那么既要建立数据库表，还要简历对应的java bean，还要写SQL等等。而分析一下我们要存储的数据，无非就是类似{ip:errorNumber}这样的key:value数据。对于这种数据，我们可以用NOSQL数据库来代替传统的关系型数据库。</code></pre>\n\n<pre>\n<code>2、页面缓存\n除了数据缓存，还有页面缓存。比如使用HTML5的localstroage或者cookie。\n\n优点：\n减轻数据库的压力\n大幅度提高访问速度\n缺点：\n需要维护缓存服务器\n提高了编码的复杂性</code></pre>\n\n<p>　　值得一提的是：</p>\n\n<p>　　缓存集群的调度算法不同与上面提到的应用服务器和数据库。最好采用&ldquo;一致性哈希算法&rdquo;，这样才能提高命中率。</p>\n\n<p>　　加入缓存后的结构：</p>\n\n<p><img src=\"http://localhost:8080/myLog/images/architect/6.png\" style=\"height:485px; width:449px\" /></p>\n\n<p><strong><a id=\"阶段七、数据库水平拆分与垂直拆分\" name=\"阶段七、数据库水平拆分与垂直拆分\"></a>阶段七、数据库水平拆分与垂直拆分</strong></p>\n\n<p>　　我们的网站演进到现在，交易、商品、用户的数据都还在同一个数据库中。尽管采取了增加缓存，读写分离的方式，但随着数据库的压力继续增加，数据库的瓶颈越来越突出，此时，我们可以有数据垂直拆分和水平拆分两种选择。</p>\n\n<p>　　7.1、数据垂直拆分</p>\n\n<p>　　垂直拆分的意思是把数据库中不同的业务数据拆分道不同的数据库中，结合现在的例子，就是把交易、商品、用户的数据分开。</p>\n\n<p>　　优点：<br />\n　　解决了原来把所有业务放在一个数据库中的压力问题。<br />\n　　可以根据业务的特点进行更多的优化<br />\n　　缺点：<br />\n　　需要维护多个数据库<br />\n　　问题：<br />\n　　需要考虑原来跨业务的事务<br />\n　　跨数据库的join<br />\n　　解决问题方案：<br />\n　　我们应该在应用层尽量避免跨数据库的事务，如果非要跨数据库，尽量在代码中控制。<br />\n　　我们可以通过第三方应用来解决，如上面提到的mycat，mycat提供了丰富的跨库join方案。<br />\n　　垂直拆分后的结构如下：</p>\n\n<p><img src=\"http://localhost:8080/myLog/images/architect/7.png\" style=\"height:485px; width:690px\" /></p>\n\n<p>　7.2、数据水平拆分</p>\n\n<p>　　数据水平拆分就是把同一个表中的数据拆分到两个甚至多个数据库中。产生数据水平拆分的原因是某个业务的数据量或者更新量到达了单个数据库的瓶颈，这时就可以把这个表拆分到两个或更多个数据库中。</p>\n\n<p>　　优点：<br />\n　　如果我们能克服以上问题，那么我们将能够很好地对数据量及写入量增长的情况。<br />\n　　问题：<br />\n　　访问用户信息的应用系统需要解决SQL路由的问题，因为现在用户信息分在了两个数据库中，需要在进行数据操作时了解需要操作的数据在哪里。<br />\n　　主键的处理也变得不同，例如原来自增字段，现在不能简单地继续使用了。<br />\n　　如果需要分页，就麻烦了。<br />\n　　解决问题方案：<br />\n　　我们还是可以通过可以解决第三方中间件，如mycat。mycat可以通过SQL解析模块对我们的SQL进行解析，再根据我们的配置，把请求转发到具体的某个数据库。<br />\n　　我们可以通过UUID保证唯一或自定义ID方案来解决。<br />\n　　mycat也提供了丰富的分页查询方案，比如先从每个数据库做分页查询，再合并数据做一次分页查询等等。<br />\n　　数据水平拆分后的结构：</p>\n\n<p><img src=\"http://localhost:8080/myLog/images/architect/8.png\" style=\"height:512px; width:712px\" /></p>\n\n<p><strong><a id=\"阶段八、应用的拆分\" name=\"阶段八、应用的拆分\"></a>阶段八、应用的拆分</strong></p>\n\n<p>　　8.1、拆分应用</p>\n\n<p>　　随着业务的发展，业务越来越多，应用越来越大。我们需要考虑如何避免让应用越来越臃肿。这就需要把应用拆开，从一个应用变为俩个甚至更多。还是以我们上面的例子，我们可以把用户、商品、交易拆分开。变成&ldquo;用户、商品&rdquo;和&ldquo;用户，交易&rdquo;两个子系统。　　</p>\n\n<p>　　拆分后的结构：</p>\n\n<p><img src=\"http://localhost:8080/myLog/images/architect/9.png\" style=\"height:602px; width:712px\" /></p>\n\n<p>问题：<br />\n这样拆分后，可能会有一些相同的代码，如用户相关的代码，商品和交易都需要用户信息，所以在两个系统中都保留差不多的操作用户信息的代码。如何保证这些代码可以复用是一个需要解决的问题。<br />\n&nbsp;　　解决问题：<br />\n&nbsp;　　通过走服务化的路线来解决<br />\n　　8.2、走服务化的道路</p>\n\n<p>　　为了解决上面拆分应用后所出现的问题，我们把公共的服务拆分出来，形成一种服务化的模式，简称SOA。</p>\n\n<p>　　采用服务化之后的系统结构：</p>\n\n<p><img src=\"http://localhost:8080/myLog/images/architect/10.png\" style=\"height:512px; width:712px\" /></p>\n\n<p>优点：<br />\n相同的代码不会散落在不同的应用中了，这些实现放在了各个服务中心，使代码得到更好的维护。<br />\n我们把对数据库的交互放在了各个服务中心，让&rdquo;前端&ldquo;的web应用更注重与浏览器交互的工作。<br />\n　　&nbsp;问题：<br />\n&nbsp;　　如何进行远程的服务调用<br />\n　　&nbsp;解决方法：<br />\n&nbsp;　　我们可以通过下面的引入消息中间件来解决</p>\n\n<p><strong><a id=\"阶段九、引入消息中间件\" name=\"阶段九、引入消息中间件\"></a>阶段九、引入消息中间件</strong></p>\n\n<p>　　随着网站的继续发展，我们的系统中可能出现不同语言开发的子模块和部署在不同平台的子系统。此时我们需要一个平台来传递可靠的，与平台和语言无关的数据，并且能够把负载均衡透明化，能在调用过程中收集调用数据并分析之，推测出网站的访问增长率等等一系列需求，对于网站应该如何成长做出预测。开源消息中间件有阿里的dubbo，可以搭配Google开源的分布式程序协调服务zookeeper实现服务器的注册与发现。<br />\n　　引入消息中间件后的结构：</p>\n\n<p><img src=\"http://localhost:8080/myLog/images/architect/12.png\" style=\"height:697px; width:712px\" /></p>\n\n<p><a id=\"总结\" name=\"总结\"></a><strong>总结</strong></p>\n\n<p>　　以上的演变过程只是一个例子，并不适合所有的网站，实际中网站演进过程与自身业务和不同遇到的问题有密切的关系，没有固定的模式。只有认真的分析和不断地探究，才能发现适合自己网站的架构。</p>', 1, '5f199b8885e24fc8b28672b872edb606', 19, '2018-02-26 11:32:34', '2018-09-22 19:37:36');
INSERT INTO `logcontent` VALUES ('49ed9b138ac24917b2b27003cd374715', -1, 'f29612168904487aadb4043df110361c', 'Nginx主模块（Main Module）', 'linux,nginx,主模块,Main Module', '主模块', '<p>主模块（Main Module）：包含一些Nginx的基本控制功能</p>\n\n<p>error_log</p>\n\n<pre>\n<code class=\"language-nginx\">语法：error_log file [ debug | info | notice | warn | error | crit ] \n默认值：${prefix}/logs/error.log\n指定Nginx服务（与FastCGI）错误日志文件位置。\n每个字段的错误日志等级默认值：\n\n1、main字段 - error\n2、HTTP字段 - crit\n3、server字段 - crit\n\nNginx支持为每个虚拟主机设置不同的错误日志文件，这一点要好于lighttpd，详细为每个虚拟主机配置不同错误日志的例子请参考：SeparateErrorLoggingPerVirtualHost和mailing list thread on separating error logging per virtual host\n如果你在编译安装Nginx时加入了--with-debug参数，你可以使用以下配置：\nerror_log LOGFILE [debug_core | debug_alloc | debug_mutex | debug_event | debug_http | debug_imap];\n注意error_log off并不能关闭日志记录功能，而会将日志文件写入一个文件名为off的文件中，如果你想关闭错误日志记录功能，应使用以下配置：\nerror_log /dev/null crit;\n同样注意0.7.53版本，nginx在使用配置文件指定的错误日志路径前将使用编译时指定的默认日志位置，如果运行nginx的用户对该位置没有写入权限，nginx将输出如下错误：\n[alert]: could not open error log file: open() \"/var/log/nginx/error.log\" failed (13: Permission denied)</code></pre>\n\n<p>include</p>\n\n<pre>\n<code class=\"language-nginx\">语法：include file | * \n默认值：none \n你可以包含一些其他的配置文件来完成你想要的功能。\n0.4.4版本以后，include指令已经能够支持文件通配符：\ninclude vhosts/*.conf;\n注意：直到0.6.7版本，这个参数包含的文件相对路径随你在编译时指定的--prefix=PATH目录而决定，默认是/usr/local/nginx，如果你不想指定这个目录下的文件，请写绝对路径。\n0.6.7版本以后指定的文件相对路径根据nginx.conf所在的目录而决定，而不是prefix目录的路径。</code></pre>\n\n<p>pid</p>\n\n<pre>\n<code class=\"language-nginx\">语法：pid file \n默认值：编译时指定 \npid /var/log/nginx.pid;\n指定pid文件，可以使用kill命令来发送相关信号，例如你如果想重新读取配置文件，则可以使用：kill -HUP `cat /var/log/nginx.pid`</code></pre>\n\n<p>user</p>\n\n<pre>\n<code class=\"language-nginx\">语法：user user [group] \n默认值：nobody nobody \n如果主进程以root运行，Nginx将会调用setuid()/setgid()来设置用户/组，如果没有指定组，那么将使用与用户名相同的组，默认情况下会使用nobody用户与nobody组（或者nogroup），或者在编译时指定的--user=USER和--group=GROUP的值。\nuser www users;</code></pre>\n\n<p>worker_cpu_affinity</p>\n\n<pre>\n<code class=\"language-nginx\">语法：worker_cpu_affinity cpumask [cpumask...] \n默认值：none \n仅支持linux系统。\n这个参数允许将工作进程指定到cpu，它调用sched_setaffinity()函数\nworker_processes     4;\nworker_cpu_affinity 0001 0010 0100 1000;\n指定每个进程到一个CPU：\nworker_processes     2;\nworker_cpu_affinity 0101 1010;\n指定第一个进程到CPU0/CPU2，指定第二个进程到CPU1/CPU3，对于HTT处理器来说是一个不错的选择。</code></pre>\n\n<p>worker_processes</p>\n\n<pre>\n<code class=\"language-nginx\">语法：worker_processes number \n默认值：1 \nworker_processes 5;\n由于以下几点原因，Nginx可能需要运行不止一个进程\n\n·使用了SMP（对称多处理技术）。\n·当服务器在磁盘I/O出现瓶颈时为了减少响应时间。\n·当使用select()/poll()限制了每个进程的最大连接数时。\n\n在事件模块中使用worker_processes和worker_connections来计算理论最大连接数（max_clients）：\nmax_clients = worker_processes * worker_connections </code></pre>\n\n<p>&nbsp;</p>\n', 1, '5f199b8885e24fc8b28672b872edb606', 8, '2018-04-14 14:36:24', '2018-09-07 18:57:47');
INSERT INTO `logcontent` VALUES ('4c20c1d766bb4c338e7b7e530b2ad545', -1, '7338e53acd514defa1a17e47016f3f4a', 'Elasticsearch（十三）', '写一致性原理,quorum机制特殊情况,quorum不齐全', '写一致性原理/quorum机制/节点数少于quorum数量，可能导致quorum不齐全，进而导致无法执行任何写操作/quorum不齐全', '<p><a name=\"写一致性原理\"></a>1、写一致性原理：consistency，one（primary shard），all（all shard），quorum（default）</p>\n\n<pre>\n<code class=\"language-markdown\">我们在发送任何一个增删改操作的时候，比如说put /index/type/id，都可以带上一个consistency参数，指明我们想要的写一致性是什么？\n\nput /index/type/id?consistency=quorum\none：要求我们这个写操作，只要有一个primary shard是active活跃可用的，就可以执行\nall：要求我们这个写操作，必须所有的primary shard和replica shard都是活跃的，才可以执行这个写操作\nquorum：默认的值，要求所有的shard中，必须是大部分的shard都是活跃的，可用的，才可以执行这个写操作</code></pre>\n\n<p><a name=\"quorum机制\"></a>2、quorum机制</p>\n\n<pre>\n<code class=\"language-markdown\">写之前必须确保大多数shard都可用，int( (primary + number_of_replicas) / 2 ) + 1，当number_of_replicas&gt;1时才生效\nquroum = int( (primary + number_of_replicas) / 2 ) + 1\n举个例子，3个primary shard，number_of_replicas=1，总共有3 + 3 * 1 = 6个shard\nquorum = int( (3 + 1) / 2 ) + 1 = 3\n所以，要求6个shard中至少有3个shard是active状态的，才可以执行这个写操作</code></pre>\n\n<p><a name=\"特殊情况\"></a>3、如果节点数少于quorum数量，可能导致quorum不齐全，进而导致无法执行任何写操作</p>\n\n<pre>\n<code class=\"language-markdown\">1个primary shard，replica=3，要求至少3个shard是active，3个shard处于active状态，如果说只有2台机器的话(1台机器有一个primary shard，另一台机器有一个replica shard，共两个shard为active状态)，出现3个shard都没法分配齐全，此时就可能会出现写操作无法执行的情况\nes提供了一种特殊的处理场景，就是说当number_of_replicas&gt;1时才生效，因为假如说，你就一个primary shard，replica=1，此时就2个shard（(1 + 1） / 2) + 1 = 2，要求必须有2个shard是活跃的，但是可能就1个node，此时就1个shard是活跃的，如果你不特殊处理的话，导致我们的单节点集群就无法工作</code></pre>\n\n<p><a name=\"quorum不齐全\"></a>4、quorum不齐全时，wait，默认1分钟，timeout，100，30s</p>\n\n<p>等待期间，期望活跃的shard数量可以增加，最后实在不行，就会timeout</p>\n\n<p>我们其实可以在写操作的时候，加一个timeout参数，比如说put /index/type/id?timeout=30，这个就是说自己去设定quorum不齐全的时候，es的timeout时长，可以缩短，也可以增长</p>\n\n<p>&nbsp;</p>\n', 2, '5f199b8885e24fc8b28672b872edb606', 5, '2017-07-12 18:04:35', '2018-09-06 16:12:58');
INSERT INTO `logcontent` VALUES ('506ea0d367f74699ab7a859831b40ebc', -1, '716b0d3ea6e04d03a9deb097be1e2cf1', '炒股就是炒心态笔记', '炒股就是炒心态', '《炒股就是炒心态》炒股如炒人生', '<p><strong><a id=\"炒股的实质是炒心态\" name=\"炒股的实质是炒心态\"></a>1/&nbsp;炒股的实质是炒心态</strong></p>\n\n<p>有的人理论很在行，实践操作却不行。心态不好，急着想发财的人，肯定是要失败的。炒股之道，技术只是基本条件，关键还在于心态。</p>\n\n<p><strong><a id=\"股市没有想象的那么简单\" name=\"股市没有想象的那么简单\"></a>2/ 股市没有想象的那么简单</strong></p>\n\n<p>要避免产生&ldquo;羊群效应&rdquo;，一定要克服盲目的从众心理，遇事要冷静分析，真正了解自己会不会做、能不能做好，心理承受能力有多强。</p>\n\n<p><strong><a id=\"了解“一赢二平七输”定律\" name=\"了解“一赢二平七输”定律\"></a>3/ 了解&ldquo;一赢二平七输&rdquo;定律</strong></p>\n\n<p>炒股前一定要做好知识准备、心理准备和经济准备等各项工作，不能盲目去炒股。</p>\n\n<p><strong><a id=\"不要做“勤劳”的股民\" name=\"不要做“勤劳”的股民\"></a>4/ 不要做&ldquo;勤劳&rdquo;的股民</strong></p>\n\n<p>勤劳&rdquo;的股民不一定能致富，最多只能获得一点微利，而且还要冒着被套牢的风险。相反，善于思考、耐心等待的人大多能致富。</p>\n\n<p>每年至少有一波或者两三波持续时间较长、上涨空间很大、多数股票能获利的行情，只要你耐心等待，伺机买进，果断出局，肯定是赢家。多分析，多思考，少操作，特别是在熊市中。&ldquo;炒股票总不能天天满仓，天天操作。&rdquo;</p>\n\n<p><strong><a id=\"了解“涨跌轮回”的道理\" name=\"了解“涨跌轮回”的道理\"></a>5/ 了解&ldquo;涨跌轮回&rdquo;的道理</strong></p>\n\n<p>世界上没有只涨不跌的股票。当大多数人不再谈论股市、不看股市行情时，也就意味着熊市快要结束了。同样的道理，当大多数人都&ldquo;勇敢&rdquo;地谈论股票市场，把看股市行情当成每一天生活的中心工作时，那么牛市也快到头了。</p>\n\n<p>大盘严重超跌就会反弹，这是股市自身运行的规律。实际上就这么简单。反弹还将继续，但大盘筑底过程中的反复却是难免的。&rdquo;</p>\n\n<p><strong><a id=\"众人皆醉唯我独醒\" name=\"众人皆醉唯我独醒\"></a>6/ 众人皆醉唯我独醒</strong></p>\n\n<p>要经常问问自己，尤其是在独处时，更应该问问自己：&ldquo;我是皆醉的人，还是独醒的人？&rdquo;&ldquo;为什么在熊市越低越敢卖？为什么在牛市越高越敢买？</p>\n\n<p><strong><a id=\"不要去买好多只股票\" name=\"不要去买好多只股票\"></a>7/ 不要去买好多只股票</strong></p>\n\n<p>股票一多，操作就容易乱。不知道股票的底细，最可能发生的事情就是&ldquo;盲目去买，盲目去卖&rdquo;。忙来忙去，结果还是亏空和绝望。最关键的是心态也被搞坏了。</p>\n\n<p><strong><a id=\"不能死在熊市里\" name=\"不能死在熊市里\"></a>8/ 不能死在熊市里</strong></p>\n\n<p>在熊市里用&ldquo;麻雀战术&rdquo;往往是最有效的操作策略。如果在熊市里还战斗不休，而且心态很差，不断地打听各种小道消息，不加分析地买进卖出，或者一心想翻本，不断地割肉抄底，企图弥补亏空，自然亏得更多。结果等不到牛市到来就被淘汰出局，因为已悲惨地死在熊市里了</p>\n\n<p>最科学的炒股方法是明白&ldquo;仓位控制、资金管理和持股操作&rdquo;</p>\n\n<p><strong><a id=\"顶上一日，底下一年\" name=\"顶上一日，底下一年\"></a>9/ 顶上一日，底下一年</strong></p>\n\n<p>股市在高位停留的时间往往很短暂，而其下跌的过程，以及在底部盘整的时间往往会很长。从我国股市19年的历史来看，除了1991年、2000年、2006年和2007年上涨时间稍长之外，达到了一年左右的时间，其余的上涨时间一般是1至3个月左右。而在历史高点的时间通常只是瞬间，比如说2007年10月的6124点。</p>\n\n<p><strong><a id=\"学会空仓\" name=\"学会空仓\"></a>10/ 涨跌的时候除看大了看势，股票质量，还要学会空仓</strong></p>\n\n<p>有所放弃才能有所收获。&ldquo;你暂时放弃了这个难以把握的机会，如大盘报复性反弹上涨，能够心静如水，仅是做观看分析。当你控制了冲动的欲望时，股市中的财富也将随之而来。&rdquo;</p>\n\n<p><strong><a id=\"耐心等待最好的时机\" name=\"耐心等待最好的时机\"></a>11/ 耐心等待最好的时机</strong></p>\n\n<p>作为一位成熟的股民，在指数高的时候持币，在指数低的时候持股。严格坚持10%左右止损原则，坚持资金安全第一原则。其实，股市更多是处于&ldquo;抄底时难抛亦难，反弹无力割肉寒&rdquo;的情形。</p>\n\n<p><strong><a id=\"适可而止，见好就收\" name=\"适可而止，见好就收\"></a>12/ 适可而止，见好就收</strong></p>\n\n<p>克服贪心，保住自己的胜利果实是非常重要的。许多人炒股之所以会失败，最主要的原因就是：&ldquo;一是频繁换股；二是贪婪满仓，不见好就收；三是高买低卖，不断亏损。&rdquo;</p>\n\n<p><strong><a id=\"知道“七炒七不炒”\" name=\"知道“七炒七不炒”\"></a>13/ 知道&ldquo;七炒七不炒&rdquo;</strong></p>\n\n<p>&ldquo;炒新不炒旧；炒小不炒大；炒短不炒长；炒低不炒高；炒冷不炒热；炒转不炒弹；炒亏不炒盈。&rdquo;</p>\n\n<p><strong><a id=\"炒股必须了解宏观政策面\" name=\"炒股必须了解宏观政策面\"></a>14/ 炒股必须了解宏观政策面</strong></p>\n\n<p>炒股要了解宏观面、基本面和技术面，一个都不能少。光知道股票知识和操作技术，肯定是不行的。有人曾说过：&ldquo;炒股必须听党的话。&rdquo;如果用&ldquo;只低头拉车，不抬头看路&rdquo;的方式去炒股，那么只能是&ldquo;亏损&rdquo;一条路。</p>\n\n<p><strong><a id=\"了解“华尔街家训”\" name=\"了解“华尔街家训”\"></a>15/ 了解&ldquo;华尔街家训&rdquo;</strong></p>\n\n<p>华尔街有如下家训：1.&ldquo;止损第一位&rdquo;。2.&ldquo;不要买太多的股票&rdquo;。3.&ldquo;看不准的时候离场&rdquo;。当对市场无法作清晰判断时，就要果断地暂时离开。4.&ldquo;忘记成本价&rdquo;。该躲避风险、该逃命的时候赶快离开，以免损失更大。5.&ldquo;切忌频繁操作&rdquo;。不能给证券公司&ldquo;义务打工&rdquo;。6.&ldquo;不要补仓&rdquo;。在股市大势不好的时候，往往补仓越多，死得越惨，因为谁都不知道底在哪里。7.&ldquo;制订好计划，然后严格遵守执行&rdquo;。有人坚持&ldquo;8%、5%&rdquo;原则，就是涨8%就抛，跌5%就抛。结果牛市大赚，熊市小亏。8.&ldquo;顺势而为&rdquo;。&ldquo;宁买当头起，莫买当头跌。&rdquo;</p>\n\n<p><strong><a id=\"股市里没有专家\" name=\"股市里没有专家\"></a>16/ 股市里没有专家</strong></p>\n\n<p>别顾虑太多，通过股市的实践来学习和丰富股市操作基本知识，这是一种最有效的方法。股市里没有顶级专家，更没有炒股&ldquo;神仙&rdquo;。</p>\n\n<p>有人善意地提醒广大股民：&ldquo;不要盲目学太多的分析方法，因为盲目学习只会让我们丧失最基本的自我保护意识。知道了什么是&lsquo;黄金分割&rsquo;、什么是&lsquo;波浪理论&rsquo;，知道&lsquo;怎样看报表&rsquo;，知道了如何分析&lsquo;市场消息&rsquo;，却容易忘掉&lsquo;止损保命&rsquo;这个最基本的原则。&rdquo;</p>\n\n<p><strong><a id=\"经常亏损就要改变操作习惯\" name=\"经常亏损就要改变操作习惯\"></a>17/ 经常亏损就要改变操作习惯</strong></p>\n\n<p>失败总是有原因的，不好好总结教训只能永远失败。</p>\n\n<p><strong><a id=\"股市是一门哲学\" name=\"股市是一门哲学\"></a>18/ 股市是一门哲学</strong></p>\n\n<p>新股民认为，&ldquo;股市就是忙忙碌碌，买进卖出。&rdquo;老股民认为，&ldquo;股市就是人生，股市更是一门哲学。&rdquo;比如说遇到好股票，你能否坚持？是涨一点就走，还是长期持有？不要被股市涨跌的表象所迷惑，相信股市像大自然一样，有黑夜就有白天；</p>\n\n<p><strong><a id=\"股市里不会掉馅饼\" name=\"股市里不会掉馅饼\"></a>19/ 股市里不会掉馅饼</strong></p>\n\n<p>在股市关键点要&ldquo;大义灭亲，说翻脸就翻脸&rdquo;。&ldquo;炒股与赛车一样，有时候要停，有时候要冲，但不要永远想着冲，否则会被撞死的。&rdquo;有人忠告：&ldquo;当别人忽悠你股市会掉馅饼的时候，你就永远记住&lsquo;股市媒体的天职是服务于销量，而教授的天职是服从于媒体，永远不要相信他们&rsquo;。&rdquo;要相信世界上没有无缘无故的爱，也没有无缘无故的恨。</p>\n\n<p><strong><a id=\"赌博心理太重，人容易丧失理智\" name=\"赌博心理太重，人容易丧失理智\"></a>20/ 赌博心理太重，人容易丧失理智</strong></p>\n\n<p>在美国这样成熟的股市里，股民年换手率只是百分之百；而在我国这样极其不成熟的股市里，股民年换手率达到了百分之一千二。这就充分说明了我们的广大股民都是在进行短线投机，在股市里搞所谓的&ldquo;赌博&rdquo;活动，而不是什么投资行为。</p>\n\n<p><strong><a id=\"牛市股票为王，熊市现金为王\" name=\"牛市股票为王，熊市现金为王\"></a>21/ 牛市股票为王，熊市现金为王</strong></p>\n\n<p>树立正确的财富观念比获取财富更为重要。梦想通过冒险的投机行为来致富，是非常不现实的，也是非常危险的事情。在股市中，你更要学习、学习、再学习。</p>\n\n<p>多听取成功者的忠告，而不要把太多的时间浪费在熊市里。要像军人那样用铁的纪律来约束自己的炒股行为，避免掉入失败的陷阱。许多失败者总是不喜欢纪律约束，所以他们常常会失败。</p>\n\n<p>股民不是输在知识和技术上，而是输在错误的理念和心态上。</p>\n\n<p>牛市看不清楚什么是好公司，而熊市则能够看清楚哪些是好公司，哪些是垃圾公司。</p>\n\n<p>股商是不可缺少的</p>\n\n<p>不断克服贪和怕的心理</p>\n', 1, '5f199b8885e24fc8b28672b872edb606', 2, '2017-09-02 22:42:29', '2018-08-31 22:21:23');
INSERT INTO `logcontent` VALUES ('574272daafed485fac935a39cd186fd2', -1, 'd7caeba238f0466d87db109b2b9724da', '隐马尔可夫模型', '隐马尔可夫', '初学', '<p>http://www.cnblogs.com/skyme/p/4651331.html</p>\n', 1, '5f199b8885e24fc8b28672b872edb606', 6, '2017-12-10 21:04:54', '2018-08-31 22:21:23');
INSERT INTO `logcontent` VALUES ('58f2d228fa7041778099a2fb9ef5793a', -1, '7338e53acd514defa1a17e47016f3f4a', '六、请求体查询', '请求体查询', '请求体查询', '<h2>请求体查询</h2>\n\n<p><em>简易</em>&nbsp;查询 &mdash;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/search-lite.html\" title=\"轻量 搜索\">query-string search</a>&mdash; 对于用命令行进行即席查询（ad-hoc）是非常有用的。&nbsp;然而，为了充分利用查询的强大功能，你应该使用&nbsp;<em>请求体</em>&nbsp;<code>search</code>&nbsp;API， 之所以称之为请求体查询(Full-Body Search)，因为大部分参数是通过 Http 请求体而非查询字符串来传递的。</p>\n\n<p>请求体查询 &mdash;下文简称&nbsp;<em>查询</em>&mdash;不仅可以处理自身的查询请求，还允许你对结果进行片段强调（高亮）、对所有或部分结果进行聚合分析，同时还可以给出&nbsp;<em>你是不是想找</em>&nbsp;的建议，这些建议可以引导使用者快速找到他想要的结果。</p>\n\n<h2>空查询</h2>\n\n<p>让我们以&nbsp;最简单的&nbsp;<code>search</code>&nbsp;API 的形式开启我们的旅程，空查询将返回所有索引库（indices)中的所有文档：</p>\n\n<pre>\nGET /_search\n{} <img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" />\n</pre>\n\n<table border=\"0\" summary=\"Callout list\">\n	<tbody>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/_empty_search.html#CO22-1\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p>这是一个空的请求体。</p>\n			</td>\n		</tr>\n	</tbody>\n</table>\n\n<p>只用一个查询字符串，你就可以在一个、多个或者&nbsp;<code>_all</code>&nbsp;索引库（indices）和一个、多个或者所有types中查询：</p>\n\n<pre>\nGET /index_2014*/type1,type2/_search\n{}</pre>\n\n<p>同时你可以使用&nbsp;<code>from</code>&nbsp;和&nbsp;<code>size</code>&nbsp;参数来分页：</p>\n\n<pre>\nGET /_search\n{\n  &quot;from&quot;: 30,\n  &quot;size&quot;: 10\n}</pre>\n\n<p><strong>一个带请求体的 GET 请求？</strong></p>\n\n<p>某些特定语言（特别是 JavaScript）的 HTTP 库是不允许&nbsp;<code>GET</code>&nbsp;请求带有请求体的。&nbsp;事实上，一些使用者对于&nbsp;<code>GET</code>&nbsp;请求可以带请求体感到非常的吃惊。</p>\n\n<p>而事实是这个RFC文档&nbsp;<a href=\"http://tools.ietf.org/html/rfc7231#page-24\" target=\"_top\">RFC 7231</a>&mdash; 一个专门负责处理 HTTP 语义和内容的文档&thinsp;&mdash;&thinsp;并没有规定一个带有请求体的&nbsp;<code>GET</code>&nbsp;请求应该如何处理！结果是，一些 HTTP 服务器允许这样子，而有一些&thinsp;&mdash;&thinsp;特别是一些用于缓存和代理的服务器&thinsp;&mdash;&thinsp;则不允许。</p>\n\n<p>对于一个查询请求，Elasticsearch 的工程师偏向于使用&nbsp;<code>GET</code>&nbsp;方式，因为他们觉得它比&nbsp;<code>POST</code>&nbsp;能更好的描述信息检索（retrieving information）的行为。然而，因为带请求体的&nbsp;<code>GET</code>&nbsp;请求并不被广泛支持，所以&nbsp;<code>search</code>&nbsp;API&nbsp;同时支持&nbsp;<code>POST</code>&nbsp;请求：</p>\n\n<pre>\nPOST /_search\n{\n  &quot;from&quot;: 30,\n  &quot;size&quot;: 10\n}</pre>\n\n<p>类似的规则可以应用于任何需要带请求体的&nbsp;<code>GET</code>&nbsp;API。</p>\n\n<p>我们将深入介绍聚合（aggregations），而现在，我们将聚焦在查询。</p>\n\n<p>相对于使用晦涩难懂的查询字符串的方式，一个带请求体的查询允许我们使用&nbsp;<em>查询领域特定语言（query domain-specific language）</em>&nbsp;或者 Query DSL 来写查询语句。</p>\n\n<h2>查询表达式</h2>\n\n<p>查询表达式(Query DSL)是一种非常灵活又富有表现力的&nbsp;查询语言。 Elasticsearch 使用它可以以简单的 JSON 接口来展现 Lucene 功能的绝大部分。在你的应用中，你应该用它来编写你的查询语句。它可以使你的查询语句更灵活、更精确、易读和易调试。</p>\n\n<p>要使用这种查询表达式，只需将查询语句传递给&nbsp;<code>query</code>&nbsp;参数：</p>\n\n<pre>\nGET /_search\n{\n    &quot;query&quot;: YOUR_QUERY_HERE\n}</pre>\n\n<p><em>空查询（empty search）</em>&nbsp;&mdash;<code>{}</code>&mdash;&nbsp;在功能上等价于使用&nbsp;<code>match_all</code>&nbsp;查询，&nbsp;正如其名字一样，匹配所有文档：</p>\n\n<pre>\nGET /_search\n{\n    &quot;query&quot;: {\n        &quot;match_all&quot;: {}\n    }\n}\n</pre>\n\n<h3>查询语句的结构</h3>\n\n<p>一个查询语句&nbsp;的典型结构：</p>\n\n<pre>\n{\n    QUERY_NAME: {\n        ARGUMENT: VALUE,\n        ARGUMENT: VALUE,...\n    }\n}</pre>\n\n<p>如果是针对某个字段，那么它的结构如下：</p>\n\n<pre>\n{\n    QUERY_NAME: {\n        FIELD_NAME: {\n            ARGUMENT: VALUE,\n            ARGUMENT: VALUE,...\n        }\n    }\n}</pre>\n\n<p>举个例子，你可以使用&nbsp;<code>match</code>&nbsp;查询语句&nbsp;来查询&nbsp;<code>tweet</code>&nbsp;字段中包含&nbsp;<code>elasticsearch</code>&nbsp;的 tweet：</p>\n\n<pre>\n{\n    &quot;match&quot;: {\n        &quot;tweet&quot;: &quot;elasticsearch&quot;\n    }\n}</pre>\n\n<p>完整的查询请求如下：</p>\n\n<pre>\nGET /_search\n{\n    &quot;query&quot;: {\n        &quot;match&quot;: {\n            &quot;tweet&quot;: &quot;elasticsearch&quot;\n        }\n    }\n}\n</pre>\n\n<h3>合并查询语句</h3>\n\n<p><em>查询语句(Query clauses)</em>&nbsp;就像一些简单的组合块&nbsp;，这些组合块可以彼此之间合并组成更复杂的查询。这些语句可以是如下形式：</p>\n\n<ul style=\"list-style-type:disc\">\n	<li><em>叶子语句（Leaf clauses）</em>&nbsp;(就像&nbsp;<code>match</code>&nbsp;语句)&nbsp;被用于将查询字符串和一个字段（或者多个字段）对比。</li>\n	<li><em>复合(Compound)</em>&nbsp;语句 主要用于&nbsp;合并其它查询语句。 比如，一个&nbsp;<code>bool</code>&nbsp;语句&nbsp;允许在你需要的时候组合其它语句，无论是&nbsp;<code>must</code>&nbsp;匹配、&nbsp;<code>must_not</code>&nbsp;匹配还是&nbsp;<code>should</code>&nbsp;匹配，同时它可以包含不评分的过滤器（filters）：</li>\n</ul>\n\n<pre>\n{\n    &quot;bool&quot;: {\n        &quot;must&quot;:     { &quot;match&quot;: { &quot;tweet&quot;: &quot;elasticsearch&quot; }},\n        &quot;must_not&quot;: { &quot;match&quot;: { &quot;name&quot;:  &quot;mary&quot; }},\n        &quot;should&quot;:   { &quot;match&quot;: { &quot;tweet&quot;: &quot;full text&quot; }},\n        &quot;filter&quot;:   { &quot;range&quot;: { &quot;age&quot; : { &quot;gt&quot; : 30 }} }\n    }\n}\n</pre>\n\n<p>一条复合语句可以合并&nbsp;<em>任何</em>&nbsp;其它查询语句，包括复合语句，了解这一点是很重要的。这就意味着，复合语句之间可以互相嵌套，可以表达非常复杂的逻辑。</p>\n\n<p>例如，以下查询是为了找出信件正文包含&nbsp;<code>business opportunity</code>&nbsp;的星标邮件，或者在收件箱正文包含<code>business opportunity</code>&nbsp;的非垃圾邮件：</p>\n\n<pre>\n{\n    &quot;bool&quot;: {\n        &quot;must&quot;: { &quot;match&quot;:   { &quot;email&quot;: &quot;business opportunity&quot; }},\n        &quot;should&quot;: [\n            { &quot;match&quot;:       { &quot;starred&quot;: true }},\n            { &quot;bool&quot;: {\n                &quot;must&quot;:      { &quot;match&quot;: { &quot;folder&quot;: &quot;inbox&quot; }},\n                &quot;must_not&quot;:  { &quot;match&quot;: { &quot;spam&quot;: true }}\n            }}\n        ],\n        &quot;minimum_should_match&quot;: 1\n    }\n}</pre>\n\n<p>到目前为止，你不必太在意这个例子的细节，我们会在后面详细解释。最重要的是你要理解到，一条复合语句可以将多条语句&thinsp;&mdash;&thinsp;叶子语句和其它复合语句&thinsp;&mdash;&thinsp;合并成一个单一的查询语句。</p>\n\n<h2>查询与过滤</h2>\n\n<p>Elasticsearch 使用的查询语言（DSL）&nbsp;拥有一套查询组件，这些组件可以以无限组合的方式进行搭配。这套组件可以在以下两种情况下使用：过滤情况（filtering context）和查询情况（query context）。</p>\n\n<p>当使用于&nbsp;<em>过滤情况</em>&nbsp;时，查询被设置成一个&ldquo;不评分&rdquo;或者&ldquo;过滤&rdquo;查询。即，这个查询只是简单的问一个问题：&ldquo;这篇文档是否匹配？&rdquo;。回答也是非常的简单，yes 或者 no ，二者必居其一。</p>\n\n<ul style=\"list-style-type:disc\">\n	<li><code>created</code>&nbsp;时间是否在&nbsp;<code>2013</code>&nbsp;与&nbsp;<code>2014</code>&nbsp;这个区间？</li>\n	<li><code>status</code>&nbsp;字段是否包含&nbsp;<code>published</code>&nbsp;这个单词？</li>\n	<li><code>lat_lon</code>&nbsp;字段表示的位置是否在指定点的&nbsp;<code>10km</code>&nbsp;范围内？</li>\n</ul>\n\n<p>当使用于&nbsp;<em>查询情况</em>&nbsp;时，查询就变成了一个&ldquo;评分&rdquo;的查询。和不评分的查询类似，也要去判断这个文档是否匹配，同时它还需要判断这个文档匹配的有 _多好_（匹配程度如何）。 此查询的典型用法是用于查找以下文档：</p>\n\n<ul style=\"list-style-type:disc\">\n	<li>查找与&nbsp;<code>full text search</code>&nbsp;这个词语最佳匹配的文档</li>\n	<li>包含&nbsp;<code>run</code>&nbsp;这个词，也能匹配&nbsp;<code>runs</code>&nbsp;、&nbsp;<code>running</code>&nbsp;、&nbsp;<code>jog</code>&nbsp;或者&nbsp;<code>sprint</code></li>\n	<li>包含&nbsp;<code>quick</code>&nbsp;、&nbsp;<code>brown</code>&nbsp;和&nbsp;<code>fox</code>&nbsp;这几个词 &mdash; 词之间离的越近，文档相关性越高</li>\n	<li>标有&nbsp;<code>lucene</code>&nbsp;、&nbsp;<code>search</code>&nbsp;或者&nbsp;<code>java</code>&nbsp;标签 &mdash; 标签越多，相关性越高</li>\n</ul>\n\n<p>一个评分查询计算每一个文档与此查询的 _相关程度_，同时将这个相关程度分配给表示相关性的字段 `_score`，并且按照相关性对匹配到的文档进行排序。这种相关性的概念是非常适合全文搜索的情况，因为全文搜索几乎没有完全 &ldquo;正确&rdquo; 的答案。</p>\n\n<p><img alt=\"注意\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/note.png\" /></p>\n\n<p>自 Elasticsearch 问世以来，查询与过滤（queries and filters）就独自成为 Elasticsearch 的组件。但从 Elasticsearch 2.0 开始，过滤（filters）已经从技术上被排除了，同时所有的查询（queries）拥有变成不评分查询的能力。</p>\n\n<p>然而，为了明确和简单，我们用 &quot;filter&quot; 这个词表示不评分、只过滤情况下的查询。你可以把 &quot;filter&quot; 、 &quot;filtering query&quot; 和 &quot;non-scoring query&quot; 这几个词视为相同的。</p>\n\n<p>相似的，如果单独地不加任何修饰词地使用 &quot;query&quot; 这个词，我们指的是 &quot;scoring query&quot; 。</p>\n\n<h3>性能差异</h3>\n\n<p>过滤查询（Filtering queries）只是简单的检查包含或者排除，这就使得计算起来非常快。考虑到至少有一个过滤查询（filtering query）的结果是 &ldquo;稀少的&rdquo;（很少匹配的文档），并且经常使用不评分查询（non-scoring queries），结果会被缓存到内存中以便快速读取，所以有各种各样的手段来优化查询结果。</p>\n\n<p>相反，评分查询（scoring queries）不仅仅要找出&nbsp;匹配的文档，还要计算每个匹配文档的相关性，计算相关性使得它们比不评分查询费力的多。同时，查询结果并不缓存。</p>\n\n<p>多亏倒排索引（inverted index），一个简单的评分查询在匹配少量文档时可能与一个涵盖百万文档的filter表现的一样好，甚至会更好。但是在一般情况下，一个filter 会比一个评分的query性能更优异，并且每次都表现的很稳定。</p>\n\n<p>过滤（filtering）的目标是减少那些需要通过评分查询（scoring queries）进行检查的文档。</p>\n\n<h3>如何选择查询与过滤</h3>\n\n<p>通常的规则是，使用&nbsp;查询（query）语句来进行&nbsp;<em>全文</em>&nbsp;搜索或者其它任何需要影响&nbsp;<em>相关性得分</em>&nbsp;的搜索。除此以外的情况都使用过滤（filters)。</p>\n\n<h2>最重要的查询</h2>\n\n<p>虽然 Elasticsearch 自带了很多的查询，但经常用到的也就那么几个。我们将详细讨论那些查询的细节，接下来我们对最重要的几个查询进行简单介绍。</p>\n\n<h3>match_all 查询</h3>\n\n<p><code>match_all</code>&nbsp;查询简单的&nbsp;匹配所有文档。在没有指定查询方式时，它是默认的查询：</p>\n\n<pre>\n{ &quot;match_all&quot;: {}}</pre>\n\n<p>它经常与 filter 结合使用--例如，检索收件箱里的所有邮件。所有邮件被认为具有相同的相关性，所以都将获得分值为&nbsp;<code>1</code>&nbsp;的中性 `_score`。</p>\n\n<h3>match 查询</h3>\n\n<p>无论你在任何字段上进行的是全文搜索还是精确查询，<code>match</code>&nbsp;查询是你可用的标准查询。</p>\n\n<p>如果你在一个全文字段上使用&nbsp;<code>match</code>&nbsp;查询，在执行查询前，它将用正确的分析器去分析查询字符串：</p>\n\n<pre>\n{ &quot;match&quot;: { &quot;tweet&quot;: &quot;About Search&quot; }}</pre>\n\n<p>如果在一个精确值的字段上使用它，&nbsp;例如数字、日期、布尔或者一个&nbsp;<code>not_analyzed</code>&nbsp;字符串字段，那么它将会精确匹配给定的值：</p>\n\n<pre>\n{ &quot;match&quot;: { &quot;age&quot;:    26           }}\n{ &quot;match&quot;: { &quot;date&quot;:   &quot;2014-09-01&quot; }}\n{ &quot;match&quot;: { &quot;public&quot;: true         }}\n{ &quot;match&quot;: { &quot;tag&quot;:    &quot;full_text&quot;  }}</pre>\n\n<p><img alt=\"提示\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/tip.png\" /></p>\n\n<p>对于精确值的查询，你可能需要使用 filter 语句来取代 query，因为 filter 将会被缓存。接下来，我们将看到一些关于 filter 的例子。</p>\n\n<p>不像我们介绍的字符串查询（query-string search），&nbsp;<code>match</code>&nbsp;查询不使用类似<code>+user_id:2 +tweet:search</code>&nbsp;的查询语法。它只是去查找给定的单词。这就意味着将查询字段暴露给你的用户是安全的；你需要控制那些允许被查询字段，不易于抛出语法异常。</p>\n\n<h3>multi_match 查询</h3>\n\n<p><code>multi_match</code>&nbsp;查询可以在多个字段上执行相同的&nbsp;<code>match</code>&nbsp;查询：</p>\n\n<pre>\n{\n    &quot;multi_match&quot;: {\n        &quot;query&quot;:    &quot;full text search&quot;,\n        &quot;fields&quot;:   [ &quot;title&quot;, &quot;body&quot; ]\n    }\n}</pre>\n\n<p>range 查询</p>\n\n<p><code>range</code>&nbsp;查询找出那些落在指定区间内的数字或者时间：</p>\n\n<pre>\n{\n    &quot;range&quot;: {\n        &quot;age&quot;: {\n            &quot;gte&quot;:  20,\n            &quot;lt&quot;:   30\n        }\n    }\n}</pre>\n\n<p>被允许的操作符如下：</p>\n\n<p><code>gt</code></p>\n\n<p>大于</p>\n\n<p><code>gte</code></p>\n\n<p>大于等于</p>\n\n<p><code>lt</code></p>\n\n<p>小于</p>\n\n<p><code>lte</code></p>\n\n<p>小于等于</p>\n\n<h3>term 查询</h3>\n\n<p><code>term</code>&nbsp;查询被用于精确值&nbsp;匹配，这些精确值可能是数字、时间、布尔或者那些&nbsp;<code>not_analyzed</code>&nbsp;的字符串：</p>\n\n<pre>\n{ &quot;term&quot;: { &quot;age&quot;:    26           }}\n{ &quot;term&quot;: { &quot;date&quot;:   &quot;2014-09-01&quot; }}\n{ &quot;term&quot;: { &quot;public&quot;: true         }}\n{ &quot;term&quot;: { &quot;tag&quot;:    &quot;full_text&quot;  }}</pre>\n\n<p><code>term</code>&nbsp;查询对于输入的文本不&nbsp;<em>分析</em>&nbsp;，所以它将给定的值进行精确查询。</p>\n\n<h3>terms 查询</h3>\n\n<p><code>terms</code>&nbsp;查询和&nbsp;<code>term</code>&nbsp;查询一样，但它允许你指定多值进行匹配。如果这个字段包含了指定值中的任何一个值，那么这个文档满足条件：</p>\n\n<pre>\n{ &quot;terms&quot;: { &quot;tag&quot;: [ &quot;search&quot;, &quot;full_text&quot;, &quot;nosql&quot; ] }}</pre>\n\n<p>和&nbsp;<code>term</code>&nbsp;查询一样，<code>terms</code>&nbsp;查询对于输入的文本不分析。它查询那些精确匹配的值（包括在大小写、重音、空格等方面的差异）。</p>\n\n<h3>exists 查询和 missing 查询</h3>\n\n<p><code>exists</code>&nbsp;查询和&nbsp;<code>missing</code>&nbsp;查询被用于查找那些指定字段中有值 (<code>exists</code>) 或无值 (<code>missing</code>) 的文档。这与SQL中的&nbsp;<code>IS_NULL</code>&nbsp;(<code>missing</code>) 和&nbsp;<code>NOT IS_NULL</code>&nbsp;(<code>exists</code>) 在本质上具有共性：</p>\n\n<pre>\n{\n    &quot;exists&quot;:   {\n        &quot;field&quot;:    &quot;title&quot;\n    }\n}</pre>\n\n<p>这些查询经常用于某个字段有值的情况和某个字段缺值的情况。</p>\n\n<h2>组合多查询</h2>\n\n<p>现实的查询需求从来都没有那么简单；它们需要在多个字段上查询多种多样的文本，并且根据一系列的标准来过滤。为了构建类似的高级查询，你需要一种能够将多查询组合成单一查询的查询方法。</p>\n\n<p>你可以用&nbsp;<code>bool</code>&nbsp;查询来实现你的需求。这种查询将多查询组合在一起，成为用户自己想要的布尔查询。它接收以下参数：</p>\n\n<p><code>must</code></p>\n\n<p>文档&nbsp;<em>必须</em>&nbsp;匹配这些条件才能被包含进来。</p>\n\n<p><code>must_not</code></p>\n\n<p>文档&nbsp;<em>必须不</em>&nbsp;匹配这些条件才能被包含进来。</p>\n\n<p><code>should</code></p>\n\n<p>如果满足这些语句中的任意语句，将增加&nbsp;<code>_score</code>&nbsp;，否则，无任何影响。它们主要用于修正每个文档的相关性得分。</p>\n\n<p><code>filter</code></p>\n\n<p><em>必须</em>&nbsp;匹配，但它以不评分、过滤模式来进行。这些语句对评分没有贡献，只是根据过滤标准来排除或包含文档。</p>\n\n<p>由于这是我们看到的第一个包含多个查询的查询，所以有必要讨论一下相关性得分是如何组合的。每一个子查询都独自地计算文档的相关性得分。一旦他们的得分被计算出来，&nbsp;<code>bool</code>&nbsp;查询就将这些得分进行合并并且返回一个代表整个布尔操作的得分。</p>\n\n<p>下面的查询用于查找&nbsp;<code>title</code>&nbsp;字段匹配&nbsp;<code>how to make millions</code>&nbsp;并且不被标识为&nbsp;<code>spam</code>&nbsp;的文档。那些被标识为&nbsp;<code>starred</code>&nbsp;或在2014之后的文档，将比另外那些文档拥有更高的排名。如果 _两者_ 都满足，那么它排名将更高：</p>\n\n<pre>\n{\n    &quot;bool&quot;: {\n        &quot;must&quot;:     { &quot;match&quot;: { &quot;title&quot;: &quot;how to make millions&quot; }},\n        &quot;must_not&quot;: { &quot;match&quot;: { &quot;tag&quot;:   &quot;spam&quot; }},\n        &quot;should&quot;: [\n            { &quot;match&quot;: { &quot;tag&quot;: &quot;starred&quot; }},\n            { &quot;range&quot;: { &quot;date&quot;: { &quot;gte&quot;: &quot;2014-01-01&quot; }}}\n        ]\n    }\n}\n</pre>\n\n<p><img alt=\"提示\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/tip.png\" /></p>\n\n<p>如果没有&nbsp;<code>must</code>&nbsp;语句，那么至少需要能够匹配其中的一条&nbsp;<code>should</code>&nbsp;语句。但，如果存在至少一条&nbsp;<code>must</code>&nbsp;语句，则对&nbsp;<code>should</code>&nbsp;语句的匹配没有要求。</p>\n\n<h3>增加带过滤器（filtering）的查询</h3>\n\n<p>如果我们不想因为文档的时间而影响得分，可以用&nbsp;<code>filter</code>&nbsp;语句来重写前面的例子：</p>\n\n<pre>\n{\n    &quot;bool&quot;: {\n        &quot;must&quot;:     { &quot;match&quot;: { &quot;title&quot;: &quot;how to make millions&quot; }},\n        &quot;must_not&quot;: { &quot;match&quot;: { &quot;tag&quot;:   &quot;spam&quot; }},\n        &quot;should&quot;: [\n            { &quot;match&quot;: { &quot;tag&quot;: &quot;starred&quot; }}\n        ],\n        &quot;filter&quot;: {\n          &quot;range&quot;: { &quot;date&quot;: { &quot;gte&quot;: &quot;2014-01-01&quot; }} <img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" />\n        }\n    }\n}\n</pre>\n\n<table border=\"0\" summary=\"Callout list\">\n	<tbody>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/combining-queries-together.html#CO23-1\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p>range 查询已经从&nbsp;<code>should</code>&nbsp;语句中移到&nbsp;<code>filter</code>&nbsp;语句</p>\n			</td>\n		</tr>\n	</tbody>\n</table>\n\n<p>通过将 range 查询移到&nbsp;<code>filter</code>&nbsp;语句中，我们将它转成不评分的查询，将不再影响文档的相关性排名。由于它现在是一个不评分的查询，可以使用各种对 filter 查询有效的优化手段来提升性能。</p>\n\n<p>所有查询都可以借鉴这种方式。将查询移到&nbsp;<code>bool</code>&nbsp;查询的&nbsp;<code>filter</code>&nbsp;语句中，这样它就自动的转成一个不评分的 filter 了。</p>\n\n<p>如果你需要通过多个不同的标准来过滤你的文档，<code>bool</code>&nbsp;查询本身也可以被用做不评分的查询。简单地将它放置到&nbsp;<code>filter</code>&nbsp;语句中并在内部构建布尔逻辑：</p>\n\n<pre>\n{\n    &quot;bool&quot;: {\n        &quot;must&quot;:     { &quot;match&quot;: { &quot;title&quot;: &quot;how to make millions&quot; }},\n        &quot;must_not&quot;: { &quot;match&quot;: { &quot;tag&quot;:   &quot;spam&quot; }},\n        &quot;should&quot;: [\n            { &quot;match&quot;: { &quot;tag&quot;: &quot;starred&quot; }}\n        ],\n        &quot;filter&quot;: {\n          &quot;bool&quot;: { <img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" />\n              &quot;must&quot;: [\n                  { &quot;range&quot;: { &quot;date&quot;: { &quot;gte&quot;: &quot;2014-01-01&quot; }}},\n                  { &quot;range&quot;: { &quot;price&quot;: { &quot;lte&quot;: 29.99 }}}\n              ],\n              &quot;must_not&quot;: [\n                  { &quot;term&quot;: { &quot;category&quot;: &quot;ebooks&quot; }}\n              ]\n          }\n        }\n    }\n}\n</pre>\n\n<table border=\"0\" summary=\"Callout list\">\n	<tbody>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/combining-queries-together.html#CO24-1\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p>将&nbsp;<code>bool</code>&nbsp;查询包裹在&nbsp;<code>filter</code>&nbsp;语句中，我们可以在过滤标准中增加布尔逻辑</p>\n			</td>\n		</tr>\n	</tbody>\n</table>\n\n<p>通过混合布尔查询，我们可以在我们的查询请求中灵活地编写 scoring 和 filtering 查询逻辑。</p>\n\n<h3>constant_score 查询</h3>\n\n<p>尽管没有&nbsp;<code>bool</code>&nbsp;查询使用这么频繁，<code>constant_score</code>&nbsp;查询也是你工具箱里有用的查询工具。它将一个不变的常量评分应用于所有匹配的文档。它被经常用于你只需要执行一个 filter 而没有其它查询（例如，评分查询）的情况下。</p>\n\n<p>可以使用它来取代只有 filter 语句的&nbsp;<code>bool</code>&nbsp;查询。在性能上是完全相同的，但对于提高查询简洁性和清晰度有很大帮助。</p>\n\n<pre>\n{\n    &quot;constant_score&quot;:   {\n        &quot;filter&quot;: {\n            &quot;term&quot;: { &quot;category&quot;: &quot;ebooks&quot; } <img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" />\n        }\n    }\n}\n</pre>\n\n<table border=\"0\" summary=\"Callout list\">\n	<tbody>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/combining-queries-together.html#CO25-1\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p><code>term</code>&nbsp;查询被放置在&nbsp;<code>constant_score</code>&nbsp;中，转成不评分的 filter。这种方式可以用来取代只有 filter 语句的&nbsp;<code>bool</code>&nbsp;查询。</p>\n			</td>\n		</tr>\n	</tbody>\n</table>\n\n<h2>验证查询</h2>\n\n<p>查询可以变得非常的复杂，尤其&nbsp;和不同的分析器与不同的字段映射结合时，理解起来就有点困难了。不过<code>validate-query</code>&nbsp;API 可以用来验证查询是否合法。</p>\n\n<pre>\nGET /gb/tweet/_validate/query\n{\n   &quot;query&quot;: {\n      &quot;tweet&quot; : {\n         &quot;match&quot; : &quot;really powerful&quot;\n      }\n   }\n}\n</pre>\n\n<p>以上&nbsp;<code>validate</code>&nbsp;请求的应答告诉我们这个查询是不合法的：</p>\n\n<pre>\n{\n  &quot;valid&quot; :         false,\n  &quot;_shards&quot; : {\n    &quot;total&quot; :       1,\n    &quot;successful&quot; :  1,\n    &quot;failed&quot; :      0\n  }\n}</pre>\n\n<h3>理解错误信息</h3>\n\n<p>为了找出&nbsp;查询不合法的原因，可以将&nbsp;<code>explain</code>&nbsp;参数&nbsp;加到查询字符串中：</p>\n\n<pre>\nGET /gb/tweet/_validate/query?explain <img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" />\n{\n   &quot;query&quot;: {\n      &quot;tweet&quot; : {\n         &quot;match&quot; : &quot;really powerful&quot;\n      }\n   }\n}\n</pre>\n\n<table border=\"0\" summary=\"Callout list\">\n	<tbody>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/validating-queries.html#CO26-1\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p><code>explain</code>&nbsp;参数可以提供更多关于查询不合法的信息。</p>\n			</td>\n		</tr>\n	</tbody>\n</table>\n\n<p>很明显，我们将查询类型(<code>match</code>)与字段名称 (<code>tweet</code>)搞混了：</p>\n\n<pre>\n{\n  &quot;valid&quot; :     false,\n  &quot;_shards&quot; :   { ... },\n  &quot;explanations&quot; : [ {\n    &quot;index&quot; :   &quot;gb&quot;,\n    &quot;valid&quot; :   false,\n    &quot;error&quot; :   &quot;org.elasticsearch.index.query.QueryParsingException:\n                 [gb] No query registered for [tweet]&quot;\n  } ]\n}</pre>\n\n<h3>理解查询语句</h3>\n\n<p>对于合法查询，使用&nbsp;<code>explain</code>&nbsp;参数将返回可读的描述，这对准确理解 Elasticsearch 是如何解析你的 query 是非常有用的：</p>\n\n<pre>\nGET /_validate/query?explain\n{\n   &quot;query&quot;: {\n      &quot;match&quot; : {\n         &quot;tweet&quot; : &quot;really powerful&quot;\n      }\n   }\n}\n</pre>\n\n<p>我们查询的每一个 index&nbsp;都会返回对应的&nbsp;<code>explanation</code>&nbsp;，因为每一个 index 都有自己的映射和分析器：</p>\n\n<pre>\n{\n  &quot;valid&quot; :         true,\n  &quot;_shards&quot; :       { ... },\n  &quot;explanations&quot; : [ {\n    &quot;index&quot; :       &quot;us&quot;,\n    &quot;valid&quot; :       true,\n    &quot;explanation&quot; : &quot;tweet:really tweet:powerful&quot;\n  }, {\n    &quot;index&quot; :       &quot;gb&quot;,\n    &quot;valid&quot; :       true,\n    &quot;explanation&quot; : &quot;tweet:realli tweet:power&quot;\n  } ]\n}</pre>\n\n<p>从&nbsp;<code>explanation</code>&nbsp;中可以看出，匹配&nbsp;<code>really powerful</code>&nbsp;的&nbsp;<code>match</code>&nbsp;查询被重写为两个针对&nbsp;<code>tweet</code>&nbsp;字段的 single-term 查询，一个single-term查询对应查询字符串分出来的一个term。</p>\n\n<p>当然，对于索引&nbsp;<code>us</code>&nbsp;，这两个 term 分别是&nbsp;<code>really</code>&nbsp;和&nbsp;<code>powerful</code>&nbsp;，而对于索引&nbsp;<code>gb</code>&nbsp;，term 则分别是<code>realli</code>&nbsp;和&nbsp;<code>power</code>&nbsp;。之所以出现这个情况，是由于我们将索引&nbsp;<code>gb</code>&nbsp;中&nbsp;<code>tweet</code>&nbsp;字段的分析器修改为&nbsp;<code>english</code>分析器。</p>\n\n<p>&nbsp;</p>\n', 1, '5f199b8885e24fc8b28672b872edb606', 4, '2018-06-14 17:08:38', '2018-08-31 22:21:23');
INSERT INTO `logcontent` VALUES ('5959c498661e4e5ba19475f3a1513fa2', -1, '3ba9a3aed3ce48f682974ebcf3552ce2', '未来30年如何“拼爹”', '拼爹', '赞同', '<h2>老婆最近一直在抱怨我陪儿子时间太少，导致儿子现在还不会骑自行车，不会背三字经，不会玩直升飞机&hellip;&hellip;而别人家的孩子，早都会了。连爹妈也来插两刀，说弟弟的儿子，都会背十首唐诗了。</h2>\n\n<p>还有我小时候据说４岁就认识好几百汉字，可以加减乘除。再加上我们周围生活的很多人，下一代基本都是牛津耶鲁此类，作为家庭教育总管的爹，鸭梨山大。</p>\n\n<p>父母到底应该在小孩教育中起什么样的作用？</p>\n\n<p>正方一派通常人数众多，认为小孩自己什么都不懂，你不逼他陪他提供条件给他，他自然是放任自流玩物丧志，所以虽然知道孩子辛苦，但是世道艰难，为了将来，父母必须殚精竭虑顶着头皮推他上轨道，这是责任。</p>\n\n<p>反方一派人不多，但是理念很超前：快乐就好，父母的作用应该是在发现而不是挖掘，多陪伴而不是逼迫。</p>\n\n<p>我的立场和反方非常接近，但是通常会被正方认为是不作为的借口，因此我最近也在反思，究竟应该如何包装一下观点。</p>\n\n<p>我一直认为，一个小孩爱不爱学习，学习成绩好不好，几乎是天注定的。学习这件事情，如果不是以考上清华北大来论的话，那拼得就是天分&hellip;&hellip;真正聪明的人，他根本不需要学习&mdash;&mdash;李寻欢的理论绝对成立，这一点只有真正见过高手的人才会明白。</p>\n\n<p>这听上去非常让人不服气，但真的就是事实。很多人这辈子听一下波意尔马略特定律就头疼，至死都不明白动量定理和动能定理的区别，对于这样的人你当然也可以使用家教，上加强班，进国际学校等等办法，最终也许依然可以保证他们进入名校甚至拿到博士后&mdash;&mdash;但也仅此而已，他们的人生可以成功，但是很难精彩。</p>\n\n<p>听到这里，我的很多朋友都会说，是的，道理是这个，但是你不知道国内现在的竞争有多激烈&hellip;&hellip;</p>\n\n<p>如果我告诉你，20年之后常春藤毕业的都必须先进监狱扫垃圾三年，而动物园给河马刷牙的年轻人可以直选卫生部长，你还会坚持现今的做法吗？</p>\n\n<p>也许这个假设很荒谬，但更荒谬的是：这么多人用假定的宏大叙述作为目标，不过是为了逃避人生出现过多的变数。很多人并非试图追求稳定、常规、传统，他们只是害怕凋零&mdash;&mdash;对于很多家长而言，他们不愿意承认的，是自己的眼界狭小而导致的失败。</p>\n\n<p>其实新的一代人，他们面临的环境和竞争已经远远超出上一代人可以理解的范畴。对于50后60后来讲，能读书就很了不起了，考上大学当然是改变人生振奋家乡的大事件。而到了70后80后，名校外企公务员，所有的独木桥上都是人，不争第一就只能回家做缩头乌龟，因此好好读书奋力向上，几乎是每个普通人最深刻的生活选择&mdash;&mdash;对此我们可以理解为改革开放的正能量。</p>\n\n<p>可是到了90后甚至00后，这个逻辑很难follow。首先独木桥早就被人挤垮了。我周围的很多年轻人，从小就是新东方直奔牛津去的，高中就跑遍欧美澳的大把，创业大赛商模PK奖状早就拿厌了。可是斯坦福麻省理工就那么几家，你是局长处长那边一堆CEO高级总监也不是吃素的，何况现在大家都吃进口奶粉，智商差异很容易集中在小数点后几位&mdash;&mdash;如果入口只有几百人的容量，能挤进地铁站的已经是英雄。况且买完票上车坐到站，小朋友们还要辛苦再挤一遍出站&hellip;&hellip;怎么，你还想开车去那边接不成？</p>\n\n<p>对于像我这样出生于小城市、至今没有上市企业挂牌、一生与诺贝尔奖无缘的人而言，怂恿儿子去走最好的小学&mdash;中学&mdash;大学&mdash;博士这条路，几乎是逼良为娼。除非我确定他在智商上胜我一筹，不行，至少还要三筹。</p>\n\n<p>对这条路不死心的人，要么是对自己的基因太过乐观，要么就是多半没见过真正聪明的人。我小时候据说算是智商高的，所以5岁被父母丢进小学，后来还去了火箭班少年班各种。这种拔苗助长果然以失败而告终，除了成绩上的天外有天外，很少有人能体会到那种和一堆比自己大几岁的同学在一起的孤独：随便一个男生都可以一脚踹倒你，随便一个女生都可以把你写的情书交给老师......所以带给我和我年少的小伙伴们共同的体会就是：决不逼孩子做任何试图超越常人的事情，不然的话世界会一下变得很凶险。</p>\n\n<p>所以差异化和多样化的竞争，才是00后们要面对的主流挑战。当上个大学根本就是basic，去个纽约也毫不special的时代来临。</p>\n\n<p>70后80后父母们在2030年这个节点很可能被现实击溃：在高大上的奋斗链条上他们已经找不到更多的诱饵来激励下一代。</p>\n\n<p>龙应台的儿子会叼着烟跟她讲：妈妈，你要接受一个事实，我很平庸，不可能像你和爸爸一样都拿到博士学位的。</p>\n\n<p>相信我的同龄人今后很多都会遇到这一幕。孩子们长大后倔强的要表达的其实不是反叛，而是对人生的放弃。这一点才是最可怕的。</p>\n\n<p>我们每个人都可以回身四顾一下，在你的朋友圈里，有多少人可以完成父母通常希望的标准轨迹：名牌大学毕业，工作优越稳定，30岁结婚生子，花园洋房定期旅游，到现在没离婚没当过小三没未老先衰，对未来和生活充满希望。</p>\n\n<p>所谓稳定的中产阶级生活，它根本不可能在一个竞争太过激烈、社会配套环境堪忧的大背景下出现。如果我们无力改变社会性的自然现实，却又执意在内部空间内把孩子们往这个方向推，最终出现的结果很可能是我们要面临更多的心灵崩溃：在不远的将来，这些强势聪明的父母们，可能更多地要面临高才生们带来的窘境：找不到老婆，呆家里啃老，吸毒&hellip;&hellip;甚至无厘头的自杀。</p>\n\n<p>我曾经和一个马上要去帝国理工的90后聊了一晚，整个晚上她没有提到任何关于学习关于就业关于父母的话题，全部精力都只集中在让叔叔帮她留住一份不般配的微弱的爱情上面&mdash;&mdash;我想说的重点是，如果你是一个公司的00后销售，你的客户类型都是这种哈佛回来的小龙女，那么你是否还会把取得成功的前提定义为你也要有金闪闪的毕业证？</p>\n\n<p>2030年左右人类要面临的世界，很可能是一个高度关联、无孔不入的智能世界。这意味着大多数依靠智商和经验去完成的行业会面临巨大挑战，比如投资、审计、咨询、工程，甚至管理。大数据建模完毕按图索骥就是，再高的学历和智商甚至外语水平面对大电脑都是白扯&mdash;&mdash;很多职业会黯然失色，就像上海一百那抓一把刚好是1斤水果糖的售货员，邮电局快准狠的接线员。</p>\n\n<p>人们对职业的追求更多将依赖天分，而且很可能还是不入流的天分。比如现在的手游设计师，将来的金鱼花纹养成师。一个会开小型挖掘机的园丁，很可能比很多名校毕业生收入更高，生活更优越，老婆更漂亮，所谓人生赢家。所以现代人很难理解为什么踢毽子的高俅能当宰相，其实你看看宋代的GDP就知道，一个社会发展成熟完备了，奇淫技巧必定完败格物致知。</p>\n\n<p>这才是真正的社会进步：我们这一代学习工程和经济，是为了让下一代去学习历史和艺术。假设这一理论在2030年能被大部分人接受，那么我们现在倒推回来就必须问：爹妈们在2015该如何拼呢？</p>\n\n<p>遗憾的是以我浅薄的身家根本无法回答这个问题，人们大可以看看教育专家或虎爸牛妈哈佛女孩们的自传。但是我想作为一个号称新派的父亲，我会努力做到的是：每天都陪小胖吃晚饭，每周带他出去看花花世界，去最近的地方上学，到最远的地方旅游，永远不指责他说人家都会背千字文会画鸡蛋了。爱的时候亲他屁屁，错的时候也要打其屁屁，给他正常的生活。</p>\n\n<p>核心是：保持儿子承担的最大压力仅来自他自己，父亲的任务是发现他的优点，欣赏他的个性，运用成人的思维和资源帮助其放大并促进应用。要对自己的儿子无比自信：上不上大学都无所谓，我的儿子就不可能平庸。</p>\n\n<p>当然，基础是爱妈妈，健康，赚钱，不缺失生活的基本格局和水准。</p>\n\n<p>父母实际上除了基因，最能够给孩子们传递的应该是生活习惯，思维方式。还有：信念。</p>\n', 1, '5f199b8885e24fc8b28672b872edb606', 6, '2017-09-02 22:47:48', '2018-08-31 22:21:23');
INSERT INTO `logcontent` VALUES ('5986727921a6498a844cb0f6f037b744', -1, '3ba9a3aed3ce48f682974ebcf3552ce2', '正则表达式记录', 'regular,正则表达式', '将一些好用的正则表达式记录下来', '<p>正则表达式匹配不包含某些字符串的技巧</p>\n\n<pre>\n<code>^((?!tl).)*$</code></pre>\n\n<p>&nbsp;</p>\n', 1, '5f199b8885e24fc8b28672b872edb606', 7, '2018-03-29 13:56:18', '2018-08-31 22:21:23');
INSERT INTO `logcontent` VALUES ('598c158ea16b47bfa56ca041c5a95faa', 1, 'd7caeba238f0466d87db109b2b9724da', '支持向量机(SVM)算法应用笔记二', 'svm', '初学', '<p><a id=\"Python环境\" name=\"Python环境\"></a>Python环境</p>\n\n<pre>\n<code>Windows版 Python 3.6.2</code></pre>\n\n<p><a id=\"Python代码实现\" name=\"Python代码实现\"></a>Python代码实现</p>\n\n<pre>\n<code># -*- coding:utf8 -*-\n\nfrom __future__ import print_function\n\nfrom time import time\nimport logging\nimport matplotlib.pyplot as plt\n\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.datasets import fetch_lfw_people\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.decomposition import RandomizedPCA\nfrom sklearn.svm import SVC\n\nlogging.basicConfig(level=logging.INFO, format=\'%(asctime)s %(message)s\')\n\nlfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)#下载人脸数据集\n\nn_samples, h, w = lfw_people.images.shape\n\nX = lfw_people.data#获取特征向量的矩阵（人脸的特征向量）\nn_features = X.shape[1]\n\ny = lfw_people.target#归类标记（对应哪个人）\ntarget_names = lfw_people.target_names#类别中的名字\nn_classes = target_names.shape[0]\n\nprint(\"Total dataset size:\")\nprint(\"n_samples: %d\" % n_samples)#实例数\nprint(\"n_features: %d\" % n_features)#特征维度数\nprint(\"n_classes: %d\" % n_classes)#类别中的名字数量（多少个人）\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)#通过train_test_split将数据集分成训练集和测试集\n\n####PCA降维处理####\nn_components = 150\nprint(\"Extracting the top %d eigenfaces from %d faces\"% (n_components, X_train.shape[0]))\nt0 = time()\npca = RandomizedPCA(n_components=n_components, whiten=True).fit(X_train)\nprint(\"done in %0.3fs\" % (time() - t0))\neigenfaces = pca.components_.reshape((n_components, h, w))\nt0 = time()\nX_train_pca = pca.transform(X_train)\nX_test_pca = pca.transform(X_test)\nprint(\"done in %0.3fs\" % (time() - t0))\n####降维完成####\n####使用SVM分类####\nt0 = time()\nparam_grid = {\'C\': [1e3, 5e3, 1e4, 5e4, 1e5],\n              \'gamma\': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\nclf = GridSearchCV(SVC(kernel=\'rbf\', class_weight=\'auto\'), param_grid)\nclf = clf.fit(X_train_pca, y_train)\nprint(\"done in %0.3fs\" % (time() - t0))\nprint(\"Best estimator found by grid search:\")\nprint(clf.best_estimator_)\n\nprint(\"Predicting people\'s names on the test set\")\nt0 = time()\ny_pred = clf.predict(X_test_pca)\nprint(\"done in %0.3fs\" % (time() - t0))\n\nprint(classification_report(y_test, y_pred, target_names=target_names))\nprint(confusion_matrix(y_test, y_pred, labels=range(n_classes)))\n\ndef plot_gallery(images, titles, h, w, n_row=3, n_col=4):\n    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n    for i in range(n_row * n_col):\n        plt.subplot(n_row, n_col, i + 1)\n        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n        plt.title(titles[i], size=12)\n        plt.xticks(())\n        plt.yticks(())\n\ndef title(y_pred, y_test, target_names, i):\n    pred_name = target_names[y_pred[i]].rsplit(\' \', 1)[-1]\n    true_name = target_names[y_test[i]].rsplit(\' \', 1)[-1]\n    return \'predicted: %s\\ntrue:      %s\' % (pred_name, true_name)\n\nprediction_titles = [title(y_pred, y_test, target_names, i)\n                     for i in range(y_pred.shape[0])]\n\nplot_gallery(X_test, prediction_titles, h, w)\n\neigenface_titles = [\"eigenface %d\" % i for i in range(eigenfaces.shape[0])]\nplot_gallery(eigenfaces, eigenface_titles, h, w)\n\nplt.show()</code></pre>\n\n<pre>\n<code>注意：\n由于在启动程序的时候会下载一个图片库，耗时比较长，如果中途停止程序，可能导致lfw-funneled.tgz文件损坏\n异常如下：\n    raise EOFError(\"Compressed file ended before the \"\nEOFError: Compressed file ended before the end-of-stream marker was reached\n解决：\n找到lfw-funneled.tgz及解压的文件夹lfw-funneled并删除。</code></pre>\n\n<p>&nbsp;</p>\n', 1, '5f199b8885e24fc8b28672b872edb606', 4, '2017-11-22 22:44:17', '2018-08-31 22:21:23');
INSERT INTO `logcontent` VALUES ('5a6c298e9e2b436eaa10376000043c75', -1, 'f29612168904487aadb4043df110361c', 'VCL', 'varnish,vcl', 'Varnish配置语言\n另收藏一大神帖\nhttp://www.bubuko.com/infodetail-1583832.html', '<p><a id=\"VCL简介\" name=\"VCL简介\"></a>VCL简介</p>\n\n<pre>\n<code>VCL（Varnish Configuration Language）：Varnish配置语言，语法简单，功能强大，类似于c、perl。主要用来配置如何处理请求和内容的缓存策略。\nVCL在执行时，会转换成二进制代码。\nVCL文件被分为多个子程序，不同的子程序在不同的时间里执行，比如一个子程序在接到请求时执行，另一个子程序在接收到后端服务器传送的文件时执行。</code></pre>\n\n<p><a id=\"基本语法介绍\" name=\"基本语法介绍\"></a>基本语法介绍</p>\n\n<pre>\n<code>1：用花括号做界定符，使用分号表示声明结束。注释用//、#、/* */\n2：赋值（=）、比较（==）、和一些布尔值（！、&amp;&amp;、||），!(取反)等类似c语法\n3：支持正则表达式，ACL匹配使用~ 操作\n4：不同于C的地方，反斜杠（\\）在VCL中没有特殊的含义。只是用来匹配URLs\n5：VCL没有用户定义的变量，只能给backend、request、document这些对象的变量赋值，大部分是手工输入的，而且给这些变量分配值的时候，必须有一个VCL兼容的单位\n6：VCL有if，但是没有循环。\n7：可以使用set来给request的header添加值，unset 或 remove 来删除某个header</code></pre>\n\n<p><a id=\"backend\" name=\"backend\"></a>backend</p>\n\n<pre>\n<code>一个backend申明创建和初始化一个backend目标：\nbackend sishuok {\n	.host = \"www.sishuok.com\";\n	.port = \"8080\";\n	.connect_timeout = 1s;\n	.first_byte_timeout = 5s;\n	.between_bytes_timeout = 2s;\n	.max_connections=1000;\n}\n一个请求可以选择一个Backend：\nif (req.http.host ~ \"^(www.)?sishuok.com$\") {\n    set req.backend = sishuok;\n}\n为了避免后端服务器过载，.max_connections 可以设置连接后端服务器的最大限制数。\n在backend中申明的timeout参数可以被覆盖，\n.connect_timeout 等待连接后端的时间；\n.first_byte_timeout 等待从backend传输过来的第一个字符的时间；\n.between_bytes_timeout 两个字符的间隔时间。</code></pre>\n\n<p><a id=\"director\" name=\"director\"></a>director</p>\n\n<pre>\n<code>》backend的逻辑分组或backend的集群。主要有随机、循环和DNS几种director。\n\n》随机 director分成三种，分别是：random、client、hash，他们采用同样的随机分发算法，只是种子数值不同，种子数分别采用随机数、客户端id，或者是缓存的hash（典型如url）。\nclient director：通过设置VCL的变量client.identity来区分客户端，值可以从session cookie 或其它相似的值来获取\nhash director：默认使用URL的hash值，可以通过 req.hash 获取到\nround-robin director：一次循环使用backend，第一个请求用第一个backend，第二个请求用第二个，以此类推。如果某个backend出了问题，它会继续尝试下一个，理论上它要尝试完所有的backend均失败，才会出错。\neg:random\ndirector directorname random {\n    .retries = 5;\n    {\n        .backend = b1;//引用已经存在的backend\n        .weight = 7;\n    }\n    {\n        .backend = { //或者是直接在这里定义backend\n            .host = \"fs2\";\n        }\n        .weight = 3;\n    }\n}\n.retries这个参数指定查找可用后端的次数。默认director中的所有后端的.retries相同。\n.weight表示这个后端的权重\n\n\n》DNS director分为两种，一种是random或者round-robin；另一种是使用.list（list的方式不支持ipv6）：\ndirector directorname dns {\n	.list = {\n		.host_header = \"www.example.com\";\n		.port = \"80\";\n		.connection_timeout = 0.4;\n		\"192.168.15.0\"/24;\n		\"192.168.16.128\"/25;\n	}\n	.ttl = 5m;\n	.suffix = \"internal.example.net\";\n}\n这段代码会制定384个后端，都使用80端口及0.4s的连接超时。\n.list声明中设置选项必须在IPS的前面。.ttl定义DNSlookups的时间。\n\n》fallback director：选择第一个健康的backend，示例：\ndirector b3 fallback {\n	{ .backend = www1; }\n	{ .backend = www2; } // 第一个不好用，才会到这里\n	{ .backend = www3; } // 前两个都不好用，才会到这里 \n}\n\n》probe（后端探针）：探测后端，确定他们是否健康，返回的状态用req.backend.healthy核对：\nbackend sishuok {\n	.host = \"www.sishuok.com\";\n	.port = \"8080\";\n	.probe = { \n		.url = \"/test.jpg\";\n		.timeout = 0.3 s;\n		.window = 8; //要检查后端服务器的次数\n		.threshold = 3; //.window里面要有多少polls成功就认为后端是健康的\n		.initial = 3; //当varnish启动的时候，要确保多少个probe正常\n	} \n}\n当然，也可以把 probe从backend中拿出来单独定义，如：\nbackend sishuok{……\n    .probe=p1;\n}\nprobe p1{……}\n\n》可能用到的参数：\n.url：访问backend的路径，缺省是”/”\n.request：设置详细的请求头，是一些字符串\n.window：要检查后端服务器的次数，默认是8\n.threshold：.window里面要有多少polls成功就认为后端是健康的，默认是3\n.initial：当varnish启动的时候，要确保多少个probe正常，默认和threshold一样\n.expected_response：期望的response code，默认是200\n.interval：定义probe多久检查一次后端，默认是5秒\n.timeout：定义probe的过期时间，默认是2秒\n》也可以指定原始的http请求，形如：\nbackend sishuok {\n	.host = \"www.sishuok.com\";\n	.port = “8080\";\n	.probe = {\n		.request =\"GET / HTTP/1.1“\n		\"Host: www.foo.bar“\n		\"Connection: close\";\n	}\n}</code></pre>\n\n<p><a id=\"ACLs\" name=\"ACLs\"></a>ACLs</p>\n\n<pre>\n<code>访问控制列表，示例如下：\nacl local {\n    \"localhost\";\n    \"192.0.2.0\"/24;\n    ! \"192.0.2.23\";\n}\n如果一个ACL中指定一个主机名，varnish不能解析，他将解析匹配到所有地址。\n如果你使用了一个否定标记（！），那么将拒绝匹配所有主机。\n下面是一个匹配的示例：\nif (client.ip ~ local) {\n    pipe;\n}</code></pre>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n', 1, '5f199b8885e24fc8b28672b872edb606', 2, '2018-03-30 15:51:27', '2018-09-07 18:57:35');
INSERT INTO `logcontent` VALUES ('5b45838467c04903947e793dc1e8e730', 1, '1dff2a5fdaeb4886938bb7c70b57acce', 'spring', 'spring,面试', 'spring面试常见问题', '<h2>1. 一般问题</h2>\n\n<h3>1.1. 不同版本的 Spring Framework 有哪些主要功能？</h3>\n\n<table style=\"width:893px\">\n	<thead>\n		<tr>\n			<th>Version</th>\n			<th>Feature</th>\n		</tr>\n	</thead>\n	<tbody>\n		<tr>\n			<td>Spring 2.5</td>\n			<td>发布于 2007 年。这是第一个支持注解的版本。</td>\n		</tr>\n		<tr>\n			<td>Spring 3.0</td>\n			<td>发布于 2009 年。它完全利用了 Java5 中的改进，并为 JEE6 提供了支持。</td>\n		</tr>\n		<tr>\n			<td>Spring 4.0</td>\n			<td>发布于 2013 年。这是第一个完全支持 JAVA8 的版本。</td>\n		</tr>\n	</tbody>\n</table>\n\n<h3>1.2. 什么是 Spring Framework？</h3>\n\n<ul>\n	<li>\n	<p>Spring 是一个开源应用框架，旨在降低应用程序开发的复杂度。</p>\n	</li>\n	<li>\n	<p>它是轻量级、松散耦合的。</p>\n	</li>\n	<li>\n	<p>它具有分层体系结构，允许用户选择组件，同时还为 J2EE 应用程序开发提供了一个有凝聚力的框架。</p>\n	</li>\n	<li>\n	<p>它可以集成其他框架，如 Structs、Hibernate、EJB 等，所以又称为框架的框架。</p>\n	</li>\n</ul>\n\n<h3>1.3. 列举 Spring Framework 的优点。</h3>\n\n<ul>\n	<li>\n	<p>由于 Spring Frameworks 的分层架构，用户可以自由选择自己需要的组件。</p>\n	</li>\n	<li>\n	<p>Spring Framework 支持 POJO(Plain Old Java Object) 编程，从而具备持续集成和可测试性。</p>\n	</li>\n	<li>\n	<p>由于依赖注入和控制反转，JDBC 得以简化。</p>\n	</li>\n	<li>\n	<p>它是开源免费的。</p>\n	</li>\n</ul>\n\n<h3>1.4. Spring Framework 有哪些不同的功能？</h3>\n\n<ul>\n	<li>\n	<p><strong>轻量级</strong>&nbsp;- Spring 在代码量和透明度方面都很轻便。</p>\n	</li>\n	<li>\n	<p><strong>IOC</strong>&nbsp;- 控制反转</p>\n	</li>\n	<li>\n	<p><strong>AOP</strong>&nbsp;- 面向切面编程可以将应用业务逻辑和系统服务分离，以实现高内聚。</p>\n	</li>\n	<li>\n	<p><strong>容器</strong>&nbsp;- Spring 负责创建和管理对象（Bean）的生命周期和配置。</p>\n	</li>\n	<li>\n	<p><strong>MVC</strong>&nbsp;- 对 web 应用提供了高度可配置性，其他框架的集成也十分方便。</p>\n	</li>\n	<li>\n	<p><strong>事务管理</strong>&nbsp;- 提供了用于事务管理的通用抽象层。Spring 的事务支持也可用于容器较少的环境。</p>\n	</li>\n	<li>\n	<p><strong>JDBC 异常</strong>&nbsp;- Spring 的 JDBC 抽象层提供了一个异常层次结构，简化了错误处理策略。</p>\n	</li>\n</ul>\n\n<h3>1.5. Spring Framework 中有多少个模块，它们分别是什么？</h3>\n\n<p><img alt=\"\" src=\"http://localhost:8080/myLog/images/spring/springframeworkruntime\" style=\"height:300px; width:400px\" /></p>\n\n<ul>\n	<li>\n	<p><strong>Spring 核心容器</strong>&nbsp;&ndash; 该层基本上是 Spring Framework 的核心。它包含以下模块：</p>\n\n	<ul>\n		<li>\n		<p>Spring Core</p>\n		</li>\n		<li>\n		<p>Spring Bean</p>\n		</li>\n		<li>\n		<p>SpEL (Spring Expression Language)</p>\n		</li>\n		<li>\n		<p>Spring Context</p>\n		</li>\n	</ul>\n	</li>\n	<li>\n	<p><strong>数据访问/集成</strong>&nbsp;&ndash; 该层提供与数据库交互的支持。它包含以下模块：</p>\n\n	<ul>\n		<li>\n		<p>JDBC (Java DataBase Connectivity)</p>\n		</li>\n		<li>\n		<p>ORM (Object Relational Mapping)</p>\n		</li>\n		<li>\n		<p>OXM (Object XML Mappers)</p>\n		</li>\n		<li>\n		<p>JMS (Java Messaging Service)</p>\n		</li>\n		<li>\n		<p>Transaction</p>\n		</li>\n	</ul>\n	</li>\n	<li>\n	<p><strong>Web</strong>&nbsp;&ndash; 该层提供了创建 Web 应用程序的支持。它包含以下模块：</p>\n\n	<ul>\n		<li>\n		<p>Web</p>\n		</li>\n		<li>\n		<p>Web &ndash; Servlet</p>\n		</li>\n		<li>\n		<p>Web &ndash; Socket</p>\n		</li>\n		<li>\n		<p>Web &ndash; Portlet</p>\n		</li>\n	</ul>\n	</li>\n	<li>\n	<p><strong>AOP</strong>&nbsp;&ndash; 该层支持面向切面编程</p>\n	</li>\n	<li>\n	<p><strong>Instrumentation</strong>&nbsp;&ndash; 该层为类检测和类加载器实现提供支持。</p>\n	</li>\n	<li>\n	<p><strong>Test</strong>&nbsp;&ndash; 该层为使用 JUnit 和 TestNG 进行测试提供支持。</p>\n	</li>\n	<li>\n	<p><strong>几个杂项模块:</strong></p>\n\n	<ul>\n		<li>\n		<p>Messaging &ndash; 该模块为 STOMP 提供支持。它还支持注解编程模型，该模型用于从 WebSocket 客户端路由和处理 STOMP 消息。</p>\n		</li>\n		<li>\n		<p>Aspects &ndash; 该模块为与 AspectJ 的集成提供支持。</p>\n		</li>\n	</ul>\n	</li>\n</ul>\n\n<h3>1.6. 什么是 Spring 配置文件？</h3>\n\n<p>Spring 配置文件是 XML 文件。该文件主要包含类信息。它描述了这些类是如何配置以及相互引入的。但是，XML 配置文件冗长且更加干净。如果没有正确规划和编写，那么在大项目中管理变得非常困难。</p>\n\n<h3>1.7. Spring 应用程序有哪些不同组件？</h3>\n\n<p>Spring 应用一般有以下组件：</p>\n\n<ul>\n	<li>\n	<p><strong>接口</strong>&nbsp;- 定义功能。</p>\n	</li>\n	<li>\n	<p><strong>Bean 类</strong>&nbsp;- 它包含属性，setter 和 getter 方法，函数等。</p>\n	</li>\n	<li>\n	<p><strong>Spring 面向切面编程（AOP）</strong>&nbsp;- 提供面向切面编程的功能。</p>\n	</li>\n	<li>\n	<p><strong>Bean 配置文件</strong>&nbsp;- 包含类的信息以及如何配置它们。</p>\n	</li>\n	<li>\n	<p><strong>用户程序</strong>&nbsp;- 它使用接口。</p>\n	</li>\n</ul>\n\n<h3>1.8. 使用 Spring 有哪些方式？</h3>\n\n<p>使用 Spring 有以下方式：</p>\n\n<ul>\n	<li>\n	<p>作为一个成熟的 Spring Web 应用程序。</p>\n	</li>\n	<li>\n	<p>作为第三方 Web 框架，使用 Spring Frameworks 中间层。</p>\n	</li>\n	<li>\n	<p>用于远程使用。</p>\n	</li>\n	<li>\n	<p>作为企业级 Java Bean，它可以包装现有的 POJO（Plain Old Java Objects）。</p>\n	</li>\n</ul>\n\n<h2>2. 依赖注入（Ioc）</h2>\n\n<h3>2.1. 什么是 Spring IOC 容器？</h3>\n\n<p>Spring 框架的核心是 Spring 容器。容器创建对象，将它们装配在一起，配置它们并管理它们的完整生命周期。Spring 容器使用依赖注入来管理组成应用程序的组件。容器通过读取提供的配置元数据来接收对象进行实例化，配置和组装的指令。该元数据可以通过 XML，Java 注解或 Java 代码提供。</p>\n\n<p><img alt=\"\" src=\"http://localhost:8080/myLog/images/spring/ioc容器\" style=\"height:252px; width:300px\" /></p>\n\n<h3>2.2. 什么是依赖注入？</h3>\n\n<p>在依赖注入中，您不必创建对象，但必须描述如何创建它们。您不是直接在代码中将组件和服务连接在一起，而是描述配置文件中哪些组件需要哪些服务。由 IoC 容器将它们装配在一起。</p>\n\n<h3>2.3. 可以通过多少种方式完成依赖注入？</h3>\n\n<p>通常，依赖注入可以通过三种方式完成，即：</p>\n\n<ul>\n	<li>\n	<p>构造函数注入</p>\n	</li>\n	<li>\n	<p>setter 注入</p>\n	</li>\n	<li>\n	<p>接口注入</p>\n	</li>\n</ul>\n\n<p>在 Spring Framework 中，仅使用构造函数和 setter 注入。</p>\n\n<h3>2.4. 区分构造函数注入和 setter 注入。</h3>\n\n<table style=\"width:893px\">\n	<thead>\n		<tr>\n			<th>构造函数注入</th>\n			<th>setter 注入</th>\n		</tr>\n	</thead>\n	<tbody>\n		<tr>\n			<td>没有部分注入</td>\n			<td>有部分注入</td>\n		</tr>\n		<tr>\n			<td>不会覆盖 setter 属性</td>\n			<td>会覆盖 setter 属性</td>\n		</tr>\n		<tr>\n			<td>任意修改都会创建一个新实例</td>\n			<td>任意修改不会创建一个新实例</td>\n		</tr>\n		<tr>\n			<td>适用于设置很多属性</td>\n			<td>适用于设置少量属性</td>\n		</tr>\n	</tbody>\n</table>\n\n<h3>2.5. spring 中有多少种 IOC 容器？</h3>\n\n<ul>\n	<li>\n	<p>BeanFactory - BeanFactory 就像一个包含 bean 集合的工厂类。它会在客户端要求时实例化 bean。</p>\n	</li>\n	<li>\n	<p>ApplicationContext - ApplicationContext 接口扩展了 BeanFactory 接口。它在 BeanFactory 基础上提供了一些额外的功能。</p>\n	</li>\n</ul>\n\n<h3>2.6. 区分 BeanFactory 和 ApplicationContext。</h3>\n\n<table style=\"width:893px\">\n	<thead>\n		<tr>\n			<th>BeanFactory</th>\n			<th>ApplicationContext</th>\n		</tr>\n	</thead>\n	<tbody>\n		<tr>\n			<td>它使用懒加载</td>\n			<td>它使用即时加载</td>\n		</tr>\n		<tr>\n			<td>它使用语法显式提供资源对象</td>\n			<td>它自己创建和管理资源对象</td>\n		</tr>\n		<tr>\n			<td>不支持国际化</td>\n			<td>支持国际化</td>\n		</tr>\n		<tr>\n			<td>不支持基于依赖的注解</td>\n			<td>支持基于依赖的注解</td>\n		</tr>\n	</tbody>\n</table>\n\n<h3>2.7. 列举 IoC 的一些好处。</h3>\n\n<p>IoC 的一些好处是：</p>\n\n<ul>\n	<li>\n	<p>它将最小化应用程序中的代码量。</p>\n	</li>\n	<li>\n	<p>它将使您的应用程序易于测试，因为它不需要单元测试用例中的任何单例或 JNDI 查找机制。</p>\n	</li>\n	<li>\n	<p>它以最小的影响和最少的侵入机制促进松耦合。</p>\n	</li>\n	<li>\n	<p>它支持即时的实例化和延迟加载服务。</p>\n	</li>\n</ul>\n\n<h3>2.8. Spring IoC 的实现机制。</h3>\n\n<p>Spring 中的 IoC 的实现原理就是工厂模式加反射机制。</p>\n\n<p>示例：</p>\n\n<pre>\n<code>interface Fruit {\n     public abstract void eat();\n}\nclass Apple implements Fruit {\n    public void eat(){\n        System.out.println(\"Apple\");\n    }\n}\nclass Orange implements Fruit {\n    public void eat(){\n        System.out.println(\"Orange\");\n    }\n}\nclass Factory {\n    public static Fruit getInstance(String ClassName) {\n        Fruit f=null;\n        try {\n            f=(Fruit)Class.forName(ClassName).newInstance();\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n        return f;\n    }\n}\nclass Client {\n    public static void main(String[] a) {\n        Fruit f=Factory.getInstance(\"io.github.dunwu.spring.Apple\");\n        if(f!=null){\n            f.eat();\n        }\n    }\n}\n</code></pre>\n\n<h2>3. Beans</h2>\n\n<h3>3.1. 什么是 spring bean？</h3>\n\n<ul>\n	<li>\n	<p>它们是构成用户应用程序主干的对象。</p>\n	</li>\n	<li>\n	<p>Bean 由 Spring IoC 容器管理。</p>\n	</li>\n	<li>\n	<p>它们由 Spring IoC 容器实例化，配置，装配和管理。</p>\n	</li>\n	<li>\n	<p>Bean 是基于用户提供给容器的配置元数据创建。</p>\n	</li>\n</ul>\n\n<h3>3.2. spring 提供了哪些配置方式？</h3>\n\n<ul>\n	<li>\n	<p>基于 xml 配置</p>\n	</li>\n</ul>\n\n<p>bean 所需的依赖项和服务在 XML 格式的配置文件中指定。这些配置文件通常包含许多 bean 定义和特定于应用程序的配置选项。它们通常以 bean 标签开头。例如：</p>\n\n<pre>\n<code>&lt;bean id=\"studentbean\" class=\"org.edureka.firstSpring.StudentBean\"&gt;\n &lt;property name=\"name\" value=\"Edureka\"&gt;&lt;/property&gt;\n&lt;/bean&gt;\n</code></pre>\n\n<ul>\n	<li>\n	<p>基于注解配置</p>\n	</li>\n</ul>\n\n<p>您可以通过在相关的类，方法或字段声明上使用注解，将 bean 配置为组件类本身，而不是使用 XML 来描述 bean 装配。默认情况下，Spring 容器中未打开注解装配。因此，您需要在使用它之前在 Spring 配置文件中启用它。例如：</p>\n\n<pre>\n<code>&lt;beans&gt;\n&lt;context:annotation-config/&gt;\n&lt;!-- bean definitions go here --&gt;\n&lt;/beans&gt;\n</code></pre>\n\n<ul>\n	<li>\n	<p>基于 Java API 配置</p>\n	</li>\n</ul>\n\n<p>Spring 的 Java 配置是通过使用 @Bean 和 @Configuration 来实现。</p>\n\n<ol>\n	<li>\n	<p>@Bean 注解扮演与&nbsp;<code>&lt;bean /&gt;</code>&nbsp;元素相同的角色。</p>\n	</li>\n	<li>\n	<p>@Configuration 类允许通过简单地调用同一个类中的其他 @Bean 方法来定义 bean 间依赖关系。</p>\n	</li>\n</ol>\n\n<p>例如：</p>\n\n<pre>\n<code>@Configuration\npublic class StudentConfig {\n    @Bean\n    public StudentBean myStudent() {\n        return new StudentBean();\n    }\n}\n</code></pre>\n\n<h3>3.3. spring 支持集中 bean scope？</h3>\n\n<p>Spring bean 支持 5 种 scope：</p>\n\n<ul>\n	<li>\n	<p><strong>Singleton</strong>&nbsp;- 每个 Spring IoC 容器仅有一个单实例。</p>\n	</li>\n	<li>\n	<p><strong>Prototype</strong>&nbsp;- 每次请求都会产生一个新的实例。</p>\n	</li>\n	<li>\n	<p><strong>Request</strong>&nbsp;- 每一次 HTTP 请求都会产生一个新的实例，并且该 bean 仅在当前 HTTP 请求内有效。</p>\n	</li>\n	<li>\n	<p><strong>Session</strong>&nbsp;- 每一次 HTTP 请求都会产生一个新的 bean，同时该 bean 仅在当前 HTTP session 内有效。</p>\n	</li>\n	<li>\n	<p><strong>Global-session</strong>&nbsp;- 类似于标准的 HTTP Session 作用域，不过它仅仅在基于 portlet 的 web 应用中才有意义。Portlet 规范定义了全局 Session 的概念，它被所有构成某个 portlet web 应用的各种不同的 portlet 所共享。在 global session 作用域中定义的 bean 被限定于全局 portlet Session 的生命周期范围内。如果你在 web 中使用 global session 作用域来标识 bean，那么 web 会自动当成 session 类型来使用。</p>\n	</li>\n</ul>\n\n<p>仅当用户使用支持 Web 的 ApplicationContext 时，最后三个才可用。</p>\n\n<h3>3.4. spring bean 容器的生命周期是什么样的？</h3>\n\n<p>spring bean 容器的生命周期流程如下：</p>\n\n<ol>\n	<li>\n	<p>Spring 容器根据配置中的 bean 定义中实例化 bean。</p>\n	</li>\n	<li>\n	<p>Spring 使用依赖注入填充所有属性，如 bean 中所定义的配置。</p>\n	</li>\n	<li>\n	<p>如果 bean 实现 BeanNameAware 接口，则工厂通过传递 bean 的 ID 来调用 setBeanName()。</p>\n	</li>\n	<li>\n	<p>如果 bean 实现 BeanFactoryAware 接口，工厂通过传递自身的实例来调用 setBeanFactory()。</p>\n	</li>\n	<li>\n	<p>如果存在与 bean 关联的任何 BeanPostProcessors，则调用 preProcessBeforeInitialization() 方法。</p>\n	</li>\n	<li>\n	<p>如果为 bean 指定了 init 方法（<code>&lt;bean&gt;</code>&nbsp;的 init-method 属性），那么将调用它。</p>\n	</li>\n	<li>\n	<p>最后，如果存在与 bean 关联的任何 BeanPostProcessors，则将调用 postProcessAfterInitialization() 方法。</p>\n	</li>\n	<li>\n	<p>如果 bean 实现 DisposableBean 接口，当 spring 容器关闭时，会调用 destory()。</p>\n	</li>\n	<li>\n	<p>如果为 bean 指定了 destroy 方法（<code>&lt;bean&gt;</code>&nbsp;的 destroy-method 属性），那么将调用它。</p>\n	</li>\n</ol>\n\n<p><img src=\"data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==\" /><img alt=\"\" src=\"http://localhost:8080/myLog/images/spring/bean生命周期\" style=\"height:219px; width:500px\" /></p>\n\n<h3>3.5. 什么是 spring 的内部 bean？</h3>\n\n<p>只有将 bean 用作另一个 bean 的属性时，才能将 bean 声明为内部 bean。为了定义 bean，Spring 的基于 XML 的配置元数据在&nbsp;<code>&lt;property&gt;</code>&nbsp;或&nbsp;<code>&lt;constructor-arg&gt;</code>&nbsp;中提供了&nbsp;<code>&lt;bean&gt;</code>&nbsp;元素的使用。内部 bean 总是匿名的，它们总是作为原型。</p>\n\n<p>例如，假设我们有一个 Student 类，其中引用了 Person 类。这里我们将只创建一个 Person 类实例并在 Student 中使用它。</p>\n\n<p>Student.java</p>\n\n<pre>\n<code>public class Student {\n    private Person person;\n    //Setters and Getters\n}\npublic class Person {\n    private String name;\n    private String address;\n    //Setters and Getters\n}\n</code></pre>\n\n<p>bean.xml</p>\n\n<pre>\n<code>&lt;bean id=“StudentBean\" class=\"com.edureka.Student\"&gt;\n    &lt;property name=\"person\"&gt;\n        &lt;!--This is inner bean --&gt;\n        &lt;bean class=\"com.edureka.Person\"&gt;\n            &lt;property name=\"name\" value=“Scott\"&gt;&lt;/property&gt;\n            &lt;property name=\"address\" value=“Bangalore\"&gt;&lt;/property&gt;\n        &lt;/bean&gt;\n    &lt;/property&gt;\n&lt;/bean&gt;\n</code></pre>\n\n<h3>3.6. 什么是 spring 装配</h3>\n\n<p>当 bean 在 Spring 容器中组合在一起时，它被称为装配或 bean 装配。 Spring 容器需要知道需要什么 bean 以及容器应该如何使用依赖注入来将 bean 绑定在一起，同时装配 bean。</p>\n\n<h3>3.7. 自动装配有哪些方式？</h3>\n\n<p>Spring 容器能够自动装配 bean。也就是说，可以通过检查 BeanFactory 的内容让 Spring 自动解析 bean 的协作者。</p>\n\n<p>自动装配的不同模式：</p>\n\n<ul>\n	<li>\n	<p><strong>no</strong>&nbsp;- 这是默认设置，表示没有自动装配。应使用显式 bean 引用进行装配。</p>\n	</li>\n	<li>\n	<p><strong>byName</strong>&nbsp;- 它根据 bean 的名称注入对象依赖项。它匹配并装配其属性与 XML 文件中由相同名称定义的 bean。</p>\n	</li>\n	<li>\n	<p><strong>byType</strong>&nbsp;- 它根据类型注入对象依赖项。如果属性的类型与 XML 文件中的一个 bean 名称匹配，则匹配并装配属性。</p>\n	</li>\n	<li>\n	<p><strong>构造函数</strong>&nbsp;- 它通过调用类的构造函数来注入依赖项。它有大量的参数。</p>\n	</li>\n	<li>\n	<p><strong>autodetect</strong>&nbsp;- 首先容器尝试通过构造函数使用 autowire 装配，如果不能，则尝试通过 byType 自动装配。</p>\n	</li>\n</ul>\n\n<h3>3.8. 自动装配有什么局限？</h3>\n\n<ul>\n	<li>\n	<p>覆盖的可能性 - 您始终可以使用&nbsp;<code>&lt;constructor-arg&gt;</code>&nbsp;和&nbsp;<code>&lt;property&gt;</code>&nbsp;设置指定依赖项，这将覆盖自动装配。</p>\n	</li>\n	<li>\n	<p>基本元数据类型 - 简单属性（如原数据类型，字符串和类）无法自动装配。</p>\n	</li>\n	<li>\n	<p>令人困惑的性质 - 总是喜欢使用明确的装配，因为自动装配不太精确。</p>\n	</li>\n</ul>\n\n<h2>4. 注解</h2>\n\n<h3>4.1. 什么是基于注解的容器配置</h3>\n\n<p>不使用 XML 来描述 bean 装配，开发人员通过在相关的类，方法或字段声明上使用注解将配置移动到组件类本身。它可以作为 XML 设置的替代方案。例如：</p>\n\n<p>Spring 的 Java 配置是通过使用 @Bean 和 @Configuration 来实现。</p>\n\n<ul>\n	<li>\n	<p>@Bean 注解扮演与&nbsp;</p>\n	元素相同的角色。</li>\n	<li>\n	<p>@Configuration 类允许通过简单地调用同一个类中的其他 @Bean 方法来定义 bean 间依赖关系。</p>\n	</li>\n</ul>\n\n<p>例如：</p>\n\n<pre>\n<code>@Configuration\npublic class StudentConfig {\n    @Bean\n    public StudentBean myStudent() {\n        return new StudentBean();\n    }\n}\n</code></pre>\n\n<h3>4.2. 如何在 spring 中启动注解装配？</h3>\n\n<p>默认情况下，Spring 容器中未打开注解装配。因此，要使用基于注解装配，我们必须通过配置<code>&lt;context：annotation-config /&gt;</code>&nbsp;元素在 Spring 配置文件中启用它。</p>\n\n<h3>4.3. @Component, @Controller, @Repository, @Service 有何区别？</h3>\n\n<ul>\n	<li>\n	<p>@Component：这将 java 类标记为 bean。它是任何 Spring 管理组件的通用构造型。spring 的组件扫描机制现在可以将其拾取并将其拉入应用程序环境中。</p>\n	</li>\n	<li>\n	<p>@Controller：这将一个类标记为 Spring Web MVC 控制器。标有它的 Bean 会自动导入到 IoC 容器中。</p>\n	</li>\n	<li>\n	<p>@Service：此注解是组件注解的特化。它不会对 @Component 注解提供任何其他行为。您可以在服务层类中使用 @Service 而不是 @Component，因为它以更好的方式指定了意图。</p>\n	</li>\n	<li>\n	<p>@Repository：这个注解是具有类似用途和功能的 @Component 注解的特化。它为 DAO 提供了额外的好处。它将 DAO 导入 IoC 容器，并使未经检查的异常有资格转换为 Spring DataAccessException。</p>\n	</li>\n</ul>\n\n<h3>4.4. @Required 注解有什么用？</h3>\n\n<p>@Required 应用于 bean 属性 setter 方法。此注解仅指示必须在配置时使用 bean 定义中的显式属性值或使用自动装配填充受影响的 bean 属性。如果尚未填充受影响的 bean 属性，则容器将抛出 BeanInitializationException。</p>\n\n<p>示例：</p>\n\n<pre>\n<code>public class Employee {\n    private String name;\n    @Required\n    public void setName(String name){\n        this.name=name;\n    }\n    public string getName(){\n        return name;\n    }\n}\n</code></pre>\n\n<h3>4.5. @Autowired 注解有什么用？</h3>\n\n<p>@Autowired 可以更准确地控制应该在何处以及如何进行自动装配。此注解用于在 setter 方法，构造函数，具有任意名称或多个参数的属性或方法上自动装配 bean。默认情况下，它是类型驱动的注入。</p>\n\n<pre>\n<code>public class Employee {\n    private String name;\n    @Autowired\n    public void setName(String name) {\n        this.name=name;\n    }\n    public string getName(){\n        return name;\n    }\n}\n</code></pre>\n\n<h3>4.6. @Qualifier 注解有什么用？</h3>\n\n<p>当您创建多个相同类型的 bean 并希望仅使用属性装配其中一个 bean 时，您可以使用@Qualifier 注解和 @Autowired 通过指定应该装配哪个确切的 bean 来消除歧义。</p>\n\n<p>例如，这里我们分别有两个类，Employee 和 EmpAccount。在 EmpAccount 中，使用@Qualifier 指定了必须装配 id 为 emp1 的 bean。</p>\n\n<p>Employee.java</p>\n\n<pre>\n<code>public class Employee {\n    private String name;\n    @Autowired\n    public void setName(String name) {\n        this.name=name;\n    }\n    public string getName() {\n        return name;\n    }\n}\n</code></pre>\n\n<p>EmpAccount.java</p>\n\n<pre>\n<code>public class EmpAccount {\n    private Employee emp;\n\n    @Autowired\n    @Qualifier(emp1)\n    public void showName() {\n        System.out.println(“Employee name : ”+emp.getName);\n    }\n}\n</code></pre>\n\n<h3>4.7. @RequestMapping 注解有什么用？</h3>\n\n<p>@RequestMapping 注解用于将特定 HTTP 请求方法映射到将处理相应请求的控制器中的特定类/方法。此注释可应用于两个级别：</p>\n\n<ul>\n	<li>\n	<p>类级别：映射请求的 URL</p>\n	</li>\n	<li>\n	<p>方法级别：映射 URL 以及 HTTP 请求方法</p>\n	</li>\n</ul>\n\n<h2>5. 数据访问</h2>\n\n<h3>5.1. spring DAO 有什么用？</h3>\n\n<p>Spring DAO 使得 JDBC，Hibernate 或 JDO 这样的数据访问技术更容易以一种统一的方式工作。这使得用户容易在持久性技术之间切换。它还允许您在编写代码时，无需考虑捕获每种技术不同的异常。</p>\n\n<h3>5.2. 列举 Spring DAO 抛出的异常。</h3>\n\n<p><img src=\"data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==\" /><img alt=\"\" src=\"http://localhost:8080/myLog/images/spring/springDao\" style=\"height:305px; width:400px\" /></p>\n\n<h3>5.3. spring JDBC API 中存在哪些类？</h3>\n\n<ul>\n	<li>\n	<p>JdbcTemplate</p>\n	</li>\n	<li>\n	<p>SimpleJdbcTemplate</p>\n	</li>\n	<li>\n	<p>NamedParameterJdbcTemplate</p>\n	</li>\n	<li>\n	<p>SimpleJdbcInsert</p>\n	</li>\n	<li>\n	<p>SimpleJdbcCall</p>\n	</li>\n</ul>\n\n<h3>5.4. 使用 Spring 访问 Hibernate 的方法有哪些？</h3>\n\n<p>我们可以通过两种方式使用 Spring 访问 Hibernate：</p>\n\n<ol>\n	<li>\n	<p>使用 Hibernate 模板和回调进行控制反转</p>\n	</li>\n	<li>\n	<p>扩展 HibernateDAOSupport 并应用 AOP 拦截器节点</p>\n	</li>\n</ol>\n\n<h3>5.5. 列举 spring 支持的事务管理类型</h3>\n\n<p>Spring 支持两种类型的事务管理：</p>\n\n<ol>\n	<li>\n	<p>程序化事务管理：在此过程中，在编程的帮助下管理事务。它为您提供极大的灵活性，但维护起来非常困难。</p>\n	</li>\n	<li>\n	<p>声明式事务管理：在此，事务管理与业务代码分离。仅使用注解或基于 XML 的配置来管理事务。</p>\n	</li>\n</ol>\n\n<h3>5.6. spring 支持哪些 ORM 框架</h3>\n\n<ul>\n	<li>\n	<p>Hibernate</p>\n	</li>\n	<li>\n	<p>iBatis</p>\n	</li>\n	<li>\n	<p>JPA</p>\n	</li>\n	<li>\n	<p>JDO</p>\n	</li>\n	<li>\n	<p>OJB</p>\n	</li>\n</ul>\n\n<h2>6. AOP</h2>\n\n<h3>6.1. 什么是 AOP？</h3>\n\n<p>AOP(Aspect-Oriented Programming), 即&nbsp;<strong>面向切面编程</strong>, 它与 OOP( Object-Oriented Programming, 面向对象编程) 相辅相成, 提供了与 OOP 不同的抽象软件结构的视角.<br />\n在 OOP 中, 我们以类(class)作为我们的基本单元, 而 AOP 中的基本单元是&nbsp;<strong>Aspect(切面)</strong></p>\n\n<h3>6.2. 什么是 Aspect？</h3>\n\n<p><code>aspect</code>&nbsp;由&nbsp;<code>pointcount</code>&nbsp;和&nbsp;<code>advice</code>&nbsp;组成, 它既包含了横切逻辑的定义, 也包括了连接点的定义. Spring AOP 就是负责实施切面的框架, 它将切面所定义的横切逻辑编织到切面所指定的连接点中.<br />\nAOP 的工作重心在于如何将增强编织目标对象的连接点上, 这里包含两个工作:</p>\n\n<ol>\n	<li>\n	<p>如何通过 pointcut 和 advice 定位到特定的 joinpoint 上</p>\n	</li>\n	<li>\n	<p>如何在 advice 中编写切面代码.</p>\n	</li>\n</ol>\n\n<p><strong>可以简单地认为, 使用 @Aspect 注解的类就是切面.</strong></p>\n\n<p><img src=\"data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==\" /><img alt=\"\" src=\"http://localhost:8080/myLog/images/spring/aspect\" style=\"height:303px; width:400px\" /></p>\n\n<h3>6.3. 什么是切点（JoinPoint）</h3>\n\n<p>程序运行中的一些时间点, 例如一个方法的执行, 或者是一个异常的处理.</p>\n\n<p>在 Spring AOP 中, join point 总是方法的执行点。</p>\n\n<h3>6.4. 什么是通知（Advice）？</h3>\n\n<p>特定 JoinPoint 处的 Aspect 所采取的动作称为 Advice。Spring AOP 使用一个 Advice 作为拦截器，在 JoinPoint &ldquo;周围&rdquo;维护一系列的拦截器。</p>\n\n<h3>6.5. 有哪些类型的通知（Advice）？</h3>\n\n<ul>\n	<li>\n	<p><strong>Before</strong>&nbsp;- 这些类型的 Advice 在 joinpoint 方法之前执行，并使用 @Before 注解标记进行配置。</p>\n	</li>\n	<li>\n	<p><strong>After Returning</strong>&nbsp;- 这些类型的 Advice 在连接点方法正常执行后执行，并使用@AfterReturning 注解标记进行配置。</p>\n	</li>\n	<li>\n	<p><strong>After Throwing</strong>&nbsp;- 这些类型的 Advice 仅在 joinpoint 方法通过抛出异常退出并使用 @AfterThrowing 注解标记配置时执行。</p>\n	</li>\n	<li>\n	<p><strong>After (finally)</strong>&nbsp;- 这些类型的 Advice 在连接点方法之后执行，无论方法退出是正常还是异常返回，并使用 @After 注解标记进行配置。</p>\n	</li>\n	<li>\n	<p><strong>Around</strong>&nbsp;- 这些类型的 Advice 在连接点之前和之后执行，并使用 @Around 注解标记进行配置。</p>\n	</li>\n</ul>\n\n<h3>6.6. 指出在 spring aop 中 concern 和 cross-cutting concern 的不同之处。</h3>\n\n<p>concern 是我们想要在应用程序的特定模块中定义的行为。它可以定义为我们想要实现的功能。</p>\n\n<p>cross-cutting concern 是一个适用于整个应用的行为，这会影响整个应用程序。例如，日志记录，安全性和数据传输是应用程序几乎每个模块都需要关注的问题，因此它们是跨领域的问题。</p>\n\n<h3>6.7. AOP 有哪些实现方式？</h3>\n\n<p>实现 AOP 的技术，主要分为两大类：</p>\n\n<ul>\n	<li>\n	<p>静态代理 - 指使用 AOP 框架提供的命令进行编译，从而在编译阶段就可生成 AOP 代理类，因此也称为编译时增强；</p>\n\n	<ul>\n		<li>\n		<p>编译时编织（特殊编译器实现）</p>\n		</li>\n		<li>\n		<p>类加载时编织（特殊的类加载器实现）。</p>\n		</li>\n	</ul>\n	</li>\n	<li>\n	<p>动态代理 - 在运行时在内存中&ldquo;临时&rdquo;生成 AOP 动态代理类，因此也被称为运行时增强。</p>\n\n	<ul>\n		<li>\n		<p>JDK 动态代理</p>\n		</li>\n		<li>\n		<p>CGLIB</p>\n		</li>\n	</ul>\n	</li>\n</ul>\n\n<h3>6.8. Spring AOP and AspectJ AOP 有什么区别？</h3>\n\n<p>Spring AOP 基于动态代理方式实现；AspectJ 基于静态代理方式实现。<br />\nSpring AOP 仅支持方法级别的 PointCut；提供了完全的 AOP 支持，它还支持属性级别的 PointCut。</p>\n\n<h3>6.9. 如何理解 Spring 中的代理？</h3>\n\n<p>将 Advice 应用于目标对象后创建的对象称为代理。在客户端对象的情况下，目标对象和代理对象是相同的。</p>\n\n<pre>\n<code>Advice + Target Object = Proxy</code></pre>\n\n<h3>6.10. 什么是编织（Weaving）？</h3>\n\n<p>为了创建一个 advice 对象而链接一个 aspect 和其它应用类型或对象，称为编织（Weaving）。在 Spring AOP 中，编织在运行时执行。请参考下图：</p>\n\n<p><img src=\"data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==\" /><img alt=\"\" src=\"http://localhost:8080/myLog/images/spring/weaving\" style=\"height:157px; width:400px\" /></p>\n\n<h2>7. MVC</h2>\n\n<h3>7.1. Spring MVC 框架有什么用？</h3>\n\n<p>Spring Web MVC 框架提供&nbsp;<strong>模型-视图-控制器</strong>&nbsp;架构和随时可用的组件，用于开发灵活且松散耦合的 Web 应用程序。 MVC 模式有助于分离应用程序的不同方面，如输入逻辑，业务逻辑和 UI 逻辑，同时在所有这些元素之间提供松散耦合。</p>\n\n<h3>7.2. 描述一下 DispatcherServlet 的工作流程</h3>\n\n<p>DispatcherServlet 的工作流程可以用一幅图来说明：</p>\n\n<p><img src=\"http://localhost:8080/myLog/images/spring/dispatcher\" style=\"height:284px; width:500px\" /></p>\n\n<ol>\n	<li>\n	<p>向服务器发送 HTTP 请求，请求被前端控制器&nbsp;<code>DispatcherServlet</code>&nbsp;捕获。</p>\n	</li>\n	<li>\n	<p><code>DispatcherServlet</code>&nbsp;根据&nbsp;<strong>-servlet.xml</strong>&nbsp;中的配置对请求的 URL 进行解析，得到请求资源标识符（URI）。然后根据该 URI，调用&nbsp;<code>HandlerMapping</code>&nbsp;获得该 Handler 配置的所有相关的对象（包括 Handler 对象以及 Handler 对象对应的拦截器），最后以<code>HandlerExecutionChain</code>&nbsp;对象的形式返回。</p>\n	</li>\n	<li>\n	<p><code>DispatcherServlet</code>&nbsp;根据获得的<code>Handler</code>，选择一个合适的&nbsp;<code>HandlerAdapter</code>。（附注：如果成功获得<code>HandlerAdapter</code>后，此时将开始执行拦截器的 preHandler(...)方法）。</p>\n	</li>\n	<li>\n	<p>提取<code>Request</code>中的模型数据，填充<code>Handler</code>入参，开始执行<code>Handler</code>（<code>Controller</code>)。 在填充<code>Handler</code>的入参过程中，根据你的配置，Spring 将帮你做一些额外的工作：</p>\n\n	<ul>\n		<li>\n		<p>HttpMessageConveter： 将请求消息（如 Json、xml 等数据）转换成一个对象，将对象转换为指定的响应信息。</p>\n		</li>\n		<li>\n		<p>数据转换：对请求消息进行数据转换。如<code>String</code>转换成<code>Integer</code>、<code>Double</code>等。</p>\n		</li>\n		<li>\n		<p>数据根式化：对请求消息进行数据格式化。 如将字符串转换成格式化数字或格式化日期等。</p>\n		</li>\n		<li>\n		<p>数据验证： 验证数据的有效性（长度、格式等），验证结果存储到<code>BindingResult</code>或<code>Error</code>中。</p>\n		</li>\n	</ul>\n	</li>\n	<li>\n	<p>Handler(Controller)执行完成后，向&nbsp;<code>DispatcherServlet</code>&nbsp;返回一个&nbsp;<code>ModelAndView</code>&nbsp;对象；</p>\n	</li>\n	<li>\n	<p>根据返回的<code>ModelAndView</code>，选择一个适合的&nbsp;<code>ViewResolver</code>（必须是已经注册到 Spring 容器中的<code>ViewResolver</code>)返回给<code>DispatcherServlet</code>。</p>\n	</li>\n	<li>\n	<p><code>ViewResolver</code>&nbsp;结合<code>Model</code>和<code>View</code>，来渲染视图。</p>\n	</li>\n	<li>\n	<p>视图负责将渲染结果返回给客户端。</p>\n	</li>\n</ol>\n\n<h3>7.3. 介绍一下 WebApplicationContext</h3>\n\n<p>WebApplicationContext 是 ApplicationContext 的扩展。它具有 Web 应用程序所需的一些额外功能。它与普通的 ApplicationContext 在解析主题和决定与哪个 servlet 关联的能力方面有所不同。</p>', 1, '5f199b8885e24fc8b28672b872edb606', 1, '2018-09-16 22:27:24', '2018-09-16 22:33:56');
INSERT INTO `logcontent` VALUES ('5e19d147430d43229c321d65bd02dbe5', -1, '3ba9a3aed3ce48f682974ebcf3552ce2', 'cmd命令执行java的main方法', 'main,java,cmd', 'main/cmd', '<p><a id=\"cmd执行main方法\" name=\"cmd执行main方法\"></a>使用cmd命令执行Java中的main方法</p>\n\n<p>结构：java -cp jar路径 main方法路径</p>\n\n<p>注意：结构中的空格不要漏，main方法路径为class文件路径并非java文件路径</p>\n\n<p><a id=\"bat脚本\" name=\"bat脚本\"></a>1.bat脚本（非必需）</p>\n\n<pre>\n<code class=\"language-bash\">cd C:\\c\\song\\bin\njava -cp ..\\resources\\lib\\*; com.cjp.threads.Test canshu1 canshu2\npause</code></pre>\n\n<p><a id=\"main方法接传参\" name=\"main方法接传参\"></a>2.main方法传参，接受参数</p>\n\n<pre>\n<code class=\"language-java\">	public static void main(String[] args)  {		\n		if(args!=null&amp;&amp;args.length&gt;0){\n			for (int i = 0; i &lt; args.length; i++) {\n				System.out.println(args[i]);\n			}\n		}\n	}</code></pre>\n\n<p>&nbsp;</p>\n', 1, '5f199b8885e24fc8b28672b872edb606', 5, '2017-07-25 15:19:14', '2018-09-08 20:10:27');
INSERT INTO `logcontent` VALUES ('5e4229f43cb34cdf9018164f10309992', 1, '4eca926aa543420baea2f03f506d042e', 'MongoDb简单语句', 'mongodb,复合查询,更新修改,聚合', '语句简单记录', '<p><a id=\"MongoDB\" name=\"MongoDB\"></a>MongoDB</p>\n\n<pre>\n<code class=\"language-sql\">创建数据库\nuse mydb\n删除数据库\ndb.dropDatabase()\n查看所有databases\nshow dbs\n查看数据库所有Collection\nshow collections\n删除Collection\ndb.esdata.drop()\n增加\ndb.esdata.insert({\"name\":\"xc\"})\n删除\ndb.esdata.remove({\"name\":\"xc\"})\n查询\ndb.esdata.findOne({})查询一条\ndb.esdata.find({})查询所有\ndb.esdata.find({}).count()查询总数\ndb.esdata.find({\'state\':\'0\'})查询state为\'0\'的值\ndb.esdata.find({\'state\':{\'$ne\':\'0\'}})查询state不为\'0\'的值{$ne(≠)$gt(&gt;)$lt(&lt;)$gte(≥)$lte(≤)}\ndb.esdata.find({\'state\':{$in: [\'0\',\'1\',\'2\']}});查询state在[\'0\',\'1\',\'2\']中的值{not in}\ndb.esdata.find({\'$and\':[{\'demin\':\'zxgg\'},{\'state\':\'0\'}]});查询demin为\'zxgg\'并且state为\'0\'的值{$or}\ndb.esdata.find({}).sort({\'createTime\':1}).skip(0).limit(100)分页：按照createTime正序查询跳过0条查询100条{(1正序-1反序)}\n如果value类型为ObjectId（\"_id\" : ObjectId(\"59363cbaa795a51a74559eeb\"),）那么查询时也要采用ObjectId进行查询\ndb.esdata.find({\'_id\':ObjectId(\"59363cbaa795a51a74559eeb\")})\n\n更新\n格式：db.esdata.update(criteria,objNew,upsert,multi)\n\n参数说明：\ncriteria：查询条件\nobjNew：update对象和一些更新操作符\nupsert：如果不存在update的记录，是否插入objNew这个新的文档，true为插入，默认为false，不插入。\nmulti：默认是false，只更新找到的第一条记录。如果为true，把按条件查询出来的记录全部更新。\ndb.esdata.update({\'state\':\'2\'},{$set:{\'state\':\'0\'}},false,true)更新所有state为\'2\'的为state为\'0\'\n\n（3）去重\n作用：获取集合中指定字段的不重复值，并以数组的形式返回\ndb.esdata.distinct(field,query,options)\n\n参数说明：\nfield -----指定要返回的字段(string)\nquery-----条件查询(document)\noptions-----其他的选项(document)\n\n（4）分组\ndb.HI_CRAWLER_ERR.aggregate(\n	{$match:{\"menuId\":\"596ebd8aacd75d19f09d1b8f\"}},\n	{$group:{_id:\'$menuName\',count:{$sum:1}}}\n	,{$sort:{count:-1}}\n	)\n\n（5）MongoDB运行状态、性能监控，分析\n获取当前数据库的信息，比如Obj总数、数据库总大小、平均Obj大小等\ndb.stat()\n\n获取服务器的状态（connections 当前连接和可用连接数，indexCounters:btree:misses 索引的不命中数，和hits的比例高就要考虑索引是否正确建立，其他的都能自解释，也不是查看mongo健康状况的关键）\ndb.serverStatus()\n\n在一台繁忙的机器或者有比较慢的命令时，你可以获取当前正在执行的操作\ndb.currentOp()\n在没有负载的机器上，该命令基本上都是返回空的：\n{ \"inprog\" : [ ] }\n有负载的机器上得到的返回值样例：\n{ \"opid\" : \"shard3:466404288\", \"active\" : false, \"waitingForLock\" : false, \"op\" : \"query\", \"ns\" : \"sd.usersEmails\", \"query\" : { }, \"client_s\" : \"10.121.13.8:34473\", \"desc\" : \"conn\" }\n如果你发现一个操作太长，把数据库卡死的话，可以用这个命令杀死他\ndb.killOp(\"shard3:466404288\")</code></pre>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>', 1, '5f199b8885e24fc8b28672b872edb606', 68, '2017-07-11 15:38:41', '2018-09-04 18:23:19');
INSERT INTO `logcontent` VALUES ('6079b70a349f42aa89ebbd98c9b6c1ed', 1, 'd7caeba238f0466d87db109b2b9724da', '最邻近规则分类KNN算法应用笔记', 'KNN算法实现', '初学', '<p><a id=\"Python环境\" name=\"Python环境\"></a>Python环境</p>\n\n<pre>\n<code>Windows版 Python 3.6.2</code></pre>\n\n<p><a id=\"Python代码实现\" name=\"Python代码实现\"></a>KNN算法Python代码实现</p>\n\n<pre>\n<code>from sklearn import neighbors\nfrom sklearn import datasets\n\nknn = neighbors.KNeighborsClassifier()\n\niris = datasets.load_iris()\n#打印数据集\nprint(iris)\n#建立模型\nknn.fit(iris.data, iris.target)\n##拿出第一条数据测试结果\npredictedLabel = knn.predict([[0.1, 0.2, 0.3, 0.4]])\nprint(\"predictedLabel is :\",predictedLabel)\n#输出结果\nprint(predictedLabel)</code></pre>\n\n<p>&nbsp;</p>\n', 1, '5f199b8885e24fc8b28672b872edb606', 14, '2017-10-29 20:33:45', '2018-08-31 22:21:23');
INSERT INTO `logcontent` VALUES ('69611cf206e84d3a9098af51e7f20044', 1, 'd7caeba238f0466d87db109b2b9724da', '中文自然语言处理', '20', '文章转载，联系:18210650520@163.com', '<h2><a id=\"第01课：中文自然语言处理的完整机器处理流程\" name=\"第01课：中文自然语言处理的完整机器处理流程\"></a>第01课：中文自然语言处理的完整机器处理流程</h2>\n\n<p>2016年全球瞩目的围棋大战中，人类以失败告终，更是激起了各种&ldquo;机器超越、控制人类&rdquo;的讨论，然而机器真的懂人类吗？机器能感受到人类的情绪吗？机器能理解人类的语言吗？如果能，那它又是如何做到呢？带着这样好奇心，本文将带领大家熟悉和回顾一个完整的自然语言处理过程，后续所有章节所有示例开发都将遵从这个处理过程。</p>\n\n<p>首先我们通过一张图（来源：网络）来了解 NLP 所包含的技术知识点，这张图从分析对象和分析内容两个不同的维度来进行表达，个人觉得内容只能作为参考，对于整个 AI 背景下的自然语言处理来说还不够完整。</p>\n\n<p><img alt=\"enter image description here\" src=\"http://images.gitbook.cn/b5701e90-5f4b-11e8-8a60-1bdde4cc4659\" style=\"height:1620px; width:400px\" /></p>\n\n<p>有机器学习相关经验的人都知道，中文自然语言处理的过程和机器学习过程大体一致，但又存在很多细节上的不同点，下面我们就来看看中文自然语言处理的基本过程有哪些呢？</p>\n\n<h3>获取语料</h3>\n\n<p>语料，即语言材料。语料是语言学研究的内容。语料是构成语料库的基本单元。所以，人们简单地用文本作为替代，并把文本中的上下文关系作为现实世界中语言的上下文关系的替代品。我们把一个文本集合称为语料库（Corpus），当有几个这样的文本集合的时候，我们称之为语料库集合(Corpora)。（定义来源：百度百科）按语料来源，我们将语料分为以下两种：</p>\n\n<p><strong>1.已有语料</strong></p>\n\n<p>很多业务部门、公司等组织随着业务发展都会积累有大量的纸质或者电子文本资料。那么，对于这些资料，在允许的条件下我们稍加整合，把纸质的文本全部电子化就可以作为我们的语料库。</p>\n\n<p><strong>2.网上下载、抓取语料</strong></p>\n\n<p>如果现在个人手里没有数据怎么办呢？这个时候，我们可以选择获取国内外标准开放数据集，比如国内的<strong>中文汉语有搜狗语料</strong>、<strong>人民日报语料</strong>。国外的因为大都是英文或者外文，这里暂时用不到。也可以选择通过爬虫自己去抓取一些数据，然后来进行后续内容。</p>\n\n<h3>语料预处理</h3>\n\n<p>这里重点介绍一下语料的预处理，在一个完整的中文自然语言处理工程应用中，语料预处理大概会占到整个50%-70%的工作量，所以开发人员大部分时间就在进行语料预处理。下面通过数据洗清、分词、词性标注、去停用词四个大的方面来完成语料的预处理工作。</p>\n\n<p><strong>1.语料清洗</strong></p>\n\n<p>数据清洗，顾名思义就是在语料中找到我们感兴趣的东西，把不感兴趣的、视为噪音的内容清洗删除，包括对于原始文本提取标题、摘要、正文等信息，对于爬取的网页内容，去除广告、标签、HTML、JS 等代码和注释等。常见的数据清洗方式有：人工去重、对齐、删除和标注等，或者规则提取内容、正则表达式匹配、根据词性和命名实体提取、编写脚本或者代码批处理等。</p>\n\n<p><strong>2.分词</strong></p>\n\n<p>中文语料数据为一批短文本或者长文本，比如：句子，文章摘要，段落或者整篇文章组成的一个集合。一般句子、段落之间的字、词语是连续的，有一定含义。而进行文本挖掘分析时，我们希望文本处理的最小单位粒度是词或者词语，所以这个时候就需要分词来将文本全部进行分词。</p>\n\n<p>常见的分词算法有：基于字符串匹配的分词方法、基于理解的分词方法、基于统计的分词方法和基于规则的分词方法，每种方法下面对应许多具体的算法。</p>\n\n<p>当前中文分词算法的主要难点有歧义识别和新词识别，比如：&ldquo;羽毛球拍卖完了&rdquo;，这个可以切分成&ldquo;羽毛 球拍 卖 完 了&rdquo;，也可切分成&ldquo;羽毛球 拍卖 完 了&rdquo;，如果不依赖上下文其他的句子，恐怕很难知道如何去理解。</p>\n\n<p><strong>3.词性标注</strong></p>\n\n<p>词性标注，就是给每个词或者词语打词类标签，如形容词、动词、名词等。这样做可以让文本在后面的处理中融入更多有用的语言信息。词性标注是一个经典的序列标注问题，不过对于有些中文自然语言处理来说，词性标注不是非必需的。比如，常见的文本分类就不用关心词性问题，但是类似情感分析、知识推理却是需要的，下图是常见的中文词性整理。</p>\n\n<p><img alt=\"enter image description here\" src=\"http://images.gitbook.cn/0fdc1ad0-5fc1-11e8-a59f-c7ac04233ce1\" /></p>\n\n<p>常见的词性标注方法可以分为基于规则和基于统计的方法。其中基于统计的方法，如基于最大熵的词性标注、基于统计最大概率输出词性和基于 HMM 的词性标注。</p>\n\n<p><strong>4.去停用词</strong></p>\n\n<p>停用词一般指对文本特征没有任何贡献作用的字词，比如标点符号、语气、人称等一些词。所以在一般性的文本处理中，分词之后，接下来一步就是去停用词。但是对于中文来说，去停用词操作不是一成不变的，停用词词典是根据具体场景来决定的，比如在情感分析中，语气词、感叹号是应该保留的，因为他们对表示语气程度、感情色彩有一定的贡献和意义。</p>\n\n<h3>特征工程</h3>\n\n<p>做完语料预处理之后，接下来需要考虑如何把分词之后的字和词语表示成计算机能够计算的类型。显然，如果要计算我们至少需要把中文分词的字符串转换成数字，确切的说应该是数学中的向量。有两种常用的表示模型分别是词袋模型和词向量。</p>\n\n<p>词袋模型（Bag of Word, BOW)，即不考虑词语原本在句子中的顺序，直接将每一个词语或者符号统一放置在一个集合（如 list），然后按照计数的方式对出现的次数进行统计。统计词频这只是最基本的方式，TF-IDF 是词袋模型的一个经典用法。</p>\n\n<p>词向量是将字、词语转换成向量矩阵的计算模型。目前为止最常用的词表示方法是 One-hot，这种方法把每个词表示为一个很长的向量。这个向量的维度是词表大小，其中绝大多数元素为 0，只有一个维度的值为 1，这个维度就代表了当前的词。还有 Google 团队的 Word2Vec，其主要包含两个模型：跳字模型（Skip-Gram）和连续词袋模型（Continuous Bag of Words，简称 CBOW），以及两种高效训练的方法：负采样（Negative Sampling）和层序 Softmax（Hierarchical Softmax）。值得一提的是，Word2Vec 词向量可以较好地表达不同词之间的相似和类比关系。除此之外，还有一些词向量的表示方式，如 Doc2Vec、WordRank 和 FastText 等。</p>\n\n<h3>特征选择</h3>\n\n<p>同数据挖掘一样，在文本挖掘相关问题中，特征工程也是必不可少的。在一个实际问题中，构造好的特征向量，是要选择合适的、表达能力强的特征。文本特征一般都是词语，具有语义信息，使用特征选择能够找出一个特征子集，其仍然可以保留语义信息；但通过特征提取找到的特征子空间，将会丢失部分语义信息。所以特征选择是一个很有挑战的过程，更多的依赖于经验和专业知识，并且有很多现成的算法来进行特征的选择。目前，常见的特征选择方法主要有 DF、 MI、 IG、 CHI、WLLR、WFO 六种。</p>\n\n<h3>模型训练</h3>\n\n<p>在特征向量选择好之后，接下来要做的事情当然就是训练模型，对于不同的应用需求，我们使用不同的模型，传统的有监督和无监督等机器学习模型， 如 KNN、SVM、Naive Bayes、决策树、GBDT、K-means 等模型；深度学习模型比如 CNN、RNN、LSTM、 Seq2Seq、FastText、TextCNN 等。这些模型在后续的分类、聚类、神经序列、情感分析等示例中都会用到，这里不再赘述。下面是在模型训练时需要注意的几个点。</p>\n\n<p><strong>1.注意过拟合、欠拟合问题，不断提高模型的泛化能力。</strong></p>\n\n<p><strong>过拟合</strong>：模型学习能力太强，以至于把噪声数据的特征也学习到了，导致模型泛化能力下降，在训练集上表现很好，但是在测试集上表现很差。</p>\n\n<p>常见的解决方法有：</p>\n\n<ul>\n	<li>增大数据的训练量；</li>\n	<li>增加正则化项，如 L1 正则和 L2 正则；</li>\n	<li>特征选取不合理，人工筛选特征和使用特征选择算法；</li>\n	<li>采用 Dropout 方法等。</li>\n</ul>\n\n<p><strong>欠拟合</strong>：就是模型不能够很好地拟合数据，表现在模型过于简单。</p>\n\n<p>常见的解决方法有：</p>\n\n<ul>\n	<li>添加其他特征项；</li>\n	<li>增加模型复杂度，比如神经网络加更多的层、线性模型通过添加多项式使模型泛化能力更强；</li>\n	<li>减少正则化参数，正则化的目的是用来防止过拟合的，但是现在模型出现了欠拟合，则需要减少正则化参数。</li>\n</ul>\n\n<p><strong>2.对于神经网络，注意梯度消失和梯度爆炸问题。</strong></p>\n\n<h3>评价指标</h3>\n\n<p>训练好的模型，上线之前要对模型进行必要的评估，目的让模型对语料具备较好的泛化能力。具体有以下这些指标可以参考。</p>\n\n<p><strong>1.错误率、精度、准确率、精确度、召回率、F1 衡量。</strong></p>\n\n<p><strong>错误率</strong>：是分类错误的样本数占样本总数的比例。对样例集 D，分类错误率计算公式如下：</p>\n\n<p><img alt=\"enter image description here\" src=\"http://images.gitbook.cn/03e7a210-5fc8-11e8-b864-0bd1f4b74dfb\" /></p>\n\n<p><strong>精度</strong>：是分类正确的样本数占样本总数的比例。这里的分类正确的样本数指的不仅是正例分类正确的个数还有反例分类正确的个数。对样例集 D，精度计算公式如下：</p>\n\n<p><img alt=\"enter image description here\" src=\"http://images.gitbook.cn/270be8f0-5fc8-11e8-b864-0bd1f4b74dfb\" /></p>\n\n<p>对于二分类问题，可将样例根据其真实类别与学习器预测类别的组合划分为真正例（True Positive）、假正例（False Positive）、真反例（True Negative)、假反例（False Negative）四种情形，令 TP、FP、TN、FN 分别表示其对应的样例数，则显然有 TP+FP++TN+FN=样例总数。分类结果的&ldquo;混淆矩阵&rdquo;（Confusion Matrix）如下：</p>\n\n<p><img alt=\"enter image description here\" src=\"http://images.gitbook.cn/96e35960-5fc8-11e8-a59f-c7ac04233ce1\" /></p>\n\n<p><strong>准确率</strong>，缩写表示用 P。准确率是针对我们预测结果而言的，它表示的是预测为正的样例中有多少是真正的正样例。定义公式如下：</p>\n\n<p><img alt=\"enter image description here\" src=\"http://images.gitbook.cn/2448b4d0-5fc9-11e8-8a60-1bdde4cc4659\" /></p>\n\n<p><strong>精确度</strong>，缩写表示用 A。精确度则是分类正确的样本数占样本总数的比例。Accuracy 反应了分类器对整个样本的判定能力（即能将正的判定为正的，负的判定为负的）。定义公式如下：</p>\n\n<p><img alt=\"enter image description here\" src=\"http://images.gitbook.cn/2bc44b70-5fc9-11e8-a59f-c7ac04233ce1\" /></p>\n\n<p><strong>召回率</strong>，缩写表示用 R。召回率是针对我们原来的样本而言的，它表示的是样本中的正例有多少被预测正确。定义公式如下：</p>\n\n<p><img alt=\"enter image description here\" src=\"http://images.gitbook.cn/320a2130-5fc9-11e8-b864-0bd1f4b74dfb\" /></p>\n\n<p><strong>F1 衡量</strong>，表达出对查准率/查全率的不同偏好。定义公式如下：</p>\n\n<p><img alt=\"enter image description here\" src=\"http://images.gitbook.cn/b74ad4c0-5fc9-11e8-a59f-c7ac04233ce1\" /></p>\n\n<p><strong>2.ROC 曲线、AUC 曲线。</strong></p>\n\n<p>ROC 全称是&ldquo;受试者工作特征&rdquo;（Receiver Operating Characteristic）曲线。我们根据模型的预测结果，把阈值从0变到最大，即刚开始是把每个样本作为正例进行预测，随着阈值的增大，学习器预测正样例数越来越少，直到最后没有一个样本是正样例。在这一过程中，每次计算出两个重要量的值，分别以它们为横、纵坐标作图，就得到了 ROC 曲线。</p>\n\n<p>ROC 曲线的纵轴是&ldquo;真正例率&rdquo;（True Positive Rate, 简称 TPR)，横轴是&ldquo;假正例率&rdquo;（False Positive Rate,简称FPR），两者分别定义为：</p>\n\n<p><img alt=\"enter image description here\" src=\"http://images.gitbook.cn/20696b90-63aa-11e8-b82b-ffbb9d1e8856\" /></p>\n\n<p><img alt=\"enter image description here\" src=\"http://images.gitbook.cn/f6e06a60-63ab-11e8-b82b-ffbb9d1e8856\" /></p>\n\n<p><strong>ROC 曲线的意义有以下几点：</strong></p>\n\n<ol>\n	<li>ROC 曲线能很容易的查出任意阈值对模型的泛化性能影响；</li>\n	<li>有助于选择最佳的阈值；</li>\n	<li>可以对不同的模型比较性能，在同一坐标中，靠近左上角的 ROC 曲所代表的学习器准确性最高。</li>\n</ol>\n\n<p>如果两条 ROC 曲线没有相交，我们可以根据哪条曲线最靠近左上角哪条曲线代表的学习器性能就最好。但是实际任务中，情况很复杂，若两个模型的 ROC 曲线发生交叉，则难以一般性的断言两者孰优孰劣。此时如果一定要进行比较，则比较合理的判断依据是比较 ROC 曲线下的面积，即AUC（Area Under ROC Curve）。</p>\n\n<p>AUC 就是 ROC 曲线下的面积，衡量学习器优劣的一种性能指标。AUC 是衡量二分类模型优劣的一种评价指标，表示预测的正例排在负例前面的概率。</p>\n\n<p>前面我们所讲的都是针对二分类问题，那么如果实际需要在多分类问题中用 ROC 曲线的话，一般性的转化为多个&ldquo;一对多&rdquo;的问题。即把其中一个当作正例，其余当作负例来看待，画出多个 ROC 曲线。</p>\n\n<h3>模型上线应用</h3>\n\n<p>模型线上应用，目前主流的应用方式就是提供服务或者将模型持久化。</p>\n\n<p>第一就是线下训练模型，然后将模型做线上部署，发布成接口服务以供业务系统使用。</p>\n\n<p>第二种就是在线训练，在线训练完成之后把模型 pickle 持久化，然后在线服务接口模板通过读取 pickle 而改变接口服务。</p>\n\n<h3>模型重构（非必须）</h3>\n\n<p>随着时间和变化，可能需要对模型做一定的重构，包括根据业务不同侧重点对上面提到的一至七步骤也进行调整，重新训练模型进行上线。</p>\n\n<h3>参考文献</h3>\n\n<ol>\n	<li>周志华《机器学习》</li>\n	<li>李航《统计学习方法》</li>\n	<li>伊恩&middot;古德费洛《深度学习》</li>\n</ol>\n\n<h2><a id=\"第02课：简单好用的中文分词利器 jieba 和 HanLP\" name=\"第02课：简单好用的中文分词利器 jieba 和 HanLP\"></a>第02课：简单好用的中文分词利器 jieba 和 HanLP</h2>\n\n<h3>前言</h3>\n\n<p>从本文开始，我们就要真正进入实战部分。首先，我们按照中文自然语言处理流程的第一步获取语料，然后重点进行中文分词的学习。中文分词有很多种，常见的比如有中科院计算所 NLPIR、哈工大 LTP、清华大学 THULAC 、斯坦福分词器、Hanlp 分词器、jieba 分词、IKAnalyzer 等。这里针对 jieba 和 HanLP 分别介绍不同场景下的中文分词应用。</p>\n\n<h3>jieba 分词</h3>\n\n<h4>jieba 安装</h4>\n\n<p>（1）Python 2.x 下 jieba 的三种安装方式，如下：</p>\n\n<ul>\n	<li>\n	<p><strong>全自动安装</strong>：执行命令&nbsp;<code>easy_install jieba</code>&nbsp;或者&nbsp;<code>pip install jieba</code>&nbsp;/<code>pip3 install jieba</code>，可实现全自动安装。</p>\n	</li>\n	<li>\n	<p><strong>半自动安装</strong>：先<a href=\"https://pypi.python.org/pypi/jieba/\">下载 jieba</a>，解压后运行&nbsp;<code>python setup.py install</code>。</p>\n	</li>\n	<li>\n	<p><strong>手动安装</strong>：将 jieba 目录放置于当前目录或者 site-packages 目录。</p>\n	</li>\n</ul>\n\n<p>安装完通过&nbsp;<code>import jieba</code>&nbsp;验证安装成功与否。</p>\n\n<p>（2）Python 3.x 下的安装方式。</p>\n\n<p>Github 上 jieba 的 Python3.x 版本的路径是：https://github.com/fxsjy/jieba/tree/jieba3k。</p>\n\n<p>通过&nbsp;<code>git clone https://github.com/fxsjy/jieba.git</code>&nbsp;命令下载到本地，然后解压，再通过命令行进入解压目录，执行&nbsp;<code>python setup.py install</code>&nbsp;命令，即可安装成功。</p>\n\n<h4>jieba 的分词算法</h4>\n\n<p>主要有以下三种：</p>\n\n<ol>\n	<li>基于统计词典，构造前缀词典，基于前缀词典对句子进行切分，得到所有切分可能，根据切分位置，构造一个有向无环图（DAG）；</li>\n	<li>基于DAG图，采用动态规划计算最大概率路径（最有可能的分词结果），根据最大概率路径分词；</li>\n	<li>对于新词(词库中没有的词），采用有汉字成词能力的 HMM 模型进行切分。</li>\n</ol>\n\n<h4>jieba 分词</h4>\n\n<p>下面我们进行 jieba 分词练习，第一步首先引入 jieba 和语料:</p>\n\n<pre>\n<code>import jieba\ncontent = \"现如今，机器学习和深度学习带动人工智能飞速的发展，并在图片处理、语音识别领域取得巨大成功。\"\n</code></pre>\n\n<p>（1）<strong>精确分词</strong></p>\n\n<p>精确分词：精确模式试图将句子最精确地切开，精确分词也是默认分词。</p>\n\n<pre>\n<code>segs_1 = jieba.cut(content, cut_all=False)\nprint(\"/\".join(segs_1))\n</code></pre>\n\n<p>其结果为：</p>\n\n<blockquote>\n<p>现如今/，/机器/学习/和/深度/学习/带动/人工智能/飞速/的/发展/，/并/在/图片/处理/、/语音/识别/领域/取得/巨大成功/。</p>\n</blockquote>\n\n<p>（2）<strong>全模式</strong></p>\n\n<p>全模式分词：把句子中所有的可能是词语的都扫描出来，速度非常快，但不能解决歧义。</p>\n\n<pre>\n<code>segs_3 = jieba.cut(content, cut_all=True)\nprint(\"/\".join(segs_3))\n</code></pre>\n\n<p>结果为：</p>\n\n<blockquote>\n<p>现如今/如今///机器/学习/和/深度/学习/带动/动人/人工/人工智能/智能/飞速/的/发展///并/在/图片/处理///语音/识别/领域/取得/巨大/巨大成功/大成/成功//</p>\n</blockquote>\n\n<p>（3）<strong>搜索引擎模式</strong></p>\n\n<p>搜索引擎模式：在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。</p>\n\n<pre>\n<code>segs_4 = jieba.cut_for_search(content)\nprint(\"/\".join(segs_4))\n</code></pre>\n\n<p>结果为：</p>\n\n<blockquote>\n<p>如今/现如今/，/机器/学习/和/深度/学习/带动/人工/智能/人工智能/飞速/的/发展/，/并/在/图片/处理/、/语音/识别/领域/取得/巨大/大成/成功/巨大成功/。</p>\n</blockquote>\n\n<p>（4）<strong>用 lcut 生成 list</strong></p>\n\n<p>jieba.cut 以及&nbsp;<code>jieba.cut_for_search</code>&nbsp;返回的结构都是一个可迭代的 Generator，可以使用 for 循环来获得分词后得到的每一个词语（Unicode）。jieba.lcut 对 cut 的结果做了封装，l 代表 list，即返回的结果是一个 list 集合。同样的，用&nbsp;<code>jieba.lcut_for_search</code>&nbsp;也直接返回 list 集合。</p>\n\n<pre>\n<code>segs_5 = jieba.lcut(content)\nprint(segs_5)\n</code></pre>\n\n<p>结果为：</p>\n\n<blockquote>\n<p>[&#39;现如今&#39;, &#39;，&#39;, &#39;机器&#39;, &#39;学习&#39;, &#39;和&#39;, &#39;深度&#39;, &#39;学习&#39;, &#39;带动&#39;, &#39;人工智能&#39;, &#39;飞速&#39;, &#39;的&#39;, &#39;发展&#39;, &#39;，&#39;, &#39;并&#39;, &#39;在&#39;, &#39;图片&#39;, &#39;处理&#39;, &#39;、&#39;, &#39;语音&#39;, &#39;识别&#39;, &#39;领域&#39;, &#39;取得&#39;, &#39;巨大成功&#39;, &#39;。&#39;]</p>\n</blockquote>\n\n<p>（5）<strong>获取词性</strong></p>\n\n<p>jieba 可以很方便地获取中文词性，通过 jieba.posseg 模块实现词性标注。</p>\n\n<pre>\n<code>import jieba.posseg as psg\nprint([(x.word,x.flag) for x in psg.lcut(content)])\n</code></pre>\n\n<p>结果为：</p>\n\n<blockquote>\n<p>[(&#39;现如今&#39;, &#39;t&#39;), (&#39;，&#39;, &#39;x&#39;), (&#39;机器&#39;, &#39;n&#39;), (&#39;学习&#39;, &#39;v&#39;), (&#39;和&#39;, &#39;c&#39;), (&#39;深度&#39;, &#39;ns&#39;), (&#39;学习&#39;, &#39;v&#39;), (&#39;带动&#39;, &#39;v&#39;), (&#39;人工智能&#39;, &#39;n&#39;), (&#39;飞速&#39;, &#39;n&#39;), (&#39;的&#39;, &#39;uj&#39;), (&#39;发展&#39;, &#39;vn&#39;), (&#39;，&#39;, &#39;x&#39;), (&#39;并&#39;, &#39;c&#39;), (&#39;在&#39;, &#39;p&#39;), (&#39;图片&#39;, &#39;n&#39;), (&#39;处理&#39;, &#39;v&#39;), (&#39;、&#39;, &#39;x&#39;), (&#39;语音&#39;, &#39;n&#39;), (&#39;识别&#39;, &#39;v&#39;), (&#39;领域&#39;, &#39;n&#39;), (&#39;取得&#39;, &#39;v&#39;), (&#39;巨大成功&#39;, &#39;nr&#39;), (&#39;。&#39;, &#39;x&#39;)]</p>\n</blockquote>\n\n<p>（6）<strong>并行分词</strong></p>\n\n<p>并行分词原理为文本按行分隔后，分配到多个 Python 进程并行分词，最后归并结果。</p>\n\n<p>用法：</p>\n\n<pre>\n<code>jieba.enable_parallel(4) # 开启并行分词模式，参数为并行进程数 。\njieba.disable_parallel() # 关闭并行分词模式 。\n</code></pre>\n\n<p>注意： 并行分词仅支持默认分词器 jieba.dt 和 jieba.posseg.dt。目前暂不支持 Windows。</p>\n\n<p>（7）<strong>获取分词结果中词列表的 top n</strong></p>\n\n<pre>\n<code>from collections import Counter\ntop5= Counter(segs_5).most_common(5)\nprint(top5)\n</code></pre>\n\n<p>结果为：</p>\n\n<blockquote>\n<p>[(&#39;，&#39;, 2), (&#39;学习&#39;, 2), (&#39;现如今&#39;, 1), (&#39;机器&#39;, 1), (&#39;和&#39;, 1)]</p>\n</blockquote>\n\n<p>（8）<strong>自定义添加词和字典</strong></p>\n\n<p>默认情况下，使用默认分词，是识别不出这句话中的&ldquo;铁甲网&rdquo;这个新词，这里使用用户字典提高分词准确性。</p>\n\n<pre>\n<code>txt = \"铁甲网是中国最大的工程机械交易平台。\"\nprint(jieba.lcut(txt))\n</code></pre>\n\n<p>结果为：</p>\n\n<blockquote>\n<p>[&#39;铁甲&#39;, &#39;网是&#39;, &#39;中国&#39;, &#39;最大&#39;, &#39;的&#39;, &#39;工程机械&#39;, &#39;交易平台&#39;, &#39;。&#39;]</p>\n</blockquote>\n\n<p>如果添加一个词到字典，看结果就不一样了。</p>\n\n<pre>\n<code>jieba.add_word(\"铁甲网\")\nprint(jieba.lcut(txt))\n</code></pre>\n\n<p>结果为：</p>\n\n<blockquote>\n<p>[&#39;铁甲网&#39;, &#39;是&#39;, &#39;中国&#39;, &#39;最大&#39;, &#39;的&#39;, &#39;工程机械&#39;, &#39;交易平台&#39;, &#39;。&#39;]</p>\n</blockquote>\n\n<p>但是，如果要添加很多个词，一个个添加效率就不够高了，这时候可以定义一个文件，然后通过&nbsp;<code>load_userdict()</code>函数，加载自定义词典，如下：</p>\n\n<pre>\n<code>jieba.load_userdict(\'user_dict.txt\')\nprint(jieba.lcut(txt))\n</code></pre>\n\n<p>结果为：</p>\n\n<blockquote>\n<p>[&#39;铁甲网&#39;, &#39;是&#39;, &#39;中国&#39;, &#39;最大&#39;, &#39;的&#39;, &#39;工程机械&#39;, &#39;交易平台&#39;, &#39;。&#39;]</p>\n</blockquote>\n\n<p><strong>注意事项：</strong></p>\n\n<p>jieba.cut 方法接受三个输入参数: 需要分词的字符串；<code>cut_all</code>&nbsp;参数用来控制是否采用全模式；HMM 参数用来控制是否使用 HMM 模型。</p>\n\n<p><code>jieba.cut_for_search</code>&nbsp;方法接受两个参数：需要分词的字符串；是否使用 HMM 模型。该方法适合用于搜索引擎构建倒排索引的分词，粒度比较细。</p>\n\n<h3>HanLP 分词</h3>\n\n<h4>pyhanlp 安装</h4>\n\n<p>其为 HanLP 的 Python 接口，支持自动下载与升级 HanLP，兼容 Python2、Python3。</p>\n\n<p>安装命令为&nbsp;<code>pip install pyhanlp</code>，使用命令 hanlp 来验证安装。</p>\n\n<p>pyhanlp 目前使用 jpype1 这个 Python 包来调用 HanLP，如果遇到：</p>\n\n<blockquote>\n<p>building &#39;_jpype&#39; extensionerror: Microsoft Visual C++ 14.0 is required. Get it with &quot;Microsoft VisualC++ Build Tools&quot;: http://landinghub.visualstudio.com/visual-cpp-build-tools</p>\n</blockquote>\n\n<p><strong>则推荐利用轻量级的 Miniconda 来下载编译好的 jpype1。</strong></p>\n\n<pre>\n<code>conda install -c conda-forge jpype1\npip install pyhanlp\n</code></pre>\n\n<p><strong>未安装 Java 时会报错</strong>：</p>\n\n<blockquote>\n<p>jpype.<em>jvmfinder.JVMNotFoundException: No JVM shared library file (jvm.dll) found. Try setting up the JAVA</em>HOME environment variable properly.</p>\n</blockquote>\n\n<p>HanLP 主项目采用 Java 开发，所以需要 Java 运行环境，请安装 JDK。</p>\n\n<h4>命令行交互式分词模式</h4>\n\n<p>在命令行界面，使用命令 hanlp segment 进入交互分词模式，输入一个句子并回车，HanLP 会输出分词结果：</p>\n\n<p><img alt=\"enter image description here\" src=\"http://images.gitbook.cn/009c2f60-616f-11e8-b864-0bd1f4b74dfb\" /></p>\n\n<p>可见，pyhanlp 分词结果是带有词性的。</p>\n\n<h4>服务器模式</h4>\n\n<p>通过 hanlp serve 来启动内置的 HTTP 服务器，默认本地访问地址为：http://localhost:8765 。</p>\n\n<p><img alt=\"enter image description here\" src=\"http://images.gitbook.cn/d29d52f0-616f-11e8-b864-0bd1f4b74dfb\" /></p>\n\n<p><img alt=\"enter image description here\" src=\"http://images.gitbook.cn/e79a06b0-6171-11e8-b864-0bd1f4b74dfb\" /></p>\n\n<p>也可以访问官网演示页面：<a href=\"http://hanlp.hankcs.com/\">http://hanlp.hankcs.com/</a>。</p>\n\n<h4>通过工具类 HanLP 调用常用接口</h4>\n\n<p>通过工具类 HanLP 调用常用接口，这种方式应该是我们在项目中最常用的方式。</p>\n\n<p>（1）分词</p>\n\n<pre>\n<code>from pyhanlp import *\ncontent = \"现如今，机器学习和深度学习带动人工智能飞速的发展，并在图片处理、语音识别领域取得巨大成功。\"\nprint(HanLP.segment(content))\n</code></pre>\n\n<p>结果为：</p>\n\n<blockquote>\n<p>[现如今/t, ，/w, 机器学习/gi, 和/cc, 深度/n, 学习/v, 带动/v, 人工智能/n, 飞速/d, 的/ude1, 发展/vn, ，/w, 并/cc, 在/p, 图片/n, 处理/vn, 、/w, 语音/n, 识别/vn, 领域/n, 取得/v, 巨大/a, 成功/a, 。/w]</p>\n</blockquote>\n\n<p>（2）自定义词典分词</p>\n\n<p>在没有使用自定义字典时的分词。</p>\n\n<pre>\n<code>txt = \"铁甲网是中国最大的工程机械交易平台。\"\nprint(HanLP.segment(txt))\n</code></pre>\n\n<p>结果为：</p>\n\n<blockquote>\n<p>[铁甲/n, 网/n, 是/vshi, 中国/ns, 最大/gm, 的/ude1, 工程/n, 机械/n, 交易/vn, 平台/n, 。/w]</p>\n</blockquote>\n\n<p>添加自定义新词：</p>\n\n<pre>\n<code>CustomDictionary.add(\"铁甲网\")\nCustomDictionary.insert(\"工程机械\", \"nz 1024\")\nCustomDictionary.add(\"交易平台\", \"nz 1024 n 1\")\nprint(HanLP.segment(txt))\n</code></pre>\n\n<p>结果为：</p>\n\n<blockquote>\n<p>[铁甲网/nz, 是/vshi, 中国/ns, 最大/gm, 的/ude1, 工程机械/nz, 交易平台/nz, 。/w]</p>\n</blockquote>\n\n<p>当然了，jieba 和 pyhanlp 能做的事还有很多，关键词提取、自动摘要、依存句法分析、情感分析等，后面章节我们将会讲到，这里不再赘述。</p>\n\n<p>参考文献：</p>\n\n<ol>\n	<li>https://github.com/fxsjy/jieba</li>\n	<li>https://github.com/hankcs/pyhanlp</li>\n</ol>\n\n<h2><a id=\"第03课：动手实战中文文本中的关键字提取\" name=\"第03课：动手实战中文文本中的关键字提取\"></a>第03课：动手实战中文文本中的关键字提取</h2>\n\n<h3>前言</h3>\n\n<p>关键词提取就是从文本里面把跟这篇文章意义最相关的一些词语抽取出来。这个可以追溯到文献检索初期，关键词是为了文献标引工作，从报告、论文中选取出来用以表示全文主题内容信息的单词或术语，在现在的报告和论文中，我们依然可以看到关键词这一项。因此，关键词在文献检索、自动文摘、文本聚类/分类等方面有着重要的应用，它不仅是进行这些工作不可或缺的基础和前提，也是互联网上信息建库的一项重要工作。</p>\n\n<p>关键词抽取从方法来说主要有两种：</p>\n\n<ul>\n	<li>\n	<p>第一种是关键词分配：就是给定一个已有的关键词库，对于新来的文档从该词库里面匹配几个词语作为这篇文档的关键词。</p>\n	</li>\n	<li>\n	<p>第二种是关键词提取：针对新文档，通过算法分析，提取文档中一些词语作为该文档的关键词。</p>\n	</li>\n</ul>\n\n<p>目前大多数应用领域的关键词抽取算法都是基于后者实现的，从逻辑上说，后者比前者在实际应用中更准确。</p>\n\n<p>下面介绍一些关于关键词抽取的常用和经典的算法实现。</p>\n\n<h3>基于 TF-IDF 算法进行关键词提取</h3>\n\n<p>在信息检索理论中，TF-IDF 是 Term Frequency - Inverse Document Frequency 的简写。TF-IDF 是一种数值统计，用于反映一个词对于语料中某篇文档的重要性。在信息检索和文本挖掘领域，它经常用于因子加权。TF-IDF 的主要思想就是：如果某个词在一篇文档中出现的频率高，也即 TF 高；并且在语料库中其他文档中很少出现，即 DF 低，也即 IDF 高，则认为这个词具有很好的类别区分能力。</p>\n\n<p>TF 为词频（Term Frequency），表示词 t 在文档 d 中出现的频率，计算公式：</p>\n\n<p><img alt=\"enter image description here\" src=\"http://images.gitbook.cn/332cea00-61b0-11e8-8a60-1bdde4cc4659\" /></p>\n\n<p>其中，ni,jni,j&nbsp;是该词&nbsp;titi&nbsp;在文件&nbsp;djdj&nbsp;中的出现次数，而分母则是在文件&nbsp;djdj&nbsp;中所有字词的出现次数之和。</p>\n\n<p>IDF 为逆文档频率（Inverse Document Frequency），表示语料库中包含词 t 的文档的数目的倒数，计算公式：</p>\n\n<p><img alt=\"enter image description here\" src=\"http://images.gitbook.cn/8412c0c0-61b0-11e8-b864-0bd1f4b74dfb\" /></p>\n\n<p>其中，<code>|D|</code>&nbsp;表示语料库中的文件总数，|{j:ti&isin;dj}||{j:ti&isin;dj}|&nbsp;包含词&nbsp;titi&nbsp;的文件数目，如果该词语不在语料库中，就会导致被除数为零，因此一般情况下使用&nbsp;1+|{j:ti&isin;dj}|1+|{j:ti&isin;dj}|。</p>\n\n<p>TF-IDF 在实际中主要是将二者相乘，也即 TF * IDF， 计算公式：</p>\n\n<p><img alt=\"enter image description here\" src=\"http://images.gitbook.cn/b78eeb70-61b2-11e8-a59f-c7ac04233ce1\" /></p>\n\n<p>因此，TF-IDF 倾向于过滤掉常见的词语，保留重要的词语。例如，某一特定文件内的高频率词语，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的 TF-IDF。</p>\n\n<p>好在 jieba 已经实现了基于 TF-IDF 算法的关键词抽取，通过命令&nbsp;<code>import jieba.analyse</code>&nbsp;引入，函数参数解释如下：</p>\n\n<pre>\n<code>jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=())\n</code></pre>\n\n<ul>\n	<li>\n	<p>sentence：待提取的文本语料；</p>\n	</li>\n	<li>\n	<p>topK：返回 TF/IDF 权重最大的关键词个数，默认值为 20；</p>\n	</li>\n	<li>\n	<p>withWeight：是否需要返回关键词权重值，默认值为 False；</p>\n	</li>\n	<li>\n	<p>allowPOS：仅包括指定词性的词，默认值为空，即不筛选。</p>\n	</li>\n</ul>\n\n<p>接下来看例子，我采用的语料来自于百度百科对人工智能的定义，获取 Top20 关键字，用空格隔开打印：</p>\n\n<pre>\n<code>import jieba.analyse\nsentence  = \"人工智能（Artificial Intelligence），英文缩写为AI。它是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。人工智能是计算机科学的一个分支，它企图了解智能的实质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器，该领域的研究包括机器人、语言识别、图像识别、自然语言处理和专家系统等。人工智能从诞生以来，理论和技术日益成熟，应用领域也不断扩大，可以设想，未来人工智能带来的科技产品，将会是人类智慧的“容器”。人工智能可以对人的意识、思维的信息过程的模拟。人工智能不是人的智能，但能像人那样思考、也可能超过人的智能。人工智能是一门极富挑战性的科学，从事这项工作的人必须懂得计算机知识，心理学和哲学。人工智能是包括十分广泛的科学，它由不同的领域组成，如机器学习，计算机视觉等等，总的说来，人工智能研究的一个主要目标是使机器能够胜任一些通常需要人类智能才能完成的复杂工作。但不同的时代、不同的人对这种“复杂工作”的理解是不同的。2017年12月，人工智能入选“2017年度中国媒体十大流行语”。\"\nkeywords = \"  \".join(jieba.analyse.extract_tags(sentence , topK=20, withWeight=False, allowPOS=()))\nprint(keywords)\n</code></pre>\n\n<p>执行结果：</p>\n\n<blockquote>\n<p>人工智能 智能 2017 机器 不同 人类 科学 模拟 一门 技术 计算机 研究 工作 Artificial Intelligence AI 图像识别 12 复杂 流行语</p>\n</blockquote>\n\n<p>下面只获取 Top10 的关键字，并修改一下词性，只选择名词和动词，看看结果有何不同？</p>\n\n<pre>\n<code>keywords =(jieba.analyse.extract_tags(sentence , topK=10, withWeight=True, allowPOS=([\'n\',\'v\'])))\nprint(keywords)\n</code></pre>\n\n<p>执行结果：</p>\n\n<blockquote>\n<p>[(&#39;人工智能&#39;, 0.9750542675762887), (&#39;智能&#39;, 0.5167124540885567), (&#39;机器&#39;, 0.20540911929525774), (&#39;人类&#39;, 0.17414426566082475), (&#39;科学&#39;, 0.17250169374402063), (&#39;模拟&#39;, 0.15723537382948452), (&#39;技术&#39;, 0.14596259315164947), (&#39;计算机&#39;, 0.14030483362639176), (&#39;图像识别&#39;, 0.12324502580309278), (&#39;流行语&#39;, 0.11242211730309279)]</p>\n</blockquote>\n\n<h3>基于 TextRank 算法进行关键词提取</h3>\n\n<p>TextRank 是由 PageRank 改进而来，核心思想将文本中的词看作图中的节点，通过边相互连接，不同的节点会有不同的权重，权重高的节点可以作为关键词。这里给出 TextRank 的公式：</p>\n\n<p><img alt=\"enter image description here\" src=\"http://images.gitbook.cn/a041fb90-61af-11e8-a59f-c7ac04233ce1\" /></p>\n\n<p>节点 i 的权重取决于节点 i 的邻居节点中 i-j 这条边的权重 / j 的所有出度的边的权重 * 节点 j 的权重，将这些邻居节点计算的权重相加，再乘上一定的阻尼系数，就是节点 i 的权重，阻尼系数 d 一般取 0.85。</p>\n\n<p>TextRank 用于关键词提取的算法如下：</p>\n\n<p>（1）把给定的文本 T 按照完整句子进行分割，即:</p>\n\n<p><img alt=\"enter image description here\" src=\"http://images.gitbook.cn/82165be0-61ad-11e8-a59f-c7ac04233ce1\" /></p>\n\n<p>（2）对于每个句子，进行分词和词性标注处理，并过滤掉停用词，只保留指定词性的单词，如名词、动词、形容词，其中&nbsp;ti,jti,j&nbsp;是保留后的候选关键词。</p>\n\n<p><img alt=\"enter image description here\" src=\"http://images.gitbook.cn/736f86a0-61af-11e8-8a60-1bdde4cc4659\" /></p>\n\n<p>（3）构建候选关键词图 G = (V,E)，其中 V 为节点集，由（2）生成的候选关键词组成，然后采用共现关系（Co-Occurrence）构造任两点之间的边，两个节点之间存在边仅当它们对应的词汇在长度为 K 的窗口中共现，K 表示窗口大小，即最多共现 K 个单词。</p>\n\n<p>（4）根据 TextRank 的公式，迭代传播各节点的权重，直至收敛。</p>\n\n<p>（5）对节点权重进行倒序排序，从而得到最重要的 T 个单词，作为候选关键词。</p>\n\n<p>（6）由（5）得到最重要的 T 个单词，在原始文本中进行标记，若形成相邻词组，则组合成多词关键词。</p>\n\n<p>同样 jieba 已经实现了基于 TextRank 算法的关键词抽取，通过命令&nbsp;<code>import jieba.analyse</code>&nbsp;引用，函数参数解释如下：</p>\n\n<pre>\n<code>jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=(\'ns\', \'n\', \'vn\', \'v\'))\n</code></pre>\n\n<p>直接使用，接口参数同 TF-IDF 相同，注意默认过滤词性。</p>\n\n<p>接下来，我们继续看例子，语料继续使用上例中的句子。</p>\n\n<pre>\n<code>result = \"  \".join(jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=(\'ns\', \'n\', \'vn\', \'v\')))\nprint(result)\n</code></pre>\n\n<p>执行结果：</p>\n\n<blockquote>\n<p>智能 人工智能 机器 人类 研究 技术 模拟 包括 科学 工作 领域 理论 计算机 年度 需要 语言 相似 方式 做出 心理学</p>\n</blockquote>\n\n<p>如果修改一下词性，只需要名词和动词，看看结果有何不同？</p>\n\n<pre>\n<code>result = \"  \".join(jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=(\'n\',\'v\')))\nprint(result)\n</code></pre>\n\n<p>执行结果：</p>\n\n<blockquote>\n<p>智能 人工智能 机器 人类 技术 模拟 包括 科学 理论 计算机 领域 年度 需要 心理学 信息 语言 识别 带来 过程 延伸</p>\n</blockquote>\n\n<h3>基于 LDA 主题模型进行关键词提取</h3>\n\n<p>其实，使用 LDA 获取文本关键词在我的第一次 Chat<a href=\"http://gitbook.cn/gitchat/activity/5ae2c9475d06502947fb1d98\">《NLP 中文短文本分类项目实践（上）》</a>已经讲过了，为了保持内容的完整性，在这里我继续写一下。</p>\n\n<p>语料是一个关于汽车的短文本，下面通过 Gensim 库完成基于 LDA 的关键字提取。整个过程的步骤为：文件加载 -&gt; jieba 分词 -&gt; 去停用词 -&gt; 构建词袋模型 -&gt; LDA 模型训练 -&gt; 结果可视化。</p>\n\n<pre>\n<code>#引入库文件\nimport jieba.analyse as analyse\nimport jieba\nimport pandas as pd\nfrom gensim import corpora, models, similarities\nimport gensim\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n#设置文件路径\ndir = \"D://ProgramData//PythonWorkSpace//study//\"\nfile_desc = \"\".join([dir,\'car.csv\'])\nstop_words = \"\".join([dir,\'stopwords.txt\'])\n#定义停用词\nstopwords=pd.read_csv(stop_words,index_col=False,quoting=3,sep=\"\\t\",names=[\'stopword\'], encoding=\'utf-8\')\nstopwords=stopwords[\'stopword\'].values\n#加载语料\ndf = pd.read_csv(file_desc, encoding=\'gbk\')\n#删除nan行\ndf.dropna(inplace=True)\nlines=df.content.values.tolist()\n#开始分词\nsentences=[]\nfor line in lines:\n    try:\n        segs=jieba.lcut(line)\n        segs = [v for v in segs if not str(v).isdigit()]#去数字\n        segs = list(filter(lambda x:x.strip(), segs))   #去左右空格\n        segs = list(filter(lambda x:x not in stopwords, segs)) #去掉停用词\n        sentences.append(segs)\n    except Exception:\n        print(line)\n        continue\n#构建词袋模型\ndictionary = corpora.Dictionary(sentences)\ncorpus = [dictionary.doc2bow(sentence) for sentence in sentences]\n#lda模型，num_topics是主题的个数，这里定义了5个\nlda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=10)\n#我们查一下第1号分类，其中最常出现的5个词是：\nprint(lda.print_topic(1, topn=5))\n#我们打印所有5个主题，每个主题显示8个词\nfor topic in lda.print_topics(num_topics=10, num_words=8):\n    print(topic[1])</code></pre>\n\n<p>执行结果如下图所示：</p>\n\n<p><img alt=\"enter image description here\" src=\"http://images.gitbook.cn/a55a6950-61b9-11e8-b864-0bd1f4b74dfb\" /></p>\n\n<pre>\n<code>#显示中文matplotlib\nplt.rcParams[\'font.sans-serif\'] = [u\'SimHei\']\nplt.rcParams[\'axes.unicode_minus\'] = False\n# 在可视化部分，我们首先画出了九个主题的7个词的概率分布图\nnum_show_term = 8 # 每个主题下显示几个词\nnum_topics  = 10  \nfor i, k in enumerate(range(num_topics)):\n    ax = plt.subplot(2, 5, i+1)\n    item_dis_all = lda.get_topic_terms(topicid=k)\n    item_dis = np.array(item_dis_all[:num_show_term])\n    ax.plot(range(num_show_term), item_dis[:, 1], \'b*\')\n    item_word_id = item_dis[:, 0].astype(np.int)\n    word = [dictionary.id2token[i] for i in item_word_id]\n    ax.set_ylabel(u\"概率\")\n    for j in range(num_show_term):\n        ax.text(j, item_dis[j, 1], word[j], bbox=dict(facecolor=\'green\',alpha=0.1))\nplt.suptitle(u\'9个主题及其7个主要词的概率\', fontsize=18)\nplt.show()</code></pre>\n\n<p>执行结果如下图所示：</p>\n\n<p><img alt=\"enter image description here\" src=\"http://images.gitbook.cn/ba1ae950-61b9-11e8-b977-f33e31f528f0\" /></p>\n\n<h3>基于 pyhanlp 进行关键词提取</h3>\n\n<p>除了 jieba，也可以选择使用 HanLP 来完成关键字提取，内部采用 TextRankKeyword 实现，语料继续使用上例中的句子。</p>\n\n<pre>\n<code>from pyhanlp import *\nresult = HanLP.extractKeyword(sentence, 20)\nprint(result)\n</code></pre>\n\n<p>执行结果：</p>\n\n<blockquote>\n<p>[人工智能, 智能, 领域, 人类, 研究, 不同, 工作, 包括, 模拟, 新的, 机器, 计算机, 门, 科学, 应用, 系统, 理论, 技术, 入选, 复杂]</p>\n</blockquote>\n\n<h3>总结</h3>\n\n<p>本节内容的重点就是掌握关键字提取的基本方法，常规的关键词提取方法如上所述，当然还有其他算法及其改进，有深入研究需求的，可以下载关键字提取方面的论文阅读。</p>\n\n<p>参考文献：</p>\n\n<ol>\n	<li>Mihalcea R, Tarau P. TextRank: Bringing order into texts[C]//Proceedings of EMNLP. 2004, 4(4): 275.</li>\n	<li>Witten I H, Paynter G W, Frank E, et al. KEA: Practical automatic keyphrase extraction[C]//Proceedings of the fourth ACM conference on Digital libraries. ACM, 1999: 254-255.</li>\n	<li>Chien L F. PAT-tree-based keyword extraction for Chinese information retrieval[C]//ACM SIGIR Forum. ACM, 1997, 31(SI): 50-58.</li>\n</ol>\n\n<h2><a id=\"第04课：了解数据必备的文本可视化技巧\" name=\"第04课：了解数据必备的文本可视化技巧\"></a>第04课：了解数据必备的文本可视化技巧</h2>\n\n<h3>为什么要文本数据可视化</h3>\n\n<p>文字是传递信息最常用的载体，随着海量文本的涌现，信息超载和数据过剩等问题日益凸显，当大段大段的文字摆在面前，已经很少有人耐心、认真把它读完，人们急需一种更高效的信息接收方式，从视觉的角度出发，文本可视化正是解药良方。所谓一图胜千言，其实就是文本可视化的一种表现。</p>\n\n<p><img alt=\"enter image description here\" src=\"http://images.gitbook.cn/8388b2b0-6280-11e8-b7c9-aba3c5c7330f\" /></p>\n\n<p>因此，文本可视化技术将文本中复杂的或者难以通过文字表达的内容和规律以视觉符号的形式表达出来，使人们能够利用与生俱来的视觉感知的并行化处理能力，快速获取文本中所蕴含的关键信息。</p>\n\n<h3>文本可视化的流程</h3>\n\n<p>文本可视化依赖于自然语言处理，因此词袋模型、命名实体识别、关键词抽取、主题分析、情感分析等是较常用的文本分析技术。文本分析的过程主要包括特征提取，通过分词、抽取、归一化等操作提取出文本词汇级的内容，利用特征构建向量空间模型并进行降维，以便将其呈现在低维空间，或者利用主题模型处理特征，最终以灵活有效的形式表示这些处理过的数据，以便进行可视化呈现。下图（来源：网络）是一个文本可视化的基本流程图：</p>\n\n<p><img alt=\"enter image description here\" src=\"http://images.gitbook.cn/5d192a50-6231-11e8-b82c-e1d608026a45\" /></p>\n\n<p>文本可视化类型，除了包含常规的图表类，如柱状图、饼图、折线图等的表现形式，在文本领域用的比较多的可视化类型有：</p>\n\n<p>（1）基于文本内容的可视化。</p>\n\n<p>基于文本内容的可视化研究包括基于词频的可视化和基于词汇分布的可视化，常用的有词云、分布图和 Document Cards 等。</p>\n\n<p>（2）基于文本关系的可视化。</p>\n\n<p>基于文本关系的可视化研究文本内外关系，帮助人们理解文本内容和发现规律。常用的可视化形式有树状图、节点连接的网络图、力导向图、叠式图和 Word Tree 等。</p>\n\n<p>（3）基于多层面信息的可视化</p>\n\n<p>基于多层面信息的可视化主要研究如何结合信息的多个方面帮助用户从更深层次理解文本数据，发现其内在规律。其中，包含时间信息和地理坐标的文本可视化近年来受到越来越多的关注。常用的有地理热力图、ThemeRiver、SparkClouds、TextFlow 和基于矩阵视图的情感分析可视化等。</p>\n\n<h3>动手实战文本可视化</h3>\n\n<h4>词云</h4>\n\n<p>在 Chat<a href=\"http://gitbook.cn/gitchat/activity/5ae2c9475d06502947fb1d98\">《NLP 中文短文本分类项目实践（上）》</a>中已经讲过如何绘制 Wordcloud，这里只给出关键代码。具体过程是分词、去停用词和统计词频，然后绘制 Wordcloud 词云，这里提供下面两种方式。</p>\n\n<pre>\n<code>#**第一种是默认的样式**\nwordcloud=WordCloud(font_path=simhei,background_color=\"white\",max_font_size=80)\nword_frequence = {x[0]:x[1] for x in words_stat.head(1000).values}\nwordcloud=wordcloud.fit_words(word_frequence)\n#**第二种是自定义图片**\ntext = \" \".join(words_stat[\'segment\'].head(100).astype(str))\nabel_mask = imread(r\"china.jpg\")  #这里设置了一张中国地图\nwordcloud2 = WordCloud(background_color=\'white\',  # 设置背景颜色 \n   mask = abel_mask,  # 设置背景图片\n   max_words = 3000,  # 设置最大现实的字数\n   font_path = simhei,  # 设置字体格式\n   width=2048,\n   height=1024,\n   scale=4.0,\n   max_font_size= 300,  # 字体最大值\n                     random_state=42).generate(text)\n# 根据图片生成词云颜色\nimage_colors = ImageColorGenerator(abel_mask)\nwordcloud2.recolor(color_func=image_colors)\n# 以下代码显示图片\nplt.imshow(wordcloud2)\nplt.axis(\"off\")\nplt.show()\nwordcloud2.to_file(r\'wordcloud_2.jpg\') #保存结果</code></pre>\n\n<p>得到的词云如下图所示：</p>\n\n<p><img alt=\"enter image description here\" src=\"http://images.gitbook.cn/25a32c00-624b-11e8-b82c-e1d608026a45\" /></p>\n\n<h4>关系图</h4>\n\n<p>关系图法，是指用连线图来表示事物相互关系的一种方法。最常见的关系图是数据库里的 E-R 图，表示实体、关系、属性三者之间的关系。在文本可视化里面，关系图也经常被用来表示有相互关系、原因与结果和目的与手段等复杂关系，下面我们来看看如何用 Python 实现关系图制作。</p>\n\n<p><strong>基本步骤</strong>：</p>\n\n<ul>\n	<li>安装 Matplotlib、NetworkX；</li>\n	<li>解决 Matplotlib 无法写中文问题。</li>\n</ul>\n\n<p>我们需要知道 NetworkX 绘制关系图的数据组织结构，节点和边都是 list 格式，边的 list 里面是成对的节点。下面我们看一个真实的例子，学生课程和上课地点的关系图。</p>\n\n<pre>\n<code>classes= df[\'class\'].values.tolist()\nclassrooms=df[\'classroom\'].values.tolist()\nnodes = list(set(classes + classrooms))\nweights = [(df.loc[index,\'class\'],df.loc[index,\'classroom\'])for index in df.index]   \nweights =  list(set(weights))\n# 设置matplotlib正常显示中文\nplt.rcParams[\'font.sans-serif\']=[\'SimHei\']   # 用黑体显示中文\nplt.rcParams[\'axes.unicode_minus\']=False \ncolors = [\'red\', \'green\', \'blue\', \'yellow\']\n#有向图\nDG = nx.DiGraph()\n#一次性添加多节点，输入的格式为列表\nDG.add_nodes_from(nodes)\n#添加边，数据格式为列表\nDG.add_edges_from(weights)\n#作图，设置节点名显示,节点大小，节点颜色\nnx.draw(DG,with_labels=True, node_size=1000, node_color = colors)\nplt.show() </code></pre>\n\n<p>得到的关系图如下图所示：</p>\n\n<p><img alt=\"enter image description here\" src=\"http://images.gitbook.cn/0af7aa00-6257-11e8-b7c9-aba3c5c7330f\" /></p>\n\n<h4>地理热力图</h4>\n\n<p>地理热力图，是以特殊高亮的形式显示用户的地理位置，借助热力图，可以直观地观察到用户的总体情况和偏好。</p>\n\n<p><strong>基本步骤</strong>：</p>\n\n<ul>\n	<li>安装 Folium；</li>\n	<li>将地理名词通过百度转换成经纬度。</li>\n</ul>\n\n<p>在通过分词得到城市名称后，将地理名词通过百度转换成经纬度。首先注册密钥，使用百度 Web 服务 API 下的 Geocoding API 接口来获取你所需要地址的经纬度坐标，并转化为 JSON 结构的数据（个人接口，百度每天限制调用6000次），接下来定义经纬度获取函数：</p>\n\n<pre>\n<code>#经纬度转换\ndef getlnglat(address):\n    url = \'http://api.map.baidu.com/geocoder/v2/\'\n    output = \'json\'\n    ak = \'sqGDDvCDEZPSz24bt4b0BpKLnMk1dv6d\'\n    add = quote(address) #由于本文城市变量为中文，为防止乱码，先用quote进行编码\n    uri = url + \'?\' + \'address=\' + add  + \'&amp;output=\' + output + \'&amp;ak=\' + ak\n    req = urlopen(uri)\n    res = req.read().decode() #将其他编码的字符串解码成unicode\n    temp = json.loads(res)  #对json数据进行解析\n    return temp</code></pre>\n\n<p>转换后数据格式：</p>\n\n<blockquote>\n<p>北京,116.39564503787867,39.92998577808024,840</p>\n\n<p>成都,104.06792346330406,30.679942845419564,291</p>\n\n<p>重庆,106.53063501341296,29.54460610888615,261</p>\n\n<p>昆明,102.71460113878045,25.049153100453157,238</p>\n\n<p>潍坊,119.14263382297052,36.71611487305138,214</p>\n\n<p>济南,117.02496706629023,36.68278472716141,212</p>\n</blockquote>\n\n<p>然后，使用 Folium 库进行热力图绘制地图：</p>\n\n<pre>\n<code>lat = np.array(cities[\"lat\"][0:num])   # 获取维度之维度值\nlon = np.array(cities[\"lng\"][0:num])   # 获取经度值\npop = np.array(cities[\"count\"][0:num],dtype=float)    # 获取人口数，转化为numpy浮点型\ndata1 = [[lat[i],lon[i],pop[i]] for i in range(num)]    #将数据制作成[lats,lons,weights]的形式\nmap_osm = folium.Map(location=[35,110],zoom_start=5)    #绘制Map，开始缩放程度是5倍\nHeatMap(data1).add_to(map_osm)  # 将热力图添加到前面建立的map里\nfile_path = dir + \"heatmap.html\"\nmap_osm.save(file_path)</code></pre>\n\n<p>得到的地图热力图如下图所示：</p>\n\n<p><img alt=\"enter image description here\" src=\"http://images.gitbook.cn/3e258970-6260-11e8-95e8-074055ff8c6f\" /></p>\n\n<p>上面列举了三种典型的文本可视化方式，当然还有更多漂亮的方式。个人在开发过程中，觉得采用前端技术实现的可视化效果最好，但是这次课程仅限于 Pyhton，所以就不讲前端知识了。</p>\n\n<p>最后，我推荐三个前端可视化学习网站，第一个是百度的&nbsp;<a href=\"http://echarts.baidu.com/echarts2/doc/example.html\">Echarts</a>，基于 Canvas，适合刚入门的新手，遵循了数据可视化的一些经典范式，只要把数据组织好，就可以轻松得到很漂亮的图表；第二个推荐&nbsp;<a href=\"https://github.com/d3/d3/wiki/Gallery\">D3.js</a>，基于 SVG 方便自己定制，D3 V4 支持 Canvas+SVG，D3.js 比 Echarts 稍微难点，适合有一定开发经验的人；第三个&nbsp;<a href=\"https://threejs.org/\">three.js</a>，是一个基于 WebGL 的 3D 图形的框架，可以让用户通过 JavaScript 搭建 WebGL 项目。</p>\n\n<h2><a id=\"第05课：面向非结构化数据转换的词袋和词向量模型\" name=\"第05课：面向非结构化数据转换的词袋和词向量模型\"></a>第05课：面向非结构化数据转换的词袋和词向量模型</h2>\n\n<p>通过前面几个小节的学习，我们现在已经学会了如何获取文本预料，然后分词，在分词之后的结果上，我们可以提取文本的关键词查看文本核心思想，进而可以通过可视化技术把文档从视觉的角度表达出来。</p>\n\n<p>下面，我们来看看，文本数据如何转换成计算机能够计算的数据。这里介绍两种常用的模型：词袋和词向量模型。</p>\n\n<h3>词袋模型（Bag of Words Model）</h3>\n\n<h4>词袋模型的概念</h4>\n\n<p>先来看张图，从视觉上感受一下词袋模型的样子。</p>\n\n<p><img alt=\"enter image description here\" src=\"http://images.gitbook.cn/bcd2fa60-62eb-11e8-8a60-1bdde4cc4659\" /></p>\n\n<p>词袋模型看起来好像一个口袋把所有词都装进去，但却不完全如此。在自然语言处理和信息检索中作为一种简单假设，词袋模型把文本（段落或者文档）被看作是无序的词汇集合，忽略语法甚至是单词的顺序，把每一个单词都进行统计，同时计算每个单词出现的次数，常常被用在文本分类中，如贝叶斯算法、LDA 和 LSA 等。</p>\n\n<h4>动手实战词袋模型</h4>\n\n<p>（1）词袋模型</p>\n\n<p>本例中，我们自己动手写代码看看词袋模型是如何操作的。</p>\n\n<p>首先，引入 jieba 分词器、语料和停用词（标点符号集合，自己可以手动添加或者用一个文本字典代替）。</p>\n\n<pre>\n<code>import jieba\n#定义停用词、标点符号\npunctuation = [\"，\",\"。\", \"：\", \"；\", \"？\"]\n#定义语料\ncontent = [\"机器学习带动人工智能飞速的发展。\",\n           \"深度学习带动人工智能飞速的发展。\",\n           \"机器学习和深度学习带动人工智能飞速的发展。\"\n          ]</code></pre>\n\n<p>接下来，我们先对语料进行分词操作，这里用到 lcut() 方法：</p>\n\n<pre>\n<code>#分词\nsegs_1 = [jieba.lcut(con) for con in content]\nprint(segs_1)\n</code></pre>\n\n<p>得到分词后的结果如下：</p>\n\n<blockquote>\n<p>[[&#39;机器&#39;, &#39;学习&#39;, &#39;带动&#39;, &#39;人工智能&#39;, &#39;飞速&#39;, &#39;的&#39;, &#39;发展&#39;, &#39;。&#39;], [&#39;深度&#39;, &#39;学习&#39;, &#39;带动&#39;, &#39;人工智能&#39;, &#39;飞速&#39;, &#39;的&#39;, &#39;发展&#39;, &#39;。&#39;], [&#39;机器&#39;, &#39;学习&#39;, &#39;和&#39;, &#39;深度&#39;, &#39;学习&#39;, &#39;带动&#39;, &#39;人工智能&#39;, &#39;飞速&#39;, &#39;的&#39;, &#39;发展&#39;, &#39;。&#39;]]</p>\n</blockquote>\n\n<p>因为中文语料带有停用词和标点符号，所以需要去停用词和标点符号，这里语料很小，我们直接去标点符号：</p>\n\n<pre>\n<code>tokenized = []\nfor sentence in segs_1:\n    words = []\n    for word in sentence:\n        if word not in punctuation:          \n            words.append(word)\n    tokenized.append(words)\nprint(tokenized)</code></pre>\n\n<p>去标点符号后，我们得到结果如下：</p>\n\n<blockquote>\n<p>[[&#39;机器&#39;, &#39;学习&#39;, &#39;带动&#39;, &#39;人工智能&#39;, &#39;飞速&#39;, &#39;的&#39;, &#39;发展&#39;], [&#39;深度&#39;, &#39;学习&#39;, &#39;带动&#39;, &#39;人工智能&#39;, &#39;飞速&#39;, &#39;的&#39;, &#39;发展&#39;], [&#39;机器&#39;, &#39;学习&#39;, &#39;和&#39;, &#39;深度&#39;, &#39;学习&#39;, &#39;带动&#39;, &#39;人工智能&#39;, &#39;飞速&#39;, &#39;的&#39;, &#39;发展&#39;]]</p>\n</blockquote>\n\n<p>下面操作就是把所有的分词结果放到一个袋子（List）里面，也就是取并集，再去重，获取对应的特征词。</p>\n\n<pre>\n<code>#求并集\nbag_of_words = [ x for item in segs_1 for x in item if x not in punctuation]\n#去重\nbag_of_words = list(set(bag_of_words))\nprint(bag_of_words)</code></pre>\n\n<p>得到的特征词结果如下：</p>\n\n<blockquote>\n<p>[&#39;飞速&#39;, &#39;的&#39;, &#39;深度&#39;, &#39;人工智能&#39;, &#39;发展&#39;, &#39;和&#39;, &#39;机器&#39;, &#39;学习&#39;, &#39;带动&#39;]</p>\n</blockquote>\n\n<p>我们以上面特征词的顺序，完成词袋化：</p>\n\n<pre>\n<code>bag_of_word2vec = []\nfor sentence in tokenized:\n    tokens = [1 if token in sentence else 0 for token in bag_of_words ]\n    bag_of_word2vec.append(tokens)</code></pre>\n\n<p>最后得到词袋向量：</p>\n\n<blockquote>\n<p>[[1, 1, 0, 1, 1, 0, 1, 1, 1], [1, 1, 1, 1, 1, 0, 0, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1]]</p>\n</blockquote>\n\n<p>上面的例子在编码时，对于 for 循环多次直接用到列表推导式。在 Python 中，列表推导式的效率比 for 快很多，尤其在数据量大的时候效果更明显，建议多使用列表推导式。</p>\n\n<p>（2）Gensim 构建词袋模型</p>\n\n<p>下面我们介绍 Gensim 库的使用，继续沿用上面的例子：</p>\n\n<pre>\n<code>from gensim import corpora\nimport gensim\n#tokenized是去标点之后的\ndictionary = corpora.Dictionary(tokenized)\n#保存词典\ndictionary.save(\'deerwester.dict\') \nprint(dictionary)</code></pre>\n\n<p>这时我们得到的结果不全，但通过提示信息可知道共9个独立的词：</p>\n\n<blockquote>\n<p>Dictionary(9 unique tokens: [&#39;人工智能&#39;, &#39;发展&#39;, &#39;学习&#39;, &#39;带动&#39;, &#39;机器&#39;]...)</p>\n</blockquote>\n\n<p>那我们如何查看所有词呢？通过下面方法，可以查看到所有词和对应的下标：</p>\n\n<pre>\n<code>#查看词典和下标 id 的映射\nprint(dictionary.token2id)\n</code></pre>\n\n<p>最后结果如下：</p>\n\n<blockquote>\n<p>{&#39;人工智能&#39;: 0, &#39;发展&#39;: 1, &#39;学习&#39;: 2, &#39;带动&#39;: 3, &#39;机器&#39;: 4, &#39;的&#39;: 5, &#39;飞速&#39;: 6, &#39;深度&#39;: 7, &#39;和&#39;: 8}</p>\n</blockquote>\n\n<p>根据得到的结果，我们同样可以得到词袋模型的特征向量。这里顺带提一下函数 doc2bow()，作用只是计算每个不同单词的出现次数，将单词转换为其整数单词 id 并将结果作为稀疏向量返回。</p>\n\n<pre>\n<code>corpus = [dictionary.doc2bow(sentence) for sentence in segs_1]\nprint(corpus )\n</code></pre>\n\n<p>得到的稀疏向量结果如下：</p>\n\n<blockquote>\n<p>[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)], [(0, 1), (1, 1), (2, 1), (3, 1), (5, 1), (6, 1), (7, 1)], [(0, 1), (1, 1), (2, 2), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)]]</p>\n</blockquote>\n\n<h3>词向量 （Word Embedding）</h3>\n\n<p>深度学习带给自然语言处理最令人兴奋的突破是词向量（Word Embedding）技术。词向量技术是将词语转化成为稠密向量。在自然语言处理应用中，词向量作为机器学习、深度学习模型的特征进行输入。因此，最终模型的效果很大程度上取决于词向量的效果。</p>\n\n<h3>词向量的概念</h3>\n\n<p>在 Word2Vec 出现之前，自然语言处理经常把字词进行独热编码，也就是 One-Hot Encoder。</p>\n\n<blockquote>\n<p>大数据 [0,0,0,0,0,0,0,1,0,&hellip;&hellip;，0,0,0,0,0,0,0]</p>\n\n<p>云计算[0,0,0,0,1,0,0,0,0,&hellip;&hellip;，0,0,0,0,0,0,0]</p>\n\n<p>机器学习[0,0,0,1,0,0,0,0,0,&hellip;&hellip;，0,0,0,0,0,0,0]</p>\n\n<p>人工智能[0,0,0,0,0,0,0,0,0,&hellip;&hellip;，1,0,0,0,0,0,0]</p>\n</blockquote>\n\n<p>比如上面的例子中，大数据 、云计算、机器学习和人工智能各对应一个向量，向量中只有一个值为1，其余都为0。所以使用 One-Hot Encoder有以下问题：</p>\n\n<ul>\n	<li>第一，词语编码是随机的，向量之间相互独立，看不出词语之间可能存在的关联关系。</li>\n	<li>第二，向量维度的大小取决于语料库中词语的多少，如果语料包含的所有词语对应的向量合为一个矩阵的话，那这个矩阵过于稀疏，并且会造成维度灾难。</li>\n</ul>\n\n<p>而解决这个问题的手段，就是使用向量表示（Vector Representations）。比如 Word2Vec 可以将 One-Hot Encoder 转化为低维度的连续值，也就是稠密向量，并且其中意思相近的词也将被映射到向量空间中相近的位置。经过降维，在二维空间中，相似的单词在空间中的距离也很接近。</p>\n\n<p>这里简单给词向量一个定义，词向量就是要用某个固定维度的向量去表示单词。也就是说要把单词变成固定维度的向量，作为机器学习（Machine Learning）或深度学习模型的特征向量输入。</p>\n\n<h4>动手实战词向量</h4>\n\n<p>（1）Word2Vec</p>\n\n<p>Word2Vec 是 Google 团队2013年推出的，自提出后被广泛应用在自然语言处理任务中，并且受到它的启发，后续出现了更多形式的词向量模型。Word2Vec 主要包含两种模型：Skip-Gram 和 CBOW，值得一提的是，Word2Vec 词向量可以较好地表达不同词之间的相似和类比关系。</p>\n\n<p>下面我们通过代码实战来体验一下 Word2Vec。通过&nbsp;<code>pip install gensim</code>&nbsp;安装好库后，即可导入使用。</p>\n\n<p><img alt=\"enter image description here\" src=\"http://images.gitbook.cn/9e0a13c0-6b01-11e8-8431-a75f9cd1b0ae\" /></p>\n\n<p>先导入 Gensim 中的 Word2Vec 和 jieba 分词器，再引入从百度百科抓取的黄河和长江的语料：</p>\n\n<pre>\n<code>from gensim.models import Word2Vec  \nimport jieba\n#定义停用词、标点符号\npunctuation = [\",\",\"。\", \":\", \";\", \".\", \"\'\", \'\"\', \"’\", \"?\", \"/\", \"-\", \"+\", \"&amp;\", \"(\", \")\"]\nsentences = [\n\"长江是中国第一大河，干流全长6397公里（以沱沱河为源），一般称6300公里。流域总面积一百八十余万平方公里，年平均入海水量约九千六百余亿立方米。以干流长度和入海水量论，长江均居世界第三位。\",\n\"黄河，中国古代也称河，发源于中华人民共和国青海省巴颜喀拉山脉，流经青海、四川、甘肃、宁夏、内蒙古、陕西、山西、河南、山东9个省区，最后于山东省东营垦利县注入渤海。干流河道全长5464千米，仅次于长江，为中国第二长河。黄河还是世界第五长河。\",\n\"黄河,是中华民族的母亲河。作为中华文明的发祥地,维系炎黄子孙的血脉.是中华民族民族精神与民族情感的象征。\",\n\"黄河被称为中华文明的母亲河。公元前2000多年华夏族在黄河领域的中原地区形成、繁衍。\",\n\"在兰州的“黄河第一桥”内蒙古托克托县河口镇以上的黄河河段为黄河上游。\",\n\"黄河上游根据河道特性的不同，又可分为河源段、峡谷段和冲积平原三部分。 \",\n\"黄河,是中华民族的母亲河。\"\n]</code></pre>\n\n<p>上面定义好语料，接下来进行分词，去标点符号操作 ：</p>\n\n<pre>\n<code>sentences = [jieba.lcut(sen) for sen in sentences]\ntokenized = []\nfor sentence in sentences:\n    words = []\n    for word in sentence:\n        if word not in punctuation:          \n            words.append(word)\n    tokenized.append(words)</code></pre>\n\n<p>这样我们获取的语料在分词之后，去掉了标点符号，如果做得更严谨，大家可以去停用词，然后进行模型训练：</p>\n\n<pre>\n<code>model = Word2Vec(tokenized, sg=1, size=100,  window=5,  min_count=2,  negative=1, sample=0.001, hs=1, workers=4)\n</code></pre>\n\n<p>参数解释如下：</p>\n\n<ul>\n	<li>sg=1 是&nbsp;<code>skip-gram</code>&nbsp;算法，对低频词敏感；默认 sg=0 为 CBOW 算法。</li>\n	<li>size 是输出词向量的维数，值太小会导致词映射因为冲突而影响结果，值太大则会耗内存并使算法计算变慢，一般值取为100到200之间。</li>\n	<li>window 是句子中当前词与目标词之间的最大距离，3表示在目标词前看3-b 个词，后面看 b 个词（b 在0-3之间随机）。</li>\n	<li><code>min_count</code>&nbsp;是对词进行过滤，频率小于&nbsp;<code>min-count</code>&nbsp;的单词则会被忽视，默认值为5。</li>\n	<li>negative 和 sample 可根据训练结果进行微调，sample 表示更高频率的词被随机下采样到所设置的阈值，默认值为 1e-3。</li>\n	<li>hs=1 表示层级 softmax 将会被使用，默认 hs=0 且 negative 不为0，则负采样将会被选择使用。</li>\n	<li>详细参数说明可查看 Word2Vec 源代码。</li>\n</ul>\n\n<p>训练后的模型可以保存与加载，如下代码所示：</p>\n\n<pre>\n<code>model.save(\'model\')  #保存模型\nmodel = Word2Vec.load(\'model\')   #加载模型</code></pre>\n\n<p>模型训练好之后，接下来就可以使用模型，可以用来计算句子或者词的相似性、最大匹配程度等。</p>\n\n<p>例如，我们判断一下黄河和黄河自己的相似度：</p>\n\n<pre>\n<code>print(model.similarity(\'黄河\', \'黄河\'))\n</code></pre>\n\n<p>结果输出为：</p>\n\n<blockquote>\n<p>1.0000000000000002</p>\n</blockquote>\n\n<p>例如，当输入黄河和长江来计算相似度的时候，结果就比较小，因为我们的语料实在太小了。</p>\n\n<pre>\n<code>print(model.similarity(\'黄河\', \'长江\'))\n</code></pre>\n\n<p>结果输出为：</p>\n\n<blockquote>\n<p>-0.036808977457324699</p>\n</blockquote>\n\n<p>下面我们预测最接近的词，预测与黄河和母亲河最接近，而与长江不接近的词：</p>\n\n<pre>\n<code>print(model.most_similar(positive=[\'黄河\', \'母亲河\'], negative=[\'长江\']))\n</code></pre>\n\n<p>得到结果如下，可以根据相似度大小找到与黄河和母亲河最接近的词（实际处理建议增大数据量和去停用词）。</p>\n\n<blockquote>\n<p>[(&#39;是&#39;, 0.14632007479667664), (&#39;以&#39;, 0.14630728960037231), (&#39;长河&#39;, 0.13878652453422546), (&#39;河道&#39;, 0.13716217875480652), (&#39;在&#39;, 0.11577725410461426), (&#39;全长&#39;, 0.10969121754169464), (&#39;内蒙古&#39;, 0.07590540498495102), (&#39;入海&#39;, 0.06970417499542236), (&#39;民族&#39;, 0.06064444035291672), (&#39;中华文明&#39;, 0.057667165994644165)]</p>\n</blockquote>\n\n<p>上面通过小数据量的语料实战，加强了对 Word2Vec 的理解，总之 Word2Vec 是一种将词变成词向量的工具。通俗点说，只有这样文本预料才转化为计算机能够计算的矩阵向量。</p>\n\n<p>（2）Doc2Vec</p>\n\n<p>Doc2Vec 是 Mikolov 在 Word2Vec 基础上提出的另一个用于计算长文本向量的工具。在 Gensim 库中，Doc2Vec 与 Word2Vec 都极为相似。但两者在对输入数据的预处理上稍有不同，Doc2vec 接收一个由 LabeledSentence 对象组成的迭代器作为其构造函数的输入参数。其中，LabeledSentence 是 Gensim 内建的一个类，它接收两个 List 作为其初始化的参数：word list 和 label list。</p>\n\n<p>Doc2Vec 也包括两种实现方式：DBOW（Distributed Bag of Words）和 DM （Distributed Memory）。DBOW 和 DM 的实现，二者在 gensim 库中的实现用的是同一个方法，该方法中参数 dm = 0 或者 dm=1 决定调用 DBOW 还是 DM。Doc2Vec 将文档语料通过一个固定长度的向量表达。</p>\n\n<p>下面是 Gensim 中 Doc2Vec 模型的实战，我们把上述语料每一句话当做一个文本，添加上对应的标签。接下来，定义数据预处理类，作用是给每个文章添加对应的标签：</p>\n\n<pre>\n<code>#定义数据预处理类，作用是给每个文章添加对应的标签\nfrom gensim.models.doc2vec import Doc2Vec,LabeledSentence\ndoc_labels = [\"长江\",\"黄河\",\"黄河\",\"黄河\",\"黄河\",\"黄河\",\"黄河\"]\nclass LabeledLineSentence(object):\n    def __init__(self, doc_list, labels_list):\n       self.labels_list = labels_list\n       self.doc_list = doc_list\n    def __iter__(self):\n        for idx, doc in enumerate(self.doc_list):\n            yield LabeledSentence(words=doc,tags=[self.labels_list[idx]])\n\n    model = Doc2Vec(documents,dm=1, size=100, window=8, min_count=5, workers=4)\n    model.save(\'model\')\n    model = Doc2Vec.load(\'model\')  </code></pre>\n\n<p>上面定义好了数据预处理函数，我们将 Word2Vec 中分词去标点后的数据，进行转换：</p>\n\n<pre>\n<code>iter_data = LabeledLineSentence(tokenized, doc_labels)\n</code></pre>\n\n<p>得到一个数据集，我开始定义模型参数，这里 dm=1，采用了 Gensim 中的 DM 实现。</p>\n\n<pre>\n<code>model = Doc2Vec(dm=1, size=100, window=8, min_count=5, workers=4)\nmodel.build_vocab(iter_data)</code></pre>\n\n<p>接下来训练模型， 设置迭代次数1000次，<code>start_alpha</code>&nbsp;为开始学习率，<code>end_alpha</code>&nbsp;与&nbsp;<code>start_alpha</code>&nbsp;线性递减。</p>\n\n<pre>\n<code>model.train(iter_data,total_examples=model.corpus_count,epochs=1000,start_alpha=0.01,end_alpha =0.001)\n</code></pre>\n\n<p>最后我们对模型进行一些预测：</p>\n\n<pre>\n<code>#根据标签找最相似的，这里只有黄河和长江，所以结果为长江，并计算出了相似度\nprint(model.docvecs.most_similar(\'黄河\'))\n</code></pre>\n\n<p>得到的结果：</p>\n\n<blockquote>\n<p>[(&#39;长江&#39;, 0.25543850660324097)]</p>\n</blockquote>\n\n<p>然后对黄河和长江标签做相似性计算：</p>\n\n<pre>\n<code>print(model.docvecs.similarity(\'黄河\',\'长江\'))\n</code></pre>\n\n<p>得到的结果：</p>\n\n<blockquote>\n<p>0.25543848271351405</p>\n</blockquote>\n\n<p>上面只是在小数据量进行的小练习，而最终影响模型准确率的因素有：文档的数量越多，文档的相似性越好，也就是基于大数据量的模型训练。在工业界，Word2Vec 和 Doc2Vec 常见的应用有：做相似词计算；相关词挖掘，在推荐系统中用在品牌、用户、商品挖掘中；上下文预测句子；机器翻译；作为特征输入其他模型等。</p>\n\n<p>总结，本文只是简单的介绍了词袋和词向量模型的典型应用，对于两者的理论和其他词向量模型，比如 TextRank 、FastText 和 GloVe 等，阅读文末给出参考文献将了解更多。</p>\n\n<p><strong>参考文献：</strong></p>\n\n<ol>\n	<li><a href=\"https://radimrehurek.com/gensim/tut1.html\">https://radimrehurek.com/gensim/tut1.html</a></li>\n	<li><a href=\"https://radimrehurek.com/gensim/models/word2vec.html\">https://radimrehurek.com/gensim/models/word2vec.html</a></li>\n	<li><a href=\"https://radimrehurek.com/gensim/summarization/summariser.html\">https://radimrehurek.com/gensim/summarization/summariser.html</a></li>\n	<li><a href=\"https://radimrehurek.com/gensim/models/fasttext.html\">https://radimrehurek.com/gensim/models/fasttext.html</a></li>\n	<li><a href=\"https://nlp.stanford.edu/projects/glove/\">https://nlp.stanford.edu/projects/glove/</a></li>\n</ol>\n\n<h2><a id=\"第06课：动手实战基于 ML 的中文短文本分类\" name=\"第06课：动手实战基于 ML 的中文短文本分类\"></a>第06课：动手实战基于 ML 的中文短文本分类</h2>\n\n<p>文本分类，属于有监督学习中的一部分，在很多场景下都有应用，下面通过小数据的实例，一步步完成中文短文本的分类实现，整个过程尽量做到少理论重实战。</p>\n\n<p><img alt=\"enter image description here\" src=\"http://images.gitbook.cn/f3726f10-7d0d-11e8-be78-bb5c0f92d7f1\" /></p>\n\n<p><strong>开发环境，我们选择</strong>：</p>\n\n<ol>\n	<li>Windows 系统</li>\n	<li>Python 3.6</li>\n	<li>Jupyter Notebook</li>\n</ol>\n\n<p>本文使用的数据是我曾经做过的一份司法数据，需求是对每一条输入数据，判断事情的主体是谁，比如报警人被老公打，报警人被老婆打，报警人被儿子打，报警人被女儿打等来进行文本有监督的分类操作。</p>\n\n<p><strong>整个过程分为以下几个步骤</strong>：</p>\n\n<ul>\n	<li>语料加载</li>\n	<li>分词</li>\n	<li>去停用词</li>\n	<li>抽取词向量特征</li>\n	<li>分别进行算法建模和模型训练</li>\n	<li>评估、计算 AUC 值</li>\n	<li>模型对比</li>\n</ul>\n\n<p><strong>基本流程如下图所示</strong>：</p>\n\n<p><img alt=\"enter image description here\" src=\"http://images.gitbook.cn/7cb83ee0-7d08-11e8-ab3b-8f5f773d8874\" /></p>\n\n<p>下面开始项目实战。</p>\n\n<p><strong>1.</strong>&nbsp;首先进行语料加载，在这之前，引入所需要的 Python 依赖包，并将全部语料和停用词字典读入内存中。</p>\n\n<p>第一步，引入依赖库，有随机数库、jieba 分词、pandas 库等：</p>\n\n<pre>\n<code>import random\nimport jieba\nimport pandas as pd\n</code></pre>\n\n<p>第二步，加载停用词字典，停用词词典为 stopwords.txt 文件，可以根据场景自己在该文本里面添加要去除的词（比如冠词、人称、数字等特定词）：</p>\n\n<pre>\n<code>#加载停用词\nstopwords=pd.read_csv(\'stopwords.txt\',index_col=False,quoting=3,sep=\"\\t\",names=[\'stopword\'], encoding=\'utf-8\')\nstopwords=stopwords[\'stopword\'].values</code></pre>\n\n<p>第三步，加载语料，语料是4个已经分好类的 csv 文件，直接用 pandas 加载即可，加载之后可以首先删除 nan 行，并提取要分词的 content 列转换为 list 列表：</p>\n\n<pre>\n<code>#加载语料\nlaogong_df = pd.read_csv(\'beilaogongda.csv\', encoding=\'utf-8\', sep=\',\')\nlaopo_df = pd.read_csv(\'beilaogongda.csv\', encoding=\'utf-8\', sep=\',\')\nerzi_df = pd.read_csv(\'beierzida.csv\', encoding=\'utf-8\', sep=\',\')\nnver_df = pd.read_csv(\'beinverda.csv\', encoding=\'utf-8\', sep=\',\')\n#删除语料的nan行\nlaogong_df.dropna(inplace=True)\nlaopo_df.dropna(inplace=True)\nerzi_df.dropna(inplace=True)\nnver_df.dropna(inplace=True)\n#转换\nlaogong = laogong_df.segment.values.tolist()\nlaopo = laopo_df.segment.values.tolist()\nerzi = erzi_df.segment.values.tolist()\nnver = nver_df.segment.values.tolist()</code></pre>\n\n<p><strong>2.</strong>&nbsp;分词和去停用词。</p>\n\n<p>第一步，定义分词、去停用词和批量打标签的函数，函数包含3个参数：<code>content_lines</code>&nbsp;参数为语料列表；sentences 参数为预先定义的 list，用来存储分词并打标签后的结果；category 参数为标签 ：</p>\n\n<pre>\n<code>#定义分词和打标签函数preprocess_text\n#参数content_lines即为上面转换的list\n#参数sentences是定义的空list，用来储存打标签之后的数据\n#参数category 是类型标签\ndef preprocess_text(content_lines, sentences, category):\n    for line in content_lines:\n        try:\n            segs=jieba.lcut(line)\n            segs = [v for v in segs if not str(v).isdigit()]#去数字\n            segs = list(filter(lambda x:x.strip(), segs))   #去左右空格\n            segs = list(filter(lambda x:len(x)&gt;1, segs)) #长度为1的字符\n            segs = list(filter(lambda x:x not in stopwords, segs)) #去掉停用词\n            sentences.append((\" \".join(segs), category))# 打标签\n        except Exception:\n            print(line)\n            continue </code></pre>\n\n<p>第二步，调用函数、生成训练数据，根据我提供的司法语料数据，分为报警人被老公打，报警人被老婆打，报警人被儿子打，报警人被女儿打，标签分别为0、1、2、3，具体如下：</p>\n\n<pre>\n<code>sentences = []\npreprocess_text(laogong, sentences,0)\npreprocess_text(laopo, sentences, 1)\npreprocess_text(erzi, sentences, 2)\npreprocess_text(nver, sentences, 3)</code></pre>\n\n<p>第三步，将得到的数据集打散，生成更可靠的训练集分布，避免同类数据分布不均匀：</p>\n\n<pre>\n<code>random.shuffle(sentences)\n</code></pre>\n\n<p>第四步，我们在控制台输出前10条数据，观察一下：</p>\n\n<pre>\n<code>for sentence in sentences[:10]:\n    print(sentence[0], sentence[1])  #下标0是词列表，1是标签</code></pre>\n\n<p>得到的结果如图所示：</p>\n\n<p><img alt=\"enter image description here\" src=\"http://images.gitbook.cn/ce9d3950-7d0c-11e8-be78-bb5c0f92d7f1\" /></p>\n\n<p><strong>3.</strong>&nbsp;抽取词向量特征。</p>\n\n<p>第一步，抽取特征，我们定义文本抽取词袋模型特征：</p>\n\n<pre>\n<code>from sklearn.feature_extraction.text import CountVectorizer\nvec = CountVectorizer(\n    analyzer=\'word\', # tokenise by character ngrams\n    max_features=4000,  # keep the most common 1000 ngrams\n)</code></pre>\n\n<p>第二步，把语料数据切分，用&nbsp;<code>sk-learn</code>&nbsp;对数据切分，分成训练集和测试集：</p>\n\n<pre>\n<code>from sklearn.model_selection import train_test_split\nx, y = zip(*sentences)\nx_train, x_test, y_train, y_test = train_test_split(x, y, random_state=1256)</code></pre>\n\n<p>第三步，把训练数据转换为词袋模型：</p>\n\n<pre>\n<code>vec.fit(x_train)\n</code></pre>\n\n<p><strong>4.</strong>&nbsp;分别进行算法建模和模型训练。</p>\n\n<p>定义朴素贝叶斯模型，然后对训练集进行模型训练，直接使用 sklearn 中的 MultinomialNB：</p>\n\n<pre>\n<code>from sklearn.naive_bayes import MultinomialNB\nclassifier = MultinomialNB()\nclassifier.fit(vec.transform(x_train), y_train)</code></pre>\n\n<p><strong>5.</strong>&nbsp;评估、计算 AUC 值。</p>\n\n<p>第一步，上面步骤1-4完成了从语料到模型的训练，训练之后，我们要用测试集来计算 AUC 值：</p>\n\n<pre>\n<code>print(classifier.score(vec.transform(x_test), y_test))\n</code></pre>\n\n<p>得到的结果评分为：0.647331786543。</p>\n\n<p>第二步，进行测试集的预测：</p>\n\n<pre>\n<code>pre = classifier.predict(vec.transform(x_test))\n</code></pre>\n\n<p><strong>6.</strong>&nbsp;模型对比。</p>\n\n<p>整个模型从语料到训练评估步骤1-5就完成了，接下来我们来看看，改变特征向量模型和训练模型对结果有什么变化。</p>\n\n<p>（1）改变特征向量模型</p>\n\n<p>下面可以把特征做得更强一点，尝试加入抽取&nbsp;<code>2-gram</code>&nbsp;和&nbsp;<code>3-gram</code>&nbsp;的统计特征，把词库的量放大一点。</p>\n\n<pre>\n<code>from sklearn.feature_extraction.text import CountVectorizer\nvec = CountVectorizer(\n    analyzer=\'word\', # tokenise by character ngrams\n    ngram_range=(1,4),  # use ngrams of size 1 and 2\n    max_features=20000,  # keep the most common 1000 ngrams\n)\nvec.fit(x_train)\n#用朴素贝叶斯算法进行模型训练\nclassifier = MultinomialNB()\nclassifier.fit(vec.transform(x_train), y_train)\n#对结果进行评分\nprint(classifier.score(vec.transform(x_test), y_test))\n</code></pre>\n\n<p>得到的结果评分为：0.649651972158，确实有一点提高，但是不太明显。</p>\n\n<p>（2）改变训练模型</p>\n\n<p>使用 SVM 训练：</p>\n\n<pre>\n<code>from sklearn.svm import SVC\nsvm = SVC(kernel=\'linear\')\nsvm.fit(vec.transform(x_train), y_train)\nprint(svm.score(vec.transform(x_test), y_test))</code></pre>\n\n<p>使用决策树、随机森林、XGBoost、神经网络等等：</p>\n\n<pre>\n<code>import xgboost as xgb  \nfrom sklearn.model_selection import StratifiedKFold  \nimport numpy as np\n# xgb矩阵赋值  \nxgb_train = xgb.DMatrix(vec.transform(x_train), label=y_train)  \nxgb_test = xgb.DMatrix(vec.transform(x_test)) </code></pre>\n\n<p>在 XGBoost 中，下面主要是调参指标，可以根据参数进行调参：</p>\n\n<pre>\n<code>params = {  \n        \'booster\': \'gbtree\',     #使用gbtree\n        \'objective\': \'multi:softmax\',  # 多分类的问题、  \n        # \'objective\': \'multi:softprob\',   # 多分类概率  \n        #\'objective\': \'binary:logistic\',  #二分类\n        \'eval_metric\': \'merror\',   #logloss\n        \'num_class\': 4,  # 类别数，与 multisoftmax 并用  \n        \'gamma\': 0.1,  # 用于控制是否后剪枝的参数,越大越保守，一般0.1、0.2这样子。  \n        \'max_depth\': 8,  # 构建树的深度，越大越容易过拟合  \n        \'alpha\': 0,   # L1正则化系数  \n        \'lambda\': 10,  # 控制模型复杂度的权重值的L2正则化项参数，参数越大，模型越不容易过拟合。  \n        \'subsample\': 0.7,  # 随机采样训练样本  \n        \'colsample_bytree\': 0.5,  # 生成树时进行的列采样  \n        \'min_child_weight\': 3,  \n        # 这个参数默认是 1，是每个叶子里面 h 的和至少是多少，对正负样本不均衡时的 0-1 分类而言  \n        # 假设 h 在 0.01 附近，min_child_weight 为 1 叶子节点中最少需要包含 100 个样本。  \n        \'silent\': 0,  # 设置成1则没有运行信息输出，最好是设置为0.  \n        \'eta\': 0.03,  # 如同学习率  \n        \'seed\': 1000,  \n        \'nthread\': -1,  # cpu 线程数  \n        \'missing\': 1 \n    }</code></pre>\n\n<p><strong>总结</strong></p>\n\n<p>上面通过真实司法数据，一步步实现中文短文本分类的方法，整个示例代码可以当做模板来用，从优化和提高模型准确率来说，主要有两方面可以尝试：</p>\n\n<ol>\n	<li>特征向量的构建，除了词袋模型，可以考虑使用 word2vec 和 doc2vec 等；</li>\n	<li>模型上可以选择有监督的分类算法、集成学习以及神经网络等。</li>\n</ol>\n\n<p>最后如果想了解更多，推荐我的两篇 Chat 文章：</p>\n\n<ul>\n	<li><a href=\"http://gitbook.cn/gitchat/activity/5ae2c9475d06502947fb1d98\">NLP 中文短文本分类项目实践（上）</a></li>\n	<li><a href=\"http://gitbook.cn/gitchat/activity/5afcf897109cd76e3c1dcd99\">NLP 中文短文本分类项目实践（下）</a></li>\n</ul>\n\n<h2><a id=\"第07课：动手实战基于 ML 的中文短文本聚类\" name=\"第07课：动手实战基于 ML 的中文短文本聚类\"></a>第07课：动手实战基于 ML 的中文短文本聚类</h2>\n\n<p>关于文本聚类，我曾在 Chat<a href=\"http://gitbook.cn/gitchat/activity/5b15556785040e095b60d67a\">《NLP 中文文本聚类之无监督学习》</a>中介绍过，文本聚类是将一个个文档由原有的自然语言文字信息转化成数学信息，以高维空间点的形式展现出来，通过计算哪些点距离比较近，从而将那些点聚成一个簇，簇的中心叫做簇心。一个好的聚类要保证簇内点的距离尽量的近，但簇与簇之间的点要尽量的远。</p>\n\n<p>如下图，以 K、M、N 三个点分别为聚类的簇心，将结果聚为三类，使得簇内点的距离尽量的近，但簇与簇之间的点尽量的远。</p>\n\n<p><img alt=\"enter image description here\" src=\"http://images.gitbook.cn/ee62afe0-7d2f-11e8-8748-9f97e9dc7c3b\" /></p>\n\n<p><strong>开发环境，我们选择：</strong>：</p>\n\n<ol>\n	<li>Windows 系统</li>\n	<li>Python 3.6</li>\n	<li>Jupyter Notebook</li>\n</ol>\n\n<p>本文继续沿用上篇文本分类中的语料来进行文本无监督聚类操作。</p>\n\n<p><strong>整个过程分为以下几个步骤</strong>：</p>\n\n<ul>\n	<li>语料加载</li>\n	<li>分词</li>\n	<li>去停用词</li>\n	<li>抽取词向量特征</li>\n	<li>实战 TF-IDF 的中文文本 K-means 聚类</li>\n	<li>实战 word2Vec 的中文文本 K-means 聚类</li>\n</ul>\n\n<p>下面开始项目实战。</p>\n\n<p><strong>1.</strong>&nbsp;首先进行语料加载，在这之前，引入所需要的 Python 依赖包，并将全部语料和停用词字典读入内存中。</p>\n\n<p>第一步，引入依赖库，有随机数库、jieba 分词、pandas 库等：</p>\n\n<pre>\n<code>import random\nimport jieba\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nimport gensim\nfrom gensim.models import Word2Vec\nfrom sklearn.preprocessing import scale\nimport multiprocessing</code></pre>\n\n<p>第二步，加载停用词字典，停用词词典为 stopwords.txt 文件，可以根据场景自己在该文本里面添加要去除的词（比如冠词、人称、数字等特定词）：</p>\n\n<pre>\n<code>#加载停用词\nstopwords=pd.read_csv(\'stopwords.txt\',index_col=False,quoting=3,sep=\"\\t\",names=[\'stopword\'], encoding=\'utf-8\')\nstopwords=stopwords[\'stopword\'].values</code></pre>\n\n<p>第三步，加载语料，语料是4个已经分好类的 csv 文件，直接用 pandas 加载即可，加载之后可以首先删除 nan 行，并提取要分词的 content 列转换为 list 列表：</p>\n\n<pre>\n<code>#加载语料\nlaogong_df = pd.read_csv(\'beilaogongda.csv\', encoding=\'utf-8\', sep=\',\')\nlaopo_df = pd.read_csv(\'beilaogongda.csv\', encoding=\'utf-8\', sep=\',\')\nerzi_df = pd.read_csv(\'beierzida.csv\', encoding=\'utf-8\', sep=\',\')\nnver_df = pd.read_csv(\'beinverda.csv\', encoding=\'utf-8\', sep=\',\')\n#删除语料的nan行\nlaogong_df.dropna(inplace=True)\nlaopo_df.dropna(inplace=True)\nerzi_df.dropna(inplace=True)\nnver_df.dropna(inplace=True)\n#转换\nlaogong = laogong_df.segment.values.tolist()\nlaopo = laopo_df.segment.values.tolist()\nerzi = erzi_df.segment.values.tolist()\nnver = nver_df.segment.values.tolist()</code></pre>\n\n<p><strong>2.</strong>&nbsp;分词和去停用词。</p>\n\n<p>第一步，定义分词、去停用词的函数，函数包含两个参数：<code>content_lines</code>&nbsp;参数为语料列表；sentences 参数为预先定义的 list，用来存储分词后的结果：</p>\n\n<pre>\n<code>#定义分词函数preprocess_text\n#参数content_lines即为上面转换的list\n#参数sentences是定义的空list，用来储存分词后的数据\n\ndef preprocess_text(content_lines, sentences):\n    for line in content_lines:\n        try:\n            segs=jieba.lcut(line)\n            segs = [v for v in segs if not str(v).isdigit()]#去数字\n            segs = list(filter(lambda x:x.strip(), segs))   #去左右空格\n            segs = list(filter(lambda x:len(x)&gt;1, segs)) #长度为1的字符\n            segs = list(filter(lambda x:x not in stopwords, segs)) #去掉停用词\n            sentences.append(\" \".join(segs))\n        except Exception:\n            print(line)\n            continue </code></pre>\n\n<p>第二步，调用函数、生成训练数据，根据我提供的司法语料数据，分为报警人被老公打，报警人被老婆打，报警人被儿子打，报警人被女儿打，具体如下：</p>\n\n<pre>\n<code>sentences = []\npreprocess_text(laogong, sentences)\npreprocess_text(laopo, sentences)\npreprocess_text(erzi, sentences)\npreprocess_text(nver, sentences)</code></pre>\n\n<p>第三步，将得到的数据集打散，生成更可靠的训练集分布，避免同类数据分布不均匀：</p>\n\n<pre>\n<code>random.shuffle(sentences)\n</code></pre>\n\n<p>第四步，我们控制台输出前10条数据，观察一下（因为上面进行了随机打散，你看到的前10条可能不一样）：</p>\n\n<pre>\n<code>for sentence in sentences[:10]:\n    print(sentenc)</code></pre>\n\n<p>得到的结果聚类和分类是不同的，这里没有标签：</p>\n\n<p><img alt=\"enter image description here\" src=\"http://images.gitbook.cn/bddb2e50-7d94-11e8-be78-bb5c0f92d7f1\" /></p>\n\n<p><strong>3.</strong>&nbsp;抽取词向量特征。</p>\n\n<p>抽取特征，将文本中的词语转换为词频矩阵，统计每个词语的&nbsp;<code>tf-idf</code>&nbsp;权值，获得词在对应文本中的&nbsp;<code>tf-idf</code>&nbsp;权重：</p>\n\n<pre>\n<code>#将文本中的词语转换为词频矩阵 矩阵元素a[i][j] 表示j词在i类文本下的词频\nvectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5)\n#统计每个词语的tf-idf权值\ntransformer = TfidfTransformer()\n# 第一个fit_transform是计算tf-idf 第二个fit_transform是将文本转为词频矩阵\ntfidf = transformer.fit_transform(vectorizer.fit_transform(sentences))\n# 获取词袋模型中的所有词语\nword = vectorizer.get_feature_names()\n# 将tf-idf矩阵抽取出来，元素w[i][j]表示j词在i类文本中的tf-idf权重\nweight = tfidf.toarray()\n#查看特征大小\nprint (\'Features length: \' + str(len(word)))</code></pre>\n\n<p><strong>4.</strong>&nbsp;实战&nbsp;<code>TF-IDF</code>&nbsp;的中文文本&nbsp;<code>K-means</code>&nbsp;聚类</p>\n\n<p>第一步，使用&nbsp;<code>k-means++</code>&nbsp;来初始化模型，当然也可以选择随机初始化，即<code>init=&quot;random&quot;</code>，然后通过 PCA 降维把上面的权重 weight 降到10维，进行聚类模型训练：</p>\n\n<pre>\n<code>numClass=4 #聚类分几簇\nclf = KMeans(n_clusters=numClass, max_iter=10000, init=\"k-means++\", tol=1e-6)  #这里也可以选择随机初始化init=\"random\"\npca = PCA(n_components=10)  # 降维\nTnewData = pca.fit_transform(weight)  # 载入N维\ns = clf.fit(TnewData)</code></pre>\n\n<p>第二步，定义聚类结果可视化函数<code>plot_cluster(result,newData,numClass)</code>，该函数包含3个参数，其中 result 表示聚类拟合的结果集；newData 表示权重 weight 降维的结果，这里需要降维到2维，即平面可视化；numClass 表示聚类分为几簇，绘制代码第一部分绘制结果 newData，第二部分绘制聚类的中心点：</p>\n\n<pre>\n<code>def plot_cluster(result,newData,numClass):\n    plt.figure(2)\n    Lab = [[] for i in range(numClass)]\n    index = 0\n    for labi in result:\n        Lab[labi].append(index)\n        index += 1\n    color = [\'oy\', \'ob\', \'og\', \'cs\', \'ms\', \'bs\', \'ks\', \'ys\', \'yv\', \'mv\', \'bv\', \'kv\', \'gv\', \'y^\', \'m^\', \'b^\', \'k^\',\n             \'g^\'] * 3 \n    for i in range(numClass):\n        x1 = []\n        y1 = []\n        for ind1 in newData[Lab[i]]:\n            # print ind1\n            try:\n                y1.append(ind1[1])\n                x1.append(ind1[0])\n            except:\n                pass\n        plt.plot(x1, y1, color[i])\n\n    #绘制初始中心点\n    x1 = []\n    y1 = []\n    for ind1 in clf.cluster_centers_:\n        try:\n            y1.append(ind1[1])\n            x1.append(ind1[0])\n        except:\n            pass\n    plt.plot(x1, y1, \"rv\") #绘制中心\n    plt.show()</code></pre>\n\n<p>第三步，对数据降维到2维，然后获得结果，最后绘制聚类结果图：</p>\n\n<pre>\n<code>pca = PCA(n_components=2)  # 输出两维\nnewData = pca.fit_transform(weight)  # 载入N维\nresult = list(clf.predict(TnewData))\nplot_cluster(result,newData,numClass)</code></pre>\n\n<p>第四步，得到的聚类结果图，4个中心点和4个簇，我们看到结果还比较好，簇的边界很清楚：</p>\n\n<p><img alt=\"enter image description here\" src=\"http://images.gitbook.cn/6d68b500-7da3-11e8-8748-9f97e9dc7c3b\" /></p>\n\n<p>第五步，上面演示的可视化过程，降维使用了 PCA，我们还可以试试 TSNE，两者同为降维工具，主要区别在于，所在的包不同（也即机制和原理不同）：</p>\n\n<pre>\n<code>from sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n</code></pre>\n\n<p>因为原理不同，导致 TSNE 保留下的属性信息，更具代表性，也即最能体现样本间的差异，但是 TSNE 运行极慢，PCA 则相对较快，下面看看 TSNE 运行的可视化结果：</p>\n\n<pre>\n<code>from sklearn.manifold import TSNE\nts =TSNE(2)\nnewData = ts.fit_transform(weight)\nresult = list(clf.predict(TnewData))\nplot_cluster(result,newData,numClass)</code></pre>\n\n<p>得到的可视化结果，为一个中心点，不同簇落在围绕中心点的不同半径之内，我们看到在这里结果并不是很好：</p>\n\n<p><img alt=\"enter image description here\" src=\"http://images.gitbook.cn/4445a4c0-7da9-11e8-8a07-2345656531ad\" /></p>\n\n<p>第六步，为了更好的表达和获取更具有代表性的信息，在展示（可视化）高维数据时，更为一般的处理，常常先用 PCA 进行降维，再使用 TSNE：</p>\n\n<pre>\n<code>from sklearn.manifold import TSNE\nnewData = PCA(n_components=4).fit_transform(weight)  # 载入N维\nnewData =TSNE(2).fit_transform(newData)\nresult = list(clf.predict(TnewData))\nplot_cluster(result,newData,numClass)</code></pre>\n\n<p>得到的可视化结果，不同簇落在围绕中心点的不同半径之内：</p>\n\n<p><img alt=\"enter image description here\" src=\"http://images.gitbook.cn/3809f890-7daa-11e8-8a07-2345656531ad\" /></p>\n\n<p><strong>总结</strong></p>\n\n<p>上面通过真实小案例，对司法数据一步步实现中文短文本聚类，从优化和提高模型准确率来说，主要有两方面可以尝试：</p>\n\n<ol>\n	<li>特征向量的构建，除了词袋模型，可以考虑使用 word2vec 和 doc2vec 等；</li>\n	<li>模型上可以采用基于密度的 DBSCAN、层次聚类等算法。</li>\n</ol>\n\n<h2><a id=\"第08课：从自然语言处理角度看 HMM 和 CRF\" name=\"第08课：从自然语言处理角度看 HMM 和 CRF\"></a>第08课：从自然语言处理角度看 HMM 和 CRF</h2>\n\n<p>近几年在自然语言处理领域中，HMM（隐马尔可夫模型）和 CRF（条件随机场）算法常常被用于分词、句法分析、命名实体识别、词性标注等。由于两者之间有很大的共同点，所以在很多应用上往往是重叠的，但在命名实体、句法分析等领域 CRF 似乎更胜一筹。通常来说如果做自然语言处理，这两个模型应该都要了解，下面我们来看看本文的内容。</p>\n\n<h3>从贝叶斯定义理解生成式模型和判别式模型</h3>\n\n<p>理解 HMM（隐马尔可夫模型）和 CRF（条件随机场）模型之前，我们先来看两个概念：生成式模型和判别式模型。</p>\n\n<p>在机器学习中，生成式模型和判别式模型都用于有监督学习，有监督学习的任务就是从数据中学习一个模型（也叫分类器），应用这一模型，对给定的输入 X 预测相应的输出 Y。这个模型的一般形式为：决策函数 Y=f(X) 或者条件概率分布 P(Y|X)。</p>\n\n<p>首先，简单从贝叶斯定理说起，若记 P(A)、P(B) 分别表示事件 A 和事件 B 发生的概率，则 P(A|B) 表示事件 B 发生的情况下事件 A 发生的概率；P(AB)表示事件 A 和事件 B 同时发生的概率。</p>\n\n<p>根据贝叶斯公式可以得出：</p>\n\n<p><img alt=\"enter image description here\" src=\"http://images.gitbook.cn/ba258630-7e8d-11e8-abd3-eb6d72babbec\" /></p>\n\n<p><strong>生成式模型：</strong>估计的是联合概率分布，P(Y, X)=P(Y|X)*P(X)，由联合概率密度分布 P(X,Y)，然后求出条件概率分布 P(Y|X) 作为预测的模型，即生成模型公式为：P(Y|X)= P(X,Y)/ P(X)。基本思想是首先建立样本的联合概率密度模型 P(X,Y)，然后再得到后验概率 P(Y|X)，再利用它进行分类，其主要关心的是给定输入 X 产生输出 Y 的生成关系。</p>\n\n<p><strong>判别式模型：</strong>估计的是条件概率分布， P(Y|X)，是给定观测变量 X 和目标变量 Y 的条件模型。由数据直接学习决策函数 Y=f(X) 或者条件概率分布 P(Y|X) 作为预测的模型，其主要关心的是对于给定的输入 X，应该预测什么样的输出 Y。</p>\n\n<p>所以，HMM 使用隐含变量生成可观测状态，其生成概率有标注集统计得到，是一个生成模型。其他常见的生成式模型有：Gaussian、 Naive Bayes、Mixtures of multinomials 等。</p>\n\n<p>而 CRF 就像一个反向的隐马尔可夫模型（HMM），通过可观测状态判别隐含变量，其概率亦通过标注集统计得来，是一个判别模型。其他常见的判别式模型有：K 近邻法、感知机、决策树、逻辑斯谛回归模型、最大熵模型、支持向量机、提升方法等。</p>\n\n<p>HMM（隐马尔可夫模型）和 CRF（条件随机场）的理论部分，推荐看周志华老师的西瓜书《机器学习》。</p>\n\n<h3>动手实战：基于 HMM 训练自己的 Python 中文分词器</h3>\n\n<h4>模型介绍</h4>\n\n<p>HMM 模型是由一个&ldquo;五元组&rdquo;组成的集合：</p>\n\n<ul>\n	<li>\n	<p>StatusSet：状态值集合，状态值集合为 (B, M, E, S)，其中 B 为词的首个字，M 为词中间的字，E 为词语中最后一个字，S 为单个字，B、M、E、S 每个状态代表的是该字在词语中的位置。</p>\n\n	<p>举个例子，对&ldquo;中国的人工智能发展进入高潮阶段&rdquo;，分词可以标注为：&ldquo;中B国E的S人B工E智B能E发B展E进B入E高B潮E阶B段E&rdquo;，最后的分词结果为：[&#39;中国&#39;, &#39;的&#39;, &#39;人工&#39;, &#39;智能&#39;, &#39;发展&#39;, &#39;进入&#39;, &#39;高潮&#39;, &#39;阶段&#39;]。</p>\n	</li>\n	<li>\n	<p>ObservedSet：观察值集合，观察值集合就是所有语料的汉字，甚至包括标点符号所组成的集合。</p>\n	</li>\n	<li>\n	<p>TransProbMatrix：转移概率矩阵，状态转移概率矩阵的含义就是从状态 X 转移到状态 Y 的概率，是一个4&times;4的矩阵，即 {B,E,M,S}&times;{B,E,M,S}。</p>\n	</li>\n	<li>\n	<p>EmitProbMatrix：发射概率矩阵，发射概率矩阵的每个元素都是一个条件概率，代表 P(Observed[i]|Status[j]) 概率。</p>\n	</li>\n	<li>\n	<p>InitStatus：初始状态分布，初始状态概率分布表示句子的第一个字属于 {B,E,M,S} 这四种状态的概率。</p>\n	</li>\n</ul>\n\n<p>将 HMM 应用在分词上，要解决的问题是：参数（ObservedSet、TransProbMatrix、EmitRobMatrix、InitStatus）已知的情况下，求解状态值序列。</p>\n\n<p>解决这个问题的最有名的方法是 Viterbi 算法。</p>\n\n<h4>语料准备</h4>\n\n<p>本次训练使用的预料&nbsp;<code>syj_trainCorpus_utf8.txt</code>&nbsp;是我爬取的短文本处理生成的。整个语料大小 264M，包含1116903条数据，UTF-8 编码，词与词之间用空格隔开，用来训练分词模型。</p>\n\n<p>语料已上传到 CSDN 资源上，下载地址请点击：<a href=\"https://download.csdn.net/download/qq_36330643/10514771\">中文自然语言处理中文分词训练语料</a>&nbsp;。</p>\n\n<p>语料格式，用空格隔开的：</p>\n\n<blockquote>\n<p>如果 继续 听任 资产阶级 自由化 的 思潮 泛滥 ，</p>\n\n<p>党 就 失去 了 凝聚力 和 战斗力 ，</p>\n\n<p>怎么 能 成为 全国 人民 的 领导 核心 ？</p>\n\n<p>中国 又 会 成为 一盘散沙 ，</p>\n\n<p>那 还有 什么 希望 ？</p>\n</blockquote>\n\n<h4>编码实现</h4>\n\n<p>（1）预定义</p>\n\n<p>首先引出库，这两个库的作用是用来模型保存的：</p>\n\n<pre>\n<code>import pickle\nimport json\n</code></pre>\n\n<p>接下来定义 HMM 中的状态，初始化概率，以及中文停顿词：</p>\n\n<pre>\n<code>STATES = {\'B\', \'M\', \'E\', \'S\'}\nEPS = 0.0001\n#定义停顿标点\nseg_stop_words = {\" \",\"，\",\"。\",\"“\",\"”\",\'“\', \"？\", \"！\", \"：\", \"《\", \"》\", \"、\", \"；\", \"·\", \"‘ \", \"’\", \"──\", \",\", \".\", \"?\", \"!\", \"`\", \"~\", \"@\", \"#\", \"$\", \"%\", \"^\", \"&amp;\", \"*\", \"(\", \")\", \"-\", \"_\", \"+\", \"=\", \"[\", \"]\", \"{\", \"}\", \'\"\', \"\'\", \"&lt;\", \"&gt;\", \"\\\\\", \"|\" \"\\r\", \"\\n\",\"\\t\"}\n</code></pre>\n\n<p>（2）面向对象封装成类</p>\n\n<p>首先，将 HMM 模型封装为独立的类&nbsp;<code>HMM_Model</code>，下面先给出类的结构定义：</p>\n\n<pre>\n<code>class HMM_Model:\n    def __init__(self):\n        pass\n    #初始化    \n    def setup(self):\n        pass\n     #模型保存   \n    def save(self, filename, code):\n        pass\n    #模型加载\n    def load(self, filename, code):\n        pass\n    #模型训练\n    def do_train(self, observes, states):\n        pass\n    #HMM计算\n    def get_prob(self):\n        pass\n    #模型预测\n    def do_predict(self, sequence):\n        pass</code></pre>\n\n<p>第一个方法&nbsp;<code>__init__()</code>&nbsp;是一种特殊的方法，被称为类的构造函数或初始化方法，当创建了这个类的实例时就会调用该方法，其中定义了数据结构和初始变量，实现如下：</p>\n\n<pre>\n<code>def __init__(self):\n        self.trans_mat = {}  \n        self.emit_mat = {} \n        self.init_vec = {}  \n        self.state_count = {} \n        self.states = {}\n        self.inited = False</code></pre>\n\n<p>其中的数据结构定义：</p>\n\n<ul>\n	<li>\n	<p><code>trans_mat</code>：状态转移矩阵，<code>trans_mat[state1][state2]</code>&nbsp;表示训练集中由 state1 转移到 state2 的次数。</p>\n	</li>\n	<li>\n	<p><code>emit_mat</code>：观测矩阵，<code>emit_mat[state][char]</code>&nbsp;表示训练集中单字 char 被标注为 state 的次数。</p>\n	</li>\n	<li>\n	<p><code>init_vec</code>：初始状态分布向量，<code>init_vec[state]</code>&nbsp;表示状态 state 在训练集中出现的次数。</p>\n	</li>\n	<li>\n	<p><code>state_count</code>：状态统计向量，<code>state_count[state]</code>表示状态 state 出现的次数。</p>\n	</li>\n	<li>\n	<p><code>word_set</code>：词集合，包含所有单词。</p>\n	</li>\n</ul>\n\n<p>第二个方法 setup()，初始化第一个方法中的数据结构，具体实现如下：</p>\n\n<pre>\n<code>#初始化数据结构    \ndef setup(self):\n    for state in self.states:\n        # build trans_mat\n        self.trans_mat[state] = {}\n        for target in self.states:\n            self.trans_mat[state][target] = 0.0\n        self.emit_mat[state] = {}\n        self.init_vec[state] = 0\n        self.state_count[state] = 0\n    self.inited = True</code></pre>\n\n<p>第三个方法 save()，用来保存训练好的模型，filename 指定模型名称，默认模型名称为 hmm.json，这里提供两种格式的保存类型，JSON 或者 pickle 格式，通过参数 code 来决定，code 的值为&nbsp;<code>code=&#39;json&#39;</code>&nbsp;或者&nbsp;<code>code = &#39;pickle&#39;</code>，默认为&nbsp;<code>code=&#39;json&#39;</code>，具体实现如下：</p>\n\n<pre>\n<code>#模型保存   \ndef save(self, filename=\"hmm.json\", code=\'json\'):\n    fw = open(filename, \'w\', encoding=\'utf-8\')\n    data = {\n        \"trans_mat\": self.trans_mat,\n        \"emit_mat\": self.emit_mat,\n        \"init_vec\": self.init_vec,\n        \"state_count\": self.state_count\n    }\n    if code == \"json\":\n        txt = json.dumps(data)\n        txt = txt.encode(\'utf-8\').decode(\'unicode-escape\')\n        fw.write(txt)\n    elif code == \"pickle\":\n        pickle.dump(data, fw)\n    fw.close()</code></pre>\n\n<p>第四个方法 load()，与第三个 save() 方法对应，用来加载模型，filename 指定模型名称，默认模型名称为 hmm.json，这里提供两种格式的保存类型，JSON 或者 pickle 格式，通过参数 code 来决定，code 的值为&nbsp;<code>code=&#39;json&#39;</code>&nbsp;或者<code>code = &#39;pickle&#39;</code>，默认为&nbsp;<code>code=&#39;json&#39;</code>，具体实现如下：</p>\n\n<pre>\n<code>#模型加载\ndef load(self, filename=\"hmm.json\", code=\"json\"):\n    fr = open(filename, \'r\', encoding=\'utf-8\')\n    if code == \"json\":\n        txt = fr.read()\n        model = json.loads(txt)\n    elif code == \"pickle\":\n        model = pickle.load(fr)\n    self.trans_mat = model[\"trans_mat\"]\n    self.emit_mat = model[\"emit_mat\"]\n    self.init_vec = model[\"init_vec\"]\n    self.state_count = model[\"state_count\"]\n    self.inited = True\n    fr.close()</code></pre>\n\n<p>第五个方法&nbsp;<code>do_train()</code>，用来训练模型，因为使用的标注数据集， 因此可以使用更简单的监督学习算法，训练函数输入观测序列和状态序列进行训练， 依次更新各矩阵数据。类中维护的模型参数均为频数而非频率， 这样的设计使得模型可以进行在线训练，使得模型随时都可以接受新的训练数据继续训练，不会丢失前次训练的结果。具体实现如下：</p>\n\n<pre>\n<code>#模型训练\ndef do_train(self, observes, states):\n    if not self.inited:\n        self.setup()\n\n    for i in range(len(states)):\n        if i == 0:\n            self.init_vec[states[0]] += 1\n            self.state_count[states[0]] += 1\n        else:\n            self.trans_mat[states[i - 1]][states[i]] += 1\n            self.state_count[states[i]] += 1\n            if observes[i] not in self.emit_mat[states[i]]:\n                self.emit_mat[states[i]][observes[i]] = 1\n            else:\n                self.emit_mat[states[i]][observes[i]] += 1</code></pre>\n\n<p>第六个方法&nbsp;<code>get_prob()</code>，在进行预测前，需将数据结构的频数转换为频率，具体实现如下：</p>\n\n<pre>\n<code>#频数转频率\ndef get_prob(self):\n    init_vec = {}\n    trans_mat = {}\n    emit_mat = {}\n    default = max(self.state_count.values())  \n\n    for key in self.init_vec:\n        if self.state_count[key] != 0:\n            init_vec[key] = float(self.init_vec[key]) / self.state_count[key]\n        else:\n            init_vec[key] = float(self.init_vec[key]) / default\n\n    for key1 in self.trans_mat:\n        trans_mat[key1] = {}\n        for key2 in self.trans_mat[key1]:\n            if self.state_count[key1] != 0:\n                trans_mat[key1][key2] = float(self.trans_mat[key1][key2]) / self.state_count[key1]\n            else:\n                trans_mat[key1][key2] = float(self.trans_mat[key1][key2]) / default\n\n    for key1 in self.emit_mat:\n        emit_mat[key1] = {}\n        for key2 in self.emit_mat[key1]:\n            if self.state_count[key1] != 0:\n                emit_mat[key1][key2] = float(self.emit_mat[key1][key2]) / self.state_count[key1]\n            else:\n                emit_mat[key1][key2] = float(self.emit_mat[key1][key2]) / default\n    return init_vec, trans_mat, emit_mat</code></pre>\n\n<p>第七个方法&nbsp;<code>do_predict()</code>，预测采用 Viterbi 算法求得最优路径， 具体实现如下：</p>\n\n<pre>\n<code>#模型预测\ndef do_predict(self, sequence):\n    tab = [{}]\n    path = {}\n    init_vec, trans_mat, emit_mat = self.get_prob()\n\n    # 初始化\n    for state in self.states:\n        tab[0][state] = init_vec[state] * emit_mat[state].get(sequence[0], EPS)\n        path[state] = [state]\n\n    # 创建动态搜索表\n    for t in range(1, len(sequence)):\n        tab.append({})\n        new_path = {}\n        for state1 in self.states:\n            items = []\n            for state2 in self.states:\n                if tab[t - 1][state2] == 0:\n                    continue\n                prob = tab[t - 1][state2] * trans_mat[state2].get(state1, EPS) * emit_mat[state1].get(sequence[t], EPS)\n                items.append((prob, state2))\n            best = max(items)  \n            tab[t][state1] = best[0]\n            new_path[state1] = path[best[1]] + [state1]\n        path = new_path\n\n    # 搜索最有路径\n    prob, state = max([(tab[len(sequence) - 1][state], state) for state in self.states])\n    return path[state]</code></pre>\n\n<p>上面实现了类&nbsp;<code>HMM_Model</code>&nbsp;的7个方法，接下来我们来实现分词器，这里先定义两个函数，这两个函数是独立的，不在类中。</p>\n\n<p>（1）定义一个工具函数</p>\n\n<p>对输入的训练语料中的每个词进行标注，因为训练数据是空格隔开的，可以进行转态标注，该方法用在训练数据的标注，具体实现如下：</p>\n\n<pre>\n<code>    def get_tags(src):\n        tags = []\n        if len(src) == 1:\n            tags = [\'S\']\n        elif len(src) == 2:\n            tags = [\'B\', \'E\']\n        else:\n            m_num = len(src) - 2\n            tags.append(\'B\')\n            tags.extend([\'M\'] * m_num)\n            tags.append(\'E\')\n        return tags\n</code></pre>\n\n<p>（2）定义一个工具函数</p>\n\n<p>根据预测得到的标注序列将输入的句子分割为词语列表，也就是预测得到的状态序列，解析成一个 list 列表进行返回，具体实现如下：</p>\n\n<pre>\n<code>    def cut_sent(src, tags):\n        word_list = []\n        start = -1\n        started = False\n\n        if len(tags) != len(src):\n            return None\n\n        if tags[-1] not in {\'S\', \'E\'}:\n            if tags[-2] in {\'S\', \'E\'}:\n                tags[-1] = \'S\'  \n            else:\n                tags[-1] = \'E\'  \n\n        for i in range(len(tags)):\n            if tags[i] == \'S\':\n                if started:\n                    started = False\n                    word_list.append(src[start:i])  \n                word_list.append(src[i])\n            elif tags[i] == \'B\':\n                if started:\n                    word_list.append(src[start:i])  \n                start = i\n                started = True\n            elif tags[i] == \'E\':\n                started = False\n                word = src[start:i+1]\n                word_list.append(word)\n            elif tags[i] == \'M\':\n                continue\n        return word_list\n</code></pre>\n\n<p>最后，我们来定义分词器类 HMMSoyoger，继承&nbsp;<code>HMM_Model</code>&nbsp;类并实现中文分词器训练、分词功能，先给出 HMMSoyoger 类的结构定义：</p>\n\n<pre>\n<code>    class HMMSoyoger(HMM_Model):\n        def __init__(self, *args, **kwargs):\n            pass\n        #加载训练数据\n        def read_txt(self, filename):\n            pass\n        #模型训练函数\n        def train(self):\n            pass\n        #模型分词预测\n        def lcut(self, sentence):\n            pass\n</code></pre>\n\n<p>第一个方法 init()，构造函数，定义了初始化变量，具体实现如下：</p>\n\n<pre>\n<code>    def __init__(self, *args, **kwargs):\n            super(HMMSoyoger, self).__init__(*args, **kwargs)\n            self.states = STATES\n            self.data = None\n</code></pre>\n\n<p>第二个方法&nbsp;<code>read_txt()</code>，加载训练语料，读入文件为 txt，并且 UTF-8 编码，防止中文出现乱码，具体实现如下：</p>\n\n<pre>\n<code>    #加载语料\n    def read_txt(self, filename):\n            self.data = open(filename, \'r\', encoding=\"utf-8\")\n</code></pre>\n\n<p>第三个方法 train()，根据单词生成观测序列和状态序列，并通过父类的<code>do_train()</code>&nbsp;方法进行训练，具体实现如下：</p>\n\n<pre>\n<code>    def train(self):\n            if not self.inited:\n                self.setup()\n\n            for line in self.data:\n                line = line.strip()\n                if not line:\n                    continue\n\n               #观测序列\n                observes = []\n                for i in range(len(line)):\n                    if line[i] == \" \":\n                        continue\n                    observes.append(line[i])\n\n                #状态序列\n                words = line.split(\" \")  \n\n                states = []\n                for word in words:\n                    if word in seg_stop_words:\n                        continue\n                    states.extend(get_tags(word))\n                #开始训练\n                if(len(observes) &gt;= len(states)):\n                    self.do_train(observes, states)\n                else:\n                    pass\n</code></pre>\n\n<p>第四个方法 lcut()，模型训练好之后，通过该方法进行分词测试，具体实现如下：</p>\n\n<pre>\n<code>    def lcut(self, sentence):\n            try:\n                tags = self.do_predict(sentence)\n                return cut_sent(sentence, tags)\n            except:\n                return sentence\n</code></pre>\n\n<p>通过上面两个类和两个方法，就完成了基于 HMM 的中文分词器编码，下面我们来进行模型训练和测试。</p>\n\n<h4>训练模型</h4>\n\n<p>首先实例化 HMMSoyoger 类，然后通过&nbsp;<code>read_txt()</code>&nbsp;方法加载语料，再通过 train() 进行在线训练，如果训练语料比较大，可能需要等待一点时间，具体实现如下：</p>\n\n<pre>\n<code>    soyoger = HMMSoyoger()\n    soyoger.read_txt(\"syj_trainCorpus_utf8.txt\")\n    soyoger.train()\n</code></pre>\n\n<h4>模型测试</h4>\n\n<p>模型训练完成之后，我们就可以进行测试：</p>\n\n<pre>\n<code>    soyoger.lcut(\"中国的人工智能发展进入高潮阶段。\")\n</code></pre>\n\n<p>得到结果为：</p>\n\n<blockquote>\n<p>[&#39;中国&#39;, &#39;的&#39;, &#39;人工&#39;, &#39;智能&#39;, &#39;发展&#39;, &#39;进入&#39;, &#39;高潮&#39;, &#39;阶段&#39;, &#39;。&#39;]</p>\n</blockquote>\n\n<pre>\n<code>soyoger.lcut(\"中文自然语言处理是人工智能技术的一个重要分支。\")\n</code></pre>\n\n<p>得到结果为：</p>\n\n<blockquote>\n<p>[&#39;中文&#39;, &#39;自然&#39;, &#39;语言&#39;, &#39;处理&#39;, &#39;是人&#39;, &#39;工智&#39;, &#39;能技&#39;, &#39;术的&#39;, &#39;一个&#39;, &#39;重要&#39;, &#39;分支&#39;, &#39;。&#39;]</p>\n</blockquote>\n\n<p>可见，最后的结果还是不错的，如果想取得更好的结果，可自行制备更大更丰富的训练数据集。</p>\n\n<h3>基于 CRF 的开源中文分词工具 Genius 实践</h3>\n\n<p>Genius 是一个基于 CRF 的开源中文分词工具，采用了 Wapiti 做训练与序列标注，支持 Python 2.x、Python 3.x。</p>\n\n<h4>安装</h4>\n\n<p>（1）下载源码</p>\n\n<p>在&nbsp;<a href=\"https://github.com/duanhongyi/genius\">Github</a>&nbsp;上下载源码地址，解压源码，然后通过&nbsp;<code>python setup.py install</code>安装。</p>\n\n<p>（2）Pypi 安装</p>\n\n<p>通过执行命令：<code>easy_install genius</code>&nbsp;或者&nbsp;<code>pip install genius</code>&nbsp;安装。</p>\n\n<h4>分词</h4>\n\n<p>首先引入 Genius，然后对 text 文本进行分词。</p>\n\n<pre>\n<code>import genius\ntext = u\"\"\"中文自然语言处理是人工智能技术的一个重要分支。\"\"\"\nseg_list = genius.seg_text(\n    text,\n    use_combine=True,\n    use_pinyin_segment=True,\n    use_tagging=True,\n    use_break=True\n)\nprint(\' \'.join([word.text for word in seg_list])\n</code></pre>\n\n<p>其中，<code>genius.seg_text</code>&nbsp;函数接受5个参数，其中 text 是必填参数：</p>\n\n<ul>\n	<li>text 第一个参数为需要分词的字。</li>\n	<li><code>use_break</code>&nbsp;代表对分词结构进行打断处理，默认值 True。</li>\n	<li><code>use_combine</code>&nbsp;代表是否使用字典进行词合并，默认值 False。</li>\n	<li><code>use_tagging</code>&nbsp;代表是否进行词性标注，默认值 True。</li>\n	<li><code>use_pinyin_segment</code>&nbsp;代表是否对拼音进行分词处理，默认值 True。</li>\n</ul>\n\n<h3>总结</h3>\n\n<p>本文首先通过贝叶斯定理，理解了判别式模型和生成式模型的区别，接着通过动手实战&mdash;&mdash;基于 HMM 训练出自己的 Python 中文分词器，并进行了模型验证，最后给出一个基于 CRF 的开源中文分词工具。</p>\n\n<h3>参考文献</h3>\n\n<ol>\n	<li><a href=\"https://github.com/duanhongyi/genius\">Genius</a></li>\n	<li>周志华《机器学习》</li>\n</ol>\n\n<h2><a id=\"第09课：一网打尽神经序列模型之 RNN 及其变种 LSTM、GRU\" name=\"第09课：一网打尽神经序列模型之 RNN 及其变种 LSTM、GRU\"></a>第09课：一网打尽神经序列模型之 RNN 及其变种 LSTM、GRU</h2>\n\n<p>首先，我们来思考下，当人工神经网络从浅层发展到深层；从全连接到卷积神经网络。在此过程中，人类在图片分类、语音识别等方面都取得了非常好的结果，那么我们为什么还需要循环神经网络呢？</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/99202320-832c-11e8-a90b-215c3565b75a\" /></p>\n\n<p>因为，上面提到的这些网络结构的层与层之间是全连接或部分连接的，但在每层之间的节点是无连接的，这样的网络结构并不能很好的处理序列数据。</p>\n\n<p>序列数据的处理，我们从语言模型 N-gram 模型说起，然后着重谈谈 RNN，并通过 RNN 的变种 LSTM 和 GRU 来实战文本分类。</p>\n\n<h3>语言模型 N-gram 模型</h3>\n\n<p>通过前面的课程，我们了解到一般自然语言处理的传统方法是将句子处理为一个词袋模型（Bag-of-Words，BoW），而不考虑每个词的顺序，比如用朴素贝叶斯算法进行垃圾邮件识别或者文本分类。在中文里有时候这种方式没有问题，因为有些句子即使把词的顺序打乱，还是可以看懂这句话在说什么，比如：</p>\n\n<blockquote>\n<p>T：研究表明，汉字的顺序并不一定能影响阅读，比如当你看完这句话后。</p>\n\n<p>F：研表究明，汉字的序顺并不定一能影阅响读，比如当你看完这句话后。</p>\n</blockquote>\n\n<p>但有时候不行，词的顺序打乱，句子意思就变得让人不可思议了，例如：</p>\n\n<blockquote>\n<p>T：我喜欢吃烧烤。</p>\n\n<p>F：烧烤喜欢吃我。</p>\n</blockquote>\n\n<p>那么，有没有模型是考虑句子中词与词之间的顺序的呢？有，语言模型中的 N-gram 就是一种。</p>\n\n<p>N-gram 模型是一种语言模型（Language Model，LM），是一个基于概率的判别模型，它的输入是一句话（词的顺序序列），输出是这句话的概率，即这些词的联合概率（Joint Probability）。<br />\n使用 N-gram 语言模型思想，一般是需要知道当前词以及前面的词，因为一个句子中每个词的出现并不是独立的。比如，如果第一个词是&ldquo;空气&rdquo;，接下来的词是&ldquo;很&rdquo;，那么下一个词很大概率会是&ldquo;新鲜&rdquo;。类似于我们人的联想，N-gram 模型知道的信息越多，得到的结果也越准确。</p>\n\n<p>在前面课程中讲解的文本分类中，我们曾用到基于 sklearn 的词袋模型，尝试加入抽取&nbsp;<code>2-gram</code>&nbsp;和&nbsp;<code>3-gram</code>&nbsp;的统计特征，把词库的量放大，获得更强的特征。</p>\n\n<p>通过 ngram_range 参数来控制，代码如下：</p>\n\n<pre>\n<code>     from sklearn.feature_extraction.text import CountVectorizer\n        vec = CountVectorizer(\n            analyzer=\'word\', # tokenise by character ngrams\n            ngram_range=(1,4),  # use ngrams of size 1 and 2\n            max_features=20000,  # keep the most common 1000 ngrams\n        )\n</code></pre>\n\n<p>因此，N-gram 模型，在自然语言处理中主要应用在如词性标注、垃圾短信分类、分词器、机器翻译和语音识别、语音识别等领域。</p>\n\n<p>然而 N-gram 模型并不是完美的，它存在如下优缺点：</p>\n\n<ul>\n	<li>\n	<p>优点：包含了前 N-1 个词所能提供的全部信息，这些词对于当前词的出现概率具有很强的约束力；</p>\n	</li>\n	<li>\n	<p>缺点：需要很大规模的训练文本来确定模型的参数，当 N 很大时，模型的参数空间过大。所以常见的 N 值一般为1，2，3等。还有因数据稀疏而导致的数据平滑问题，解决方法主要是拉普拉斯平滑和内插与回溯。</p>\n	</li>\n</ul>\n\n<p>所以，根据 N-gram 的优缺点，它的进化版 NNLM（Neural Network based Language Model）诞生了。</p>\n\n<p>NNLM 由 Bengio 在2003年提出，它是一个很简单的模型，由四层组成，输入层、嵌入层、隐层和输出层，模型结构如下图（来自百度图片）：</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/ab99c060-832c-11e8-9f90-95294e517933\" /></p>\n\n<p>NNLM 接收的输入是长度为 N 的词序列，输出是下一个词的类别。首先，输入是词序列的 index 序列，例如词&ldquo;我&rdquo;在字典（大小为|V|）中的 index 是10，词&ldquo;是&rdquo;的 index 是23， &ldquo;小明&rdquo;的 index 是65，则句子&ldquo;我是小明&rdquo;的 index 序列就是 10、 23、65。嵌入层（Embedding）是一个大小为&nbsp;<code>|V|&times;K</code>&nbsp;的矩阵，从中取出第10、23、65行向量拼成 3&times;K 的矩阵就是 Embedding 层的输出了。隐层接受拼接后的 Embedding 层输出作为输入，以 tanh 为激活函数，最后送入带 softmax 的输出层，输出概率。</p>\n\n<p>NNLM 最大的缺点就是参数多，训练慢，要求输入定长 N 这一点很不灵活，同时不能利用完整的历史信息。</p>\n\n<p>因此，针对 NNLM 存在的问题，Mikolov 在2010年提出了 RNNLM，有兴趣可以阅读相关<a href=\"http://www.fit.vutbr.cz/~imikolov/rnnlm/thesis.pdf\" target=\"_blank\">论文</a>，其结构实际上是用 RNN 代替 NNLM 里的隐层，这样做的好处，包括减少模型参数、提高训练速度、接受任意长度输入、利用完整的历史信息。同时，RNN 的引入意味着可以使用 RNN 的其他变体，像 LSTM、BLSTM、GRU 等等，从而在序列建模上进行更多更丰富的优化。</p>\n\n<p>以上，从词袋模型说起，引出语言模型 N-gram 以及其优化模型 NNLM 和 RNNLM，后续内容从 RNN 说起，来看看其变种 LSTM 和 GRU 模型如何处理类似序列数据。</p>\n\n<h3>RNN 以及变种 LSTM 和 GRU 原理</h3>\n\n<h4>RNN 为序列数据而生</h4>\n\n<p>RNN 称为循环神经网路，因为这种网络有&ldquo;记忆性&rdquo;，主要应用在自然语言处理（NLP）和语音领域。RNN 具体的表现形式为网络会对前面的信息进行记忆并应用于当前输出的计算中，即隐藏层之间的节点不再无连接而是有连接的，并且隐藏层的输入不仅包括输入层的输出还包括上一时刻隐藏层的输出。</p>\n\n<p>理论上，RNN 能够对任何长度的序列数据进行处理，但由于该网络结构存在&ldquo;梯度消失&rdquo;问题，所以在实际应用中，解决梯度消失的方法有：梯度裁剪（Clipping Gradient）和 LSTM（<code>Long Short-Term Memory</code>）。</p>\n\n<p>下图是一个简单的 RNN 经典结构：</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/6772fa00-8354-11e8-a90b-215c3565b75a\" /></p>\n\n<p>RNN 包含输入单元（Input Units），输入集标记为&nbsp;\\{x_0,x_1,...,x_t,x_t...\\}{x0​,x1​,...,xt​,xt​...}；输出单元（Output Units）的输出集则被标记为&nbsp;\\{y_0,y_1,...,y_t,...\\}{y0​,y1​,...,yt​,...}；RNN 还包含隐藏单元（Hidden Units），我们将其输出集标记为&nbsp;\\{h_0,h_1,...,h_t,...\\}{h0​,h1​,...,ht​,...}，这些隐藏单元完成了最为主要的工作。</p>\n\n<h4>LSTM 结构</h4>\n\n<p>LSTM 在1997年由&ldquo;Hochreiter &amp; Schmidhuber&rdquo;提出，目前已经成为 RNN 中的标准形式，用来解决上面提到的 RNN 模型存在&ldquo;长期依赖&rdquo;的问题。</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/b986e5f0-8353-11e8-81ea-e357bbe10665\" /></p>\n\n<p>LSTM 通过三个&ldquo;门&rdquo;结构来控制不同时刻的状态和输出。所谓的&ldquo;门&rdquo;结构就是使用了 Sigmoid 激活函数的全连接神经网络和一个按位做乘法的操作，Sigmoid 激活函数会输出一个0~1之间的数值，这个数值代表当前有多少信息能通过&ldquo;门&rdquo;，0表示任何信息都无法通过，1表示全部信息都可以通过。其中，&ldquo;遗忘门&rdquo;和&ldquo;输入门&rdquo;是 LSTM 单元结构的核心。下面我们来详细分析下三种&ldquo;门&rdquo;结构。</p>\n\n<ul>\n	<li>\n	<p>遗忘门，用来让 LSTM&ldquo;忘记&rdquo;之前没有用的信息。它会根据当前时刻节点的输入&nbsp;X_tXt​、上一时刻节点的状态&nbsp;Ct&minus;1Ct&minus;1&nbsp;和上一时刻节点的输出&nbsp;h_{t-1}ht&minus;1​&nbsp;来决定哪些信息将被遗忘。</p>\n	</li>\n	<li>\n	<p>输入门，LSTM 来决定当前输入数据中哪些信息将被留下来。在 LSTM 使用遗忘门&ldquo;忘记&rdquo;部分信息后需要从当前的输入留下最新的记忆。输入门会根据当前时刻节点的输入&nbsp;X_tXt​、上一时刻节点的状态&nbsp;C_{t-1}Ct&minus;1​&nbsp;和上一时刻节点的输出&nbsp;h_{t-1}ht&minus;1​&nbsp;来决定哪些信息将进入当前时刻节点的状态&nbsp;C_tCt​，模型需要记忆这个最新的信息。</p>\n	</li>\n	<li>\n	<p>输出门，LSTM 在得到最新节点状态&nbsp;C_tCt​&nbsp;后，结合上一时刻节点的输出&nbsp;h_{t-1}ht&minus;1​&nbsp;和当前时刻节点的输入&nbsp;X_tXt​&nbsp;来决定当前时刻节点的输出。</p>\n	</li>\n</ul>\n\n<h4>GRU 结构</h4>\n\n<p>GRU（Gated Recurrent Unit）是2014年提出来的新的 RNN 架构，它是简化版的 LSTM。下面是 LSTM 和 GRU 的结构比较图（来自于网络）：</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/9a01d620-8364-11e8-9e1d-f13ca808ab04\" /></p>\n\n<p>在超参数均调优的前提下，据说效果和 LSTM 差不多，但是参数少了1/3，不容易过拟合。如果发现 LSTM 训练出来的模型过拟合比较严重，可以试试 GRU。</p>\n\n<h3>实战基于 Keras 的 LSTM 和 GRU 文本分类</h3>\n\n<p>上面讲了那么多，但是 RNN 的知识还有很多，比如双向 RNN 等，这些需要自己去学习，下面，我们来实战一下基于 LSTM 和 GRU 的文本分类。</p>\n\n<p>本次开发使用 Keras 来快速构建和训练模型，使用的数据集还是第06课使用的司法数据。</p>\n\n<p>整个过程包括：</p>\n\n<ol>\n	<li>语料加载</li>\n	<li>分词和去停用词</li>\n	<li>数据预处理</li>\n	<li>使用 LSTM 分类</li>\n	<li>使用 GRU 分类</li>\n</ol>\n\n<p>第一步，引入数据处理库，停用词和语料加载：</p>\n\n<pre>\n<code>    #引入包\n    import random\n    import jieba\n    import pandas as pd\n    \n    #加载停用词\n    stopwords=pd.read_csv(\'stopwords.txt\',index_col=False,quoting=3,sep=\"\\t\",names=[\'stopword\'], encoding=\'utf-8\')\n    stopwords=stopwords[\'stopword\'].values\n    \n    #加载语料\n    laogong_df = pd.read_csv(\'beilaogongda.csv\', encoding=\'utf-8\', sep=\',\')\n    laopo_df = pd.read_csv(\'beilaogongda.csv\', encoding=\'utf-8\', sep=\',\')\n    erzi_df = pd.read_csv(\'beierzida.csv\', encoding=\'utf-8\', sep=\',\')\n    nver_df = pd.read_csv(\'beinverda.csv\', encoding=\'utf-8\', sep=\',\')\n    #删除语料的nan行\n    laogong_df.dropna(inplace=True)\n    laopo_df.dropna(inplace=True)\n    erzi_df.dropna(inplace=True)\n    nver_df.dropna(inplace=True)\n    #转换\n    laogong = laogong_df.segment.values.tolist()\n    laopo = laopo_df.segment.values.tolist()\n    erzi = erzi_df.segment.values.tolist()\n    nver = nver_df.segment.values.tolist()\n</code></pre>\n\n<p>第二步，分词和去停用词：</p>\n\n<pre>\n<code>    #定义分词和打标签函数preprocess_text\n    #参数content_lines即为上面转换的list\n    #参数sentences是定义的空list，用来储存打标签之后的数据\n    #参数category 是类型标签\n    def preprocess_text(content_lines, sentences, category):\n        for line in content_lines:\n            try:\n                segs=jieba.lcut(line)\n                segs = [v for v in segs if not str(v).isdigit()]#去数字\n                segs = list(filter(lambda x:x.strip(), segs)) #去左右空格\n                segs = list(filter(lambda x:len(x)&gt;1, segs))#长度为1的字符\n                segs = list(filter(lambda x:x not in stopwords, segs)) #去掉停用词\n                sentences.append((\" \".join(segs), category))# 打标签\n            except Exception:\n                print(line)\n                continue \n    \n    #调用函数、生成训练数据\n    sentences = []\n    preprocess_text(laogong, sentences,0)\n    preprocess_text(laopo, sentences, 1)\n    preprocess_text(erzi, sentences, 2)\n    preprocess_text(nver, sentences, 3)\n</code></pre>\n\n<p>第三步，先打散数据，使数据分布均匀，然后获取特征和标签列表：</p>\n\n<pre>\n<code>    #打散数据，生成更可靠的训练集\n    random.shuffle(sentences)\n    \n    #控制台输出前10条数据，观察一下\n    for sentence in sentences[:10]:\n        print(sentence[0], sentence[1])\n    #所有特征和对应标签\n    all_texts = [ sentence[0] for sentence in sentences]\n    all_labels = [ sentence[1] for sentence in sentences]\n</code></pre>\n\n<p>第四步，使用 LSTM 对数据进行分类：</p>\n\n<pre>\n<code>    #引入需要的模块\n    from keras.preprocessing.text import Tokenizer\n    from keras.preprocessing.sequence import pad_sequences\n    from keras.utils import to_categorical\n    from keras.layers import Dense, Input, Flatten, Dropout\n    from keras.layers import LSTM, Embedding,GRU\n    from keras.models import Sequential\n    \n    #预定义变量\n    MAX_SEQUENCE_LENGTH = 100    #最大序列长度\n    EMBEDDING_DIM = 200    #embdding 维度\n    VALIDATION_SPLIT = 0.16    #验证集比例\n    TEST_SPLIT = 0.2    #测试集比例\n    #keras的sequence模块文本序列填充\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(all_texts)\n    sequences = tokenizer.texts_to_sequences(all_texts)\n    word_index = tokenizer.word_index\n    print(\'Found %s unique tokens.\' % len(word_index))\n    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n    labels = to_categorical(np.asarray(all_labels))\n    print(\'Shape of data tensor:\', data.shape)\n    print(\'Shape of label tensor:\', labels.shape)\n    \n    #数据切分\n    p1 = int(len(data)*(1-VALIDATION_SPLIT-TEST_SPLIT))\n    p2 = int(len(data)*(1-TEST_SPLIT))\n    x_train = data[:p1]\n    y_train = labels[:p1]\n    x_val = data[p1:p2]\n    y_val = labels[p1:p2]\n    x_test = data[p2:]\n    y_test = labels[p2:]\n    \n    #LSTM训练模型\n    model = Sequential()\n    model.add(Embedding(len(word_index) + 1, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n    model.add(LSTM(200, dropout=0.2, recurrent_dropout=0.2))\n    model.add(Dropout(0.2))\n    model.add(Dense(64, activation=\'relu\'))\n    model.add(Dense(labels.shape[1], activation=\'softmax\'))\n    model.summary()\n    #模型编译\n    model.compile(loss=\'categorical_crossentropy\',\n                  optimizer=\'rmsprop\',\n                  metrics=[\'acc\'])\n    print(model.metrics_names)\n    model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=10, batch_size=128)\n    model.save(\'lstm.h5\')\n    #模型评估\n    print(model.evaluate(x_test, y_test))\n</code></pre>\n\n<p>训练过程结果为：</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/1646b580-8382-11e8-a90b-215c3565b75a\" /></p>\n\n<p>第五步，使用 GRU 进行文本分类，上面就是完整的使用 LSTM 进行 文本分类，如果使用 GRU 只需要改变模型训练部分：</p>\n\n<pre>\n<code>    model = Sequential()\n    model.add(Embedding(len(word_index) + 1, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n    model.add(GRU(200, dropout=0.2, recurrent_dropout=0.2))\n    model.add(Dropout(0.2))\n    model.add(Dense(64, activation=\'relu\'))\n    model.add(Dense(labels.shape[1], activation=\'softmax\'))\n    model.summary()\n    \n    model.compile(loss=\'categorical_crossentropy\',\n                  optimizer=\'rmsprop\',\n                  metrics=[\'acc\'])\n    print(model.metrics_names)\n    model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=10, batch_size=128)\n    model.save(\'lstm.h5\')\n    \n    print(model.evaluate(x_test, y_test))\n</code></pre>\n\n<p>训练过程结果：</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/80bb3620-8382-11e8-878d-d106b6fbe32a\" /></p>\n\n<h3>总结</h3>\n\n<p>本文从词袋模型谈起，旨在引出语言模型 N-gram 以及其优化模型 NNLM 和 RNNLM，并通过 RNN 以及其变种 LSTM 和 GRU 模型，理解其如何处理类似序列数据的原理，并实战基于 LSTM 和 GRU 的中文文本分类。</p>\n\n<p><strong>参考文献：</strong></p>\n\n<ol>\n	<li>\n	<p><a href=\"https://blog.csdn.net/u012328159/article/details/72847297\" target=\"_blank\">Hinton 神经网络公开课编程题2&mdash;&mdash;神经概率语言模型（NNLM）</a></p>\n	</li>\n	<li>\n	<p><a href=\"http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/\" target=\"_blank\">Recurrent Neural Networks Tutorial, Part 3 &ndash; Backpropagation Through Time and Vanishing Gradients</a></p>\n	</li>\n</ol>\n\n<h2><a id=\"第10课：动手实战基于 CNN 的电影推荐系统\" name=\"第10课：动手实战基于 CNN 的电影推荐系统\"></a>第10课：动手实战基于 CNN 的电影推荐系统</h2>\n\n<p>本文从深度学习卷积神经网络入手，基于 Github 的开源项目来完成 MovieLens 数据集的电影推荐系统。</p>\n\n<h3>什么是推荐系统呢？</h3>\n\n<p>什么是推荐系统呢？首先我们来看看几个常见的推荐场景。</p>\n\n<p>如果你经常通过豆瓣电影评分来找电影，你会发现下图所示的推荐：</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/ae7b9690-84cc-11e8-b31a-735b9fbd81d4\" /></p>\n\n<p>如果你喜欢购物，根据你的选择和购物行为，平台会给你推荐相似商品：</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/452fa580-84ce-11e8-bd72-e72df50fbc8a\" /></p>\n\n<p>在互联网的很多场景下都可以看到推荐的影子。因为推荐可以帮助用户和商家满足不同的需求：</p>\n\n<ul>\n	<li>\n	<p>对用户而言：找到感兴趣的东西，帮助发现新鲜、有趣的事物。</p>\n	</li>\n	<li>\n	<p>对商家而言：提供个性化服务，提高信任度和粘性，增加营收。</p>\n	</li>\n</ul>\n\n<p>常见的推荐系统主要包含两个方面的内容，基于用户的推荐系统（UserCF）和基于物品的推荐系统（ItemCF）。两者的区别在于，UserCF 给用户推荐那些和他有共同兴趣爱好的用户喜欢的商品，而 ItemCF 给用户推荐那些和他之前喜欢的商品类似的商品。这两种方式都会遭遇冷启动问题。</p>\n\n<p>下面是 UserCF 和 ItemCF 的对比：</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/f5728ff0-84d0-11e8-9658-97176c275e7c\" /></p>\n\n<h3>CNN 是如何应用在文本处理上的？</h3>\n\n<p>提到卷积神经网络（CNN），相信大部分人首先想到的是图像分类，比如 MNIST 手写体识别，CAFRI10 图像分类。CNN 已经在图像识别方面取得了较大的成果，随着近几年的不断发展，在文本处理领域，基于文本挖掘的文本卷积神经网络被证明是有效的。</p>\n\n<p>首先，来看看 CNN 是如何应用到 NLP 中的，下面是一个简单的过程图：</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/5ccd6290-84d3-11e8-815d-196036bcf3c7\" /></p>\n\n<p>和图像像素处理不一样，自然语言通常是一段文字，那么在特征矩阵中，矩阵的每一个行向量（比如 word2vec 或者 doc2vec）代表一个 Token，包括词或者字符。如果一段文字包含有 n 个词，每个词有 m 维的词向量，那么我们可以构造出一个&nbsp;<code>n*m</code>&nbsp;的词向量矩阵，在 NLP 处理过程中，让过滤器宽度和矩阵宽度保持一致整行滑动。</p>\n\n<h3>动手实战基于 CNN 的电影推荐系统</h3>\n\n<p>将 CNN 的技术应用到自然语言处理中并与电影推荐相结合，来训练一个基于文本的卷积神经网络，实现电影个性化推荐系统。</p>\n\n<p>首先感谢作者 chengstone 的分享，源码请访问下面网址：</p>\n\n<ul>\n	<li><a href=\"https://github.com/chengstone/movie_recommender\">Github</a></li>\n</ul>\n\n<p>在验证了 CNN 应用在自然语言处理上是有效的之后，从推荐系统的个性化推荐入手，在文本上，把 CNN 成果应用到电影的个性化推荐上。并在特征工程中，对训练集和测试集做了相应的特征处理，其中有部分字段是类型性变量，特征工程上可以采用&nbsp;<code>one-hot</code>&nbsp;编码，但是对于 UserID、MovieID 这样非常稀疏的变量，如果使用&nbsp;<code>one-hot</code>，那么数据的维度会急剧膨胀，对于这份数据集来说是不合适的。</p>\n\n<p>具体算法设计如下：</p>\n\n<p><strong>1.</strong>&nbsp;定义用户嵌入矩阵。</p>\n\n<p>用户的特征矩阵主要是通过用户信息嵌入网络来生成的，在预处理数据的时候，我们将 UserID、MovieID、性别、年龄、职业特征全部转成了数字类型，然后把这个数字当作嵌入矩阵的索引，在网络的第一层就使用嵌入层，这样数据输入的维度保持在（N，32）和（N，16）。然后进行全连接层，转成（N，128）的大小，再进行全连接层，转成（N，200）的大小，这样最后输出的用户特征维度相对比较高，也保证了能把每个用户所带有的特征充分携带并通过特征表达。</p>\n\n<p>具体流程如下：</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/3983eae0-88cd-11e8-a341-85988e434d41\" /></p>\n\n<p><strong>2.</strong>&nbsp;生成用户特征。</p>\n\n<p>生成用户特征是在用户嵌入矩阵网络输出结果的基础上，通过2层全连接层实现的。第一个全连接层把特征矩阵转成（N，128）的大小，再进行第二次全连接层，转成（N，200）的大小，这样最后输出的用户特征维度相对比较高，也保证了能把每个用户所带有的特征充分携带并通过特征表达。</p>\n\n<p>具体流程如下：</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/747e2f20-88cd-11e8-9944-4f7451c47515\" /></p>\n\n<p><strong>3.</strong>&nbsp;定义电影 ID 嵌入矩阵。</p>\n\n<p>通过电影 ID 和电影类型分别生成电影 ID 和电影类型特征，电影类型的多个嵌入向量做加和输出。电影 ID 的实现过程和上面一样，但是对于电影类型的处理相较于上面，稍微复杂一点。因为电影类型有重叠性，一个电影可以属于多个类别，当把电影类型从嵌入矩阵索引出来之后是一个（N，32）形状的矩阵，因为有多个类别，这里采用的处理方式是矩阵求和，把类别加上去，变成（1，32）形状，这样使得电影的类别信息不会丢失。</p>\n\n<p>具体流程如下：</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/9483d0e0-88cd-11e8-8555-ffb97409ea91\" /></p>\n\n<p><strong>4.</strong>&nbsp;文本卷积神经网络设计。</p>\n\n<p>文本卷积神经网络和单纯的 CNN 网络结构有点不同，因为自然语言通常是一段文字与图片像素组成的矩阵是不一样的。在电影文本特征矩阵中，矩阵的每一个行构成的行向量代表一个 Token，包括词或者字符。如果一段文字有 n 个词，每个词有 m 维的词向量，那么我们可以构造出一个&nbsp;<code>n*m</code>&nbsp;的矩阵。而且 NLP 处理过程中，会有多个不同大小的过滤器串行执行，且过滤器宽度和矩阵宽度保持一致，是整行滑动。在执行完卷积操作之后采用了 ReLU 激活函数，然后采用最大池化操作，最后通过全连接并 Dropout 操作和 Softmax 输出。这里电影名称的处理比较特殊，并没有采用循环神经网络，而采用的是文本在 CNN 网络上的应用。</p>\n\n<p>对于电影数据集，我们对电影名称做 CNN 处理，其大致流程，从嵌入矩阵中得到电影名对应的各个单词的嵌入向量，由于电影名称比较特殊一点，名称长度有一定限制，这里过滤器大小使用时，就选择2、3、4、5长度。然后对文本嵌入层使用滑动2、3、4、5个单词尺寸的卷积核做卷积和最大池化，然后 Dropout 操作，全连接层输出。</p>\n\n<p>具体流程如下：</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/49e32370-8e3a-11e8-8ee0-a17ea463076e\" /></p>\n\n<p>具体过程描述：</p>\n\n<p>（1）首先输入一个&nbsp;<code>32*32</code>&nbsp;的矩阵；</p>\n\n<p>（2）第一次卷积核大小为&nbsp;<code>2*2</code>，得到&nbsp;<code>31*31</code>&nbsp;的矩阵，然后通过&nbsp;<code>[1,14,1,1]</code>的&nbsp;<code>max-pooling</code>&nbsp;操作，得到的矩阵为&nbsp;<code>18*31</code>；</p>\n\n<p>（3）第二次卷积核大小为&nbsp;<code>3*3</code>，得到&nbsp;<code>16*29的矩阵，然后通过[1,13,1,1]</code>&nbsp;的<code>max-pooling</code>&nbsp;操作，得到的矩阵为&nbsp;<code>4*29</code>；</p>\n\n<p>（4）第三次卷积核大小&nbsp;<code>4*4</code>，得到&nbsp;<code>1*26</code>&nbsp;的矩阵，然后通过&nbsp;<code>[1,12,1,1]</code>&nbsp;的<code>max-pooling</code>&nbsp;操作，得到的矩阵为&nbsp;<code>1*26</code>；</p>\n\n<p>（5）第四次卷积核大小&nbsp;<code>5*5</code>，得到&nbsp;<code>1*22</code>&nbsp;的矩阵，然后通过&nbsp;<code>[1,11,1,1]</code>&nbsp;的<code>max-pooling</code>&nbsp;操作，得到的矩阵为&nbsp;<code>1*22</code>；</p>\n\n<p>（6）最后通过 Dropout 和全连接层，<code>len(window_sizes) * filter_num =32</code>，得到&nbsp;<code>1*32</code>的矩阵。</p>\n\n<p><strong>5.</strong>&nbsp;电影各层做一个全连接层。</p>\n\n<p>将上面几步生成的特征向量，通过2个全连接层连接在一起，第一个全连接层是电影 ID 特征和电影类型特征先全连接，之后再和 CNN 生成的电影名称特征全连接，生成最后的特征集。</p>\n\n<p>具体流程如下：</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/336d6d10-88ce-11e8-bee1-4fab1cb0896f\" /></p>\n\n<p><strong>6.</strong>&nbsp;完整的基于 CNN 的电影推荐流程。</p>\n\n<p>把以上实现的模块组合成整个算法，将网络模型作为回归问题进行训练，得到训练好的用户特征矩阵和电影特征矩阵进行推荐。</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/abffaae0-88ce-11e8-950d-3f19ed380d25\" /></p>\n\n<h3>基于 CNN 的电影推荐系统代码调参过程</h3>\n\n<p>在训练过程中，我们需要对算法预先设置一些超参数，这里给出的最终的设置结果：</p>\n\n<pre>\n<code>    # 设置迭代次数\n    num_epochs = 5\n    # 设置BatchSize大小\n    batch_size = 256\n    #设置dropout保留比例\n    dropout_keep = 0.5\n    # 设置学习率\n    learning_rate = 0.0001\n    # 设置每轮显示的batches大小\n    show_every_n_batches = 20\n</code></pre>\n\n<p>首先对数据集进行划分，按照 4:1 的比例划分为训练集和测试集，下面给出的是算法模型最终训练集合测试集使用的划分结果：</p>\n\n<pre>\n<code>    #将数据集分成训练集和测试集，随机种子不固定\n    train_X,test_X, train_y, test_y = train_test_split(features,  \n                                                 targets_values,  \n                                                 test_size = 0.3,  \n                                                 random_state = 0) \n</code></pre>\n\n<p>接下来是具体模型训练过程。训练过程，要不断调参，根据经验调参粒度可以选择从粗到细分阶段进行。</p>\n\n<p>调参过程对比：</p>\n\n<p>（1）第一步，先固定，<code>learning_rate=0.01</code>&nbsp;和&nbsp;<code>num_epochs=10</code>，测试<code>batch_size=128</code>&nbsp;对迭代时间和 Loss 的影响；</p>\n\n<p>（2）第二步，先固定，<code>learning_rate=0.01</code>&nbsp;和&nbsp;<code>num_epochs=10</code>，测试<code>batch_size=256</code>&nbsp;对迭代时间和 Loss 的影响；</p>\n\n<p>（3）第三步，先固定，<code>learning_rate=0.01</code>&nbsp;和&nbsp;<code>num_epochs=10</code>，测试<code>batch_size=512</code>&nbsp;对迭代时间和 Loss 的影响；</p>\n\n<p>（4）第四步，先固定，<code>learning_rate=0.01</code>&nbsp;和&nbsp;<code>num_epochs=5</code>，测试<code>batch_size=128</code>&nbsp;对迭代时间和 Loss 的影响；</p>\n\n<p>（5）第五步，先固定，<code>learning_rate=0.01</code>&nbsp;和&nbsp;<code>num_epochs=5</code>，测试<code>batch_size=256</code>&nbsp;对迭代时间和 Loss 的影响；</p>\n\n<p>（6）第六步，先固定，<code>learning_rate=0.01</code>&nbsp;和&nbsp;<code>num_epochs=5</code>，测试<code>batch_size=512</code>&nbsp;对迭代时间和 Loss 的影响；</p>\n\n<p>（7）第七步，先固定，<code>batch_size=256</code>&nbsp;和&nbsp;<code>num_epochs=5</code>，测试<code>learning_rate=0.001</code>&nbsp;对 Loss 的影响；</p>\n\n<p>（8）第八步，先固定，<code>batch_size=256</code>&nbsp;和&nbsp;<code>num_epochs=5</code>，测试<code>learning_rate=0.0005</code>&nbsp;对 Loss 的影响；</p>\n\n<p>（9）第九步，先固定，<code>batch_size=256</code>&nbsp;和&nbsp;<code>num_epochs=5</code>，测试<code>learning_rate=0.0001</code>&nbsp;对 Loss 的影响；</p>\n\n<p>（10）第十步，先固定，<code>batch_size=256</code>&nbsp;和&nbsp;<code>num_epochs=5</code>，测试<code>learning_rate=0.00005</code>&nbsp;对 Loss 的影响。</p>\n\n<p>得到的调参结果对比表如下：</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/64b2fe20-88cf-11e8-81e9-19f90a93f5ff\" /></p>\n\n<p>通过上面（1）-（6）步调参比较，在&nbsp;<code>learning_rate</code>、<code>batch_size</code>&nbsp;相同的情况下，<code>num_epochs</code>&nbsp;对于训练时间影响较大；而在<code>learning_rate</code>、<code>num_epochs</code>&nbsp;相同情况下，<code>batch_size</code>&nbsp;对 Loss 的影响较大，<code>batch_size</code>&nbsp;选择512，Loss 有抖动情况，权衡之下，最终确定后续调参固定采用&nbsp;<code>batch_size=256</code>、<code>num_epochs=5</code>&nbsp;的超参数值，后续（7）-（10）步，随着&nbsp;<code>learning_rate</code>&nbsp;逐渐减小，发现 Loss 是先逐渐减小，而在<code>learning_rate=0.00005</code>&nbsp;时反而增大，最终选择出学习率为<code>learning_rate=0.0001</code>&nbsp;的超参数值。</p>\n\n<h3>基于 CNN 的电影推荐系统电影推荐</h3>\n\n<p>在上面，完成模型训练验证之后，实际来进行推荐电影，这里使用生产的用户特征矩阵和电影特征矩阵做电影推荐，主要有三种方式的推荐。</p>\n\n<p><strong>1.</strong>&nbsp;推荐同类型的电影。</p>\n\n<p>思路是：计算当前看的电影特征向量与整个电影特征矩阵的余弦相似度，取相似度最大的&nbsp;<code>top_k</code>&nbsp;个，这里加了些随机选择在里面，保证每次的推荐稍稍有些不同。</p>\n\n<pre>\n<code>    def recommend_same_type_movie(movie_id_val, top_k = 20):\n\n        loaded_graph = tf.Graph()  #\n        with tf.Session(graph=loaded_graph) as sess:  #\n            # Load saved model\n            loader = tf.train.import_meta_graph(load_dir + \'.meta\')\n            loader.restore(sess, load_dir)\n\n            norm_movie_matrics = tf.sqrt(tf.reduce_sum(tf.square(movie_matrics), 1, keep_dims=True))\n            normalized_movie_matrics = movie_matrics / norm_movie_matrics\n\n            #推荐同类型的电影\n            probs_embeddings = (movie_matrics[movieid2idx[movie_id_val]]).reshape([1, 200])\n            probs_similarity = tf.matmul(probs_embeddings, tf.transpose(normalized_movie_matrics))\n            sim = (probs_similarity.eval())\n            print(\"您看的电影是：{}\".format(movies_orig[movieid2idx[movie_id_val]]))\n            print(\"以下是给您的推荐：\")\n            p = np.squeeze(sim)\n            p[np.argsort(p)[:-top_k]] = 0\n            p = p / np.sum(p)\n            results = set()\n            while len(results) != 5:\n                c = np.random.choice(3883, 1, p=p)[0]\n                results.add(c)\n            for val in (results):\n                print(val)\n                print(movies_orig[val])\n            return result\n</code></pre>\n\n<p><strong>2.</strong>&nbsp;推荐您喜欢的电影。</p>\n\n<p>思路是：使用用户特征向量与电影特征矩阵计算所有电影的评分，取评分最高的<code>top_k</code>&nbsp;个，同样加了些随机选择部分。</p>\n\n<pre>\n<code>    def recommend_your_favorite_movie(user_id_val, top_k = 10):\n\n        loaded_graph = tf.Graph()  #\n        with tf.Session(graph=loaded_graph) as sess:  #\n            # Load saved model\n            loader = tf.train.import_meta_graph(load_dir + \'.meta\')\n            loader.restore(sess, load_dir)\n\n            #推荐您喜欢的电影\n            probs_embeddings = (users_matrics[user_id_val-1]).reshape([1, 200])\n            probs_similarity = tf.matmul(probs_embeddings, tf.transpose(movie_matrics))\n            sim = (probs_similarity.eval())\n\n            print(\"以下是给您的推荐：\")\n            p = np.squeeze(sim)\n            p[np.argsort(p)[:-top_k]] = 0\n            p = p / np.sum(p)\n            results = set()\n            while len(results) != 5:\n                c = np.random.choice(3883, 1, p=p)[0]\n                results.add(c)\n            for val in (results):\n                print(val)\n                print(movies_orig[val])\n\n            return results\n</code></pre>\n\n<p><strong>3.</strong>&nbsp;看过这个电影的人还看了（喜欢）哪些电影。</p>\n\n<p>（1）首先选出喜欢某个电影的&nbsp;<code>top_k</code>&nbsp;个人，得到这几个人的用户特征向量；</p>\n\n<p>（2）然后计算这几个人对所有电影的评分 ；</p>\n\n<p>（3）选择每个人评分最高的电影作为推荐；</p>\n\n<p>（4）同样加入了随机选择。</p>\n\n<pre>\n<code>    def recommend_other_favorite_movie(movie_id_val, top_k = 20):\n        loaded_graph = tf.Graph()  #\n        with tf.Session(graph=loaded_graph) as sess:  #\n            # Load saved model\n            loader = tf.train.import_meta_graph(load_dir + \'.meta\')\n            loader.restore(sess, load_dir)\n            probs_movie_embeddings = (movie_matrics[movieid2idx[movie_id_val]]).reshape([1, 200])\n            probs_user_favorite_similarity = tf.matmul(probs_movie_embeddings, tf.transpose(users_matrics))\n            favorite_user_id = np.argsort(probs_user_favorite_similarity.eval())[0][-top_k:]\n\n            print(\"您看的电影是：{}\".format(movies_orig[movieid2idx[movie_id_val]]))\n\n            print(\"喜欢看这个电影的人是：{}\".format(users_orig[favorite_user_id-1]))\n            probs_users_embeddings = (users_matrics[favorite_user_id-1]).reshape([-1, 200])\n            probs_similarity = tf.matmul(probs_users_embeddings, tf.transpose(movie_matrics))\n            sim = (probs_similarity.eval())\n            p = np.argmax(sim, 1)\n            print(\"喜欢看这个电影的人还喜欢看：\")\n            results = set()\n            while len(results) != 5:\n                c = p[random.randrange(top_k)]\n                results.add(c)\n            for val in (results):\n                print(val)\n                print(movies_orig[val])\n            return results\n</code></pre>\n\n<h3>基于 CNN 的电影推荐系统不足</h3>\n\n<p>这里讨论一下基于上述方法所带来的不足：</p>\n\n<ol>\n	<li>\n	<p>由于一个新的用户在刚开始的时候并没有任何行为记录，所以系统会出现冷启动的问题；</p>\n	</li>\n	<li>\n	<p>由于神经网络是一个黑盒子过程，我们并不清楚在反向传播的过程中的具体细节，也不知道每一个卷积层抽取的特征细节，所以此算法缺乏一定的可解释性；</p>\n	</li>\n	<li>\n	<p>一般来说，在工业界，用户的数据量是海量的，而卷积神经网络又要耗费大量的计算资源，所以进行集群计算是非常重要的。但是由于本课程所做实验环境有限，还是在单机上运行，所以后期可以考虑在服务器集群上全量跑数据，这样获得的结果也更准确。</p>\n	</li>\n</ol>\n\n<h3>总结</h3>\n\n<p>上面通过&nbsp;<a href=\"https://github.com/chengstone/movie_recommender\">Github</a>&nbsp;上一个开源的项目，梳理了 CNN 在文本推荐上的应用，并通过模型训练调参，给出一般的模型调参思路，最后建议大家自己把源码下载下来跑跑模型，效果更好。</p>\n\n<h3>参考文献及推荐阅读</h3>\n\n<ol>\n	<li>\n	<p><a href=\"https://blog.csdn.net/dream_catcher_10/article/details/50733172\">推荐系统</a></p>\n	</li>\n	<li>\n	<p>Deep Convolutional Neural Networks for Sentiment Analysis of ShortTexts,CND Santos ,M Gattit ,2014.</p>\n	</li>\n	<li>\n	<p>推荐系统实践，p50-60，p120-130，项亮。</p>\n	</li>\n</ol>\n\n<h2><a id=\"第11课：动手实战基于 LSTM 轻松生成各种古诗\" name=\"第11课：动手实战基于 LSTM 轻松生成各种古诗\"></a>第11课：动手实战基于 LSTM 轻松生成各种古诗</h2>\n\n<p>目前循环神经网络（RNN）已经广泛用于自然语言处理中，可以处理大量的序列数据，可以说是最强大的神经网络模型之一。人们已经给 RNN 找到了越来越多的事情做，比如画画和写诗，微软的小冰都已经出版了一本诗集了。</p>\n\n<p>而其实训练一个能写诗的神经网络并不难，下面我们就介绍如何简单快捷地建立一个会写诗的网络模型。</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/0fdb0170-8db8-11e8-80d1-2d51ff7e1c55\" /></p>\n\n<p>本次开发环境如下：</p>\n\n<ul>\n	<li>Python 3.6</li>\n	<li>Keras 环境</li>\n	<li>Jupyter Notebook</li>\n</ul>\n\n<p>整个过程分为以下步骤完成：</p>\n\n<ol>\n	<li>语料准备</li>\n	<li>语料预处理</li>\n	<li>模型参数配置</li>\n	<li>构建模型</li>\n	<li>训练模型</li>\n	<li>模型作诗</li>\n	<li>绘制模型网络结构图</li>\n</ol>\n\n<p>下面一步步来构建和训练一个会写诗的模型。</p>\n\n<p><strong>第一</strong>，语料准备。一共四万多首古诗，每行一首诗，标题在预处理的时候已经去掉了。</p>\n\n<p><strong>第二</strong>，文件预处理。首先，机器并不懂每个中文汉字代表的是什么，所以要将文字转换为机器能理解的形式，这里我们采用 One-Hot 的形式，这样诗句中的每个字都能用向量来表示，下面定义函数&nbsp;<code>preprocess_file()</code>&nbsp;来处理。</p>\n\n<pre>\n<code>    puncs = [\']\', \'[\', \'（\', \'）\', \'{\', \'}\', \'：\', \'《\', \'》\']\n    def preprocess_file(Config):\n        # 语料文本内容\n        files_content = \'\'\n        with open(Config.poetry_file, \'r\', encoding=\'utf-8\') as f:\n            for line in f:\n                # 每行的末尾加上\"]\"符号代表一首诗结束\n                for char in puncs:\n                    line = line.replace(char, \"\")\n                files_content += line.strip() + \"]\"\n\n        words = sorted(list(files_content))\n        words.remove(\']\')\n        counted_words = {}\n        for word in words:\n            if word in counted_words:\n                counted_words[word] += 1\n            else:\n                counted_words[word] = 1\n\n        # 去掉低频的字\n        erase = []\n        for key in counted_words:\n            if counted_words[key] &lt;= 2:\n                erase.append(key)\n        for key in erase:\n            del counted_words[key]\n        del counted_words[\']\']\n        wordPairs = sorted(counted_words.items(), key=lambda x: -x[1])\n\n        words, _ = zip(*wordPairs)\n        # word到id的映射\n        word2num = dict((c, i + 1) for i, c in enumerate(words))\n        num2word = dict((i, c) for i, c in enumerate(words))\n        word2numF = lambda x: word2num.get(x, 0)\n        return word2numF, num2word, words, files_content\n</code></pre>\n\n<p>在每行末尾加上&nbsp;<code>]</code>&nbsp;符号是为了标识这首诗已经结束了。我们给模型学习的方法是，给定前六个字，生成第七个字，所以在后面生成训练数据的时候，会以6的跨度，1的步长截取文字，生成语料。如果出现了&nbsp;<code>]</code>&nbsp;符号，说明&nbsp;<code>]</code>&nbsp;符号之前的语句和之后的语句是两首诗里面的内容，两首诗之间是没有关联关系的，所以我们后面会舍弃掉包含&nbsp;<code>]</code>&nbsp;符号的训练数据。</p>\n\n<p><strong>第三</strong>，模型参数配置。预先定义模型参数和加载语料以及模型保存名称，通过类 Config 实现。</p>\n\n<pre>\n<code>class Config(object):\n    poetry_file = \'poetry.txt\'\n    weight_file = \'poetry_model.h5\'\n    # 根据前六个字预测第七个字\n    max_len = 6\n    batch_size = 512\n    learning_rate = 0.001\n</code></pre>\n\n<p><strong>第四</strong>，构建模型，通过 PoetryModel 类实现，类的代码结构如下：</p>\n\n<pre>\n<code>    class PoetryModel(object):\n        def __init__(self, config):\n            pass\n\n        def build_model(self):\n            pass\n\n        def sample(self, preds, temperature=1.0):\n            pass\n\n        def generate_sample_result(self, epoch, logs):\n            pass\n\n        def predict(self, text):\n            pass\n\n        def data_generator(self):\n            pass\n        def train(self):\n            pass\n</code></pre>\n\n<p>类中定义的方法具体实现功能如下：</p>\n\n<p>（1）init 函数定义，通过加载 Config 配置信息，进行语料预处理和模型加载，如果模型文件存在则直接加载模型，否则开始训练。</p>\n\n<pre>\n<code>    def __init__(self, config):\n            self.model = None\n            self.do_train = True\n            self.loaded_model = False\n            self.config = config\n\n            # 文件预处理\n            self.word2numF, self.num2word, self.words, self.files_content = preprocess_file(self.config)\n            if os.path.exists(self.config.weight_file):\n                self.model = load_model(self.config.weight_file)\n                self.model.summary()\n            else:\n                self.train()\n            self.do_train = False\n            self.loaded_model = True\n</code></pre>\n\n<p>（2）<code>build_model</code>&nbsp;函数主要用 Keras 来构建网络模型，这里使用 LSTM 的 GRU 来实现，当然直接使用 LSTM 也没问题。</p>\n\n<pre>\n<code>    def build_model(self):\n            \'\'\'建立模型\'\'\'\n            input_tensor = Input(shape=(self.config.max_len,))\n            embedd = Embedding(len(self.num2word)+1, 300, input_length=self.config.max_len)(input_tensor)\n            lstm = Bidirectional(GRU(128, return_sequences=True))(embedd)\n            dropout = Dropout(0.6)(lstm)\n            lstm = Bidirectional(GRU(128, return_sequences=True))(embedd)\n            dropout = Dropout(0.6)(lstm)\n            flatten = Flatten()(lstm)\n            dense = Dense(len(self.words), activation=\'softmax\')(flatten)\n            self.model = Model(inputs=input_tensor, outputs=dense)\n            optimizer = Adam(lr=self.config.learning_rate)\n            self.model.compile(loss=\'categorical_crossentropy\', optimizer=optimizer, metrics=[\'accuracy\'])\n</code></pre>\n\n<p>（3）sample 函数，在训练过程的每个 epoch 迭代中采样。</p>\n\n<pre>\n<code>    def sample(self, preds, temperature=1.0):\n        \'\'\'\n        当temperature=1.0时，模型输出正常\n        当temperature=0.5时，模型输出比较open\n        当temperature=1.5时，模型输出比较保守\n        在训练的过程中可以看到temperature不同，结果也不同\n        \'\'\'\n        preds = np.asarray(preds).astype(\'float64\')\n        preds = np.log(preds) / temperature\n        exp_preds = np.exp(preds)\n        preds = exp_preds / np.sum(exp_preds)\n        probas = np.random.multinomial(1, preds, 1)\n        return np.argmax(probas)\n</code></pre>\n\n<p>（4）训练过程中，每个 epoch 打印出当前的学习情况。</p>\n\n<pre>\n<code>    def generate_sample_result(self, epoch, logs):  \n            print(\"\\n==================Epoch {}=====================\".format(epoch))\n            for diversity in [0.5, 1.0, 1.5]:\n                print(\"------------Diversity {}--------------\".format(diversity))\n                start_index = random.randint(0, len(self.files_content) - self.config.max_len - 1)\n                generated = \'\'\n                sentence = self.files_content[start_index: start_index + self.config.max_len]\n                generated += sentence\n                for i in range(20):\n                    x_pred = np.zeros((1, self.config.max_len))\n                    for t, char in enumerate(sentence[-6:]):\n                        x_pred[0, t] = self.word2numF(char)\n\n                    preds = self.model.predict(x_pred, verbose=0)[0]\n                    next_index = self.sample(preds, diversity)\n                    next_char = self.num2word[next_index]\n                    generated += next_char\n                    sentence = sentence + next_char\n                print(sentence)\n</code></pre>\n\n<p>（5）predict 函数，用于根据给定的提示，来进行预测。</p>\n\n<p>根据给出的文字，生成诗句，如果给的 text 不到四个字，则随机补全。</p>\n\n<pre>\n<code>    def predict(self, text):\n            if not self.loaded_model:\n                return\n            with open(self.config.poetry_file, \'r\', encoding=\'utf-8\') as f:\n                file_list = f.readlines()\n            random_line = random.choice(file_list)\n            # 如果给的text不到四个字，则随机补全\n            if not text or len(text) != 4:\n                for _ in range(4 - len(text)):\n                    random_str_index = random.randrange(0, len(self.words))\n                    text += self.num2word.get(random_str_index) if self.num2word.get(random_str_index) not in [\',\', \'。\',\n                                                                                                               \'，\'] else self.num2word.get(\n                        random_str_index + 1)\n            seed = random_line[-(self.config.max_len):-1]\n            res = \'\'\n            seed = \'c\' + seed\n            for c in text:\n                seed = seed[1:] + c\n                for j in range(5):\n                    x_pred = np.zeros((1, self.config.max_len))\n                    for t, char in enumerate(seed):\n                        x_pred[0, t] = self.word2numF(char)\n                    preds = self.model.predict(x_pred, verbose=0)[0]\n                    next_index = self.sample(preds, 1.0)\n                    next_char = self.num2word[next_index]\n                    seed = seed[1:] + next_char\n                res += seed\n            return res\n</code></pre>\n\n<p>（6）&nbsp;<code>data_generator</code>&nbsp;函数，用于生成数据，提供给模型训练时使用。</p>\n\n<pre>\n<code>     def data_generator(self):\n            i = 0\n            while 1:\n                x = self.files_content[i: i + self.config.max_len]\n                y = self.files_content[i + self.config.max_len]\n                puncs = [\']\', \'[\', \'（\', \'）\', \'{\', \'}\', \'：\', \'《\', \'》\', \':\']\n                if len([i for i in puncs if i in x]) != 0:\n                    i += 1\n                    continue\n                if len([i for i in puncs if i in y]) != 0:\n                    i += 1\n                    continue\n                y_vec = np.zeros(\n                    shape=(1, len(self.words)),\n                    dtype=np.bool\n                )\n                y_vec[0, self.word2numF(y)] = 1.0\n                x_vec = np.zeros(\n                    shape=(1, self.config.max_len),\n                    dtype=np.int32\n                )\n                for t, char in enumerate(x):\n                    x_vec[0, t] = self.word2numF(char)\n                yield x_vec, y_vec\n                i += 1\n</code></pre>\n\n<p>（7）train 函数，用来进行模型训练，其中迭代次数&nbsp;<code>number_of_epoch</code>&nbsp;，是根据训练语料长度除以&nbsp;<code>batch_size</code>&nbsp;计算的，如果在调试中，想用更小一点的<code>number_of_epoch</code>&nbsp;，可以自定义大小，把 train 函数的第一行代码注释即可。</p>\n\n<pre>\n<code>    def train(self):\n            #number_of_epoch = len(self.files_content) // self.config.batch_size\n            number_of_epoch = 10\n            if not self.model:\n                self.build_model()\n            self.model.summary()\n            self.model.fit_generator(\n                generator=self.data_generator(),\n                verbose=True,\n                steps_per_epoch=self.config.batch_size,\n                epochs=number_of_epoch,\n                callbacks=[\n                    keras.callbacks.ModelCheckpoint(self.config.weight_file, save_weights_only=False),\n                    LambdaCallback(on_epoch_end=self.generate_sample_result)\n                ]\n            )\n</code></pre>\n\n<p><strong>第五</strong>，整个模型构建好以后，接下来进行模型训练。</p>\n\n<pre>\n<code>    model = PoetryModel(Config)\n</code></pre>\n\n<p>训练过程中的第1-2轮迭代：</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/2a5e35c0-8dbe-11e8-80d1-2d51ff7e1c55\" /></p>\n\n<p>训练过程中的第9-10轮迭代：</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/30ce9df0-8dbe-11e8-aa21-25f031a4e022\" /></p>\n\n<p>虽然训练过程写出的诗句不怎么能看得懂，但是可以看到模型从一开始标点符号都不会用 ，到最后写出了有一点点模样的诗句，能看到模型变得越来越聪明了。</p>\n\n<p><strong>第六</strong>，模型作诗，模型迭代10次之后的测试，首先输入几个字，模型根据输入的提示，做出诗句。</p>\n\n<pre>\n<code>    text = input(\"text:\")\n    sentence = model.predict(text)\n    print(sentence)\n</code></pre>\n\n<p>比如输入：小雨，模型做出的诗句为：</p>\n\n<blockquote>\n<p>输入：text：小雨</p>\n\n<p>结果：小妃侯里守。雨封即客寥。俘剪舟过槽。傲老槟冬绛。</p>\n</blockquote>\n\n<p><strong>第七</strong>，绘制网络结构图。</p>\n\n<p>模型结构绘图，采用 Keras自带的功能实现：</p>\n\n<pre>\n<code>    plot_model(model.model, to_file=\'model.png\')\n</code></pre>\n\n<p>得到的模型结构图如下：</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/d431b450-8dbe-11e8-80d1-2d51ff7e1c55\" /></p>\n\n<p>本节使用 LSTM 的变形 GRU 训练出一个能作诗的模型，当然大家可以替换训练语料为歌词或者小说，让机器人自动创作不同风格的歌曲或者小说。</p>\n\n<p><strong>参考文献以及推荐阅读：</strong></p>\n\n<ol>\n	<li><a href=\"https://blog.csdn.net/qiansg123/article/details/80131355\">基于 Keras 和 LSTM 的文本生成</a></li>\n</ol>\n\n<h2><a id=\"第12课：完全基于情感词典的文本情感分析\" name=\"第12课：完全基于情感词典的文本情感分析\"></a>第12课：完全基于情感词典的文本情感分析</h2>\n\n<p>目前情感分析在中文自然语言处理中比较火热，很多场景下，我们都需要用到情感分析。比如，做金融产品量化交易，需要根据爬取的舆论数据来分析政策和舆论对股市或者基金期货的态度；电商交易，根据买家的评论数据，来分析商品的预售率等等。</p>\n\n<p>下面我们通过以下几点来介绍中文自然语言处理情感分析：</p>\n\n<ol>\n	<li>中文情感分析方法简介；</li>\n	<li>SnowNLP 快速进行评论数据情感分析；</li>\n	<li>基于标注好的情感词典来计算情感值；</li>\n	<li>pytreebank 绘制情感树；</li>\n	<li>股吧数据情感分类。</li>\n</ol>\n\n<h3>中文情感分析方法简介</h3>\n\n<p>情感倾向可认为是主体对某一客体主观存在的内心喜恶，内在评价的一种倾向。它由两个方面来衡量：一个情感倾向方向，一个是情感倾向度。</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/0b26dd00-8a3b-11e8-affa-b587dc6ff574\" /></p>\n\n<p>目前，情感倾向分析的方法主要分为两类：一种是基于情感词典的方法；一种是基于机器学习的方法，如基于大规模语料库的机器学习。前者需要用到标注好的情感词典；后者则需要大量的人工标注的语料作为训练集，通过提取文本特征，构建分类器来实现情感的分类。</p>\n\n<p>文本情感分析的分析粒度可以是词语、句子、段落或篇章。</p>\n\n<p>段落篇章级情感分析主要是针对某个主题或事件进行情感倾向判断，一般需要构建对应事件的情感词典，如电影评论的分析，需要构建电影行业自己的情感词典，这样效果会比通用情感词典更好；也可以通过人工标注大量电影评论来构建分类器。句子级的情感分析大多通过计算句子里包含的所有情感词的值来得到。</p>\n\n<p>篇章级的情感分析，也可以通过聚合篇章中所有的句子的情感倾向来计算得出。因此，针对句子级的情感倾向分析，既能解决短文本的情感分析，同时也是篇章级文本情感分析的基础。</p>\n\n<p>中文情感分析的一些难点，比如句子是由词语根据一定的语言规则构成的，应该把句子中词语的依存关系纳入到句子情感的计算过程中去，不同的依存关系，进行情感倾向计算是不一样的。文档的情感，根据句子对文档的重要程度赋予不同权重，调整其对文档情感的贡献程度等。</p>\n\n<h3>SnowNLP 快速进行评论数据情感分析</h3>\n\n<p>如果有人问，有没有比较快速简单的方法能判断一句话的情感倾向，那么 SnowNLP 库就是答案。</p>\n\n<p>SnowNLP 主要可以进行中文分词、词性标注、情感分析、文本分类、转换拼音、繁体转简体、提取文本关键词、提取摘要、分割句子、文本相似等。</p>\n\n<p>需要注意的是，用 SnowNLP 进行情感分析，官网指出进行电商评论的准确率较高，其实是因为它的语料库主要是电商评论数据，但是可以自己构建相关领域语料库，替换单一的电商评论语料，准确率也挺不错的。</p>\n\n<p><strong>1.</strong>&nbsp;SnowNLP 安装。</p>\n\n<p>（1） 使用 pip 安装：</p>\n\n<pre>\n<code>pip install snownlp==0.11.1\n</code></pre>\n\n<p>（2）使用 Github 源码安装。</p>\n\n<p>首先，下载 SnowNLP 的&nbsp;<a href=\"https://github.com/isnowfy/snownlp\">Github</a>&nbsp;源码并解压，在解压目录，通过下面命令安装：</p>\n\n<pre>\n<code>python  setup.py install \n</code></pre>\n\n<p>以上方式，二选一安装完成之后，就可以引入 SnowNLP 库使用了。</p>\n\n<pre>\n<code>from snownlp import SnowNLP\n</code></pre>\n\n<p><strong>2.</strong>&nbsp;评论语料获取情感值。</p>\n\n<p>首先，SnowNLP 对情感的测试值为0到1，值越大，说明情感倾向越积极。下面我们通过 SnowNLP 测试在京东上找的好评、中评、差评的结果。</p>\n\n<p>首先，引入 SnowNLP 库：</p>\n\n<pre>\n<code>from snownlp import SnowNLP\n</code></pre>\n\n<p>（1） 测试一条京东的好评数据：</p>\n\n<pre>\n<code>SnowNLP(u\'本本已收到，体验还是很好，功能方面我不了解，只看外观还是很不错很薄，很轻，也有质感。\').sentiments\n</code></pre>\n\n<p>得到的情感值很高，说明买家对商品比较认可，情感值为：</p>\n\n<blockquote>\n<p>0.999950702449061</p>\n</blockquote>\n\n<p>（2）测试一条京东的中评数据：</p>\n\n<pre>\n<code>SnowNLP(u\'屏幕分辨率一般，送了个极丑的鼠标。\').sentiments\n</code></pre>\n\n<p>得到的情感值一般，说明买家对商品看法一般，甚至不喜欢，情感值为：</p>\n\n<blockquote>\n<p>0.03251402883400323</p>\n</blockquote>\n\n<p>（3）测试一条京东的差评数据：</p>\n\n<pre>\n<code>SnowNLP(u\'很差的一次购物体验，细节做得极差了，还有发热有点严重啊，散热不行，用起来就是烫得厉害，很垃圾！！！\').sentiments\n</code></pre>\n\n<p>得到的情感值一般，说明买家对商品不认可，存在退货嫌疑，情感值为：</p>\n\n<blockquote>\n<p>0.0036849517156107847</p>\n</blockquote>\n\n<p>以上就完成了简单快速的情感值计算，对评论数据是不是很好用呀！！！</p>\n\n<p>使用 SnowNLP 来计算情感值，官方推荐的是电商评论数据计算准确度比较高，难道非评论数据就不能使用 SnowNLP 来计算情感值了吗？当然不是，虽然 SnowNLP 默认提供的模型是用评论数据训练的，但是它还支持我们根据现有数据训练自己的模型。</p>\n\n<p>首先我们来看看自定义训练模型的<strong>源码 Sentiment 类</strong>，代码定义如下：</p>\n\n<pre>\n<code>class Sentiment(object):\n\n    def __init__(self):\n        self.classifier = Bayes()\n\n    def save(self, fname, iszip=True):\n        self.classifier.save(fname, iszip)\n\n    def load(self, fname=data_path, iszip=True):\n        self.classifier.load(fname, iszip)\n\n    def handle(self, doc):\n        words = seg.seg(doc)\n        words = normal.filter_stop(words)\n        return words\n\n    def train(self, neg_docs, pos_docs):\n        data = []\n        for sent in neg_docs:\n            data.append([self.handle(sent), \'neg\'])\n        for sent in pos_docs:\n            data.append([self.handle(sent), \'pos\'])\n        self.classifier.train(data)\n\n    def classify(self, sent):\n        ret, prob = self.classifier.classify(self.handle(sent))\n        if ret == \'pos\':\n            return prob\n        return 1-prob\n</code></pre>\n\n<p>通过源代码，我们可以看到，可以使用 train方法训练数据，并使用 save 方法和 load 方法保存与加载模型。下面训练自己的模型，训练集 pos.txt 和 neg.txt 分别表示积极和消极情感语句，两个 TXT 文本中每行表示一句语料。</p>\n\n<p>下面代码进行自定义模型训练和保存：</p>\n\n<pre>\n<code>from snownlp import sentiment\nsentiment.train(\'neg.txt\', \'pos.txt\')\nsentiment.save(\'sentiment.marshal\')\n</code></pre>\n\n<h3>基于标注好的情感词典来计算情感值</h3>\n\n<p>这里我们使用一个行业标准的情感词典&mdash;&mdash;玻森情感词典，来自定义计算一句话、或者一段文字的情感值。</p>\n\n<p>整个过程如下：</p>\n\n<ol>\n	<li>加载玻森情感词典；</li>\n	<li>jieba 分词；</li>\n	<li>获取句子得分。</li>\n</ol>\n\n<p>首先引入包：</p>\n\n<pre>\n<code>import pandas as pd\nimport jieba\n</code></pre>\n\n<p>接下来加载情感词典：</p>\n\n<pre>\n<code>df = pd.read_table(\"bosonnlp//BosonNLP_sentiment_score.txt\",sep= \" \",names=[\'key\',\'score\'])\n</code></pre>\n\n<p>查看一下情感词典前5行：</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/d07af1c0-8a38-11e8-974b-497483da0812\" /></p>\n\n<p>将词 key 和对应得分 score 转成2个 list 列表，目的是找到词 key 的时候，能对应获取到 score 值：</p>\n\n<pre>\n<code>key = df[\'key\'].values.tolist()\nscore = df[\'score\'].values.tolist()\n</code></pre>\n\n<p>定义分词和统计得分函数：</p>\n\n<pre>\n<code>def getscore(line):\n    segs = jieba.lcut(line)  #分词\n    score_list  = [score[key.index(x)] for x in segs if(x in key)]\n    return  sum(score_list)  #计算得分\n</code></pre>\n\n<p>最后来进行结果测试：</p>\n\n<pre>\n<code>line = \"今天天气很好，我很开心\"\nprint(round(getscore(line),2))\n\nline = \"今天下雨，心情也受到影响。\"\nprint(round(getscore(line),2))\n</code></pre>\n\n<p>获得的情感得分保留2位小数：</p>\n\n<blockquote>\n<p>5.26</p>\n\n<p>-0.96</p>\n</blockquote>\n\n<h3>pytreebank 绘制情感树</h3>\n\n<p><strong>1.</strong>&nbsp;安装 pytreebank。</p>\n\n<p>在 Github 上下载&nbsp;<a href=\"https://github.com/JonathanRaiman/pytreebank\">pytreebank 源码</a>，解压之后，进入解压目录命令行，执行命令：</p>\n\n<pre>\n<code>python setup.py install\n</code></pre>\n\n<p>最后通过引入命令，判断是否安装成功：</p>\n\n<pre>\n<code>import pytreebank\n</code></pre>\n\n<p>提示，如果在 Windows 下安装之后，报错误：</p>\n\n<pre>\n<code>UnicodeDecodeError: \'gbk\' codec can\'t decode byte 0x92 in position 24783: illegal multibyte sequence \n</code></pre>\n\n<p>这是由于编码问题引起的，可以在安装目录下报错的文件中报错的代码地方加个<code>encoding=&#39;utf-8&#39;</code>&nbsp;编码：</p>\n\n<pre>\n<code>import_tag( \"script\", contents=format_replacements(open(scriptname,encoding=\'utf-8\').read(), replacements), type=\"text/javascript\" )\n</code></pre>\n\n<p><strong>2.</strong>&nbsp;绘制情感树。</p>\n\n<p>首先引入 pytreebank 包：</p>\n\n<pre>\n<code>    import pytreebank\n</code></pre>\n\n<p>然后，加载用来可视化的 JavaScript 和 CSS 脚本：</p>\n\n<pre>\n<code>pytreebank.LabeledTree.inject_visualization_javascript()\n</code></pre>\n\n<p>绘制情感树，把句子首先进行组合再绘制图形：</p>\n\n<pre>\n<code>line = \'(4 (0 你) (3 (2 是) (3 (3 (3 谁) (2 的)) (2 谁))))\'\npytreebank.create_tree_from_string(line).display()\n</code></pre>\n\n<p>得到的情感树如下：</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/d23c18d0-8a2f-11e8-b6c7-73392c05a2ed\" /></p>\n\n<h3>股吧数据情感分类</h3>\n\n<p>历经89天的煎熬之后，7月15日中兴终于盼来了解禁，在此首先恭喜中兴，解禁了，希望再踏征程。</p>\n\n<p>但在7月15日之前，随着中美贸易战不断升级，中兴股价又上演了一场&ldquo;跌跌不休&rdquo;的惨状，我以中美贸易战背景下中兴通讯在股吧解禁前一段时间的评论数据，来进行情感数据人工打标签和分类。其中，把消极 、中性 、积极分别用0、1、2来表示。</p>\n\n<p>整个文本分类流程主要包括以下6个步骤：</p>\n\n<ul>\n	<li>中文语料；</li>\n	<li>分词；</li>\n	<li>复杂规则；</li>\n	<li>特征向量；</li>\n	<li>算法建模；</li>\n	<li>情感分析。</li>\n</ul>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/d4586f00-81b1-11e8-b718-fd519f27386c\" /></p>\n\n<p>本次分类算法采用 CNN，首先引入需要的包：</p>\n\n<pre>\n<code>import pandas as pd\nimport numpy as np\nimport jieba\nimport random\nimport keras\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.layers import Embedding\nfrom keras.layers import Conv1D, GlobalMaxPooling1D\nfrom keras.datasets import imdb\nfrom keras.models import model_from_json\nfrom keras.utils import np_utils\nimport matplotlib.pyplot as plt\n</code></pre>\n\n<p>继续引入停用词和语料文件：</p>\n\n<pre>\n<code>dir = \"D://ProgramData//PythonWorkSpace//chat//chat8//\"\nstopwords=pd.read_csv(dir +\"stopwords.txt\",index_col=False,quoting=3,sep=\"\\t\",names=[\'stopword\'], encoding=\'utf-8\')\nstopwords=stopwords[\'stopword\'].values\ndf_data1 = pd.read_csv(dir+\"data1.csv\",encoding=\'utf-8\')\ndf_data1.head()\n</code></pre>\n\n<p>下图展示数据的前5行：</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/7b3e3740-8a28-11e8-b1ae-51065196ffc8\" /></p>\n\n<p>接着进行数据预处理，把消极、中性、积极分别为0、1、2的预料分别拿出来：</p>\n\n<pre>\n<code>#把内容有缺失值的删除\ndf_data1.dropna(inplace=True)\n#抽取文本数据和标签\ndata_1 = df_data1.loc[:,[\'content\',\'label\']]\n#把消极  中性  积极分别为0、1、2的预料分别拿出来\ndata_label_0 = data_1.loc[data_1[\'label\'] ==0,:]\ndata_label_1 = data_1.loc[data_1[\'label\'] ==1,:]\ndata_label_2 = data_1.loc[data_1[\'label\'] ==2,:]\n</code></pre>\n\n<p>接下来，定义中文分词函数：</p>\n\n<pre>\n<code>#定义分词函数\ndef preprocess_text(content_lines, sentences, category):\n    for line in content_lines:\n        try:\n            segs=jieba.lcut(line)\n            segs = filter(lambda x:len(x)&gt;1, segs)\n            segs = [v for v in segs if not str(v).isdigit()]#去数字\n            segs = list(filter(lambda x:x.strip(), segs)) #去左右空格\n            segs = filter(lambda x:x not in stopwords, segs)\n            temp = \" \".join(segs)\n            if(len(temp)&gt;1):\n                sentences.append((temp, category))\n        except Exception:\n            print(line)\n            continue \n</code></pre>\n\n<p>生成训练的分词数据，并进行打散，使其分布均匀：</p>\n\n<pre>\n<code>#获取数据\ndata_label_0_content = data_label_0[\'content\'].values.tolist()\ndata_label_1_content = data_label_1[\'content\'].values.tolist()\ndata_label_2_content = data_label_2[\'content\'].values.tolist()\n#生成训练数据\nsentences = []\npreprocess_text(data_label_0_content, sentences, 0)\npreprocess_text(data_label_1_content, sentences, 1)\npreprocess_text(data_label_2_content, sentences,2)\n#我们打乱一下顺序，生成更可靠的训练集\nrandom.shuffle(sentences)\n</code></pre>\n\n<p>对数据集进行切分，按照训练集合测试集7:3的比例：</p>\n\n<pre>\n<code>#所以把原数据集分成训练集的测试集，咱们用sklearn自带的分割函数。\nfrom sklearn.model_selection import train_test_split\nx, y = zip(*sentences)\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3,random_state=1234)\n</code></pre>\n\n<p>然后，对特征构造词向量：</p>\n\n<pre>\n<code>#抽取特征，我们对文本抽取词袋模型特征\nfrom sklearn.feature_extraction.text import CountVectorizer\nvec = CountVectorizer(\n    analyzer=\'word\', #tokenise by character ngrams\n    max_features=4000,  #keep the most common 1000 ngrams\n)\nvec.fit(x_train)\n</code></pre>\n\n<p>定义模型参数：</p>\n\n<pre>\n<code># 设置参数\nmax_features = 5001\nmaxlen = 100\nbatch_size = 32\nembedding_dims = 50\nfilters = 250\nkernel_size = 3\nhidden_dims = 250\nepochs = 10\nnclasses = 3\n</code></pre>\n\n<p>输入特征转成 Array 和标签处理，打印训练集和测试集的 shape：</p>\n\n<pre>\n<code>x_train = vec.transform(x_train)\nx_test = vec.transform(x_test)\nx_train = x_train.toarray()\nx_test = x_test.toarray()\ny_train = np_utils.to_categorical(y_train,nclasses)\ny_test = np_utils.to_categorical(y_test,nclasses)\nx_train = sequence.pad_sequences(x_train, maxlen=maxlen)\nx_test = sequence.pad_sequences(x_test, maxlen=maxlen)\nprint(\'x_train shape:\', x_train.shape)\nprint(\'x_test shape:\', x_test.shape)\n</code></pre>\n\n<p>定义一个绘制 Loss 曲线的类：</p>\n\n<pre>\n<code>class LossHistory(keras.callbacks.Callback):\n    def on_train_begin(self, logs={}):\n        self.losses = {\'batch\':[], \'epoch\':[]}\n        self.accuracy = {\'batch\':[], \'epoch\':[]}\n        self.val_loss = {\'batch\':[], \'epoch\':[]}\n        self.val_acc = {\'batch\':[], \'epoch\':[]}\n\n    def on_batch_end(self, batch, logs={}):\n        self.losses[\'batch\'].append(logs.get(\'loss\'))\n        self.accuracy[\'batch\'].append(logs.get(\'acc\'))\n        self.val_loss[\'batch\'].append(logs.get(\'val_loss\'))\n        self.val_acc[\'batch\'].append(logs.get(\'val_acc\'))\n\n    def on_epoch_end(self, batch, logs={}):\n        self.losses[\'epoch\'].append(logs.get(\'loss\'))\n        self.accuracy[\'epoch\'].append(logs.get(\'acc\'))\n        self.val_loss[\'epoch\'].append(logs.get(\'val_loss\'))\n        self.val_acc[\'epoch\'].append(logs.get(\'val_acc\'))\n\n    def loss_plot(self, loss_type):\n        iters = range(len(self.losses[loss_type]))\n        plt.figure()\n        # acc\n        plt.plot(iters, self.accuracy[loss_type], \'r\', label=\'train acc\')\n        # loss\n        plt.plot(iters, self.losses[loss_type], \'g\', label=\'train loss\')\n        if loss_type == \'epoch\':\n            # val_acc\n            plt.plot(iters, self.val_acc[loss_type], \'b\', label=\'val acc\')\n            # val_loss\n            plt.plot(iters, self.val_loss[loss_type], \'k\', label=\'val loss\')\n        plt.grid(True)\n        plt.xlabel(loss_type)\n        plt.ylabel(\'acc-loss\')\n        plt.legend(loc=\"upper right\")\n        plt.show()\n</code></pre>\n\n<p>然后，初始化上面类的对象，并作为模型的回调函数输入，训练模型：</p>\n\n<pre>\n<code>history = LossHistory()\nprint(\'Build model...\')\nmodel = Sequential()\n\nmodel.add(Embedding(max_features,\n                        embedding_dims,\n                        input_length=maxlen))\nmodel.add(Dropout(0.5))\nmodel.add(Conv1D(filters,\n                     kernel_size,\n                     padding=\'valid\',\n                     activation=\'relu\',\n                     strides=1))\nmodel.add(GlobalMaxPooling1D())\nmodel.add(Dense(hidden_dims))\nmodel.add(Dropout(0.5))\nmodel.add(Activation(\'relu\'))\nmodel.add(Dense(nclasses))\nmodel.add(Activation(\'softmax\'))\nmodel.compile(loss=\'categorical_crossentropy\',\n                  optimizer=\'adam\',\n                  metrics=[\'accuracy\'])\nmodel.fit(x_train, y_train,\n              batch_size=batch_size,\n              epochs=epochs,\n              validation_data=(x_test, y_test),callbacks=[history])\n</code></pre>\n\n<p>得到的模型迭代次数为10轮的训练过程：</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/f329cce0-8c2d-11e8-b9de-5bb0fbe09c97\" /></p>\n\n<p>最后绘制 Loss 图像：</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/0886db00-8c2e-11e8-8e9b-93ccda1ef1a2\" /></p>\n\n<p>关于本次分类，这里重点讨论的一个知识点就是数据分布不均匀的情况，我们都知道，本次贸易战中兴公司受影响很大，导致整个股票价格处于下跌趋势，所以整个舆论上，大多数评论都是消极的态度，导致数据分布极不均匀。</p>\n\n<p>那数据分布不均匀一般怎么处理呢？从以下几个方面考虑：</p>\n\n<ul>\n	<li>\n	<p>数据采样，包括上采样、下采样和综合采样；</p>\n	</li>\n	<li>\n	<p>改变分类算法，在传统分类算法的基础上对不同类别采取不同的加权方式，使得模型更看重少数类；</p>\n	</li>\n	<li>\n	<p>采用合理的性能评价指标；</p>\n	</li>\n	<li>\n	<p>代价敏感。</p>\n	</li>\n</ul>\n\n<p>总结，本文通过第三方、基于词典等方式计算中文文本情感值，以及通过情感树来进行可视化，然而这些内容只是情感分析的入门知识，情感分析还涉及句法依存等，最后通过一个 CNN 分类模型，提供一种有监督的情感分类思路。</p>\n\n<p><strong>参考文献及推荐阅读：</strong></p>\n\n<ol>\n	<li>\n	<p><a href=\"https://gitbook.cn/gitchat/activity/5b3341ed28f60a20b62890f8\">基于情感词典的中文自然语言处理情感分析（上）</a></p>\n	</li>\n	<li>\n	<p><a href=\"https://gitbook.cn/gitchat/activity/5b3f2a34041b5c0e72c93383\">基于情感词典的中文自然语言处理情感分析（下）</a></p>\n	</li>\n</ol>\n\n<h2><a id=\"第13课：动手制作自己的简易聊天机器人\" name=\"第13课：动手制作自己的简易聊天机器人\"></a>第13课：动手制作自己的简易聊天机器人</h2>\n\n<h3>自动问答简介</h3>\n\n<p>自动聊天机器人，也称为自动问答系统，由于所使用的场景不同，叫法也不一样。自动问答（Question Answering，QA）是指利用计算机自动回答用户所提出的问题以满足用户知识需求的任务。不同于现有搜索引擎，问答系统是信息服务的一种高级形式，系统返回用户的不再是基于关键词匹配排序的文档列表，而是精准的自然语言答案。近年来，随着人工智能的飞速发展，自动问答已经成为倍受关注且发展前景广泛的研究方向。</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/e893ccf0-8e48-11e8-8ee0-a17ea463076e\" /></p>\n\n<p>自动问答主要研究的内容和关键科学问题如下：</p>\n\n<ol>\n	<li>\n	<p><strong>问句理解</strong>：给定用户问题，自动问答首先需要理解用户所提问题。用户问句的语义理解包含词法分析、句法分析、语义分析等多项关键技术，需要从文本的多个维度理解其中包含的语义内容。</p>\n	</li>\n	<li>\n	<p><strong>文本信息抽取</strong>：自动问答系统需要在已有语料库、知识库或问答库中匹配相关的信息，并抽取出相应的答案。</p>\n	</li>\n	<li>\n	<p><strong>知识推理</strong>：自动问答中，由于语料库、知识库和问答库本身的覆盖度有限，并不是所有问题都能直接找到答案。这就需要在已有的知识体系中，通过知识推理的手段获取这些隐含的答案。</p>\n	</li>\n</ol>\n\n<p>纵观自动问答研究的发展态势和技术现状，以下研究方向或问题将可能成为未来整个领域和行业重点关注的方向：基于深度学习的端到端自动问答，多领域、多语言的自动问答，面向问答的深度推理，篇章阅读理解、对话等。</p>\n\n<h3>基于 Chatterbot 制作中文聊天机器人</h3>\n\n<p>ChatterBot 是一个构建在 Python 上，基于一系列规则和机器学习算法完成的聊天机器人，具有结构清晰，可扩展性好，简单实用的特点。</p>\n\n<p>Chatterbot 安装有两种方式：</p>\n\n<ul>\n	<li>使用&nbsp;<code>pip install chatterbot</code>&nbsp;安装；</li>\n	<li>直接在<a href=\"https://github.com/gunthercox/ChatterBot\">&nbsp;Github Chatterbot</a>&nbsp;下载这个项目，通过&nbsp;<code>python setup.py install</code>&nbsp;安装，其中 examples 文件夹中包含几个例子，可以根据例子加深自己的理解。</li>\n</ul>\n\n<p>安装过程如果出现错误，主要是需要安装这些依赖库：</p>\n\n<pre>\n<code>chatterbot-corpus&gt;=1.1,&lt;1.2\nmathparse&gt;=0.1,&lt;0.2\nnltk&gt;=3.2,&lt;4.0\npymongo&gt;=3.3,&lt;4.0\npython-dateutil&gt;=2.6,&lt;2.7\npython-twitter&gt;=3.0,&lt;4.0\nsqlalchemy&gt;=1.2,&lt;1.3\npint&gt;=0.8.1\n</code></pre>\n\n<p><strong>1.</strong>&nbsp;手动设置一点语料，体验基于规则的聊天机器人回答。</p>\n\n<pre>\n<code>from chatterbot import ChatBot\nfrom chatterbot.trainers import ListTrainer\nChinese_bot = ChatBot(\"Training demo\") #创建一个新的实例\nChinese_bot.set_trainer(ListTrainer)\nChinese_bot.train([\n    \'亲，在吗？\',\n    \'亲，在呢\',\n    \'这件衣服的号码大小标准吗？\',\n    \'亲，标准呢，请放心下单吧。\',\n    \'有红色的吗？\',\n    \'有呢，目前有白红蓝3种色调。\',\n])\n</code></pre>\n\n<p>下面进行测试：</p>\n\n<pre>\n<code># 测试一下\nquestion = \'亲，在吗\'\nprint(question)\nresponse = Chinese_bot.get_response(question)\nprint(response)\nprint(\"\\n\")\nquestion = \'有红色的吗？\'\nprint(question)\nresponse = Chinese_bot.get_response(question)\nprint(response)\n</code></pre>\n\n<p>从得到的结果可以看出，这应该完全是基于规则的判断：</p>\n\n<blockquote>\n<p>亲，在吗</p>\n\n<p>亲，在呢</p>\n\n<p>有红色的吗？</p>\n\n<p>有呢，目前有白红蓝3种色调。</p>\n</blockquote>\n\n<p><strong>2.</strong>&nbsp;训练自己的语料。</p>\n\n<p>本次使用的语料来自 QQ 群的聊天记录，导出的 QQ 聊天记录稍微处理一下即可使用，整个过程如下。</p>\n\n<p>（1）首先载入语料，第二行代码主要是想把每句话后面的换行&nbsp;<code>\\n</code>&nbsp;去掉。</p>\n\n<pre>\n<code>lines = open(\"QQ.txt\",\"r\",encoding=\'gbk\').readlines()\nsec = [ line.strip() for line in lines]\n</code></pre>\n\n<p>（2）接下来就可以训练模型了，由于整个语料比较大，训练过程也比较耗时。</p>\n\n<pre>\n<code>from chatterbot import ChatBot\nfrom chatterbot.trainers import ListTrainer\nChinese_bot = ChatBot(\"Training\")\nChinese_bot.set_trainer(ListTrainer)\nChinese_bot.train(sec)\n</code></pre>\n\n<p>这里需要注意，如果训练过程很慢，可以在第一步中加入如下代码，即只取前1000条进行训练：</p>\n\n<pre>\n<code>sec = sec[0:1000]\n</code></pre>\n\n<p>（3）最后，对训练好的模型进行测试，可见训练数据是 QQ 群技术对话，也看得出程序员们都很努力，整体想的都是学习。</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/cc2657e0-8fce-11e8-8472-d52f55e7330e\" /></p>\n\n<p>以上只是简单的 Chatterbot 演示，如果想看更好的应用，推荐看官方文档。</p>\n\n<h3>基于 Seq2Seq 制作中文聊天机器人</h3>\n\n<p>前面，我们在第09课<a href=\"https://gitbook.cn/gitchat/column/5b10b073aafe4e5a7516708b/topic/5b1110c0f96a5f71f138462e\">《一网打尽神经序列模型之 RNN 及其变种 LSTM、GRU》</a>中讲了序列数据处理模型，从&nbsp;<code>N-gram</code>&nbsp;语言模型到 RNN 及其变种。这里我们讲另外一个基于深度学习的 Seq2Seq 模型。</p>\n\n<p>从 RNN 结构说起，根据输出和输入序列不同数量 RNN ，可以有多种不同的结构，不同结构自然就有不同的引用场合。</p>\n\n<ul>\n	<li>One To One 结构，仅仅只是简单的给一个输入得到一个输出，此处并未体现序列的特征，例如图像分类场景。</li>\n	<li>One To Many 结构，给一个输入得到一系列输出，这种结构可用于生产图片描述的场景。</li>\n	<li>Many To One 结构，给一系列输入得到一个输出，这种结构可用于文本情感分析，对一些列的文本输入进行分类，看是消极还是积极情感。</li>\n	<li>Many To Many 结构，给一系列输入得到一系列输出，这种结构可用于翻译或聊天对话场景，将输入的文本转换成另外一系列文本。</li>\n	<li>同步 Many To Many 结构，它是经典的 RNN 结构，前一输入的状态会带到下一个状态中，而且每个输入都会对应一个输出，我们最熟悉的应用场景是字符预测，同样也可以用于视频分类，对视频的帧打标签。</li>\n</ul>\n\n<p>在 Many To Many 的两种模型中，第四和第五种是有差异的，经典 RNN 结构的输入和输出序列必须要等长，它的应用场景也比较有限。而第四种，输入和输出序列可以不等长，这种模型便是 Seq2Seq 模型，即 Sequence to Sequence。它实现了从一个序列到另外一个序列的转换，比如 Google 曾用 Seq2Seq 模型加 Attention 模型实现了翻译功能，类似的还可以实现聊天机器人对话模型。经典的 RNN 模型固定了输入序列和输出序列的大小，而 Seq2Seq 模型则突破了该限制。</p>\n\n<p>Seq2Seq 属于&nbsp;<code>Encoder-Decoder</code>&nbsp;结构，这里看看常见的&nbsp;<code>Encoder-Decoder</code>&nbsp;结构。基本思想就是利用两个 RNN，一个 RNN 作为 Encoder，另一个 RNN 作为 Decoder。Encoder 负责将输入序列压缩成指定长度的向量，这个向量就可以看成是这个序列的语义，这个过程称为编码，如下图，获取语义向量最简单的方式就是直接将最后一个输入的隐状态作为语义向量。也可以对最后一个隐含状态做一个变换得到语义向量，还可以将输入序列的所有隐含状态做一个变换得到语义变量。</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/33ed6e10-8e56-11e8-aa21-25f031a4e022\" /></p>\n\n<p>具体理论知识这里不再赘述，下面重点看看，如何通过 Keras 实现一个<code>LSTM_Seq2Seq</code>&nbsp;自动问答机器人。</p>\n\n<p><strong>1.</strong>&nbsp;语料准备。</p>\n\n<p>语料我们使用 Tab 键&nbsp;<code>\\t</code>&nbsp;把问题和答案区分，每一对为一行。其中，语料为爬虫爬取的工程机械网站的问答。</p>\n\n<p><strong>2.</strong>&nbsp;模型构建和训练。</p>\n\n<p>第一步，引入需要的包：</p>\n\n<pre>\n<code>from keras.models import Model\nfrom keras.layers import Input, LSTM, Dense\nimport numpy as np\nimport pandas as pd\n</code></pre>\n\n<p>第二步，定义模型超参数、迭代次数、语料路径：</p>\n\n<pre>\n<code>#Batch size 的大小\nbatch_size = 32  \n# 迭代次数epochs\nepochs = 100\n# 编码空间的维度Latent dimensionality \nlatent_dim = 256  \n# 要训练的样本数\nnum_samples = 5000 \n#设置语料的路径\ndata_path = \'D://nlp//ch13//files.txt\'\n</code></pre>\n\n<p>第三步，把语料向量化：</p>\n\n<pre>\n<code>#把数据向量话\ninput_texts = []\ntarget_texts = []\ninput_characters = set()\ntarget_characters = set()\n\nwith open(data_path, \'r\', encoding=\'utf-8\') as f:\n    lines = f.read().split(\'\\n\')\nfor line in lines[: min(num_samples, len(lines) - 1)]:\n    #print(line)\n    input_text, target_text = line.split(\'\\t\')\n    # We use \"tab\" as the \"start sequence\" character\n    # for the targets, and \"\\n\" as \"end sequence\" character.\n    target_text = target_text[0:100]\n    target_text = \'\\t\' + target_text + \'\\n\'\n    input_texts.append(input_text)\n    target_texts.append(target_text)\n\n    for char in input_text:\n        if char not in input_characters:\n            input_characters.add(char)\n    for char in target_text:\n        if char not in target_characters:\n            target_characters.add(char)\n\ninput_characters = sorted(list(input_characters))\ntarget_characters = sorted(list(target_characters))\nnum_encoder_tokens = len(input_characters)\nnum_decoder_tokens = len(target_characters)\nmax_encoder_seq_length = max([len(txt) for txt in input_texts])\nmax_decoder_seq_length = max([len(txt) for txt in target_texts])\n\nprint(\'Number of samples:\', len(input_texts))\nprint(\'Number of unique input tokens:\', num_encoder_tokens)\nprint(\'Number of unique output tokens:\', num_decoder_tokens)\nprint(\'Max sequence length for inputs:\', max_encoder_seq_length)\nprint(\'Max sequence length for outputs:\', max_decoder_seq_length)\n\ninput_token_index = dict(\n    [(char, i) for i, char in enumerate(input_characters)])\ntarget_token_index = dict(\n    [(char, i) for i, char in enumerate(target_characters)])\n\nencoder_input_data = np.zeros(\n    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),dtype=\'float32\')\ndecoder_input_data = np.zeros(\n    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),dtype=\'float32\')\ndecoder_target_data = np.zeros(\n    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),dtype=\'float32\')\n\nfor i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n    for t, char in enumerate(input_text):\n        encoder_input_data[i, t, input_token_index[char]] = 1.\n    for t, char in enumerate(target_text):\n        # decoder_target_data is ahead of decoder_input_data by one timestep\n        decoder_input_data[i, t, target_token_index[char]] = 1.\n        if t &gt; 0:\n            # decoder_target_data will be ahead by one timestep\n            # and will not include the start character.\n            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n</code></pre>\n\n<p>第四步，<code>LSTM_Seq2Seq</code>&nbsp;模型定义、训练和保存：</p>\n\n<pre>\n<code>encoder_inputs = Input(shape=(None, num_encoder_tokens))\nencoder = LSTM(latent_dim, return_state=True)\nencoder_outputs, state_h, state_c = encoder(encoder_inputs)\n# 输出 `encoder_outputs` \nencoder_states = [state_h, state_c]\n\n# 状态 `encoder_states` \ndecoder_inputs = Input(shape=(None, num_decoder_tokens))\ndecoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\ndecoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n                       initial_state=encoder_states)\ndecoder_dense = Dense(num_decoder_tokens, activation=\'softmax\')\ndecoder_outputs = decoder_dense(decoder_outputs)\n\n# 定义模型\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n\n# 训练\nmodel.compile(optimizer=\'rmsprop\', loss=\'categorical_crossentropy\')\nmodel.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n          batch_size=batch_size,\n          epochs=epochs,\n          validation_split=0.2)\n# 保存模型\nmodel.save(\'s2s.h5\')\n</code></pre>\n\n<p>第五步，Seq2Seq 的 Encoder 操作：</p>\n\n<pre>\n<code>encoder_model = Model(encoder_inputs, encoder_states)\n\ndecoder_state_input_h = Input(shape=(latent_dim,))\ndecoder_state_input_c = Input(shape=(latent_dim,))\ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\ndecoder_outputs, state_h, state_c = decoder_lstm(\n    decoder_inputs, initial_state=decoder_states_inputs)\ndecoder_states = [state_h, state_c]\ndecoder_outputs = decoder_dense(decoder_outputs)\ndecoder_model = Model(\n    [decoder_inputs] + decoder_states_inputs,\n    [decoder_outputs] + decoder_states)\n</code></pre>\n\n<p>第六步，把索引和分词转成序列：</p>\n\n<pre>\n<code>reverse_input_char_index = dict(\n    (i, char) for char, i in input_token_index.items())\nreverse_target_char_index = dict(\n    (i, char) for char, i in target_token_index.items())\n</code></pre>\n\n<p>第七步，定义预测函数，先使用预模型预测，然后编码成汉字结果：</p>\n\n<pre>\n<code>def decode_sequence(input_seq):\n    # Encode the input as state vectors.\n    states_value = encoder_model.predict(input_seq)\n    #print(states_value)\n\n    # Generate empty target sequence of length 1.\n    target_seq = np.zeros((1, 1, num_decoder_tokens))\n    # Populate the first character of target sequence with the start character.\n    target_seq[0, 0, target_token_index[\'\\t\']] = 1.\n\n    # Sampling loop for a batch of sequences\n    # (to simplify, here we assume a batch of size 1).\n    stop_condition = False\n    decoded_sentence = \'\'\n    while not stop_condition:\n        output_tokens, h, c = decoder_model.predict(\n            [target_seq] + states_value)\n\n        # Sample a token\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_char = reverse_target_char_index[sampled_token_index]\n        decoded_sentence += sampled_char\n        if (sampled_char == \'\\n\' or\n           len(decoded_sentence) &gt; max_decoder_seq_length):\n            stop_condition = True\n\n        # Update the target sequence (of length 1).\n        target_seq = np.zeros((1, 1, num_decoder_tokens))\n        target_seq[0, 0, sampled_token_index] = 1.\n        # 更新状态\n        states_value = [h, c]\n    return decoded_sentence\n</code></pre>\n\n<p><strong>3.</strong>&nbsp;模型预测。</p>\n\n<p>首先，定义一个预测函数：</p>\n\n<pre>\n<code>def predict_ans(question):\n        inseq = np.zeros((len(question), max_encoder_seq_length, num_encoder_tokens),dtype=\'float16\')\n        decoded_sentence = decode_sequence(inseq)\n        return decoded_sentence\n</code></pre>\n\n<p>然后就可以预测了：</p>\n\n<pre>\n<code>print(\'Decoded sentence:\', predict_ans(\"挖机履带掉了怎么装上去\"))\n</code></pre>\n\n<h3>总结</h3>\n\n<p>本文我们首先基于 Chatterbot 制作了中文聊天机器人，并用 QQ 群对话语料自己尝试训练。然后通过 LSTM 和 Seq2Seq 模型，根据爬取的语料，训练了一个自动问答的模型，通过以上两种方式，我们们对自动问答有了一个简单的入门。</p>\n\n<p><strong>参考文献及推荐阅读：</strong></p>\n\n<ol>\n	<li><a href=\"http://www.cipsc.org.cn/\">《中文信息处理发展报告（2016）》</a></li>\n	<li><a href=\"http://chatterbot.readthedocs.io/en/stable/training.html\">ChatterBot 文档</a></li>\n	<li><a href=\"https://github.com/gunthercox/ChatterBot\">ChatterBot 的 GitHub</a></li>\n	<li><a href=\"https://arxiv.org/abs/1409.3215\">Sutskever, Vinyals and Le (2014)</a></li>\n	<li><a href=\"https://blog.csdn.net/starzhou/article/details/78171936\">漫谈四种神经网络序列解码模型</a></li>\n	<li><a href=\"https://jingyan.baidu.com/article/d621e8da5b3b482865913f99.html\">怎样导出 QQ 群里的所有聊天记录？</a></li>\n	<li><a href=\"https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py\">Keras 中的&nbsp;<code>LSTM_Seq2Seq</code>&nbsp;例子</a></li>\n</ol>\n\n<h2><a id=\"第14课：动手实战中文命名实体提取\" name=\"第14课：动手实战中文命名实体提取\"></a>第14课：动手实战中文命名实体提取</h2>\n\n<p>命名实体识别（Named EntitiesRecognition，NER）是自然语言处理的一个基础任务。其目的是识别语料中人名、地名、组织机构名等命名实体，比如，<a href=\"https://baike.baidu.com/item/2015%E5%B9%B4%E4%B8%AD%E5%9B%BD%E5%91%BD%E5%90%8D%E7%9A%84124%E4%B8%AA%E5%9B%BD%E9%99%85%E6%B5%B7%E5%BA%95%E5%9C%B0%E7%90%86%E5%AE%9E%E4%BD%93%E5%90%8D%E7%A7%B0%E4%BF%A1%E6%81%AF/18705238\">2015年中国国家海洋局对124个国际海底地理实体的命名</a>。</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/ed101af0-9617-11e8-9c35-b59aad3fef8b\" /></p>\n\n<p>由于命名实体数量不断增加，通常不可能在词典中穷尽列出，且其构成方法具有各自的一些规律性，因而，通常把对这些词的识别从词汇形态处理（如汉语切分）任务中独立处理，称为命名实体识别。</p>\n\n<p>命名实体识别技术是信息抽取、信息检索、机器翻译、问答系统等多种自然语言处理技术必不可少的组成部分。</p>\n\n<h3>常见的命名实体识别方法综述</h3>\n\n<p>命名实体是命名实体识别的研究主体，一般包括三大类（实体类、时间类和数字类）和七小类（人名、地名、机构名、时间、日期、货币和百分比）命名实体。评判一个命名实体是否被正确识别包括两个方面：实体的边界是否正确和实体的类型是否标注正确。</p>\n\n<p>命名实体识别的主要技术方法分为：基于规则和词典的方法、基于统计的方法、二者混合的方法等。</p>\n\n<p><strong>1.基于规则和词典的方法。</strong></p>\n\n<p>基于规则的方法多采用语言学专家手工构造规则模板，选用特征包括统计信息、标点符号、关键字、指示词和方向词、位置词（如尾字）、中心词等方法，以模式和字符串相匹配为主要手段，这类系统大多依赖于知识库和词典的建立。基于规则和词典的方法是命名实体识别中最早使用的方法，一般而言，当提取的规则能比较精确地反映语言现象时，基于规则的方法性能要优于基于统计的方法。但是这些规则往往依赖于具体语言、领域和文本风格，编制过程耗时且难以涵盖所有的语言现象，特别容易产生错误，系统可移植性不好，对于不同的系统需要语言学专家重新书写规则。基于规则的方法的另外一个缺点是代价太大，存在系统建设周期长、移植性差而且需要建立不同领域知识库作为辅助以提高系统识别能力等问题。</p>\n\n<p><strong>2.基于统计的方法。</strong></p>\n\n<p>基于统计机器学习的方法主要包括隐马尔可夫模型（HiddenMarkovMode，HMM）、最大熵（MaxmiumEntropy，ME）、支持向量机（Support VectorMachine，SVM）、条件随机场（ConditionalRandom Fields，CRF）等。</p>\n\n<p>在基于统计的这四种学习方法中，最大熵模型结构紧凑，具有较好的通用性，主要缺点是训练时间长复杂性高，有时甚至导致训练代价难以承受，另外由于需要明确的归一化计算，导致开销比较大。而条件随机场为命名实体识别提供了一个特征灵活、全局最优的标注框架，但同时存在收敛速度慢、训练时间长的问题。一般说来，最大熵和支持向量机在正确率上要比隐马尔可夫模型高一些，但隐马尔可夫模型在训练和识别时的速度要快一些，主要是由于在利用 Viterbi 算法求解命名实体类别序列时的效率较高。隐马尔可夫模型更适用于一些对实时性有要求以及像信息检索这样需要处理大量文本的应用，如短文本命名实体识别。</p>\n\n<p>基于统计的方法对特征选取的要求较高，需要从文本中选择对该项任务有影响的各种特征，并将这些特征加入到特征向量中。依据特定命名实体识别所面临的主要困难和所表现出的特性，考虑选择能有效反映该类实体特性的特征集合。主要做法是通过对训练语料所包含的语言信息进行统计和分析，从训练语料中挖掘出特征。有关特征可以分为具体的单词特征、上下文特征、词典及词性特征、停用词特征、核心词特征以及语义特征等。</p>\n\n<p>基于统计的方法对语料库的依赖也比较大，而可以用来建设和评估命名实体识别系统的大规模通用语料库又比较少。</p>\n\n<p><strong>3.混合方法。</strong></p>\n\n<p>自然语言处理并不完全是一个随机过程，单独使用基于统计的方法使状态搜索空间非常庞大，必须借助规则知识提前进行过滤修剪处理。目前几乎没有单纯使用统计模型而不使用规则知识的命名实体识别系统，在很多情况下是使用混合方法：</p>\n\n<ol>\n	<li>统计学习方法之间或内部层叠融合。</li>\n	<li>规则、词典和机器学习方法之间的融合，其核心是融合方法技术。在基于统计的学习方法中引入部分规则，将机器学习和人工知识结合起来。</li>\n	<li>将各类模型、算法结合起来，将前一级模型的结果作为下一级的训练数据，并用这些训练数据对模型进行训练，得到下一级模型。</li>\n</ol>\n\n<h3>命名实体识别的一般流程</h3>\n\n<p>如下图所示，一般的命名实体流程主要分为四个步骤：</p>\n\n<ol>\n	<li>对需要进行提取的文本语料进行分词；</li>\n	<li>获取需要识别的领域标签，并对分词结果进行标签标注；</li>\n	<li>对标签标注的分词进行抽取；</li>\n	<li>将抽取的分词组成需要的领域的命名实体。</li>\n</ol>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/b4114e00-9616-11e8-afe4-2b97d4c05a56\" /></p>\n\n<h3>动手实战命名实体识别</h3>\n\n<p>命名实体的类别，我们在<a href=\"https://gitbook.cn/gitchat/column/5b10b073aafe4e5a7516708b/topic/5b10f818b1d64f71299765a0\">第01课《中文自然语言处理的完整机器处理流程》</a>中已经给出了，这里不再赘述，下面通过 jieba 分词包和 pyhanlp 来实战命名实体识别和提取。</p>\n\n<p><strong>1.jieba 进行命名实体识别和提取。</strong></p>\n\n<p>第一步，引入 jieba 包：</p>\n\n<pre>\n<code>    import jieba\n    import jieba.analyse\n    import jieba.posseg as posg\n</code></pre>\n\n<p>第二步，使用 jieba 进行词性切分，allowPOS 指定允许的词性，这里选择名词 n 和地名 ns：</p>\n\n<pre>\n<code>    sentence=u\'\'\'上线三年就成功上市,拼多多上演了互联网企业的上市奇迹,却也放大平台上存在的诸多问题，拼多多在美国上市。\'\'\'\n    kw=jieba.analyse.extract_tags(sentence,topK=10,withWeight=True,allowPOS=(\'n\',\'ns\'))\n    for item in kw:\n        print(item[0],item[1])\n</code></pre>\n\n<p>在这里，我们可以得到打印出来的结果：</p>\n\n<blockquote>\n<p>上市 1.437080435586</p>\n\n<p>上线 0.820694551317</p>\n\n<p>奇迹 0.775434839431</p>\n\n<p>互联网 0.712189275429</p>\n\n<p>平台 0.6244340485550001</p>\n\n<p>企业 0.422177218495</p>\n\n<p>美国 0.415659623166</p>\n\n<p>问题 0.39635135730800003</p>\n</blockquote>\n\n<p>可以看得出，上市和上线应该是动词，这里给出的结果不是很准确。接下来，我们使用 textrank 算法来试试：</p>\n\n<pre>\n<code>    kw=jieba.analyse.textrank(sentence,topK=20,withWeight=True,allowPOS=(\'ns\',\'n\'))\n    for item in kw:\n        print(item[0],item[1])\n</code></pre>\n\n<p>这次得到的结果如下，可见，两次给出的结果还是不一样的。</p>\n\n<blockquote>\n<p>上市 1.0</p>\n\n<p>奇迹 0.572687398431635</p>\n\n<p>企业 0.5710407272273452</p>\n\n<p>互联网 0.5692560484441649</p>\n\n<p>上线 0.23481844682115297</p>\n\n<p>美国 0.23481844682115297</p>\n</blockquote>\n\n<p><strong>2.pyhanlp 进行命名实体识别和提取。</strong></p>\n\n<p>第一步，引入pyhanlp包：</p>\n\n<pre>\n<code>    from pyhanlp import *\n</code></pre>\n\n<p>第二步，进行词性切分：</p>\n\n<pre>\n<code>    sentence=u\'\'\'上线三年就成功上市,拼多多上演了互联网企业的上市奇迹,却也放大平台上存在的诸多问题，拼多多在美国上市。\'\'\'\n    analyzer = PerceptronLexicalAnalyzer()\n    segs = analyzer.analyze(sentence)\n    arr = str(segs).split(\" \")\n</code></pre>\n\n<p>第三步，定义一个函数，从得到的结果中，根据词性获取指定词性的词：</p>\n\n<pre>\n<code>    def get_result(arr):\n        re_list = []\n        ner = [\'n\',\'ns\']\n        for x in arr:\n            temp = x.split(\"/\")\n            if(temp[1] in ner):\n                re_list.append(temp[0])\n        return re_list\n</code></pre>\n\n<p>第四步，我们获取结果：</p>\n\n<pre>\n<code>    result = get_result(arr)\n    print(result)\n</code></pre>\n\n<p>得到的结果如下，可见比 jieba 更准确：</p>\n\n<blockquote>\n<pre>\n<code>[\'互联网\', \'企业\', \'奇迹\', \'平台\', \'问题\', \'美国\']\n</code></pre>\n</blockquote>\n\n<h3>总结</h3>\n\n<p>本文对命名实体识别的方法进行了总结，并给出一般的处理流程，最后通过简单的 jieba 分词和 pyhanlp 分词根据词性获取实体对象，后续大家也可以尝试通过哈工大和斯坦福的包来处理，下篇我们通过条件随机场 CRF 来训练一个命名实体识别模型。</p>\n\n<p><strong>参考文献及推荐阅读</strong></p>\n\n<ol>\n	<li>命名实体识别研究[J]，国防科技大学计算机学院-张晓艳、王挺、陈火旺。</li>\n	<li><a href=\"https://www.lookfor404.com/category/note/%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB/\">命名实体识别</a></li>\n</ol>\n\n<h2><a id=\"第15课：基于 CRF 的中文命名实体识别模型实现\" name=\"第15课：基于 CRF 的中文命名实体识别模型实现\"></a>第15课：基于 CRF 的中文命名实体识别模型实现</h2>\n\n<p>命名实体识别在越来越多的场景下被应用，如自动问答、知识图谱等。非结构化的文本内容有很多丰富的信息，但找到相关的知识始终是一个具有挑战性的任务，命名实体识别也不例外。</p>\n\n<p>前面我们用隐马尔可夫模型（HMM）自己尝试训练过一个分词器，其实 HMM 也可以用来训练命名实体识别器，但在本文，我们讲另外一个算法&mdash;&mdash;条件随机场（CRF），来训练一个命名实体识别器。</p>\n\n<h3>浅析条件随机场（CRF）</h3>\n\n<p>条件随机场（Conditional Random Fields，简称 CRF）是给定一组输入序列条件下另一组输出序列的条件概率分布模型，在自然语言处理中得到了广泛应用。</p>\n\n<p>首先，我们来看看什么是随机场。&ldquo;随机场&rdquo;的名字取的很玄乎，其实理解起来不难。随机场是由若干个位置组成的整体，当按照某种分布给每一个位置随机赋予一个值之后，其全体就叫做随机场。</p>\n\n<p>还是举词性标注的例子。假如我们有一个十个词形成的句子需要做词性标注。这十个词每个词的词性可以在我们已知的词性集合（名词，动词&hellip;&hellip;)中去选择。当我们为每个词选择完词性后，这就形成了一个随机场。</p>\n\n<p>了解了随机场，我们再来看看马尔科夫随机场。马尔科夫随机场是随机场的特例，它假设随机场中某一个位置的赋值仅仅与和它相邻的位置的赋值有关，和与其不相邻的位置的赋值无关。</p>\n\n<p>继续举十个词的句子词性标注的例子。如果我们假设所有词的词性只和它相邻的词的词性有关时，这个随机场就特化成一个马尔科夫随机场。比如第三个词的词性除了与自己本身的位置有关外，还只与第二个词和第四个词的词性有关。</p>\n\n<p>理解了马尔科夫随机场，再理解 CRF 就容易了。CRF 是马尔科夫随机场的特例，它假设马尔科夫随机场中只有 X 和 Y 两种变量，X 一般是给定的，而 Y 一般是在给定 X 的条件下我们的输出。这样马尔科夫随机场就特化成了条件随机场。</p>\n\n<p>在我们十个词的句子词性标注的例子中，X 是词，Y 是词性。因此，如果我们假设它是一个马尔科夫随机场，那么它也就是一个 CRF。</p>\n\n<p>对于 CRF，我们给出准确的数学语言描述：设 X 与 Y 是随机变量，P(Y|X) 是给定 X 时 Y 的条件概率分布，若随机变量 Y 构成的是一个马尔科夫随机场，则称条件概率分布 P(Y|X) 是条件随机场。</p>\n\n<h3>基于 CRF 的中文命名实体识别模型实现</h3>\n\n<p>在常规的命名实体识别中，通用场景下最常提取的是时间、人物、地点及组织机构名，因此本模型也将提取以上四种实体。</p>\n\n<p><strong>1.开发环境。</strong></p>\n\n<p>本次开发所选用的环境为：</p>\n\n<ul>\n	<li><code>Sklearn_crfsuite</code></li>\n	<li>Python 3.6</li>\n	<li>Jupyter Notebook</li>\n</ul>\n\n<p><strong>2.数据预处理。</strong></p>\n\n<p>本模型使用人民日报1998年标注数据，进行预处理。语料库词性标记中，对应的实体词依次为 t、nr、ns、nt。对语料需要做以下处理：</p>\n\n<ul>\n	<li>将语料全角字符统一转为半角；</li>\n	<li>合并语料库分开标注的姓和名，例如：<code>温/nr 家宝/nr</code>；</li>\n	<li>合并语料库中括号中的大粒度词，例如：<code>[国家/n 环保局/n]nt</code>；</li>\n	<li>合并语料库分开标注的时间，例如：<code>（/w 一九九七年/t 十二月/t 三十一日/t ）/w</code>。</li>\n</ul>\n\n<p>首先引入需要用到的库：</p>\n\n<pre>\n<code>    import re\n    import sklearn_crfsuite\n    from sklearn_crfsuite import metrics\n    from sklearn.externals import joblib\n</code></pre>\n\n<p>数据预处理，定义 CorpusProcess 类，我们还是先给出类实现框架：</p>\n\n<pre>\n<code>class CorpusProcess(object):\n\n    def __init__(self):\n        \"\"\"初始化\"\"\"\n        pass\n\n    def read_corpus_from_file(self, file_path):\n        \"\"\"读取语料\"\"\"\n        pass\n\n    def write_corpus_to_file(self, data, file_path):\n        \"\"\"写语料\"\"\"\n        pass\n\n    def q_to_b(self,q_str):\n        \"\"\"全角转半角\"\"\"\n        pass\n\n    def b_to_q(self,b_str):\n        \"\"\"半角转全角\"\"\"\n        pass\n\n    def pre_process(self):\n        \"\"\"语料预处理 \"\"\"\n        pass\n\n    def process_k(self, words):\n        \"\"\"处理大粒度分词,合并语料库中括号中的大粒度分词,类似：[国家/n  环保局/n]nt \"\"\"\n        pass\n\n    def process_nr(self, words):\n        \"\"\" 处理姓名，合并语料库分开标注的姓和名，类似：温/nr  家宝/nr\"\"\"\n        pass\n\n    def process_t(self, words):\n        \"\"\"处理时间,合并语料库分开标注的时间词，类似： （/w  一九九七年/t  十二月/t  三十一日/t  ）/w   \"\"\"\n        pass\n\n    def pos_to_tag(self, p):\n        \"\"\"由词性提取标签\"\"\"\n        pass\n\n    def tag_perform(self, tag, index):\n        \"\"\"标签使用BIO模式\"\"\"\n        pass\n\n    def pos_perform(self, pos):\n        \"\"\"去除词性携带的标签先验知识\"\"\"\n        pass\n\n    def initialize(self):\n        \"\"\"初始化 \"\"\"\n        pass\n\n    def init_sequence(self, words_list):\n        \"\"\"初始化字序列、词性序列、标记序列 \"\"\"\n        pass\n\n    def extract_feature(self, word_grams):\n        \"\"\"特征选取\"\"\"\n        pass\n\n    def segment_by_window(self, words_list=None, window=3):\n        \"\"\"窗口切分\"\"\"\n        pass\n\n    def generator(self):\n        \"\"\"训练数据\"\"\"\n        pass\n</code></pre>\n\n<p>由于整个代码实现过程较长，我这里给出重点步骤，最后会在&nbsp;<strong>Github 上连同语料代码一同给出</strong>，下面是关键过程实现。</p>\n\n<p>对语料中的句子、词性，实体分类标记进行区分。标签采用&ldquo;BIO&rdquo;体系，即实体的第一个字为&nbsp;<code>B_*</code>，其余字为&nbsp;<code>I_*</code>，非实体字统一标记为 O。大部分情况下，标签体系越复杂，准确度也越高，但这里采用简单的 BIO 体系也能达到相当不错的效果。这里模型采用&nbsp;<code>tri-gram</code>&nbsp;形式，所以在字符列中，要在句子前后加上占位符。</p>\n\n<pre>\n<code>def init_sequence(self, words_list):\n            \"\"\"初始化字序列、词性序列、标记序列 \"\"\"\n            words_seq = [[word.split(u\'/\')[0] for word in words] for words in words_list]\n            pos_seq = [[word.split(u\'/\')[1] for word in words] for words in words_list]\n            tag_seq = [[self.pos_to_tag(p) for p in pos] for pos in pos_seq]\n            self.pos_seq = [[[pos_seq[index][i] for _ in range(len(words_seq[index][i]))]\n                            for i in range(len(pos_seq[index]))] for index in range(len(pos_seq))]\n            self.tag_seq = [[[self.tag_perform(tag_seq[index][i], w) for w in range(len(words_seq[index][i]))]\n                            for i in range(len(tag_seq[index]))] for index in range(len(tag_seq))]\n            self.pos_seq = [[u\'un\']+[self.pos_perform(p) for pos in pos_seq for p in pos]+[u\'un\'] for pos_seq in self.pos_seq]\n            self.tag_seq = [[t for tag in tag_seq for t in tag] for tag_seq in self.tag_seq]\n            self.word_seq = [[u\'&lt;BOS&gt;\']+[w for word in word_seq for w in word]+[u\'&lt;EOS&gt;\'] for word_seq in words_seq] \n</code></pre>\n\n<p>处理好语料之后，紧接着进行模型定义和训练，定义&nbsp;<code>CRF_NER</code>&nbsp;类，我们还是采用先给出类实现框架，再具体讲解其实现：</p>\n\n<pre>\n<code>    class CRF_NER(object):\n        def __init__(self):\n            \"\"\"初始化参数\"\"\"\n            pass\n\n        def initialize_model(self):\n            \"\"\"初始化\"\"\"\n            pass\n\n        def train(self):\n            \"\"\"训练\"\"\"\n            pass\n\n        def predict(self, sentence):\n            \"\"\"预测\"\"\"\n            pass\n        def load_model(self):\n            \"\"\"加载模型 \"\"\"\n            pass\n        def save_model(self):\n            \"\"\"保存模型\"\"\"\n            pass\n</code></pre>\n\n<p>在&nbsp;<code>CRF_NER</code>&nbsp;类中，分别完成了语料预处理和模型训练、保存、预测功能，具体实现如下。</p>\n\n<p>第一步，init 函数实现了模型参数定义和 CorpusProcess 的实例化和语料预处理：</p>\n\n<pre>\n<code>    def __init__(self):\n            \"\"\"初始化参数\"\"\"\n            self.algorithm = \"lbfgs\"\n            self.c1 =\"0.1\"\n            self.c2 = \"0.1\"\n            self.max_iterations = 100 #迭代次数\n            self.model_path = dir + \"model.pkl\"\n            self.corpus = CorpusProcess()  #Corpus 实例\n            self.corpus.pre_process()  #语料预处理\n            self.corpus.initialize()  #初始化语料\n            self.model = None\n</code></pre>\n\n<p>第二步，给出模型定义，了解&nbsp;<code>sklearn_crfsuite.CRF</code>&nbsp;详情可查该<a href=\"https://sklearn-crfsuite.readthedocs.io/en/latest/api.html#sklearn_crfsuite.CRF\">文档</a>。</p>\n\n<pre>\n<code>    def initialize_model(self):\n            \"\"\"初始化\"\"\"\n            algorithm = self.algorithm\n            c1 = float(self.c1)\n            c2 = float(self.c2)\n            max_iterations = int(self.max_iterations)\n            self.model = sklearn_crfsuite.CRF(algorithm=algorithm, c1=c1, c2=c2,\n                                              max_iterations=max_iterations, all_possible_transitions=True)\n</code></pre>\n\n<p>第三步，模型训练和保存，分为训练集和测试集：</p>\n\n<pre>\n<code>    def train(self):\n            \"\"\"训练\"\"\"\n            self.initialize_model()\n            x, y = self.corpus.generator()\n            x_train, y_train = x[500:], y[500:]\n            x_test, y_test = x[:500], y[:500]\n            self.model.fit(x_train, y_train)\n            labels = list(self.model.classes_)\n            labels.remove(\'O\')\n            y_predict = self.model.predict(x_test)\n            metrics.flat_f1_score(y_test, y_predict, average=\'weighted\', labels=labels)\n            sorted_labels = sorted(labels, key=lambda name: (name[1:], name[0]))\n            print(metrics.flat_classification_report(y_test, y_predict, labels=sorted_labels, digits=3))\n            self.save_model()\n</code></pre>\n\n<p>第四至第六步中 predict、<code>load_model</code>、<code>save_model</code>&nbsp;方法的实现，大家可以在文末给出的地址中查看源码，这里就不堆代码了。</p>\n\n<p>最后，我们来看看模型训练和预测的过程和结果：</p>\n\n<pre>\n<code>    ner = CRF_NER()\n    model = ner.train()\n</code></pre>\n\n<p>经过模型训练，得到的准确率和召回率如下：</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/f6165800-97be-11e8-b78f-09922e3c574f\" /></p>\n\n<p>进行模型预测，其结果还不错，如下：</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/1aa70020-97bf-11e8-911c-dd974a01956f\" /></p>\n\n<p>基于 CRF 的中文命名实体识别模型实现先讲到这儿，项目源码和涉及到的语料，大家可以到：<a href=\"https://github.com/sujeek/chinese_nlp\">Github</a>&nbsp;上查看。</p>\n\n<h3>总结</h3>\n\n<p>本文浅析了条件随机场，并使用&nbsp;<code>sklearn_crfsuite.CRF</code>&nbsp;模型，对人民日报1998年标注数据进行了模型训练和预测，以帮助大家加强对条件随机场的理解。</p>\n\n<p><strong>参考资料及推荐阅读</strong></p>\n\n<ol>\n	<li><a href=\"https://blog.csdn.net/a819825294/article/details/53893231\">条件随机场（CRF）</a></li>\n	<li><a href=\"https://www.cnblogs.com/pinard/p/7048333.html\">条件随机场CRF（一）从随机场到线性链条件随机场</a></li>\n	<li><a href=\"https://blog.csdn.net/sinat_33741547/article/details/79131223\">命名实体：基于 CRF 的中文命名实体识别模型</a></li>\n	<li><a href=\"http://x-algo.cn/index.php/2016/02/15/conditional-random-field-crf-theory-and-implementation/\">条件随机场（CRF）理论及应用</a></li>\n</ol>\n\n<h2><a id=\"第16课：动手实战中文句法依存分析\" name=\"第16课：动手实战中文句法依存分析\"></a>第16课：动手实战中文句法依存分析</h2>\n\n<p>句法分析是自然语言处理（NLP）中的关键技术之一，其基本任务是确定句子的句法结构或者句子中词汇之间的依存关系。主要包括两方面的内容：一是确定语言的语法体系，即对语言中合法句子的语法结构给予形式化的定义；另一方面是句法分析技术，即根据给定的语法体系，自动推导出句子的句法结构，分析句子所包含的句法单位和这些句法单位之间的关系。</p>\n\n<p>句法分析被用在很多场景中，比如搜索引擎用户日志分析和关键词识别，比如信息抽取、自动问答、机器翻译等其他自然语言处理相关的任务。</p>\n\n<h3>语法体系</h3>\n\n<p>句法分析需要遵循某一语法体系，根据该体系的语法确定语法树的表示形式，我们看下面这个句子：</p>\n\n<blockquote>\n<p>西门子将努力参与中国的三峡工程建设。</p>\n</blockquote>\n\n<p>用可视化的工具&nbsp;<a href=\"http://nlp.stanford.edu:8080/parser/index.jsp\">Stanford Parser</a>&nbsp;来看看句法分析的整个过程：</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/e7813240-9ea0-11e8-bf0f-5103efdb7be8\" /></p>\n\n<p>短语结构树由终节点、非终结点以及短语标记三部分组成。句子分裂的语法规则为若干终结点构成一个短语，作为非终结点参与下一次规约，直至结束。如下图：</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/115a7610-97eb-11e8-b78f-09922e3c574f\" /></p>\n\n<h3>句法分析技术</h3>\n\n<h4>依存句法分析</h4>\n\n<h5><strong>依存句法</strong></h5>\n\n<p>依存句法（Dependency Parsing， DP）通过分析语言单位内成分之间的依存关系揭示其句法结构。</p>\n\n<p>直观来讲，依存句法的目的在于分析识别句子中的&ldquo;主谓宾&rdquo;、&ldquo;定状补&rdquo;这些语法成分，并分析各成分之间的关系。</p>\n\n<p>依存句法的结构没有非终结点，词与词之间直接发生依存关系，构成一个依存对，其中一个是核心词，也叫支配词，另一个叫修饰词，也叫从属词。</p>\n\n<p>依存关系用一个有向弧表示，叫做依存弧。依存弧的方向为由从属词指向支配词，当然反过来也是可以的，按个人习惯统一表示即可。</p>\n\n<p>例如，下面这个句子：</p>\n\n<blockquote>\n<p>国务院总理李克强调研上海外高桥时提出，支持上海积极探索新机制。</p>\n</blockquote>\n\n<p>依存句法的分析结果见下（利用哈工大 LTP）：</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/c6000940-97eb-11e8-b78f-09922e3c574f\" /></p>\n\n<p>从分析结果中我们可以看到，句子的核心谓词为&ldquo;提出&rdquo;，主语是&ldquo;李克强&rdquo;，提出的宾语是&ldquo;支持上海&hellip;&hellip;&rdquo;，&ldquo;调研&hellip;&hellip;时&rdquo;是&ldquo;提出&rdquo;的（时间） 状语，&ldquo;李克强&rdquo;的修饰语是&ldquo;国务院总理&rdquo;，&ldquo;支持&rdquo;的宾语是&ldquo;探索新机制&rdquo;。</p>\n\n<p>有了上面的依存句法分析结果，我们就可以比较容易的看到，&ldquo;提出者&rdquo;是&ldquo;李克强&rdquo;，而不是&ldquo;上海&rdquo;或&ldquo;外高桥&rdquo;，即使它们都是名词，而且距离&ldquo;提出&rdquo;更近。</p>\n\n<h5><strong>依存关系</strong></h5>\n\n<p>依存句法通过分析语言单位内成分之前的依存关系解释其句法结构，主张句子中核心动词是支配其他成分的中心成分。而它本身却不受其他任何成分的支配，所有受支配成分都以某种关系从属于支配者。</p>\n\n<p>在20世纪70年代，Robinson 提出依存句法中关于依存关系的四条公理，在处理中文信息的研究中，中国学者提出了依存关系的第五条公理，分别如下：</p>\n\n<ol>\n	<li>一个句子中只有一个成分是独立的；</li>\n	<li>句子的其他成分都从属于某一成分；</li>\n	<li>任何一个成分都不能依存于两个或两个以上的成分；</li>\n	<li>如果成分 A 直接从属成分 B，而成分 C 在句子中位于 A 和 B 之间，那么，成分 C 或者从属于 A，或者从属于 B，或者从属于 A 和 B 之间的某一成分；</li>\n	<li>中心成分左右两边的其他成分相互不发生关系。</li>\n</ol>\n\n<p>句子成分之间相互支配与被支配、依存与被依存的现象，普遍存在于汉语的词汇（合成语）、短语、单句、段落、篇章等能够独立运用和表达的语言之中，这一特点体现了依存关系的普遍性。依存句法分析可以反映出句子各成分之间的语义修饰关系，它可以获得长距离的搭配信息，并与句子成分的物理位置无关。</p>\n\n<p>依存句法分析标注关系（共14种）及含义如下表所示：</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/c564e540-97ec-11e8-a5a1-17130ea31e3a\" /></p>\n\n<h4>语义依存分析</h4>\n\n<p>语义依存分析（Semantic Dependency Parsing， SDP），分析句子各个语言单位之间的语义关联，并将语义关联以依存结构呈现。使用语义依存刻画句子语义，好处在于不需要去抽象词汇本身，而是通过词汇所承受的语义框架来描述该词汇，而论元的数目相对词汇来说数量总是少了很多。</p>\n\n<p>语义依存分析目标是跨越句子表层句法结构的束缚，直接获取深层的语义信息。例如以下三个句子，用不同的表达方式表达了同一个语义信息，即张三实施了一个吃的动作，吃的动作是对苹果实施的。</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/3be55a10-97ed-11e8-b78f-09922e3c574f\" /></p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/430a6c40-97ed-11e8-8841-e548fce76345\" /></p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/4dcd8cc0-97ed-11e8-911c-dd974a01956f\" /></p>\n\n<p>语义依存分析不受句法结构的影响，将具有直接语义关联的语言单元直接连接依存弧并标记上相应的语义关系。这也是语义依存分析与依存句法分析的重要区别。</p>\n\n<p>语义依存关系分为三类，分别是主要语义角色，每一种语义角色对应存在一个嵌套关系和反关系；事件关系，描述两个事件间的关系；语义依附标记，标记说话者语气等依附性信息。</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/8accc8c0-97ed-11e8-8841-e548fce76345\" /></p>\n\n<h4>Pyhanlp 实战依存句法</h4>\n\n<p>最后，我们通过 Pyhanlp 库实现依存句法的实战练习。这个过程中，我们选用 Dependency Viewer 工具进行可视化展示。可视化时， txt 文档需要采用 UTF-8 编码。</p>\n\n<p>首先，引入包，然后可直接进行分析：</p>\n\n<pre>\n<code>    from pyhanlp import *\n    sentence = \"徐先生还具体帮助他确定了把画雄鹰、松鼠和麻雀作为主攻目标。\"\n    print(HanLP.parseDependency(sentence))\n</code></pre>\n\n<p>得到的结果：</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/8c1d52b0-97ef-11e8-a5a1-17130ea31e3a\" /></p>\n\n<p>然后，我们将结果保存在 txt 文件中：</p>\n\n<pre>\n<code>    f = open(\"D://result.txt\",\'a+\')\n    print((HanLP.parseDependency(sentence )),file = f)\n</code></pre>\n\n<p>最后，通过 Dependency Viewer 工具进行可视化，如果出现乱码，记得把 txt 文档保存为 UTF-8 式即可，得到的可视化结果如下图所示：</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/ab05d260-97f4-11e8-a5a1-17130ea31e3a\" /></p>\n\n<h3>总结</h3>\n\n<p>本文，首先为大家介绍了语法体系，以及如何根据语法体系确定一个句子的语法树，为后面的句法分析奠定基础。</p>\n\n<p>接着，介绍了依存句法，它的目的是通过分析语言单位内成分之间的依存关系揭示其句法结构，随之讲解了依存句法中的五大依存关系。</p>\n\n<p>最后，进一步介绍了区别于依存句法的语义依存，其目的是分析句子各个语言单位之间的语义关联，并将语义关联以依存结构呈现。</p>\n\n<p>文章结尾，通过 Pyhanlp 实战以及可视化，带大家进一步加深对中文依存句法的了解。</p>\n\n<p><strong>参考资料以及推荐阅读：</strong></p>\n\n<ol>\n	<li><a href=\"https://blog.csdn.net/liu_zhlai/article/details/52444422\">中文依存句法分析概述及应用</a></li>\n	<li><a href=\"http://ir.hit.edu.cn/demo/ltp\">LTP 依存分析模块所使用的依存关系标记含义</a></li>\n	<li><a href=\"http://hanlp.linrunsoft.com/doc/_build/html/dependency_parser.html#id2\">依存句法解析</a></li>\n	<li><a href=\"https://blog.csdn.net/sinat_33741547/article/details/79258045\">依存分析：中文依存句法分析简介</a></li>\n	<li><a href=\"https://www.cnblogs.com/CheeseZH/p/5768389.html\">依存句法分析与语义依存分析的区别</a></li>\n	<li><a href=\"https://github.com/HIT-SCIR/pyltp\">pyltp:the python extension for LTP</a></li>\n	<li><a href=\"http://nlp.nju.edu.cn/tanggc/tools/DependencyViewer.html\">Dependency Viewer</a></li>\n</ol>\n\n<h2><a id=\"第17课：基于 CRF 的中文句法依存分析模型实现\" name=\"第17课：基于 CRF 的中文句法依存分析模型实现\"></a>第17课：基于 CRF 的中文句法依存分析模型实现</h2>\n\n<p>句法分析是自然语言处理中的关键技术之一，其基本任务是确定句子的句法结构或者句子中词汇之间的依存关系。主要包括两方面的内容，一是确定语言的语法体系，即对语言中合法句子的语法结构给予形式化的定义；另一方面是句法分析技术，即根据给定的语法体系，自动推导出句子的句法结构，分析句子所包含的句法单位和这些句法单位之间的关系。</p>\n\n<p>依存关系本身是一个树结构，每一个词看成一个节点，依存关系就是一条有向边。本文主要通过清华大学的句法标注语料库，来实现基于 CRF 的中文句法依存分析模型。</p>\n\n<h3>清华大学句法标注语料库</h3>\n\n<p>清华大学的句法标注语料，包括训练集（train.conll）和开发集合文件（dev.conll）。训练集大小 5.41M，共185541条数据。测试集大小为 578kb，共19302条数据。</p>\n\n<p>语料本身格式如下图所示：</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/ea8a9910-97fa-11e8-8841-e548fce76345\" /></p>\n\n<p>通过上图，我们可以看出，每行语料包括有8个标签，分别是 ID、FROM、lEMMA、CPOSTAG、POSTAG、FEATS、HEAD、DEPREL。详细介绍如下图：</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/fa233670-97fa-11e8-b78f-09922e3c574f\" /></p>\n\n<h3>模型的实现</h3>\n\n<p>通过上面对句法依存关键技术的定义，我们明白了，句法依存的基本任务是确定句子的句法结构或者句子中词汇之间的依存关系。同时，我们也对此次模型实现的语料有了基本了解。</p>\n\n<p>有了这些基础内容，我们便可以开始着手开发了。</p>\n\n<p>本模型的实现过程，我们将主要分为训练集和测试集数据预处理、语料特征生成、模型训练及预测三大部分来实现，最终将通过模型预测得到正确的预测结果。</p>\n\n<p>本次实战演练，我们选择以下模型和软件：</p>\n\n<ul>\n	<li>Sklearn_crfsuite</li>\n	<li>Python3.6</li>\n	<li>Jupyter Notebook</li>\n</ul>\n\n<h4>训练集和测试集数据预处理</h4>\n\n<p>由于上述给定的语料，在模型中，我们不能直接使用，必须先经过预处理，把上述语料格式重新组织成具有词性、方向和距离的格式。</p>\n\n<p>首先，我们通过一个 Python 脚本&nbsp;<code>get_parser_train_test_input.py</code>，生成所需要的训练集和测试集，执行如下命令即可：</p>\n\n<pre>\n<code>    cat train.conll | python get_parser_train_test_input.py &gt; train.data \n    cat dev.conll | python get_parser_train_test_input.py &gt; dev.data \n</code></pre>\n\n<p>上面的脚本通过 cat 命令和管道符把内容传递给脚本进行处理。这里需要注意的是，脚本需要在 Linux 环境下执行，且语料和脚本应放在同一目录下。</p>\n\n<p><code>get_parser_train_test_input.py</code>&nbsp;这一脚本的目的，就是重新组织语料，组织成可以使用 CRF 算法的格式，具有词性、方向和距离的格式。我们认为，如果词 A 依赖词 B，A 就是孩子，B 就是父亲。按照这种假设得到父亲节点的粗词性和详细词性，以及和依赖次之间的距离。</p>\n\n<p>我们打开该脚本，看看它的代码，如下所示，重要的代码给出了注释。</p>\n\n<pre>\n<code>    #coding=utf-8\n    \'\'\'词A依赖词B，A就是孩子，B就是父亲\'\'\'\n    import sys \n\n    sentence = [\"Root\"]\n    def do_parse(sentence):\n        if len(sentence) == 1:return \n        for line in sentence[1:]:\n            line_arr = line.strip().split(\"\\t\")\n            c_id = int(line_arr[0])\n            f_id = int(line_arr[6])\n            if f_id == 0:\n                print(\"\\t\".join(line_arr[2:5])+\"\\t\" + \"0_Root\")\n                continue\n            f_post,f_detail_post = sentence[f_id].strip().split(\"\\t\")[3:5] #得到父亲节点的粗词性和详细词性\n            c_edge_post = f_post #默认是依赖词的粗粒度词性，但是名词除外；名词取细粒度词性\n            if f_post == \"n\":\n                c_edge_post = f_detail_post\n            #计算是第几个出现这种词行\n            diff = f_id - c_id #确定要走几步\n            step = 1 if f_id &gt; c_id  else -1 #确定每一步方向\n            same_post_num = 0 #中间每一步统计多少个一样的词性\n            cmp_idx = 4 if f_post == \"n\" else 3  #根据是否是名词决定取的是粗or详细词性\n            for i in range(0, abs(diff)):\n                idx = c_id + (i+1)*step\n                if sentence[idx].strip().split(\"\\t\")[cmp_idx] == c_edge_post:\n                    same_post_num += step\n\n            print(\"\\t\".join(line_arr[2:5])+\"\\t\" + \"%d_%s\"%(same_post_num, c_edge_post))\n        print(\"\")\n\n    for line in sys.stdin:\n        line = line.strip()\n        line_arr = line.split(\"\\t\")\n        if  line == \"\" or line_arr[0] == \"1\":\n            do_parse(sentence)\n            sentence = [\"Root\"]\n        if line ==\"\":continue \n        sentence.append(line)\n</code></pre>\n\n<p>整个脚本按行读入，每行按 Tab 键分割，首先得到父亲节点的词性，然后根据词性是否是名词 n 进行判断，默认是依赖词的粗粒度词性，如果是名词取细粒度词性。</p>\n\n<p>脚本处理完，数据集的格式如下：</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/6ceea0a0-984a-11e8-911c-dd974a01956f\" /></p>\n\n<p>根据依存文法，决定两个词之间依存关系的主要有两个因素：方向和距离。正如上图中第四列类别标签所示，该列可以定义为以下形式：</p>\n\n<blockquote>\n<p>[+|-]dPOS</p>\n</blockquote>\n\n<p>其中，<code>[+|-]</code>&nbsp;表示中心词在句子中相对坐标轴的方向；POS 代表中心词具有的词性类别；d 表示与中心词词性相同的词的数量，即距离。</p>\n\n<h4>语料特征生成</h4>\n\n<p>语料特征提取，主要采用 N-gram 模型来完成。这里我们使用 3-gram 完成提取，将词性与词语两两进行匹配，分别返回特征集合和标签集合，需要注意整个语料采用的是 UTF-8 编码格式。</p>\n\n<p>整个编码过程中，我们首先需要引入需要的库，然后对语料进行读文件操作。语料采用 UTF-8 编码格式，以句子为单位，按 Tab 键作分割处理，从而实现句子 3-gram 模型的特征提取。具体实现如下。</p>\n\n<pre>\n<code>    import sklearn_crfsuite\n    from sklearn_crfsuite import metrics\n    from sklearn.externals import joblib\n</code></pre>\n\n<p>首先引入需要用到的库，如上面代码所示。其目的是使用模型<code>sklearn_crfsuite .CRF</code>，metrics 用来进行模型性能测试，joblib 用来保存和加载训练好的模型。</p>\n\n<p>接着，定义包含特征处理方法的类，命名为 CorpusProcess，类结构定义如下：</p>\n\n<pre>\n<code>class CorpusProcess(object):\n\n    def __init__(self):\n        \"\"\"初始化\"\"\"\n        pass\n\n    def read_corpus_from_file(self, file_path):\n        \"\"\"读取语料\"\"\"\n        pass\n\n    def write_corpus_to_file(self, data, file_path):\n        \"\"\"写语料\"\"\"\n        pass\n\n    def process_sentence(self,lines):\n        \"\"\"处理句子\"\"\"\n        pass   \n\n    def initialize(self):\n        \"\"\"语料初始化\"\"\"\n        pass   \n\n    def generator(self, train=True):\n        \"\"\"特征生成器\"\"\"\n        pass  \n\n    def extract_feature(self, sentences):\n        \"\"\"提取特征\"\"\"\n        pass\n</code></pre>\n\n<p>下面介绍下 CorpusProcess 类中各个方法的具体实现。</p>\n\n<p>第1步， 实现 init 构造函数，目的初始化预处理好的语料的路径：</p>\n\n<pre>\n<code>    def __init__(self):\n            \"\"\"初始化\"\"\"\n            self.train_process_path =  dir +  \"data//train.data\"   #预处理之后的训练集\n            self.test_process_path =  dir +  \"data//dev.data\"  #预处理之后的测试集\n</code></pre>\n\n<p>这里的路径可以自定义，这里的语料之前已经完成了预处理过程。</p>\n\n<p>第2-3步，<code>read_corpus_from_file</code>&nbsp;方法和&nbsp;<code>write_corpus_to_file</code>&nbsp;方法，分别定义了语料文件的读和写操作：</p>\n\n<pre>\n<code>    def read_corpus_from_file(self, file_path):\n        \"\"\"读取语料\"\"\"\n        f = open(file_path, \'r\',encoding=\'utf-8\')\n        lines = f.readlines()\n        f.close()\n        return lines\n\n    def write_corpus_to_file(self, data, file_path):\n        \"\"\"写语料\"\"\"\n        f = open(file_path, \'w\')\n        f.write(str(data))\n        f.close()\n</code></pre>\n\n<p>这一步，主要用 open 函数来实现语料文件的读和写。</p>\n\n<p>第4-5步，<code>process_sentence</code>&nbsp;方法和 initialize 方法，用来处理句子和初始化语料，把语料按句子结构用 list 存储起来，存储到内存中：</p>\n\n<pre>\n<code>    def process_sentence(self,lines):\n            \"\"\"处理句子\"\"\"\n            sentence = []\n            for line in lines:\n                if not line.strip():\n                    yield sentence\n                    sentence = []\n                else:\n                    lines = line.strip().split(u\'\\t\')\n                    result = [line for line in lines]\n                    sentence.append(result)   \n\n        def initialize(self):\n            \"\"\"语料初始化\"\"\"\n            train_lines = self.read_corpus_from_file(self.train_process_path)\n            test_lines = self.read_corpus_from_file(self.test_process_path)\n            self.train_sentences = [sentence for sentence in self.process_sentence(train_lines)]\n            self.test_sentences = [sentence for sentence in self.process_sentence(test_lines)] \n</code></pre>\n\n<p>这一步，通过&nbsp;<code>process_sentence</code>&nbsp;把句子收尾的空格去掉，然后通过 initialize 函数调用上面&nbsp;<code>read_corpus_from_file</code>&nbsp;方法读取语料，分别加载训练集和测试集。</p>\n\n<p>第6步，特征生成器，分别用来指定生成训练集或者测试集的特征集：</p>\n\n<pre>\n<code>    def generator(self, train=True):\n        \"\"\"特征生成器\"\"\"\n        if train: \n            sentences = self.train_sentences\n        else: \n            sentences = self.test_sentences\n        return self.extract_feature(sentences)    \n</code></pre>\n\n<p>这一步，对训练集和测试集分别处理，如果参数 train 为 True，则表示处理训练集，如果是 False，则表示处理测试集。</p>\n\n<p>第7步，特征提取，简单的进行&nbsp;<code>3-gram</code>&nbsp;的抽取，将词性与词语两两进行匹配，分别返回特征集合和标签集合：</p>\n\n<pre>\n<code>    def extract_feature(self, sentences):\n            \"\"\"提取特征\"\"\"\n            features, tags = [], []\n            for index in range(len(sentences)):\n                feature_list, tag_list = [], []\n                for i in range(len(sentences[index])):\n                    feature = {\"w0\": sentences[index][i][0],\n                               \"p0\": sentences[index][i][1],\n                               \"w-1\": sentences[index][i-1][0] if i != 0 else \"BOS\",\n                               \"w+1\": sentences[index][i+1][0] if i != len(sentences[index])-1 else \"EOS\",\n                               \"p-1\": sentences[index][i-1][1] if i != 0 else \"un\",\n                               \"p+1\": sentences[index][i+1][1] if i != len(sentences[index])-1 else \"un\"}\n                    feature[\"w-1:w0\"] = feature[\"w-1\"]+feature[\"w0\"]\n                    feature[\"w0:w+1\"] = feature[\"w0\"]+feature[\"w+1\"]\n                    feature[\"p-1:p0\"] = feature[\"p-1\"]+feature[\"p0\"]\n                    feature[\"p0:p+1\"] = feature[\"p0\"]+feature[\"p+1\"]\n                    feature[\"p-1:w0\"] = feature[\"p-1\"]+feature[\"w0\"]\n                    feature[\"w0:p+1\"] = feature[\"w0\"]+feature[\"p+1\"]\n                    feature_list.append(feature)\n                    tag_list.append(sentences[index][i][-1])\n                features.append(feature_list)\n                tags.append(tag_list)\n            return features, tags    \n</code></pre>\n\n<p>经过第6步，确定处理的是训练集还是测试集之后，通过&nbsp;<code>extract_feature</code>&nbsp;对句子进行特征抽取，使用 3-gram 模型，得到特征集合和标签集合的对应关系。</p>\n\n<h4>模型训练及预测</h4>\n\n<p>在完成特征工程和特征提取之后，接下来，我们要进行模型训练和预测，要预定义模型需要的一些参数，并初始化模型对象，进而完成模型训练和预测，以及模型的保存与加载。</p>\n\n<p>首先，我们定义模型 ModelParser 类，进行初始化参数、模型初始化，以及模型训练、预测、保存和加载，类的结构定义如下：</p>\n\n<pre>\n<code>    class ModelParser(object):\n\n        def __init__(self):\n            \"\"\"初始化参数\"\"\"\n            pass\n\n        def initialize_model(self):\n            \"\"\"模型初始化\"\"\"\n            pass\n\n        def train(self):\n            \"\"\"训练\"\"\"\n            pass\n\n        def predict(self, sentences):\n            \"\"\"模型预测\"\"\"\n            pass\n\n        def load_model(self, name=\'model\'):\n            \"\"\"加载模型 \"\"\"\n            pass\n\n        def save_model(self, name=\'model\'):\n            \"\"\"保存模型\"\"\"\n            pass\n</code></pre>\n\n<p>接下来，我们分析 ModelParser 类中方法的具体实现。</p>\n\n<p>第1步，init 方法实现算法模型参数和语料预处理 CorpusProcess 类的实例化和初始化：</p>\n\n<pre>\n<code>    def __init__(self):\n            \"\"\"初始化参数\"\"\"\n            self.algorithm = \"lbfgs\"\n            self.c1 = 0.1\n            self.c2 = 0.1\n            self.max_iterations = 100\n            self.model_path = \"model.pkl\"\n            self.corpus = CorpusProcess()  #初始化CorpusProcess类\n            self.corpus.initialize()  #语料预处理\n            self.model = None\n</code></pre>\n\n<p>这一步，init 方法初始化参数以及 CRF 模型的参数，算法选用 LBFGS，c1 和 c2 分别为0.1，最大迭代次数100次。然后定义模型保存的文件名称，以及完成对 CorpusProcess 类 的初始化。</p>\n\n<p>第2-3步，<code>initialize_model</code>&nbsp;方法和 train 实现模型定义和训练：</p>\n\n<pre>\n<code>     def initialize_model(self):\n            \"\"\"模型初始化\"\"\"\n            algorithm = self.algorithm\n            c1 = float(self.c1)\n            c2 = float(self.c2)\n            max_iterations = int(self.max_iterations)\n            self.model = sklearn_crfsuite.CRF(algorithm=algorithm, c1=c1, c2=c2,\n                                              max_iterations=max_iterations, all_possible_transitions=True)\n\n        def train(self):\n            \"\"\"训练\"\"\"\n            self.initialize_model()\n            x_train, y_train = self.corpus.generator()\n            self.model.fit(x_train, y_train)\n            labels = list(self.model.classes_)\n            x_test, y_test = self.corpus.generator(train=False)\n            y_predict = self.model.predict(x_test)\n            metrics.flat_f1_score(y_test, y_predict, average=\'weighted\', labels=labels)\n            sorted_labels = sorted(labels, key=lambda name: (name[1:], name[0]))\n            print(metrics.flat_classification_report(y_test, y_predict, labels=sorted_labels, digits=3))\n            self.save_model()\n</code></pre>\n\n<p>这一步，<code>initialize_model</code>&nbsp;方法实现 了&nbsp;<code>sklearn_crfsuite.CRF</code>&nbsp;模型的初始化。然后在 train 方法中，先通过 fit 方法训练模型，再通过<code>metrics.flat_f1_score</code>&nbsp;对测试集进行 F1 性能测试，最后将模型保存。</p>\n\n<p>第4-6步，分别实现模型预测、保存和加载方法，具体代码大家可以访问<a href=\"https://github.com/sujeek/chinese_nlp\">Github</a>。</p>\n\n<p>最后，实例化类，并进行模型训练：</p>\n\n<pre>\n<code>    model = ModelParser()\n    model.train()\n</code></pre>\n\n<p>对模型进行预测，预测数据输入格式为三维，表示完整的一句话：</p>\n\n<blockquote>\n<p>[[[&#39;坚决&#39;, &#39;a&#39;, &#39;ad&#39;, &#39;1_v&#39;],</p>\n\n<pre>\n<code> [\'惩治\', \'v\', \'v\', \'0_Root\'],\n\n [\'贪污\', \'v\', \'v\', \'1_v\'],\n\n [\'贿赂\', \'n\', \'n\', \'-1_v\'],\n\n [\'等\', \'u\', \'udeng\', \'-1_v\'],\n\n [\'经济\', \'n\', \'n\', \'1_v\'],\n\n [\'犯罪\', \'v\', \'vn\', \'-2_v\']]]\n</code></pre>\n</blockquote>\n\n<p>模型预测的结果如下图所示：</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/87377f80-985a-11e8-b78f-09922e3c574f\" /></p>\n\n<p>预测的结果，和原始语料预处理得到的标签格式保持一致。</p>\n\n<p>语料和代码下载，请访问：<a href=\"https://github.com/sujeek/chinese_nlp\">Github</a>。</p>\n\n<h3>总结</h3>\n\n<p>本文通过清华大学的句法标注语料库，实现了基于 CRF 的中文句法依存分析模型。借此实例，相信大家对句法依存已有了一个完整客观的认识。</p>\n\n<p><strong>参考文献及推荐阅读</strong></p>\n\n<ol>\n	<li><a href=\"https://github.com/scrapinghub/python-crfsuite/blob/master/examples/CoNLL%202002.ipynb\">使用 CoNLL 2002 数据的英文句法依存分析</a></li>\n	<li><a href=\"http://x-algo.cn/index.php/2016/03/02/crf-dependency-parsing/\">CRF++ 依存句法分析</a></li>\n	<li><a href=\"https://blog.csdn.net/sinat_33741547/article/details/79321401\">依存分析：基于序列标注的中文依存句法分析模型实现</a></li>\n</ol>\n\n<h2><a id=\"第18课：模型部署上线的几种服务发布方式\" name=\"第18课：模型部署上线的几种服务发布方式\"></a>第18课：模型部署上线的几种服务发布方式</h2>\n\n<p>在前面所有的模型训练和预测中，我们训练好的模型都是直接通过控制台或者 Jupyter Notebook 来进行预测和交互的，在一个系统或者项目中使用这种方式显然不可能，那在 Web 应用中如何使用我们训练好的模型呢？本文将通过以下四个方面对该问题进行讲解：</p>\n\n<ol>\n	<li>微服务架构简介；</li>\n	<li>模型的持久化与加载方式；</li>\n	<li>Flask 和 Bottle 微服务框架；</li>\n	<li>Tensorflow Serving 模型部署和服务。</li>\n</ol>\n\n<h3>微服务架构简介</h3>\n\n<p>微服务是指开发一个单个小型的但有业务功能的服务，每个服务都有自己的处理和轻量通讯机制，可以部署在单个或多个服务器上。微服务也指一种松耦合的、有一定的有界上下文的面向服务架构。也就是说，如果每个服务都要同时修改，那么它们就不是微服务，因为它们紧耦合在一起；如果你需要掌握一个服务太多的上下文场景使用条件，那么它就是一个有上下文边界的服务，这个定义来自 DDD 领域驱动设计。</p>\n\n<p>相对于单体架构和 SOA，它的主要特点是组件化、松耦合、自治、去中心化，体现在以下几个方面：</p>\n\n<ol>\n	<li>\n	<p>一组小的服务：服务粒度要小，而每个服务是针对一个单一职责的业务能力的封装，专注做好一件事情；</p>\n	</li>\n	<li>\n	<p>独立部署运行和扩展：每个服务能够独立被部署并运行在一个进程内。这种运行和部署方式能够赋予系统灵活的代码组织方式和发布节奏，使得快速交付和应对变化成为可能。</p>\n	</li>\n	<li>\n	<p>独立开发和演化：技术选型灵活，不受遗留系统技术约束。合适的业务问题选择合适的技术可以独立演化。服务与服务之间采取与语言无关的 API 进行集成。相对单体架构，微服务架构是更面向业务创新的一种架构模式。</p>\n	</li>\n	<li>\n	<p>独立团队和自治：团队对服务的整个生命周期负责，工作在独立的上下文中，自己决策自己治理，而不需要统一的指挥中心。团队和团队之间通过松散的社区部落进行衔接。</p>\n	</li>\n</ol>\n\n<p>由此，我们可以看到整个微服务的思想，与我们现在面对信息爆炸、知识爆炸做事情的思路是相通的：通过解耦我们所做的事情，分而治之以减少不必要的损耗，使得整个复杂的系统和组织能够快速地应对变化。</p>\n\n<p>我们为什么采用微服务呢？</p>\n\n<blockquote>\n<p>&ldquo;让我们的系统尽可能快地响应变化&rdquo;</p>\n\n<p>&mdash;&mdash;Rebecca Parson</p>\n</blockquote>\n\n<p>下面是一个简单的微服务模型架构设计：</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/ce581560-9868-11e8-9965-cbd322341912\" /></p>\n\n<h3>模型的持久化与加载方式</h3>\n\n<p>开发过 J2EE 应用的人应该对持久化的概念很清楚。通俗得讲，就是临时数据（比如内存中的数据，是不能永久保存的）持久化为持久数据（比如持久化至数据库中，能够长久保存）。</p>\n\n<p>那我们训练好的模型一般都是存储在内存中，这个时候就需要用到持久化方式，在 Python 中，常用的模型持久化方式有三种，并且都是以文件的方式持久化。</p>\n\n<p><strong>1.JSON（JavaScript Object Notation）格式。</strong></p>\n\n<p>JSON 是一种轻量级的数据交换格式，易于人们阅读和编写。使用 JSON 函数需要导入 JSON 库：</p>\n\n<pre>\n<code>import json\n</code></pre>\n\n<p>它拥有两个格式处理函数：</p>\n\n<ul>\n	<li>json.dumps：将 Python 对象编码成 JSON 字符串；</li>\n	<li>json.loads：将已编码的 JSON 字符串解码为 Python 对象。</li>\n</ul>\n\n<p>下面看一个例子。</p>\n\n<p>首先我们创建一个 List 对象 data，然后把 data 编码成 JSON 字符串保存在 data.json 文件中，之后再读取 data.json 文件中的字符串解码成 Python 对象，代码如下：</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/9a645770-986b-11e8-9195-95b521cd20b9\" /></p>\n\n<p><strong>2. pickle 模块</strong></p>\n\n<p>pickle 提供了一个简单的持久化功能。可以将对象以文件的形式存放在磁盘上。pickle 模块只能在 Python 中使用，Python 中几乎所有的数据类型（列表、字典、集合、类等）都可以用 pickle 来序列化。pickle 序列化后的数据，可读性差，人一般无法识别。</p>\n\n<p>使用的时候需要引入库：</p>\n\n<pre>\n<code>import pickle\n</code></pre>\n\n<p>它有以下两个方法：</p>\n\n<ul>\n	<li>\n	<p>pickle.dump(obj, file[, protocol])：序列化对象，并将结果数据流写入到文件对象中。参数 protocol 是序列化模式，默认值为0，表示以文本的形式序列化。protocol 的值还可以是1或2，表示以二进制的形式序列化。</p>\n	</li>\n	<li>\n	<p>pickle.load(file)：反序列化对象。将文件中的数据解析为一个 Python 对象。</p>\n	</li>\n</ul>\n\n<p>我们继续延用上面的例子。实现的不同点在于，这次文件打开时用了<code>with...as...</code>&nbsp;语法，使用 pickle 保存结果，文件保存为 data.pkl，代码如下。</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/191e6640-986d-11e8-9195-95b521cd20b9\" /></p>\n\n<p><strong>3. sklearn 中的 joblib 模块。</strong></p>\n\n<p>使用 joblib，首先需要引入包：</p>\n\n<pre>\n<code>from sklearn.externals import joblib\n</code></pre>\n\n<p>使用方法如下，基本和 JSON、pickle一样，这里不再详细讲解。第17课中，进行模型保存时使用的就是这种方式，可以看代码，回顾一下。</p>\n\n<pre>\n<code>joblib.dump(model, model_path)  #模型保存\njoblib.load(model_path)  #模型加载\n</code></pre>\n\n<h3>Flask 和 Bottle 微服务框架</h3>\n\n<p>通过上面，我们对微服务和 Python 中三种模型持久化和加载方式有了基本了解。下面我们看看，Python 中如何把模型发布成一个微服务的。</p>\n\n<p>这里给出两个微服务框架&nbsp;<a href=\"http://www.bottlepy.com/docs/dev/\">Bottle</a>&nbsp;和&nbsp;<a href=\"http://docs.jinkan.org/docs/flask/\">Flask</a>。</p>\n\n<p>Bottle 是一个非常小巧但高效的微型 Python Web 框架，它被设计为仅仅只有一个文件的 Python 模块，并且除 Python 标准库外，它不依赖于任何第三方模块。</p>\n\n<p>Bottle 本身主要包含以下四个模块，依靠它们便可快速开发微 Web 服务：</p>\n\n<ul>\n	<li>路由（Routing）：将请求映射到函数，可以创建十分优雅的 URL；</li>\n	<li>模板（Templates）：可以快速构建 Python 内置模板引擎，同时还支持 Mako、Jinja2、Cheetah 等第三方模板引擎；</li>\n	<li>工具集（Utilites）：用于快速读取 form 数据，上传文件，访问 Cookies，Headers 或者其它 HTTP 相关的 metadata；</li>\n	<li>服务器（Server）：内置 HTTP 开发服务器，并且支持 paste、fapws3、 bjoern、Google App Engine、Cherrypy 或者其它任何 WSGI HTTP 服务器。</li>\n</ul>\n\n<p>Flask 也是一个 Python 编写的 Web 微框架，可以让我们使用 Python 语言快速实现一个网站或 Web 服务。并使用方式和 Bottle 相似，Flask 依赖 Jinja2 模板和 Werkzeug WSGI 服务。Werkzeug 本质是 Socket 服务端，其用于接收 HTTP 请求并对请求进行预处理，然后触发 Flask 框架，开发人员基于 Flask 框架提供的功能对请求进行相应的处理，并返回给用户，如果返回给用户的内容比较复杂时，需要借助 Jinja2 模板来实现对模板的处理，即将模板和数据进行渲染，将渲染后的字符串返回给用户浏览器。</p>\n\n<p>Bottle 和 Flask 在使用上相似，而且 Flask 的文档资料更全，发布的服务更稳定，因此下面重点以 Flask 为例，来说明模型的微服务发布过程。</p>\n\n<p>如果大家想进一步了解这两个框架，可以参考说明文档。</p>\n\n<p><strong>1.安装。</strong></p>\n\n<p>对 Bottle 和 Flask 进行安装，分别执行如下命令即可安装成功：</p>\n\n<pre>\n<code>pip install bottle\npip install Flask\n</code></pre>\n\n<p>安装好之后，分别进入需要的包就可以写微服务程序了。这两个框架在使用时，用法、语法结构都差不多，网上 Flask 的中文资料相对多一些，所以这里用 Flask 来举例。</p>\n\n<p><strong>2. 第一个最小的 Flask 应用。</strong></p>\n\n<p>第一个最小的 Flask 应用看起来会是这样:</p>\n\n<pre>\n<code>from flask import Flask\napp = Flask(__name__)\n\n@app.route(\'/\')\ndef hello_world():\n    return \'Hello World!\'\n\nif __name__ == \'__main__\':\n    app.run()\n</code></pre>\n\n<p>把它保存为 hello.py（或是类似的），然后用 Python 解释器来运行：</p>\n\n<pre>\n<code>python hello.py\n</code></pre>\n\n<p>或者直接在 Jupyter Notebook 里面执行，都没有问题。服务启动将在控制台打印如下消息：</p>\n\n<blockquote>\n<p>Running on http://127.0.0.1:5000/</p>\n</blockquote>\n\n<p>意思就是，可以通过 localhost 和 5000 端口，在浏览器访问：</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/616e7240-98b9-11e8-9965-cbd322341912\" /></p>\n\n<p>这时我们就得到了服务在浏览器上的返回结果，于是也成功构建了与浏览器交互的服务。</p>\n\n<p>如果要修改服务对应的 IP 地址和端口怎么办？只需要修改这行代码，即可修改 IP 地址和端口：</p>\n\n<pre>\n<code>app.run(host=\'192.168.31.19\',port=8088)\n</code></pre>\n\n<p><strong>3. Flask 发布一个预测模型。</strong></p>\n\n<p>首先，我们这里使用第17课保存的模型&ldquo;model.pkl&rdquo;。如果不使用浏览器，常规的控制台交互，我们这样就可以实现：</p>\n\n<pre>\n<code>    from sklearn.externals import joblib\n    model_path = \"D://达人课//中文自然语言处理入门实战课程//ch18//model.pkl\"\n    model = joblib.load(model_path)\n    sen =[[[\'坚决\', \'a\', \'ad\', \'1_v\'],\n               [\'惩治\', \'v\', \'v\', \'0_Root\'],\n               [\'贪污\', \'v\', \'v\', \'1_v\'],\n               [\'贿赂\', \'n\', \'n\', \'-1_v\'],\n               [\'等\', \'u\', \'udeng\', \'-1_v\'],\n               [\'经济\', \'n\', \'n\', \'1_v\'],\n               [\'犯罪\', \'v\', \'vn\', \'-2_v\']]]\n    print(model.predict(sen))\n</code></pre>\n\n<p>如果你现在有个需求，要求你的模型和浏览器进行交互，那 Flask 就可以实现。</p>\n\n<p>在第一个最小的 Flask 应用基础上，我们增加模型预测接口，这里注意：<strong>启动之前把 IP 地址修改为自己本机的地址或者服务器工作站所在的 IP 地址。</strong></p>\n\n<p>完整的代码如下，首先在启动之前先把模型预加载到内存中，然后重新定义 predict 函数，接受一个参数 sen：</p>\n\n<pre>\n<code>    from sklearn.externals import joblib\n    from flask import Flask,request\n    app = Flask(__name__)\n\n    @app.route(\'/\')\n    def hello_world():\n        return \'Hello World!\'\n\n    @app.route(\'/predict/&lt;sen&gt;\')\n    def predict(sen):\n        result = model.predict(sen)\n        return str(result)\n\n    if __name__ == \'__main__\':\n        model_path = \"D://ch18//model.pkl\"\n        model = joblib.load(model_path)\n        app.run(host=\'192.168.31.19\')\n</code></pre>\n\n<p>启动 Flask 服务之后，在浏览器地址中输入：</p>\n\n<blockquote>\n<p>http://192.168.31.19:5000/predict/[[[&#39;坚决&#39;, &#39;a&#39;, &#39;ad&#39;, &#39;1<em>v&#39;], [&#39;惩治&#39;, &#39;v&#39;, &#39;v&#39;, &#39;0</em>Root&#39;], [&#39;贪污&#39;, &#39;v&#39;, &#39;v&#39;, &#39;1<em>v&#39;], [&#39;贿赂&#39;, &#39;n&#39;, &#39;n&#39;, &#39;-1</em>v&#39;], [&#39;等&#39;, &#39;u&#39;, &#39;udeng&#39;, &#39;-1<em>v&#39;], [&#39;经济&#39;, &#39;n&#39;, &#39;n&#39;, &#39;1</em>v&#39;], [&#39;犯罪&#39;, &#39;v&#39;, &#39;vn&#39;, &#39;-2_v&#39;]]]</p>\n</blockquote>\n\n<p>得到预测结果，这样就完成了微服务的发布，并实现了模型和前端浏览器的交互。</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/4177b380-988b-11e8-bb33-7f37906bb913\" /></p>\n\n<h3>Tensorflow Serving 模型部署和服务</h3>\n\n<p>TensorFlow Serving 是一个用于机器学习模型 Serving 的高性能开源库。它可以将训练好的机器学习模型部署到线上，使用 gRPC 作为接口接受外部调用。更加让人眼前一亮的是，它支持模型热更新与自动模型版本管理。这意味着一旦部署 TensorFlow Serving 后，你再也不需要为线上服务操心，只需要关心你的线下模型训练。</p>\n\n<p>同样，TensorFlow Serving 可以将模型部署在移动端，如安卓或者 iOS 系统的 App 应用上。关于 Tensorflow Serving 模型部署和服务，这里不在列举示例，直接参考文末的推荐阅读。</p>\n\n<h3>总结</h3>\n\n<p>本节对微服务架构做了简单介绍，并介绍了三种机器学习模型持久化和加载的方式，接着介绍了 Python 的两个轻量级微服务框架 Bottle 和 Flask。随后，我们通过 Flask 制作了一个简单的微服务预测接口，实现模型的预测和浏览器交互功能，最后简单介绍了 TensorFlow Servin 模型的部署和服务功能。</p>\n\n<p>学完上述内容，读者可轻易实现自己训练的模型和 Web 应用的结合，提供微服务接口，实现模型上线应用。</p>\n\n<p><strong>参考文献以及推荐阅读</strong></p>\n\n<ol>\n	<li><a href=\"http://www.bottlepy.com/docs/dev/\">Bottle 文档</a></li>\n	<li><a href=\"http://docs.jinkan.org/docs/flask/\">Flask 文档</a></li>\n	<li><a href=\"https://blog.csdn.net/heyc861221/article/details/80129169\">面向机器智能的 TensorFlow 实践：产品环境中模型的部署</a></li>\n	<li><a href=\"https://blog.csdn.net/shin627077/article/details/78592729\">Tensorflow Serving 服务部署与访问（Python + Java）</a></li>\n</ol>\n\n<h2><a id=\"第19课：知识挖掘与知识图谱概述\" name=\"第19课：知识挖掘与知识图谱概述\"></a>第19课：知识挖掘与知识图谱概述</h2>\n\n<p>搜索技术日新月异，如今它不再是搜索框中输入几个单词那么简单了。不仅输入方式多样化，并且还要在非常短的时间内给出一个精准而又全面的答案。目前，谷歌给出的解决方案就是&mdash;&mdash;知识图谱（Knowledge Graph）。</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/99711d60-9dc7-11e8-a606-7910b86fc801\" /></p>\n\n<h3>知识图谱能做什么？</h3>\n\n<p>知识图谱想做的，就是在不同数据（来自现实世界）之间建立联系，从而带给我们更有意义的搜索结果。</p>\n\n<p>比如，在上图中，用 Google 搜索自然语言处理，右侧会显示研究领域和相关概念。点击这些知识点，又可以深入了解；再比如，搜索一个人名时，右侧会给出此人的生平、背景、居住位置、作品等信息。</p>\n\n<p>这就是知识图谱，它不再是单一的信息，而是一个多元的信息网络。</p>\n\n<h3>知识图谱的源头</h3>\n\n<p>知识图谱的雏形好几年前就已出现，一家名为 Metaweb 的小公司，将现实世界中实体（人或事）的各种数据信息存储在系统中，并在数据之间建立起联系，从而发展出有别于传统关键词搜索的技术。</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/83c3ba70-9dc9-11e8-991f-1fa5582600fd\" /></p>\n\n<p>谷歌认为这一系统很有发展潜力，于2010年收购了 Metaweb。那时 Metawab 已经存储了1200万个节点（Reference Point，相当于一个词条或者一个页面），谷歌收购后的两年中，大大加速这一进程，现已有超过5.7亿个节点并在它们之间建了180亿个有效连接（这可是一个相当大的数字，维基百科英文版也才有大约400万个节点）。</p>\n\n<h3>知识图谱的通用表示方法</h3>\n\n<p>本质上，知识图谱是一种揭示实体之间关系的语义网络 ，可以对现实世界的事物及其相互关系进行形式化地描述 。现在的知识图谱己被用来泛指各种大规模的知识库 。</p>\n\n<p>三元组是知识图谱的一种通用表示方式，即&nbsp;G=(E，R，S)G=(E，R，S)，其中&nbsp;E=e1，e2，&hellip;，e|E|E=e1，e2，&hellip;，e|E|&nbsp;是知识库中的实体集合，共包含&nbsp;|E||E|&nbsp;种不同实体，R=r1，r2，&hellip;,r|E|R=r1，r2，&hellip;,r|E|&nbsp;是知识库中的关系集合，共包含&nbsp;|R||R|&nbsp;种不同关系，S&sube;E&times;R&times;ES&sube;E&times;R&times;E&nbsp;代表知识库中的三元组集合。</p>\n\n<p>三元组的基本形式主要包括实体 A、关系、实体 B 和概念、属性、属性值等，实体是知识图谱中的最基本元素，不同的实体间存在不同的关系。概念主要指集合、类别、对象类型、事物的种类，例如人物、地理等；属性主要指对象可能具有的属性、特征、特性、特点以及参数，例如国籍、生日等；属性值主要指对象指定属性的值，例如中国、1988&mdash;09&mdash;08等。每个实体（概念的外延）可用一个全局唯一确定的 ID 来标识，每个属性&mdash;属性值对可用来刻画实体的内在特性，而关系可用来连接两个实体，刻画它们之间的关联。</p>\n\n<p>如下图是实体 A 与实体 B 组成的一个简单三元组形式。</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/c31662b0-9dd1-11e8-8538-cb29bbd5eb8b\" /></p>\n\n<h3>知识图谱的架构</h3>\n\n<p>知识图谱的架构主要包括自身的逻辑结构以及体系架构，分别说明如下。</p>\n\n<p><strong>1. 知识图谱的逻辑结构。</strong></p>\n\n<p>知识图谱在逻辑上可分为模式层与数据层两个层次，数据层主要是由一系列的事实组成，而知识将以事实为单位进行存储。如果用（实体 A，关系，实体 B）、（实体、属性，属性值）这样的三元组来表达事实，可选择图数据库作为存储介质，例如开源的 Neo4j、Twitter 的 FlockDB、Sones 的 GraphDB 等。模式层构建在数据层之上，主要是通过本体库来规范数据层的一系列事实表达。本体是结构化知识库的概念模板，通过本体库而形成的知识库不仅层次结构较强，并且冗余程度较小。</p>\n\n<p><strong>2. 知识图谱的体系架构。</strong></p>\n\n<p>知识图谱的体系架构是指其构建模式结构，如图下图所示。</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/03944f10-9ddb-11e8-b6f3-454e1d4b65e0\" /></p>\n\n<p>知识图谱主要有自顶向下与自底向上两种构建方式。自顶向下指的是先为知识图谱定义好本体与数据模式，再将实体加入到知识库。该构建方式需要利用一些现有的结构化知识库作为其基础知识库，例如 Freebase 项目就是采用这种方式，它的绝大部分数据是从维基百科中得到的。自底向上指的是从一些开放链接数据中提取出实体，选择其中置信度较高的加入到知识库，再构建顶层的本体模式。目前，大多数知识图谱都采用自底向上的方式进行构建，其中最典型就是 Google 的 Knowledge Vault。</p>\n\n<h3>知识图谱的关键技术</h3>\n\n<p>大规模知识库的构建与应用需要多种智能信息处理技术的支持。这就涉及到当下异常火爆的人工智能中的自然语言处理（NLP）技术。</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/2cc327b0-9dec-11e8-b6f3-454e1d4b65e0\" /></p>\n\n<p>所谓自然语言，就是我们平时所说的话（包括语音或文字），但这些话计算机如何能&ldquo;理解&rdquo;？过程很复杂，下面是其中的几个关键步骤。</p>\n\n<p><strong>1. 知识抽取。</strong></p>\n\n<p>知识抽取技术，可以从一些公开的半结构化、非结构化的数据中提取出实体、关系、属性等知识要素。</p>\n\n<p>知识抽取主要包含实体抽取、关系抽取、属性抽取等，涉及到的 NLP 技术有命名实体识别、句法依存、实体关系识别等。</p>\n\n<p><strong>2. 知识表示。</strong></p>\n\n<p>知识表示形成的综合向量对知识库的构建、推理、融合以及应用均具有重要的意义。</p>\n\n<p>基于三元组的知识表示形式受到了人们广泛的认可，但是其在计算效率、数据稀疏性等方面却面临着诸多问题。近年来，以深度学习为代表的表示学习技术取得了重要的进展，可以将实体的语义信息表示为稠密低维实值向量，进而在低维空间中高效计算实体、关系及其之间的复杂语义关联。</p>\n\n<p>知识表示学习主要包含的 NLP 技术有语义相似度计算、复杂关系模型，知识代表模型如距离模型、双线性模型、神经张量模型、矩阵分解模型、翻译模型等。</p>\n\n<p><strong>3.知识融合。</strong></p>\n\n<p>由于知识图谱中的知识来源广泛，存在知识质量良莠不齐、来自不同数据源的知识重复、知识间的关联不够明确等问题，所以必须要进行知识的融合。知识融合是高层次的知识组织，使来自不同知识源的知识在同一框架规范下进行异构数据整合、消歧、加工、推理验证、更新等步骤，达到数据、信息、方法、经验以及人的思想的融合，形成高质量的知识库。</p>\n\n<p>在知识融合过程中，实体对齐、知识加工是两个重要的过程。</p>\n\n<p><strong>4.知识推理。</strong></p>\n\n<p>知识推理则是在已有的知识库基础上进一步挖掘隐含的知识，从而丰富、扩展知识库。在推理的过程中，往往需要关联规则的支持。由于实体、实体属性以及关系的多样性，人们很难穷举所有的推理规则，一些较为复杂的推理规则往往是手动总结的。对于推理规则的挖掘，主要还是依赖于实体以及关系间的丰富情况。知识推理的对象可以是实体、实体的属性、实体间的关系、本体库中概念的层次结构等。</p>\n\n<p>知识推理方法主要可分为基于逻辑的推理与基于图的推理两种类别。</p>\n\n<h3>大规模开放知识库</h3>\n\n<p>互联网的发展为知识工程提供了新的机遇。从一定程度上看，是互联网的出现帮助突破了传统知识工程在知识获取方面的瓶颈。从1998年 Tim Berners Lee 提出语义网至今，涌现出大量以互联网资源为基础的新一代知识库。这类知识库的构建方法可以分为三类：互联网众包、专家协作和互联网挖掘，如下图所示：</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/84543f00-9de7-11e8-8538-cb29bbd5eb8b\" /></p>\n\n<p>下面介绍几个知名的中文知识图谱资源：</p>\n\n<ul>\n	<li>\n	<p>OpenKG.CN：中文开放知识图谱联盟旨在通过建设开放的社区来促进中文知识图谱数据的开放与互联，促进中文知识图谱工具的标准化和技术普及。</p>\n	</li>\n	<li>\n	<p>Zhishi.me ：Zhishi.me 是中文常识知识图谱。主要通过从开放的百科数据中抽取结构化数据，已融合了百度百科，互动百科以及维基百科中的中文数据。</p>\n	</li>\n	<li>\n	<p>CN-DBPeidia：CN-DBpedia 是由复旦大学知识工场实验室研发并维护的大规模通用领域结构化百科。</p>\n	</li>\n	<li>\n	<p>cnSchema.org: cnSchema.org 是一个基于社区维护的开放的知识图谱 Schema 标准。cnSchema 的词汇集包括了上千种概念分类、数据类型、属性和关系等常用概念定义，以支持知识图谱数据的通用性、复用性和流动性。</p>\n	</li>\n</ul>\n\n<h3>知识图谱的典型应用</h3>\n\n<p>知识图谱为互联网上海量、异构、动态的大数据表达、组织、管理以及利用提供了一种更为有效的方式，使得网络的智能化水平更高，更加接近于人类的认知思维。</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/176dd730-9deb-11e8-b6f3-454e1d4b65e0\" /></p>\n\n<p>基于大规模开放知识库或知识图谱的应用，目前尚处在持续不断的发展与探索的阶段。下面列出了一些国内外比较出色的应用。</p>\n\n<p><strong>1. 语义检索。</strong></p>\n\n<p>谷歌公司通过建立 Google Knowledge Graph，实现了对知识的体系化组织与展示，试图从用户搜索意图感知、以及查询扩展的角度，直接提供给用户想要的知识。</p>\n\n<p><strong>2. 智能问答。</strong></p>\n\n<p>IBM 公司通过搭建知识图谱，并通过自然语言处理和机器学习等技术，开发出了 Watson 系统。在2011年2月的美国问答节目《Jeopardy!》上，Watson 战胜了这一节目的两位冠军选手，可与1996年同样来自 IBM 的&ldquo;深蓝&rdquo;战胜国际象棋大师卡斯帕罗夫产生的影响相提并论，被认为是人工智能历史上的一个里程碑。</p>\n\n<p><strong>3. 领域专家快速生成。</strong></p>\n\n<p>构建面向特定领域、特定主题的大规模知识库是实现对某一领域深度分析和计算的重要基础，OpenKN 通过实现端到端的开放知识库构建工具集，实现了在给定部分种子（Seed）的情况下，从无到有的生成领域知识库，进而形成领域专家。</p>\n\n<p><strong>4. 行业生态深度分析与预测。</strong></p>\n\n<p>利用开放大数据可以帮助企业发现潜伏在数据中的威胁，将结构化网络日志、文本数据、开源和第三方数据整合进一个单一的环境，屏蔽可疑的信号与噪声，有效保护用户网络，可在信用卡欺诈行为识别、医疗行业疾病预测、电商商品推荐、强化组织数据安全、不一致性验证、异常分析、金融量化交易、法律分析服务等多方面提供有价值的服务。</p>\n\n<h3>知识图谱的前景与挑战</h3>\n\n<p>在关注到知识图谱在自然语言处理、人工智能等领域展现巨大潜力的同时，也不难发现知识图谱中的知识获取、知识表示、知识推理等技术依然面临着一些困难与挑战，在未来的一段时间内，知识图谱将是大数据智能的前沿研究问题，有很多重要的开放性问题亟待学术界和产业界协力解决。我们认为，未来知识图谱研究有以下几个重要挑战：</p>\n\n<ul>\n	<li>\n	<p>知识类型与表示。知识图谱主要采用（实体1、关系、实体2）三元组的形式来表示知识，这种方法可以较好地表示很多事实性知识。然而，人类知识类型多样，面对很多复杂知识，三元组就束手无策了。例如，人们的购物记录信息、新闻事件等，包含大量实体及其之间的复杂关系，更不用说人类大量的涉及主观感受、主观情感和模糊的知识了。</p>\n	</li>\n	<li>\n	<p>知识获取。如何从互联网大数据萃取知识，是构建知识图谱的重要问题。目前已经提出各种知识获取方案，并已成功抽取大量有用的知识。但在抽取知识的准确率、覆盖率和效率等方面，都仍不如人意，有极大的提升空间。</p>\n	</li>\n	<li>\n	<p>知识融合。来自不同数据的抽取知识可能存在大量噪音和冗余，或者使用了不同的语言。如何将这些知识有机融合起来，建立更大规模的知识图谱，是实现大数据智能的必由之路。</p>\n	</li>\n	<li>\n	<p>知识应用。目前大规模知识图谱的应用场景和方式还比较有限，如何有效实现知识图谱的应用，利用知识图谱实现深度知识推理，提高大规模知识图谱计算效率，需要人们不断锐意发掘用户需求，探索更重要的应用场景，提出新的应用算法。</p>\n	</li>\n</ul>\n\n<h3>总结</h3>\n\n<p>本文对知识图谱的起源、定义、架构、大规模知识库、应用以及未来挑战等内容，进行了全面阐述。</p>\n\n<p>知识抽取、知识表示、知识融合以及知识推理为构建知识图谱的四大核心技术，本文就当前产业界的需求介绍了它在智能搜索、深度问答、社交网络以及一些垂直行业中的实际应用。此外，还总结了目前知识图谱面临的主要挑战，并对其未来的研究方向进行了展望。</p>\n\n<p>知识图谱的重要性不仅在于它是一个拥有强大语义处理能力与开放互联能力的知识库，并且还是一把开启智能机器大脑的钥匙，能够打开 Web3.0 时代的知识宝库，为相关学科领域开启新的发展方向。</p>\n\n<p><strong>参考资料以及推荐阅读</strong></p>\n\n<ol>\n	<li>柳絮飞.《知识图谱：谷歌打造未来搜索》，电脑爱好者，2013年。</li>\n	<li>徐增林，盛泳潘，贺丽荣，王雅芳.《知识图谱技术综述》，电子科技大学统计机器智能与学习实验室，2016年7月。</li>\n	<li><a href=\"https://blog.csdn.net/qingqingpiaoguo/article/details/53366240\">知识图谱&mdash;&mdash;机器大脑中的知识库</a></li>\n	<li><a href=\"https://www.sohu.com/a/137145473_160850\">人工智能2.0时代的开放知识计算</a></li>\n</ol>\n\n<h2><a id=\"第20课：Neo4j 从入门到构建一个简单知识图谱\" name=\"第20课：Neo4j 从入门到构建一个简单知识图谱\"></a>第20课：Neo4j 从入门到构建一个简单知识图谱</h2>\n\n<p>Neo4j 对于大多数人来说，可能是比较陌生的。其实，Neo4j 是一个图形数据库，就像传统的关系数据库中的 Oracel 和 MySQL一样，用来持久化数据。Neo4j 是最近几年发展起来的新技术，属于 NoSQL 数据库中的一种。</p>\n\n<p>本文主要从 Neo4j 为什么被用来做知识图谱，Neo4j 的简单安装，在 Neo4j 浏览器中创建节点和关系，Neo4j 的 Python 接口操作以及用 Neo4j 构建一个简单的农业知识图谱五个方面来讲。</p>\n\n<h3>Neo4j 为什么被用来做知识图谱</h3>\n\n<p>从第19课《知识挖掘与知识图谱概述》中，我们已经明白，知识图谱是一种基于图的数据结构，由节点和边组成。其中节点即实体，由一个全局唯一的 ID 标示，关系（也称属性）用于连接两个节点。通俗地讲，知识图谱就是把所有不同种类的信息连接在一起而得到一个关系网络，提供了从&ldquo;关系&rdquo;的角度去分析问题的能力。</p>\n\n<p>而 Neo4j 作为一种经过特别优化的图形数据库，有以下优势：</p>\n\n<ul>\n	<li>\n	<p><strong>数据存储</strong>：不像传统数据库整条记录来存储数据，Neo4j 以图的结构存储，可以存储图的节点、属性和边。属性、节点都是分开存储的，属性与节点的关系构成边，这将大大有助于提高数据库的性能。</p>\n	</li>\n	<li>\n	<p><strong>数据读写</strong>：在 Neo4j 中，存储节点时使用了&nbsp;<code>Index-free Adjacency</code>&nbsp;技术，即每个节点都有指向其邻居节点的指针，可以让我们在时间复杂度为 O(1) 的情况下找到邻居节点。另外，按照官方的说法，在 Neo4j 中边是最重要的，是&nbsp;<code>First-class Entities</code>，所以单独存储，更有利于在图遍历时提高速度，也可以很方便地以任何方向进行遍历。</p>\n	</li>\n	<li>\n	<p><strong>资源丰富</strong>：Neo4j 作为较早的一批图形数据库之一，其文档和各种技术博客较多。</p>\n	</li>\n	<li>\n	<p><strong>同类对比</strong>：Flockdb 安装过程中依赖太多，安装复杂；Orientdb，Arangodb 与 Neo4j 做对比，从易用性来说都差不多，但是从稳定性来说，neo4j 是最好的。</p>\n	</li>\n</ul>\n\n<p>综合上述以及因素，我认为 Neo4j 是做知识图谱比较简单、灵活、易用的图形数据库。</p>\n\n<h3>Neo4j 的简单安装</h3>\n\n<p>Neo4j 是基于 Java 的图形数据库，运行 Neo4j 需要启动 JVM 进程，因此必须安装 Java SE 的 JDK。从 Oracle 官方网站下载&nbsp;<a href=\"http://www.oracle.com/technetwork/java/javase/downloads/index.html\">Java SE JDK</a>，选择版本 JDK8 以上版本即可。</p>\n\n<p>下面简单介绍下 Neo4j 在 Linux 和 Windows 的安装过程。首先去<a href=\"https://neo4j.com/download/other-releases/#releases\">官网</a>下载对应版本。解压之后，Neo4j 应用程序有如下主要的目录结构：</p>\n\n<ul>\n	<li>bin 目录：用于存储 Neo4j 的可执行程序；</li>\n	<li>conf 目录：用于控制 Neo4j 启动的配置文件；</li>\n	<li>data 目录：用于存储核心数据库文件；</li>\n	<li>plugins 目录：用于存储 Neo4j 的插件。</li>\n</ul>\n\n<h4>Linux 系统下的安装</h4>\n\n<p>通过 tar 解压命令解压到一个目录下：</p>\n\n<pre>\n<code>tar -xzvf neo4j-community-3.3.1-unix.tar.gz\n</code></pre>\n\n<p>然后进入 Neo4j 解压目录：</p>\n\n<pre>\n<code>cd /usr/local/neo4j/neo4j-community-3.1.0\n</code></pre>\n\n<p>通过启动命令，可以实现启动、控制台、停止服务：</p>\n\n<pre>\n<code>bin/neo4j  start/console/stop（启动/控制台/停止）\n</code></pre>\n\n<p>通过&nbsp;<code>cypher-shell</code>&nbsp;命令，可以进入命令行：</p>\n\n<pre>\n<code>bin/cypher-shell\n</code></pre>\n\n<h4>Windows 系统下的安装</h4>\n\n<p>启动 DOS 命令行窗口，切换到解压目录 bin 下，以管理员身份运行命令，分别为启动服务、停止服务、重启服务和查询服务的状态：</p>\n\n<pre>\n<code>bin\\neo4j start\nbin\\neo4j stop\nbin\\neo4j restart\nbin\\neo4j status\n</code></pre>\n\n<p>把 Neo4j 安装为服务（Windows Services），可通过以下命令：</p>\n\n<pre>\n<code>bin\\neo4j install-service\nbin\\neo4j uninstall-service\n</code></pre>\n\n<p>Neo4j 的配置文档存储在 conf 目录下，Neo4j 通过配置文件 neo4j.conf 控制服务器的工作。默认情况下，不需要进行任意配置，就可以启动服务器。</p>\n\n<p>下面我们在 Windows 环境下启动 Neo4j：</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/a76e8bd0-a0fd-11e8-8279-afd456e25292\" /></p>\n\n<p>Neo4j 服务器具有一个集成的浏览器，在一个运行的服务器实例上访问： http://localhost:7474/，打开浏览器，显示启动页面：</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/d39a6080-a0fd-11e8-aa70-d319955d5dbe\" /></p>\n\n<p>默认的 Host 是&nbsp;<code>bolt://localhost:7687</code>，默认的用户是 neo4j，其默认的密码是 neo4j，第一次成功登录到 Neo4j 服务器之后，需要重置密码。访问 Graph Database 需要输入身份验证，Host 是 Bolt 协议标识的主机。登录成功后界面：</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/456d52d0-a0fe-11e8-aa70-d319955d5dbe\" /></p>\n\n<p>到此为止，我们就完成了 Neo4j 的基本安装过程，更详细的参数配置，可以参考官方文档。</p>\n\n<h3>在 Neo4j 浏览器中创建节点和关系</h3>\n\n<p>下面，我们简单编写 Cypher 命令，Cypher 命令可以通过&nbsp;<a href=\"https://www.w3cschool.cn/neo4j/\">Neo4j 教程</a>学习，在浏览器中通过 Neo4j 创建两个节点和两个关系。</p>\n\n<p>在&nbsp;<code>$</code>&nbsp;命令行中，编写 Cypher 脚本代码，点击 Play 按钮完成创建，依次执行下面的语句：</p>\n\n<pre>\n<code>CREATE (n:Person { name: \'Andres\', title: \'Developer\' }) return n;\n</code></pre>\n\n<p>作用是创建一个 Person，并包含属性名字和职称。</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/eb544300-a100-11e8-8279-afd456e25292\" /></p>\n\n<p>下面这条语句也创建了一个 Person 对象，属性中只是名字和职称不一样。</p>\n\n<pre>\n<code>CREATE (n:Person { name: \'Vic\', title: \'Developer\' }) return n;\n</code></pre>\n\n<p>紧接着，通过下面两行命令进行两个 Person 的关系匹配：</p>\n\n<pre>\n<code>match(n:Person{name:\"Vic\"}),(m:Person{name:\"Andres\"}) create (n)-[r:Friend]-&gt;(m) return r;\n\nmatch(n:Person{name:\"Vic\"}),(m:Person{name:\"Andres\"}) create (n)&lt;-[r:Friend]-(m) return r;\n</code></pre>\n\n<p>最后，在创建完两个节点和关系之后，查看数据库中的图形：</p>\n\n<pre>\n<code>match(n) return n;\n</code></pre>\n\n<p>如下图，返回两个 Person 节点，以及其关系网，两个 Person 之间组成 Friend 关系：</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/fbbae950-a101-11e8-82f2-9d33eeaf0679\" /></p>\n\n<h3>Neo4j 的 Python 操作</h3>\n\n<p>既然 Neo4j 作为一个图库数据库，那我们在项目中使用的时候，必然不能通过上面那种方式完成任务，一般都要通过代码来完成数据的持久化操作。其中，对于 Java 编程者来说，可通过&nbsp;<a href=\"https://docs.spring.io/spring-data/neo4j/docs/current/reference/html/\">Spring Data Neo4j</a>&nbsp;达到这一目的。</p>\n\n<p>而对于 Python 开发者来说，Py2neo 库也可以完成对 Neo4j 的操作，操作过程如下。</p>\n\n<p>首先 安装 Py2neo。Py2neo 的安装过程非常简单，在命令行通过下面命令即可安装成功。</p>\n\n<pre>\n<code>pip install py2neo\n</code></pre>\n\n<p>安装好之后，我们来看一下简单的图关系构建，看下面代码：</p>\n\n<pre>\n<code>from py2neo.data import Node, Relationship\na = Node(\"Person\", name=\"Alice\")\nb = Node(\"Person\", name=\"Bob\")\nab = Relationship(a, \"KNOWS\", b)\n</code></pre>\n\n<p>第一行代码，首先引入 Node 和 Relationship 对象，紧接着，创建 a 和 b 节点对象，最后一行匹配 a 和 b 之间的工作雇佣关系。接着来看看 ab 对象的内容是什么：</p>\n\n<pre>\n<code>print(ab)\n</code></pre>\n\n<p>通过 print 打印出 ab 的内容：</p>\n\n<pre>\n<code>(Alice)-[:KNOWS {}]-&gt;(Bob)\n</code></pre>\n\n<p>通过这样，就完成了 Alice 和 Bob 之间的工作关系，如果有多组关系将构建成 Person 之间的一个关系网。</p>\n\n<p>了解更多 Py2neo 的使用方法，建议查看官方文档。</p>\n\n<h3>用 Neo4j 构建一个简单的农业知识图谱</h3>\n\n<p>我们来看一个基于开源语料的简单农业知识图谱，由于过程比较繁杂，数据和知识图谱数据预处理过程这里不再赘述，下面，我们重点看基于 Neo4j 来创建知识图谱的过程。</p>\n\n<p>整个过程主要包含以下步骤：</p>\n\n<ul>\n	<li>环境准备</li>\n	<li>语料准备</li>\n	<li>语料加载</li>\n	<li>知识图谱查询展示</li>\n</ul>\n\n<h4>Neo4j 环境准备。</h4>\n\n<p>根据上面对 Neo4j 环境的介绍，这里默认你已经搭建好 Neo4j 的环境，并能正常访问，如果没有环境，请自行搭建好 Neo4j 的可用环境。</p>\n\n<h4>数据语料介绍。</h4>\n\n<p>本次提供的语料是已经处理好的数据，包含6个 csv 文件，文件内容和描述如下。</p>\n\n<ul>\n	<li>attributes.csv：文件大小 2M，内容是通过互动百科页面得到的部分实体的属性，包含字段：Entity、AttributeName、Attribute，分别表示实体、属性名称、属性值。文件前5行结构如下：</li>\n</ul>\n\n<pre>\n<code>Entity,AttributeName,Attribute\n密度板,别名,纤维板\n葡萄蔓枯病,主要为害部位,枝蔓\n坎德拉,性别,男\n坎德拉,国籍,法国\n坎德拉,场上位置,后卫\n</code></pre>\n\n<ul>\n	<li><code>hudong_pedia.csv</code>：文件大小 94.6M，内容是已经爬好的农业实体的百科页面的结构化数据，包含字段：title、url、image、openTypeList、detail、baseInfoKeyList、baseInfoValueList，分别表示名称、百科 URL 地址、图片、分类类型、详情、关键字、依据来源。文件前2行结构如下：</li>\n</ul>\n\n<pre>\n<code>\"title\",\"url\",\"image\",\"openTypeList\",\"detail\",\"baseInfoKeyList\",\"baseInfoValueList\"\n\"菊糖\",\"http://www.baike.com/wiki/菊糖\",\"http://a0.att.hudong.com/72/85/20200000013920144736851207227_s.jpg\",\"健康科学##分子生物学##化学品##有机物##科学##自然科学##药品##药学名词##药物中文名称列表\",\"[药理作用] 诊断试剂 人体内不含菊糖，静注后，不被机体分解、结合、利用和破坏，经肾小球滤过，通过测定血中和尿中的菊糖含量，可以准确计算肾小球的滤过率。菊糖广泛存在于植物组织中,约有3.6万种植物中含有菊糖,尤其是菊芋、菊苣块根中含有丰富的菊糖[6,8]。菊芋(Jerusalem artichoke)又名洋姜,多年生草本植物,在我国栽种广泛,其适应性广、耐贫瘠、产量高、易种植,一般亩产菊芋块茎为2 000～4 000 kg,菊芋块茎除水分外,还含有15%～20%的菊糖,是加工生产菊糖及其制品的良好原料。\",\"中文名：\",\"菊糖\"\n\"密度板\",\"http://www.baike.com/wiki/密度板\",\"http://a0.att.hudong.com/64/31/20200000013920144728317993941_s.jpg\",\"居家##巧克力包装##应用科学##建筑材料##珠宝盒##礼品盒##科学##糖果盒##红酒盒##装修##装饰材料##隔断##首饰盒\",\"密度板（英文：Medium Density Fiberboard (MDF)）也称纤维板，是以木质纤维或其他植物纤维为原料，施加脲醛树脂或其他适用的胶粘剂制成的人造板材。按其密度的不同，分为高密度板、中密度板、低密度板。密度板由于质软耐冲击，也容易再加工，在国外是制作家私的一种良好材料，但由于国家关于高密度板的标准比国际标准低数倍，所以，密度板在中国的使用质量还有待提高。\",\"中文名：##全称：##别名：##主要材料：##分类：##优点：\",\"密度板##中密度板纤维板##纤维板##以木质纤维或其他植物纤维##高密度板、中密度板、低密度板##表面光滑平整、材质细密性能稳定\"\n</code></pre>\n\n<ul>\n	<li>\n	<p><code>hudong_pedia2.csv</code>：文件大小 41M，内容结构和&nbsp;<code>hudong_pedia.csv</code>&nbsp;文件保持一致，只是增加数据量，作为&nbsp;<code>hudong_pedia.csv</code>&nbsp;数据的补充。</p>\n	</li>\n	<li>\n	<p><code>new_node.csv</code>：文件大小 2.28M，内容是节点名称和标签，包含字段：title、lable，分别表示节点名称、标签，文件前5行结构如下：</p>\n	</li>\n</ul>\n\n<pre>\n<code>title,lable\n药物治疗,newNode\n膳食纤维,newNode\nBoven Merwede,newNode\n亚美尼亚苏维埃百科全书,newNode\n</code></pre>\n\n<ul>\n	<li><code>wikidata_relation.csv</code>：文件大小 1.83M，内容是实体和关系，包含字段 HudongItem1、relation、HudongItem2，分别表示实体1、关系、实体2，文件前5行结构如下：</li>\n</ul>\n\n<pre>\n<code>HudongItem1,relation,HudongItem2\n菊糖,instance of,化合物\n菊糖,instance of,多糖\n瓦尔,instance of,河流\n菊糖,subclass of,食物\n瓦尔,origin of the watercourse,莱茵河\n</code></pre>\n\n<ul>\n	<li><code>wikidata_relation2.csv</code>：大小 7.18M，内容结构和<code>wikidata_relation.csv</code>&nbsp;一致，作为&nbsp;<code>wikidata_relation.csv</code>&nbsp;数据的补充。</li>\n</ul>\n\n<h4>语料加载。</h4>\n\n<p>语料加载，利用 Neo4j 的&nbsp;<code>LOAD CSV WITH HEADERS FROM...</code>&nbsp;功能进行加载，具体操作过程如下。</p>\n\n<p>首先，依次执行以下命令：</p>\n\n<pre>\n<code>// 将hudong_pedia.csv 导入\nLOAD CSV WITH HEADERS  FROM \"file:///hudong_pedia.csv\" AS line  \nCREATE (p:HudongItem{title:line.title,image:line.image,detail:line.detail,url:line.url,openTypeList:line.openTypeList,baseInfoKeyList:line.baseInfoKeyList,baseInfoValueList:line.baseInfoValueList})\n</code></pre>\n\n<p>执行成功之后，控制台显示成功：</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/160bd4e0-a1ca-11e8-aa70-d319955d5dbe\" /></p>\n\n<p>上面这张图，表示数据加载成功，并显示加载的数据条数和耗费的时间。</p>\n\n<pre>\n<code>// 新增了hudong_pedia2.csv\nLOAD CSV WITH HEADERS  FROM \"file:///hudong_pedia2.csv\" AS line  \nCREATE (p:HudongItem{title:line.title,image:line.image,detail:line.detail,url:line.url,openTypeList:line.openTypeList,baseInfoKeyList:line.baseInfoKeyList,baseInfoValueList:line.baseInfoValueList})\n\n// 创建索引\nCREATE CONSTRAINT ON (c:HudongItem)\nASSERT c.title IS UNIQUE\n</code></pre>\n\n<p>以上命令的意思是，将&nbsp;<code>hudong_pedia.csv</code>&nbsp;和&nbsp;<code>hudong_pedia2.csv</code>&nbsp;导入 Neo4j 作为结点，然后对 titile 属性添加 UNIQUE（唯一约束/索引）。</p>\n\n<p><strong>注意：</strong>&nbsp;如果导入的时候出现 Neo4j JVM 内存溢出错误，可以在导入前，先把 Neo4j 下的&nbsp;<code>conf/neo4j.conf</code>&nbsp;中的&nbsp;<code>dbms.memory.heap.initial_size</code>&nbsp;和<code>dbms.memory.heap.max_size</code>&nbsp;调大点。导入完成后再把值改回去即可。</p>\n\n<p>下面继续执行数据导入命令：</p>\n\n<pre>\n<code>// 导入新的节点\nLOAD CSV WITH HEADERS FROM \"file:///new_node.csv\" AS line\nCREATE (:NewNode { title: line.title })\n\n//添加索引\nCREATE CONSTRAINT ON (c:NewNode)\nASSERT c.title IS UNIQUE\n\n//导入hudongItem和新加入节点之间的关系\nLOAD CSV  WITH HEADERS FROM \"file:///wikidata_relation2.csv\" AS line\nMATCH (entity1:HudongItem{title:line.HudongItem}) , (entity2:NewNode{title:line.NewNode})\nCREATE (entity1)-[:RELATION { type: line.relation }]-&gt;(entity2)\n\nLOAD CSV  WITH HEADERS FROM \"file:///wikidata_relation.csv\" AS line\nMATCH (entity1:HudongItem{title:line.HudongItem1}) , (entity2:HudongItem{title:line.HudongItem2})\nCREATE (entity1)-[:RELATION { type: line.relation }]-&gt;(entity2)\n</code></pre>\n\n<p>执行完这些命令后，我们导入&nbsp;<code>new_node.csv</code>&nbsp;新节点，并对 titile 属性添加 UNIQUE（唯一约束/索引），导入&nbsp;<code>wikidata_relation.csv</code>&nbsp;和<code>wikidata_relation2.csv</code>，并给节点之间创建关系。</p>\n\n<p>紧接着，继续导入实体属性，并创建实体之间的关系：</p>\n\n<pre>\n<code>LOAD CSV WITH HEADERS FROM \"file:///attributes.csv\" AS line\nMATCH (entity1:HudongItem{title:line.Entity}), (entity2:HudongItem{title:line.Attribute})\nCREATE (entity1)-[:RELATION { type: line.AttributeName }]-&gt;(entity2);\n\nLOAD CSV WITH HEADERS FROM \"file:///attributes.csv\" AS line\nMATCH (entity1:HudongItem{title:line.Entity}), (entity2:NewNode{title:line.Attribute})\nCREATE (entity1)-[:RELATION { type: line.AttributeName }]-&gt;(entity2);\n\nLOAD CSV WITH HEADERS FROM \"file:///attributes.csv\" AS line\nMATCH (entity1:NewNode{title:line.Entity}), (entity2:NewNode{title:line.Attribute})\nCREATE (entity1)-[:RELATION { type: line.AttributeName }]-&gt;(entity2);\n\nLOAD CSV WITH HEADERS FROM \"file:///attributes.csv\" AS line\nMATCH (entity1:NewNode{title:line.Entity}), (entity2:HudongItem{title:line.Attribute})\nCREATE (entity1)-[:RELATION { type: line.AttributeName }]-&gt;(entity2)  \n</code></pre>\n\n<p>这里注意，建索引的时候带了 label，因此只有使用 label 时才会使用索引，这里我们的实体有两个 label，所以一共做&nbsp;<code>2*2=4</code>&nbsp;次。当然也可以建立全局索引，即对于不同的 label 使用同一个索引。</p>\n\n<p>以上过程，我们就完成了语料加载，并创建了实体之间的关系和属性匹配，下面我们来看看 Neo4j 图谱关系展示。</p>\n\n<h4>知识图谱查询展示</h4>\n\n<p>最后通过 cypher 语句查询来看看农业图谱展示。</p>\n\n<p>首先，展示 HudongItem 实体，执行如下命令：</p>\n\n<pre>\n<code>MATCH (n:HudongItem) RETURN n LIMIT 25\n</code></pre>\n\n<p>对 HudongItem 实体进行查询，返回结果的25条数据，结果如下图：</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/2608f910-a1cd-11e8-a041-3fe38238552b\" /></p>\n\n<p>接着，展示 NewNode 实体，执行如下命令：</p>\n\n<pre>\n<code>MATCH (n:NewNode) RETURN n LIMIT 25\n</code></pre>\n\n<p>对 NewNode 实体进行查询，返回结果的25条数据，结果如下图：</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/2ee13980-a1cd-11e8-aeb1-374a1fc4569b\" /></p>\n\n<p>之后，展示 RELATION 直接的关系，执行如下命令：</p>\n\n<pre>\n<code>MATCH p=()-[r:RELATION]-&gt;() RETURN p LIMIT 25\n</code></pre>\n\n<p>展示实体属性关系，结果如下图：</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/36cbbe90-a1cd-11e8-aa70-d319955d5dbe\" /></p>\n\n<h3>总结</h3>\n\n<p>本节内容到此结束，回顾下整篇文章，主要讲了以下内容：</p>\n\n<ol>\n	<li>解释了 Neo4j 被用来做知识图谱的原因；</li>\n	<li>Neo4j 的简单安装以及在 Neo4j 浏览器中创建节点和关系；</li>\n	<li>Neo4j 的 Python 接口操作及使用；</li>\n	<li>从五个方面讲解了如何使用 Neo4j 构建一个简单的农业知识图谱。</li>\n</ol>\n\n<p>最后，强调一句，知识图谱未来会通过自然语言处理技术和搜索技术结合应用会越来越广，工业界所出的地位也会越来越重要。</p>\n\n<p><strong>参考文献及推荐阅读</strong></p>\n\n<ol>\n	<li><a href=\"https://neo4j.com/download/other-releases/#releases\">Neo4j 官网</a></li>\n	<li><a href=\"https://neo4j.com/docs/operations-manual/3.2/\">The Neo4j Developer Manual</a></li>\n	<li><a href=\"https://www.w3cschool.cn/neo4j/\">Neo4j 教程</a></li>\n	<li><a href=\"https://www.jianshu.com/p/a2497a33390f\">Py2neo&mdash;&mdash;Neo4j &amp; Python 的配合使用</a></li>\n</ol>\n\n<h2><a id=\"第21课：中文自然语言处理的应用、现状和未来\" name=\"第21课：中文自然语言处理的应用、现状和未来\"></a>第21课：中文自然语言处理的应用、现状和未来</h2>\n\n<p>自然语言理解和自然语言生成是自然语言处理的两大内核，机器翻译是自然语言理解方面最早的研究工作。自然语言处理的主要任务是：研究表示语言能力和语言应用的模型，建立和实现计算框架并提出相应的方法不断地完善模型，根据这样的语言模型设计有效地实现自然语言通信的计算机系统，并研讨关于系统的评测技术，最终实现用自然语言与计算机进行通信。目前，具有一定自然语言处理能力的典型应用包括计算机信息检索系统、多语种翻译系统等。</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/610b88d0-9fab-11e8-9539-994b9a0319a0\" /></p>\n\n<p>微软创始人比尔&middot;盖茨曾经表示，&ldquo;语言理解是人工智能领域皇冠上的明珠&rdquo;。</p>\n\n<p>语言是逻辑思维和交流的工具，宇宙万物中，只有人类才具有这种高级功能。要实现人与计算机间采用自然语言通信，必须使计算机同时具备自然语言理解和自然语言生成两大功能。</p>\n\n<p>因此，NLP 作为人工智能的一个子领域，其主要目的就包括两个方面：自然语言理解，让计算机理解自然语言文本的意义；自然语言生成，让计算机能以自然语言文本来表达给定的意图、思想等。自然语言是人类智慧的结晶，自然语言处理是人工智能中最为困难的问题之一，而对自然语言处理的研究也是充满魅力和挑战的。</p>\n\n<h3>NLP 领域发展现状如何？</h3>\n\n<p>近年来，自然语言处理处于快速发展阶段。各种词表、语义语法词典、语料库等数据资源的日益丰富，词语切分、词性标注、句法分析等技术的快速进步，各种新理论、新方法、新模型的出现推动了自然语言处理研究的繁荣。互联网与移动互联网和世界经济社会一体化的潮流对自然语言处理技术的迫切需求，为自然语言处理研究发展提供了强大的市场动力。</p>\n\n<p>我国直到上世纪80年代中期才开始较大规模和较系统的自然语言处理研究，尽管较国际水平尚有较大差距，但已经有了比较稳定的研究内容，包括语料库、知识库等数据资源建设，词语切分、句法分析等基础技术，以及信息检索、机器翻译等应用技术。</p>\n\n<p>当前国内外出现了一批基于 NLP 技术的应用系统，例如 IBM 的 Watson 在电视问答节目中战胜人类冠军；苹果公司的 Siri 个人助理被大众广为测试；谷歌、微软、百度等公司纷纷发布个人智能助理；科大讯飞牵头研发高考机器人&hellip;&hellip;但相比于性能趋于饱和的计算机视觉和语音识别技术，自然语言处理因技术难度太大、应用场景太复杂，研究成果还未达到足够的高度。</p>\n\n<h3>自然语言处理中句子级分析技术</h3>\n\n<p>目前，自然语言处理的对象有词、句子、篇章和段落、文本等，但是大多归根到底在句子的处理上，自然语言处理中的自然语言句子级分析技术，可以大致分为词法分析、句法分析、语义分析三个层面。</p>\n\n<p>第一层面的词法分析包括汉语分词和词性标注两部分。和大部分西方语言不同，汉语书面语词语之间没有明显的空格标记，文本中的句子以字串的形式出现。因此汉语自然语言处理的首要工作就是要将输入的字串切分为单独的词语，然后在此基础上进行其他更高级的分析，这一步骤称为分词。</p>\n\n<p>除了分词，词性标注也通常认为是词法分析的一部分。给定一个切好词的句子，词性标注的目的是为每一个词赋予一个类别，这个类别称为词性标记，比如，名词（Noun）、动词（Verb）、形容词（Adjective）等。一般来说，属于相同词性的词，在句法中承担类似的角色。</p>\n\n<p>第二个层面的句法分析是对输入的文本句子进行分析以得到句子的句法结构的处理过程。对句法结构进行分析，一方面是语言理解的自身需求，句法分析是语言理解的重要一环，另一方面也为其它自然语言处理任务提供支持。例如句法驱动的统计机器翻译需要对源语言或目标语言（或者同时两种语言）进行句法分析；语义分析通常以句法分析的输出结果作为输入以便获得更多的指示信息。</p>\n\n<p>根据句法结构表示形式的不同，最常见的句法分析任务可以分为以下三种：</p>\n\n<ol>\n	<li>短语结构句法分析，该任务也被称作成分句法分析，作用是识别出句子中的短语结构以及短语之间的层次句法关系；</li>\n	<li>依存句法分析，作用是识别句子中词汇与词汇之间的相互依存关系；</li>\n	<li>深层文法句法分析，即利用深层文法，例如词汇化树邻接文法、词汇功能文法、组合范畴文法等，对句子进行深层的句法以及语义分析。</li>\n</ol>\n\n<p>上述几种句法分析任务比较而言，依存句法分析属于浅层句法分析。其实现过程相对简单，比较适合在多语言环境下的应用，但是依存句法分析所能提供的信息也相对较少。深层文法句法分析可以提供丰富的句法和语义信息，但是采用的文法相对复杂，分析器的运行复杂度也较高，这使得深层句法分析当前不适合处理大规模数据。短语结构句法分析介于依存句法分析和深层文法句法分析之间。</p>\n\n<p>第三个层面是语义分析。语义分析的最终目的是理解句子表达的真实语义。但是，语义应该采用什么表示形式一直困扰着研究者们，至今这个问题也没有一个统一的答案。</p>\n\n<p>语义角色标注是目前比较成熟的浅层语义分析技术。基于逻辑表达的语义分析也得到学术界的长期关注。出于机器学习模型复杂度、效率的考虑，自然语言处理系统通常采用级联的方式，即分词、词性标注、句法分析、语义分析分别训练模型。实际使用时，给定输入句子，逐一使用各个模块进行分析，最终得到所有结果。</p>\n\n<h3>深度学习背景下的自然语言处理</h3>\n\n<p>近年来，随着研究工作的深入，研究者们开始从传统机器学习转向深度学习。2006年开始，有人利用深层神经网络在大规模无标注语料上无监督的为每个词学到了一个分布式表示，形式上把每个单词表示成一个固定维数的向量，当作词的底层特征。在此特征基础上，完成了词性标注、命名实体识别和语义角色标注等多个任务，后来有人利用递归神经网络完成了句法分析、情感分析和句子表示等多个任务，这也为语言表示提供了新的思路。</p>\n\n<p>面向自然语言处理的深度学习研究工作，目前尚处于起步阶段，尽管已有的深度学习算法模型如循环神经网络、递归神经网络和卷积神经网络等已经有较为显著的应用，但还没有重大突破。围绕适合自然语言处理领域的深度学习模型构建等研究应该有着非常广阔的空间。</p>\n\n<p>在当前已有的深度学习模型研究中，难点是在模型构建过程中参数的优化调整方面。主要有深度网络层数、正则化问题及网络学习速率等，可能的解决方案比如有采用多核机提升网络训练速度，针对不同应用场合，选择合适的优化算法等。</p>\n\n<h3>自然语言处理未来的研究方向</h3>\n\n<p>纵观自然语言处理技术研究发展的态势和现状，以下研究方向或问题将可能成为自然语言处理未来研究必须攻克的堡垒：</p>\n\n<p><img alt=\"enter image description here\" src=\"https://images.gitbook.cn/0f2c36e0-9fab-11e8-9539-994b9a0319a0\" /></p>\n\n<ol>\n	<li>\n	<p>词法和句法分析方面：包括多粒度分词、新词发现、词性标注等；</p>\n	</li>\n	<li>\n	<p>语义分析方面：包括词义消歧、非规范文本的语义分析。其中，非规范划化文本主要指社交平台上比较口语化、弱规范甚至不规范的短文本，因其数据量巨大和实时性而具有研究和应用价值，被广泛用于舆情监控、情感分析和突发事件发现等任务；</p>\n	</li>\n	<li>\n	<p>语言认知模型方面：比如使用深度神经网络处理自然语言，建立更有效、可解释的语言计算模型，例如，词嵌入的发现。还有目前词的表示是通过大量的语料库学习得到的，如何通过基于少量样本来发现新词、低频词也急需探索；</p>\n	</li>\n	<li>\n	<p>知识图谱方面：如何构建能够融合符号逻辑和表示学习的大规模高精度的知识图谱；</p>\n	</li>\n	<li>\n	<p>文本分类与聚类方面：通过有监督、半监督和无监督学习，能够准确进行分类和聚类。当下大多数语料都是没有标签的，未来在无监督或者半监督方面更有需求；</p>\n	</li>\n	<li>\n	<p>信息抽取方面：对于多源异构信息，如何准确进行关系、事件的抽取等。信息抽取主要从面向开放域的可扩展信息抽取技术、自学习与自适应和自演化的信息抽取系统以及面向多源异构数据的信息融合技术方向发展；</p>\n	</li>\n	<li>\n	<p>情感分析方面：包括基于上下文感知的情感分析、跨领域跨语言情感分析、基于深度学习的端到端情感分析、情感解释、反讽分析、立场分析等；</p>\n	</li>\n	<li>\n	<p>自动文摘方面：如何表达要点信息？如何评估信息单元的重要性？这些都要随着语义分析、篇章理解、深度学习等技术快速发展；</p>\n	</li>\n	<li>\n	<p>信息检索方面：包括意图搜索、语义搜索等，都将有可能出现在各种场景的垂直领域，将以知识化推理为检索运行方式，以自然语言多媒体交互为手段的智能化搜索与推荐技术；</p>\n	</li>\n	<li>\n	<p>自动问答方面：包括深度推理问答、多轮问答等各种形式的自动问答系统；</p>\n	</li>\n	<li>\n	<p>机器翻译方面：包括面向小数据的机器翻译、非规范文本的机器翻译和篇章级机器翻译等。</p>\n	</li>\n</ol>\n\n<h3>总结</h3>\n\n<p>本文，从 NLP 的概念出发，首先指出了自然语言处理的两大内核：自然语言理解和自然语言生成；然后简单介绍了国内外 NLP 研究发展现状；紧接着重点介绍了最常用、应用最广的自然语言处理中句子级分析技术，最后在深度学习背景下，指出了自然语言处理未来可能遇到的挑战和重点研究方向，为后期的学习提供指导和帮助。</p>\n\n<h3>关于本课程结束寄语</h3>\n\n<p>首先，非常感谢 GitChat 给我们提供这样一个学习平台，非常感谢编辑老师的辛苦指导，非常感谢各位同学能够报名参加本课程。</p>\n\n<p>近年来，深度学习正在逐渐的填平领域鸿沟，继在图像和语音领域取得的巨大成功后，深度学习在同属人类认知范畴的自然语言处理方面也在不断取得更好的效果。</p>\n\n<p>在《中文自然语言处理入门实战》达人课中，相信各位同学通过一些小数据量的&ldquo;简易版&rdquo;实例，已经体会到了中文自然语言处理的精妙，并完成了中文自然语言处理从0到1的过程。但如何更好地把这些技术应用在工业生产中，不仅需要过硬的技术更要求熟练掌握相关核心算法的原理，做到知其然并知其所以然。</p>\n\n<p>因此，我计划在《中文自然语言处理入门实战》课程的基础上，推出《中文自然语言处理核心算法精要剖析》课程，本课程共包含28节，包含了中文自然语言处理核心算法精要，有针对性的面向想要深入学习中文自然语言处理和想要从事 NLP 相关工作的人，重点学习这些核心算法的原理，注重理论与实战结合，助力在中文自然语言处理上理解的更深、做的更好。</p>\n\n<p>对《中文自然语言处理核心算法精要剖析》感兴趣的同学，请继续关注我，课程后期将尽快上线。</p>\n\n<p><strong>参考以及推荐阅读</strong></p>\n\n<ul>\n	<li><a href=\"http://36kr.com/p/5096134.html\">今日头条李航：深度学习 NLP 的现有优势与未来挑战</a></li>\n</ul>', 1, '5f199b8885e24fc8b28672b872edb606', 20, '2018-09-05 11:05:57', '2018-09-14 11:06:46');
INSERT INTO `logcontent` VALUES ('69b20ab181d941fa966cc590d497adca', 1, '4eca926aa543420baea2f03f506d042e', '高性能Mysql笔记二', '高性能Mysql（第三版）,Schema与数据类型优化', '高性能Mysql（第三版）--Schema与数据类型优化', '<h2><a id=\"选择优化的数据类型\" name=\"选择优化的数据类型\"></a>选择优化的数据类型</h2>\n\n<pre>\n<code class=\"language-markdown\">1、一般情况下，应该尽量使用可以正确存储数据的最小数据类型。更小的数据类型通常更快，因为它们占用更少的磁盘、内存和CPU缓存，并且处理时需要的CPU周期也更少。但是要确保没有低估需要存储的值的范围，因为在schema中增加数据类型的范围非常耗时。\n\n2、简单数据类型的操作通常需要更少的CPU周期。例如，整型比字符操作代价更低，因为字符集和校对规则（排序规则）使字符比较比整型比较更复杂。\n\n3、尽量使用相同的数据类型存储相似或相关的值，尤其是要在关联条件中使用的列。\n\n4、尽量避免NULL（除InnoDB）（性能提升较小）：\n如果查询中包含可为NULL的列，对MySQL来说更难优化，为NULL的列使得索引、索引统计和值比较更复杂，使用更多的存储空间，还需要特殊处理。\n为NULL的列被索引时，每个索引记录需要一个额外的字节，在MyISAM里甚至还可能导致固定大小的索引变成可变大小的索引。InnoDB使用单独的位（bit）存储NULL值，所以对于稀疏数据有很好的空间效率。除非真实需要，否则应避免使用NULL值。</code></pre>\n\n<pre>\n<code>整数类型：\nTINYINT，SMALLINT，MEDIUMINT，INT，BIGINT。分别使用8，16，24，32，64位存储空间。它们可以存储的值的范围从−2（N−1）到2（N−1）−1，其中N是存储空间的位数。\n整数类型的UNSIGNED属性，表示不允许负值，占用相同的存储空间使正数的上限提高一倍，并具有相同的性能。\n类型选择决定了在内存和磁盘如何保存数据，整数计算一般使用BIGINT（一些聚合函数是例外，可能使用DECIMAL/DOUBLE进行计算）。\n对整数类型指定宽度是没有意义的，如int(10)和int(1)，它不会限制值的合法范围。\n\n实数类型：\n实数是带有小数部分的数字，但不只是为了存储小数部分，也可以使用DECIMAL存储比BIGINT还大的整数。MySQL既支持精确类型，也支持不精确类型。\nFLOAT和DOUBLE类型支持使用标准的浮点运算进行近似计算。DECIMAL类型用于存储精确的小数。Version(5.0上)DECIMAL类型支持精确计算（CPU不支持DECIMAL的直接计算，会转换为DOUBLE类型。效率低于原生浮点运算）。\n指定浮点列所需要的精度，会隐式转换为其他数据类型，损失精度，建议只指定数据类型，不指定精度。\n\n字符串类型：\nVARCHAR类型用于存储可变长字符串。\n它比定长类型更节省空间（使用ROW_FORMAT=FIXED创建除外，不推荐），使用VARCHAR(1)和VARCHAR(10)存储字符串的空间开销是一样的，但更长的列会消耗更多的内存，因为MySQL通常会分配固定大小的内存块来保存内部值。尤其使用内存临时表或磁盘临时表进行排序或操作时会特别糟糕。VARCHAR需要使用1或2个额外字节记录字符串的长度：如果列的最大长度小于等于255字节，则只使用1个字节表示，否则使用2个字节。\n由于行是变长的，在UPDATE时，如果一个行占用的空间增长大于页内剩余空间，不同存储引擎的处理方式不同，MyISAM会将行拆成不同的片段存储，InnoDB则需要分裂页来使行可以放进页内。其他一些存储引擎也许不在原数据位置更新数据。\n存储和检索时不去除末尾空格。\nInnoDB可以把过长的VARCHAR存储为BLOB。\n\nCHAR类型是定长的：根据定义的字符串长度分配足够的空间。\n适合存储很短的字符串，或者所有值都接近同一个长度。\n无记录字节，所以某些情况下CHAR比VARCHAR在存储空间上更有效率。\n存储时去除所有的末尾空格。\n\nBLOB和TEXT都是为存储很大的数据而设计的字符串数据类型，分别采用二进制和字符方式存储。\n当BLOB和TEXT值太大时，存储引擎在存储时通常会做特殊处理。InnoDB会使用专门的“外部”存储区域来进行存储，此时每个值在行内需要1～4个字节存储一个指针，然后在外部存储区域存储实际的值。\nBLOB类型存储的是二进制数据，无排序规则或字符集，而TEXT类型有字符集和排序规则。MySQL对BLOB和TEXT列进行排序与其他类型是不同的：它只对每个列的最前max_sort_length字节而不是整个字符串做排序。如果只需要排序前面一小部分字符，则可以减小max_sort_length的配置，或者使用ORDER BY SUSTRING（column，length）。\nMySQL不能将BLOB和TEXT列全部长度的字符串进行索引，也不能使用这些索引消除排序。\n\n磁盘临时表和文件排序，如果查询使用了BLOB或TEXT列并且需要使用隐式临时表，将使用MyISAM磁盘临时表，导致严重的性能开销。尽量避免使用BLOB和TEXT类型。如果实在无法避免，可以在所有用到BLOB字段的地方都使用SUBSTRING将列值转换为字符串（排序也适用），确保截取后字符串足够短，使临时表大小小于max_heap_table_size或tmp_table_size，便可使用内存临时表了，超过以后MySQL会将内存临时表转换为MyISAM磁盘临时表。如果EXPLAIN执行计划的Extra列包含”Using temporary”，则说明这个查询使用了隐式临时表。\n\n枚举，有时可以使用枚举列代替常用的字符串类型。\n枚举列可以把一些不重复的字符串存储成一个预定义的集合。MySQL在存储枚举时非常紧凑，会根据列表值的数量压缩到一个或者两个字节中。MySQL在内部会将每个值在列表中的位置保存为整数，并且在表的.frm文件中保存“数字-字符串”映射关系的“查找表”。\n枚举字段是按照内部存储的整数而不是定义的字符串进行排序的。\n特定情况下，把CHAR/VARCHAR列与枚举列进行关联可能会比直接关联CHAR/VARCHAR列更慢（使用空间和查询速度衡量）。\n枚举的缺点是字符串列表固定，添加或删除字符串必须使用ALTER TABLE。对于可能会改变的字符串，使用枚举不是一个好主意，除非能接受只在列表末尾添加元素。\n\n日期和时间类型：MySQL能存储的最小时间粒度为秒。\nDATETIME类型能保存大范围的值，从1001年到9999年，精度为秒。它把日期和时间封装到格式为YYYYMMDDHHMMSS的整数中，与时区无关。使用8个字节的存储空间。默认情况下，MySQL以一种可排序的、无歧义的格式显示DATETIME值。\nTIMETAMP类型保存了从1970年1月1日午夜（格林尼治标准时间）以来的秒数，它和UNIX时间戳相同，只使用4个字节的存储空间，因此它的范围比DATETIME小得多：只能表示从1970年到2038年，TIMESTAMP显示的值依赖于时区，MySQL服务器、操作系统，以及客户端连接都有时区设置。默认为NOT NULL。\nMySQL提供了FROM_UNIXTIME()函数把Unix时间戳转换为日期，并提供了UNIX_TIMESTAMP()函数把日期转换为Unix时间戳。\n在插入一行记录时，MySQL默认更新第一个TIMESTAMP列的值（除非明确指定列），可以配置任何TIMESTAMP列的插入和更新行为。\n\n选择标识符：为标识列选择合适的数据类型非常重要。一般来说更有可能用标识列与其他值进行比较（例如，在关联操作中），或者通过标识列寻找其他列。标识列也可能作为外键使用，所以为标识列选择数据类型时，应该选择跟关联表中对应列相同的类型，混用不同数据类型可能导致性能问题或在比较操作时隐式类型转换导致难以发现的错误。\n整数通常是标识列最好的选择，因为它们很快并且可以使用AUTO_INCREMENT。慎重选择字符串类型，很消耗空间且通常比数字类型慢。尤其是在MyISAM表里使用字符串作为标识列时要特别小心。MyISAM默认对字符串使用压缩索引，这会导致查询慢得多，对于完全“随机”的字符串也需要多加注意，例如MD5()、SHA1()或者UUID()产生的字符串。插入值会随机地写到索引的不同位置，所以使得INSERT变慢，导致页分裂、磁盘随机访问，以及对于聚簇存储引擎产生聚簇索引碎片。SELECT会变慢，因为逻辑上相邻的行会分布在磁盘和内存的不同地方。随机值导致缓存对所有类型的查询语句效果都很差，因为会使得缓存赖以工作的访问局部性原理失效。如果整个数据集都一样的“热”，那么缓存任何一部分特定数据到内存都没有好处；如果工作集比内存大，缓存将会有很多刷新和不命中。如果存储UUID值，应该移除“-”符号；更好的做法是，用UNHEX()函数转换UUID值为16字节的数字，并且存储在一个BINARY（16）列中。检索时可以通过HEX()函数格式化为十六进制格式。UUID()生成的值与加密散列函数例如SHA1()生成的值有不同的特征：UUID值虽然分布也不均匀，但还是有一定顺序的。尽管如此，但还是不如递增的整数好用。不推荐选择EMUM和SET类型。</code></pre>\n\n<h2><a id=\"MySQL schema设计中的陷阱\" name=\"MySQL schema设计中的陷阱\"></a>MySQL schema设计中的陷阱</h2>\n\n<pre>\n<code>合理设计schema：不好的schema迁移程序、程序自动生成的schema、对象关系映射（ORM）系统等都可能会导致严重的性能问题，有些程序存储默认使用很大的VARCHAR列，或者对需要在关联时比较的列使用不同的数据类型，导致严重的性能问题。\n1、太多列：MySQL的存储引擎API工作时需要在服务器层和存储引擎层之间通过行缓冲格式拷贝数据，然后在服务器层将缓冲内容解码\n   成各个列。从行缓冲中将编码过的列转换成行数据结构的操作代价是非常高的。MyISAM的定长行结构实际上与服务器层的行结构正好\n   匹配，所以不需要转换。然而，MyISAM的变长行结构和InnoDB的行结构则总是需要转换。转换的代价依赖于列的数量。\n2、太多关联：MySQL限制了每个关联操作最多只能有61张表\n3、滥用枚举：在MySQL中，当需要在枚举列表中增加一个新值时就要做一次ALTER TABLE操作。\n   Version(5.0↓)ALTER TABLE是一种阻塞操作；Version(5.0↑)如果不是在列表的末尾增加值也会一样需要ALTER TABLE\n4、变相的枚举：枚举列允许在列中存储一组定义值中的单个值，集合列则允许在列中存储一组定义值中的一个或多个\n   值。有时候这可能比较容易导致混乱。</code></pre>\n\n<h2><a id=\"范式和反范式\" name=\"范式和反范式\"></a>范式和反范式</h2>\n\n<pre>\n<code>范式：\n第一范式:确保每列的原子性(每列都是不可再分的最小数据单元)\n例如:顾客表(姓名、编号、地址、……)其中\"地址\"列还可以细分为国家、省、市、区等。\n第二范式:基于第一范式,确保表中的每列都和主键相关\n例如:订单表(订单编号、产品编号、定购日期、价格)，订单编号为主键，产品编号和主键列没有直接的关系，应删除该列。\n第三范式:更进一层,目标是确保每列都和主键列直接相关,而不是间接相关\n例如:订单表(订单编号，定购日期，顾客编号，顾客姓名)，顾客姓名&gt;顾客编号&gt;订单编号，经过了传递依赖，应去掉\"顾客姓名\"列。\n\n范式的优点：\n1、更新操作通常比反范式化要快。\n2、只有很少或者没有重复数据，所以只需要修改更少的数据。\n3、范式化的表通常更小，可以更好地放在内存里，所以执行操作会更快。\n4、很少有多余的数据意味着检索列表数据时更少需要DISTINCT或者GROUP BY语句。\n\n范式的缺点：\n稍微复杂一些的查询语句在符合范式的schema上都可能需要至少一次关联。这不但代价昂贵，也可能使一些索引策略无效。例如，范式化可能将列存放在不同的表中，而这些列如果在一个表中本可以属于同一个索引\n\n混用范式化：\n范式化和反范式化的schema各有优劣，范式是好的，但是反范式（大多数情况下意味着重复数据）有时也是必需的，并且能带来好处。</code></pre>\n\n<h2>&nbsp;</h2>', 1, '5f199b8885e24fc8b28672b872edb606', 7, '2018-08-31 17:17:55', '2018-09-21 20:11:20');
INSERT INTO `logcontent` VALUES ('6dfce85938b8480f968ac137f7801b25', -1, '7338e53acd514defa1a17e47016f3f4a', 'Elasticsearch（二）', 'Near Realtime（NRT）近实时,Cluster集群,Node节点,Document&field文档,Index索引,Type类型,shardreplica定义,优势', 'Elasticsearch核心概念：NRT、索引、分片、副本等', '<p><a name=\"核心概念\"></a>1、elasticsearch的核心概念</p>\n\n<p>（1）near realtime（NRT）：近实时，两个意思，从写入数据到数据可以被搜索到的时间；基于es执行搜索和分析的时间</p>\n\n<p>（2）cluster：集群，包含多个节点，每个节点属于哪个集群通过配置（集群名称，默认是elasticsearch）决定的</p>\n\n<p>（3）node：节点，只要cluster.name（集群名）设置一致，并且节点在同一网段下，启动的es会自动发现对方，组成集群</p>\n\n<p>（4）index：索引，包含一堆有相似结构的文档数据，简单举例比如一个商品index，其中可能包含蔬菜type，水果type，其中蔬菜type中可能包括黄瓜document</p>\n\n<p>（5）type：类型，每个索引里都可以有一个或多个type，type是index中的一个逻辑数据分类，一个type下的document，都有相同的field</p>\n\n<p>（6）document&amp;field：文档，es中的最小数据单元，一个document可以是一条数据，通常用JSON数据结构表示，每个index下的type中，都可以去存储多个document。一个document里面有多个field，每个field就是一个数据字段。</p>\n\n<p>（7）shard：单台机器无法存储大量数据，es可以将一个索引中的数据切分为多个shard，分布在多台服务器上存储。shard可以横向扩展，存储更多数据，让搜索和分析等操作分布到多台服务器上去执行，提升吞吐量和性能，每个shard就是一个lucune实例，完整的建立索引和处理请求的能力</p>\n\n<p>（8）replica：如果一个服务器故障或宕机，此时shard可能就会丢失，因此可以为每个shard创建多个replica副本。replica可以在shard故障时提供备用服务，保证数据不丢失，多个replica还可以提升搜索操作的吞吐量和性能。primary shard（建立索引时一次设置，不能修改，默认5个），replica shard（随时修改数量，默认1个），默认每个索引5个primary shard，5个replica shard（每个primary shard一个replica shard），最小的高可用配置，是2台服务器。</p>\n\n<p>&nbsp;</p>\n\n<p><a name=\"Elasticsearch与数据库对应\"></a>2、Elasticsearch与数据库对应（近似）</p>\n\n<pre>\n<code class=\"language-markdown\">Elasticsearch            数据库\n-----------------------------------------\nindex                      库\ntype                       表\ndocument                   行\nfield                      字段</code></pre>\n\n<p><a name=\"shard定义\"></a>3、shard全称primary shard，replica全称replica shard；</p>\n\n<p>primary shard和replica shard不能在同一台机器上（为了容错）</p>\n\n<p>index会被拆分为多个shard，每个shard只存放一部分数据，这些shard会散落到多台服务器上</p>\n\n<p><a name=\"优势\"></a>4、优势:</p>\n\n<p>性能提升：数据分布在多个shard，多台服务器上，所有的操作，都会在多台服务器上并行分布式执行，提升吞吐量和性能</p>\n\n<p>高可用性：如果某个节点宕机，部分数据丢失，请求会直接发送至replica节点上去，服务继续提供</p>\n', 2, '5f199b8885e24fc8b28672b872edb606', 18, '2017-07-12 18:03:42', '2018-09-06 16:12:58');
INSERT INTO `logcontent` VALUES ('722ecc63c4664202a73229e42ca6eba7', -1, 'd7caeba238f0466d87db109b2b9724da', '回归中的相关度和R平方值', 'Pearson Correlation Coefficient', '初学', '<p><strong><a id=\"皮尔逊相关系数 (Pearson Correlation Coefficient)\" name=\"皮尔逊相关系数 (Pearson Correlation Coefficient)\"></a>皮尔逊相关系数 (Pearson Correlation Coefficient):&nbsp;</strong></p>\n\n<p>衡量两个值线性相关强度的量<br />\n取值范围 [-1, 1]:正向相关: &gt;0, 负向相关：&lt;0, 无相关性：=0<br />\n<img src=\"http://localhost:8080/myLog/images/PearsonCorrelationCoefficient/1.png\" style=\"height:40px; width:266px\" /></p>\n\n<p><strong><a id=\"计算方法举例\" name=\"计算方法举例\"></a>计算方法举例：</strong></p>\n\n<table border=\"1\" cellspacing=\"0\" style=\"border-collapse:collapse; border:0.5pt solid #000000; height:171px; width:600pt\">\n	<tbody>\n		<tr>\n			<td style=\"background-color:#d0cece; height:14.25pt; text-align:center; vertical-align:middle; width:154.50pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">X</span></span></span></td>\n			<td style=\"background-color:#d0cece; height:14.25pt; text-align:center; vertical-align:middle; width:96.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">Y</span></span></span></td>\n			<td style=\"background-color:#d0cece; height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">X_bar（平均值）</span></span></span></td>\n			<td style=\"background-color:#d0cece; height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">Y_bar（平均值）</span></span></span></td>\n			<td style=\"background-color:#d0cece; height:14.25pt; text-align:center; vertical-align:middle; width:104.25pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">X-X_bar</span></span></span></td>\n			<td style=\"background-color:#d0cece; height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">Y-Y_bar</span></span></span></td>\n			<td style=\"background-color:#d0cece; height:14.25pt; text-align:center; vertical-align:middle; width:138.75pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">(X-X_bar)(Y-Y_bar)</span></span></span></td>\n			<td style=\"background-color:#d0cece; height:14.25pt; text-align:center; vertical-align:middle; width:96.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">(X-X_bar)^2</span></span></span></td>\n			<td style=\"background-color:#d0cece; height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">(Y-Y_bar)^2</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:154.50pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">1</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:96.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">10</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">9.333333333</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">33.66666667</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:104.25pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">-8.333333333</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">-23.66666667</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:138.75pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">197.2222222</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:96.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">69.44444444</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">560.1111111</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:154.50pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">3</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:96.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">12</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">9.333333333</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">33.66666667</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:104.25pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">-6.333333333</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">-21.66666667</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:138.75pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">137.2222222</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:96.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">40.11111111</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">469.4444444</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:154.50pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">8</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:96.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">24</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">9.333333333</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">33.66666667</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:104.25pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">-1.333333333</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">-9.666666667</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:138.75pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">12.88888889</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:96.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">1.777777778</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">93.44444444</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:154.50pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">7</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:96.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">21</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">9.333333333</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">33.66666667</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:104.25pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">-2.333333333</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">-12.66666667</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:138.75pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">29.55555556</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:96.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">5.444444444</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">160.4444444</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:154.50pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">9</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:96.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">34</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">9.333333333</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">33.66666667</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:104.25pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">-0.333333333</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">0.333333333</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:138.75pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">-0.111111111</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:96.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">0.111111111</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">0.111111111</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:154.50pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">28</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:96.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">101</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">9.333333333</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">33.66666667</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:104.25pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">18.66666667</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">67.33333333</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:138.75pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">1256.888889</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:96.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">348.4444444</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">4533.777778</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:154.50pt\">&nbsp;</td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:96.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">sum</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">56</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">202</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:104.25pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">0</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">0</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:138.75pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">1633.666667</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:96.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">465.3333333</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">5817.333333</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:14.25pt; text-align:general; vertical-align:middle; width:154.50pt\">&nbsp;</td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:96.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">&rho;</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\">0.992931735</td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\">&nbsp;</td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:104.25pt\">&nbsp;</td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\">&nbsp;</td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:138.75pt\">&nbsp;</td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:96.00pt\">&nbsp;</td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\">&nbsp;</td>\n		</tr>\n	</tbody>\n</table>\n\n<p><strong><a id=\"R平方值\" name=\"R平方值\"></a>R平方值:</strong><br />\n定义：决定系数，反应因变量的全部变异能通过回归关系被自变量解释的比例。<br />\n描述：如R平方为0.8，则表示回归关系可以解释因变量80%的变异。换句话说，如果我们能控制自变量不变，则因变量的变异程度会减少80%<br />\n简单线性回归：R^2 = r * r<br />\n多元线性回归：<sub><img src=\"http://localhost:8080/myLog/images/PearsonCorrelationCoefficient/2.jpg\" style=\"height:45px; width:119px\" /></sub><br />\n<sub><img src=\"http://localhost:8080/myLog/images/PearsonCorrelationCoefficient/3.png\" style=\"height:35px; width:376px\" /></sub><br />\n其中SS<sub>R</sub>为模型可以解释的值，SS<sub>E</sub>为误差值，关系为SS<sub>T</sub> = SS<sub>R</sub>+SS<sub>E</sub></p>\n\n<p>R平方也有其局限性：R平方随着自变量的增加会变大，R平方和样本量是有关系的。因此，我们要到R平方进行修正。修正的方法：<br />\n<sub><img src=\"http://localhost:8080/myLog/images/PearsonCorrelationCoefficient/4.png\" style=\"height:35px; width:178px\" /></sub><br />\nN:样本量<br />\nP:要预测的值量</p>\n', 1, '5f199b8885e24fc8b28672b872edb606', 4, '2017-11-27 21:38:06', '2018-08-31 22:21:23');
INSERT INTO `logcontent` VALUES ('731f75e371364871b7dddbea5bb773a8', -1, '716b0d3ea6e04d03a9deb097be1e2cf1', '全世界聪明人都在学的70种思维', '逻辑思维,形象思维,质疑思维,移植思维,发散思维,逆向思维,平面思维', '《全世界聪明人都在学的70种思维》', '<p><a id=\"逻辑思维\" name=\"逻辑思维\"></a><span style=\"font-size:14px\"><strong>逻辑思维:精确把握事情的本质</strong></span></p>\n\n<pre>\n<code>表象和本质是事物的两个方面，事物的表象既可以真实地反映事物的本质，也可以掩盖甚至曲解事物的本质，所以我们要透过事物的表象认识事物的本质。逻辑思维正是一种透过现象看本质，把握事物的本质特征的抽象思维方式。</code></pre>\n\n<p><a id=\"形象思维\" name=\"形象思维\"></a><span style=\"font-size:14px\"><strong>形象思维:最常用的认知思维工具</strong></span></p>\n\n<pre>\n<code>反映和认识世界的重要思维形式</code></pre>\n\n<p><a id=\"质疑思维\" name=\"质疑思维\"></a><span style=\"font-size:14px\"><strong>质疑思维:尽信书不如无书</strong></span></p>\n\n<p><a id=\"移植思维\" name=\"移植思维\"></a><strong><span style=\"font-size:14px\">移植思维:用联想将创意跨界</span></strong></p>\n\n<p><a id=\"发散思维\" name=\"发散思维\"></a><span style=\"font-size:14px\"><strong>发散思维:让自己来场头脑风暴</strong></span></p>\n\n<pre>\n<code>一位妈妈从市场买回一条活鱼，女儿走过来看妈妈杀鱼，妈妈看似无意的问女儿：“你想怎么吃”，“煎着吃”，妈妈又问：“还能怎么吃？”“油炸”，“你还能想出几种吃法吗？”“烧鱼汤，还可以蒸、醋溜、吃生鱼片还可以腌咸鱼、晒鱼干吃”，妈妈首先夸奖女儿聪明，又提醒女儿“一条鱼还可以鱼头烧汤，鱼身煎或者鱼三吃四吃。”这一番对话实际上就是对孩子进行发散性思维训练</code></pre>\n\n<p><span style=\"font-size:14px\"><strong><a id=\"逆向思维\" name=\"逆向思维\"></a>逆向思维:不妨反过来想一想</strong></span></p>\n\n<p><span style=\"font-size:14px\"><strong><a id=\"平面思维\" name=\"平面思维\"></a>平面思维:由点及面的思维导图</strong></span></p>', 1, '5f199b8885e24fc8b28672b872edb606', 5, '2017-09-02 22:16:40', '2018-09-02 18:21:16');
INSERT INTO `logcontent` VALUES ('77a8cb467be846c4bc33279f51af255b', 1, '4eca926aa543420baea2f03f506d042e', '高性能Mysql笔记七', '高性能Mysql（第三版）', '高性能Mysql（第三版）--复制', '<h2>复制概述</h2>\n\n<pre>\n<code>MySQL内建的复制功能是构建基于MySQL的大规模,高性能应用的基础.同时也是高可用性,可扩展性,灾难恢复,备份及数据仓库等工作的基础\n\nMySQL支持两种复制方式：基于行的复制和基于语句的复制。这两种方式都是通过在主库上记录二进制日志、在备库重放日志的方式来实现异步的数据复制。这意味着同一时间点主库与备库可能存在延迟。\n\n复制解决的问题\n数据分布: 基于行的复制会比传统的基于语句的复制模式对带宽压力更大。你可以随意地停止或开始复制，并在不同的地理位置来分布数据备份\n负载均衡: 将读操作分布到多个服务器上，实现对读密集型应用的优化，可以简单地对机器名做硬编码或使用DNS轮询等\n备份: 复制对于备份是一项很有意义的技术补充，但复制既不是备份也不能够取代备份。\n高可用性和故障切换: 避免单点失败\nMySQL升级测试: 一种普遍做法是使用一个更高版本的MySQL作为备库保证实例升级前查询能够在备库按照预期执行\n\n复制如何工作（在主库上并发运行的查询在备库只能串行化执行）\n1、在主库上记录二进制日志。每次准备提交事务完成数据更新前，主库将更新事件记录到二进制日志中（按事务提交的顺序而非语句的执行顺序来记录二进制日志）。在记录二进制日志后，主库通知存储引擎可以提交事务了\n2、备库将主库的二进制日志复制到其本地的中继日志中。首先，备库会启动一个工作线程，称为I/O线程，I/O线程跟主库建立一个普通的客户端连接，然后在主库上启动一个特殊的二进制转储线程，这个二进制转储线程会读取主库上二进制日志中的事件。它不会对事件进行轮询。如果该线程追赶上了主库，它将进入睡眠状态，直到主库发送信号量通知其有新的事件产生时才会被唤醒，备库I/O线程会将接收到的事件记录到中继日志中。\n3、备库的SQL线程从中继日志中读取事件并在备库执行，从而实现备库数据的更新。当SQL线程追赶上I/O线程时，中继日志通常已经在系统缓存中，所以中继日志的开销很低。SQL线程执行的事件也可以通过配置选项来决定是否写入其自己的二进制日志中\n</code></pre>\n\n<h2>配置复制</h2>\n\n<pre>\n<code>MySQL服务器配置复制（主库/备库均为新库）\n1、在每台服务器上创建复制账号（赋予REPLICATION SLAVE、REPLICATION CLIENT权限）。\n主/备库权限赋予说明：\n（1）用来监控和管理复制的账号需要REPLICATION CLIENT权限。\n（2）有需要可以方便地交换主备库的角色。\n2、需要打开二进制日志并指定一个唯一的server_id，在主库的my.cnf文件中增加或修改配置文件，重启服务器。备库一样操作。\n3、使用CHANGE MASTER TO语句开始执行复制\n\n（主库老库，备库新库---常见）\n初始化备库或者从其他服务器克隆数据到备库\n1、有三个条件来让主库和备库保持同步\n（1）在某个时间点的主库的数据快照。\n（2）主库当前的二进制日志文件，和获得数据快照时在该二进制日志文件中的偏移量，我们把这两个值称为日志文件坐标。通过这两个值可以确定二进制日志的位置。可以通过SHOW MASTER STATUS命令来获取这些值。\n（3）从快照时间到现在的二进制日志。\n2、从别的服务器克隆备库的方法\n（2）冷备份：关闭主库，把数据复制到备库。重启主库后，会使用一个新的二进制日志文件，我们在备库通过执行CHANGE MASTER TO指向这个文件的起始处。缺点：需要关闭主库\n（3）热备份：只针对MyISAM表，可以在主库运行时使用mysqlhotcopy或rsync来复制数据\n（4）mysqldump：只针对InnoDB表，使用命令来转储主库数据并将其加载到备库，然后设置相应的二进制日志坐标\n（5）快照或备份：把备份或快照恢复到备库，然后使用CHANGE MASTER TO指定二进制日志的坐标（如果使用备份，需要确保从备份的时间点开始的主库二进制日志都要存在）\n（6）Xtrabackup：开源的热备份工具，能够在备份时不阻塞服务器的操作，可以在不影响主库的情况下设置备库。通过克隆主库或另一个已存在的备库的方式来建立备库</code></pre>\n\n<h2>复制的原理</h2>\n\n<pre>\n<code>基于语句的复制\n5.0及之前的版本中只支持基于SQL语句的复制（也称为逻辑复制）\n优点\n实现简单\n二进制日志里的事件更加紧凑，节省带宽\n出现问题容易定位\n缺点\n存在无法被正确复制的sql，如语句中包含CURRENT_USER()函数等，存储过程和触发器在使用基于语句的复制模式时也可能存在问题。\n更新必须是串行的。这需要更多的锁\n\n基于行的复制\n5.1开始支持基于行的复制,将实际数据记录在二进制日志中\n优点\n可以正确地复制每一行\n无需建立执行计划并查询，性能占用低\n快速定位主/备数据不一致的情况（在备库更新一个不存在的记录会报错并停止复制）\n某些sql语句重放的代价很高（查询复杂，结果很少），可以更高效地复制数据。\n缺点\n出现问题不容易定位（执行过程黑盒）\n某些sql语句重放的代价很低（UPDATE a SET b=0）,全表更新，使用行复制开销很大\n二进制日志庞大。给主库记录日志和复制增加额外的负载，日志记录慢导致并发度降低\n\n复制过程使用的文件\n二进制日志文件\n中继日志文件\n二进制日志名称.index（用于记录磁盘上的二进制日志文件，如果删除将导致MySQL识别不了二进制日志文件）\nmysql-relay-bin-index（中继日志的索引文件）\nmaster.info（保存备库连接到主库所需要的信息，如果删除将导致备库在重启后无法连接到主库。这个文件以文本的方式记录了复制用户的密码）\nrelay-log.info（包含了当前备库复制的二进制日志和中继日志坐标，如果删除将导致备库重启后无法获知开始位置开始复制，可能导致重放已经执行过的语句）</code></pre>\n\n<h2>复制拓扑</h2>\n\n<pre>\n<code>可以在任意个主库和备库之间建立复制。限制：每一个备库只能有一个主库\n主库与备库为一对多的关系。log_slave_updates选项可以让备库变成其他服务器的主库，实现主库=&gt;备库=&gt;备库。\n\n一主库多备库\n在有少量写和大量读时，这种配置非常有效。可以把读请求分摊到多个备库上，直到备库给主库造成了太大的负担（每个备库在主库上开启一个线程，可以使用金字塔形方案），或者主备之间的带宽成为瓶颈为止\n\n用途\n1、不同的角色使用不同的备库（添加不同的索引或使用不同的存储引擎）。\n2、把一台备库当作待用的主库，除了复制没有其他数据传输。\n3、将一台备库放到远程数据中心，用作灾难恢复。\n4、延迟一个或多个备库，以备灾难恢复。\n5、把一台备库作为备份、培训、开发或者测试使用服务器。\n\n主动-主动模式下的主-主复制\n模式:\n主（主动）⇆主（主动）\n主-主复制（也叫双主复制或双向复制）包含两台服务器，每一个都被配置成对方的主库和备库\n用途\n一个可能的应用场景是两个处于不同地理位置的办公室，并且都需要一份可写的数据拷贝。\n最大的问题是如何解决冲突，两个可写的互主服务器导致的问题非常多。这通常发生在两台服务器同时修改一行记录，或同时在两台服务器上向一个包含AUTO_INCREMENT列的表里插入数据（设置auto_increment_increment和auto_increment_offset。通过这两个选项可以让MySQL自动为INSERT语句选择不互相冲突的值）。\n\n主动-被动模式下的主-主复制\n模式:\n主（主动）⇉主（被动）\n其中的一台服务器是只读的被动服务器\n如何配置主动-被动模式下的主-主复制\n1、确保两台服务器上有相同的数据。\n2、启用二进制日志，选择唯一的服务器ID，并创建复制账号。\n3、启用备库更新的日志记录，这是故障转移和故障恢复的关键。\n4、把被动服务器配置成只读，防止可能与主动服务器上的更新产生冲突，这一点是可选的。\n5、启动每个服务器的MySQL实例。\n6、将每个主库设置为对方的备库，使用新创建的二进制日志开始工作。\n\n拥有备库的主-主结构\n模式:\n主（主动）⇆主（主动）\n  ⇊          ⇊\n 备库        备库\n为每个主库增加一个备库，增加冗余\n\n环形复制\n模式:\n主库⇐主库\n   ⇘⇗ \n   主库\n环形结构可以有三个或更多的主库。每个服务器都是在它之前的服务器的备库，是在它之后的服务器的主库，失效几率高，不推荐\n缺点：如果发起事件的节点失效，该事件将陷入无限循环，因为在循环中无法找到发起事件的节点服务器ID来终止复制\n\n拥有备库的环形复制\n模式:\n  备库     备库\n    ⇖     ⇗\n    主库⇐主库\n       ⇘⇗ \n       主库\n        ⇓\n       备库\n缺点：解决了单点失效循环问题，但依然不稳定\n\n树或金字塔型\n      主库\n    ⇙     ⇘\n  备库     备库\n ⇙   ⇘   ⇙   ⇘\n备库 备库 备库 备库\n优点：减轻主库负担，缺点：中间层错误影响下方备库\n\n备库使用方案\n用作负载均衡：\n将主库数据划分到不同的备库里，每个备库只拥有主库的一部分数据，实现读取的负载均衡，降低主库读负担，每个备库通过选项replicate_wild_do_table选项来限制给定数据库的数据\n用作分离功能：\n将某个备库用作其他用途\n用作数据归档（慎用）\n备库上保留主库删除的数据，实现方法一种是在主库上选择性地禁止二进制日志，第二种是在备库上使用replicate_ignore_db规则，第三种办法是利用binlog_ignore_db来过滤复制事件\n用作全文检索\n许多应用要求合并事务和全文检索\n用作只读备库\n将备库设为只读，除了复制线程和拥有超级权限的用户以及临时表操作外禁止普通用户修改操作\n用作模拟多主库复制\nMySQL不支持多主库复制（一个备库拥有多个主库）。但是可以通过把一台备库轮流指向多台主库的方式来模拟这种结构\n用作日志服务器\n创建没有数据的日志服务器。它唯一的目的就是更加容易重放并且/或者过滤二进制日志事件，无须执行二进制日志\n日志服务器相比mysqlbinlog的优势\n1、复制作为应用二进制日志的方法已经被大量的用户所测试，能够证明是可行的。mysqlbinlog并不能确保像复制那样工作，并且可能无法正确生成二进制日志中的数据更新。\n2、复制的速度更快，因为无须将语句从日志导出来并传送给MySQL。\n3、可以很容易观察到复制过程。能够更方便处理错误。例如，可以跳过执行失败的语句。更方便过滤复制事件。\n4、有时候mysqlbinlog会因为日志记录格式更改而无法读取二进制日志。</code></pre>\n\n<h2>复制管理和维护</h2>\n\n<pre>\n<code>监控复制: SHOW MASTER STATUS查看主库的二进制日志位置和配置, SHOW BINLOG EVENTS查看复制事件\n测量备库延迟: 使用Percona Toolkit的pt-hearbeat\n确定主备是否一致：使用Percona Toolkit的pt-table-checksum\n从主库重新同步备库（单独处理某个备库）: 使用Percona Toolkit的pt-table-sync（基于复制工作，要求复制功能正常）\n备库提升为主库：有两种场景需要将备库替换为主库，一种是计划内的提升，一种是计划外的提升。\n主-主配置中交换角色：</code></pre>\n\n<h2>复制的问题和解决方案</h2>\n\n<pre>\n<code>数据损坏或丢失\n主库意外关闭: 主库开启sync_binlog避免事件丢失,使用Percona Toolkit中的pt-table-checksum检查主备一致性\n备库意外关闭: 重启后观察MySQL错误日志,想方法获取备库指向主库的日志偏移量\n主库上的二进制日志损坏: 跳过所有损坏的事件,手动找到一个完好的事件开始\n备库上的中继日志损坏: 5.5版本后能在崩溃后自动重新获取中继日志\n二进制日志与InnoDB事务日志不同步: 除非备库中继日志有保存,否则没有任何办法恢复丢失的事务\n\n复制容易出现的问题\n1、当对非事务型表的更新发生错误时，例如查询在完成前被kill，就可能导致主库和备库的数据不一致，如果使用myisam,在关闭Mysql前需要确保已经运行了stop slave,否则在服务器关闭时会kill所有正在运行的查询。\n2、如果是事务型,失败的更新会在主库上回滚而且不会记录到二进制日志\n3、避免混用事务和非事务: 如果备库发生死锁而主库没有,事务型会回滚而非事务型则不会造成不同步\n4、主库和备库使用不同存储引擎容易导致问题\n5、避免不唯一和未定义备库服务器id\n6、避免在主库上创建备库上没有的表,因为复制可能中断\n7、基于语句复制时,主库上没有安全使用临时表的方法.丢失临时表: 备库崩溃时,任何复制线程拥有的临时表都会丢失,重启备库后所有依赖临时表的语句都会失败\n8、InnoDB加锁读引起的锁争用: 将大命令拆成小命令可以有效减少锁竞争\n9、过大的复制延迟: 定位执行慢的语句,改善机器配置</code></pre>\n\n<h2>MySQL复制的高级特性</h2>\n\n<pre>\n<code>半同步复制: 当提交事务,客户端收到查询结束反馈前必须保证二进制日志已经传输到至少一台备库上,主库将事务提交到磁盘上之后会增加一些延迟\n复制心跳: 保证备库一直与主库相联系,如果出现断开的网络连接,备库会注意到丢失的心跳数据</code></pre>\n\n<h2>其他复制技术</h2>\n\n<pre>\n<code>Percona XtraDB Cluster的同步复制\nTungsten Replicator</code></pre>\n\n<p>&nbsp;</p>', 1, '5f199b8885e24fc8b28672b872edb606', 7, '2018-09-02 15:10:21', '2018-09-11 09:08:28');
INSERT INTO `logcontent` VALUES ('78c2b07adf4240b3b1b8f3604843a27c', -1, '7338e53acd514defa1a17e47016f3f4a', 'Elasticsearch（十）', '内置脚本,外部脚本,upsert操作', '内置脚本/外部脚本/upsert操作', '<p><a name=\"内置脚本\"></a>1、内置脚本</p>\n\n<pre>\n<code class=\"language-json\">es，其实是有个内置的脚本支持的，可以基于groovy脚本实现各种各样的复杂操作\n\n基于groovy脚本，如何执行partial update\n\n添加一条数据\nPUT e/p/4\n{\n  \"num\":0,\n  \"tags\":[]\n}\n使用partial update操作\nPOST e/p/4/_update\n{\n    \"doc\": {\n      \"num\": \"1\"\n    }\n  \n}\n使用内置脚本操作\nPOST e/p/4/_update\n{\n  \"script\": \"ctx._source.num+=1\"\n}\n=====结果=====\n{\n  \"_index\": \"e\",\n  \"_type\": \"p\",\n  \"_id\": \"4\",\n  \"_version\": 3,\n  \"result\": \"updated\",\n  \"_shards\": {\n    \"total\": 2,\n    \"successful\": 2,\n    \"failed\": 0\n  }\n}</code></pre>\n\n<p><a name=\"外部脚本\"></a>2、外部脚本</p>\n\n<pre>\n<code class=\"language-json\">用脚本添加字段内容\n在elasticsearch-5.2.0\\config\\scripts路径下，建立文件test-add-tags.groovy\n内容为ctx._source.tags+=new_tag\n保存\nPOST /test_index/test_type/11/_update\n{\n  \"script\": {\n    \"lang\": \"groovy\", \n    \"file\": \"test-add-tags\",\n    \"params\": {\n      \"new_tag\": \"tag1\"\n    }\n  }\n}\n======结果=======\n{\n  \"_index\": \"e\",\n  \"_type\": \"p\",\n  \"_id\": \"4\",\n  \"_version\": 5,\n  \"found\": true,\n  \"_source\": {\n    \"num\": \"1\",\n    \"tags\": [\n      \"tag1\"\n    ]\n  }\n}\n\n用脚本删除文档\n（删除失败版）\nPOST e/p/4/_update\n{\n  \"script\": {\n    \"lang\": \"groovy\",\n    \"file\": \"test-delete-document\",\n    \"params\": {\n      \"count\":1\n    }\n  }\n}\n======结果未删除（count为“1”而不是1，result为“noop”）=====\n{\n  \"_index\": \"e\",\n  \"_type\": \"p\",\n  \"_id\": \"4\",\n  \"_version\": 5,\n  \"result\": \"noop\",\n  \"_shards\": {\n    \"total\": 0,\n    \"successful\": 0,\n    \"failed\": 0\n  }\n}\n（删除成功版）\nPOST e/p/4/_update\n{\n  \"script\": {\n    \"lang\": \"groovy\",\n    \"file\": \"test-delete-document\",\n    \"params\": {\n      \"count\":\"1\"\n    }\n  }\n}\n======结果删除result为“deleted”=====\n{\n  \"_index\": \"e\",\n  \"_type\": \"p\",\n  \"_id\": \"4\",\n  \"_version\": 6,\n  \"result\": \"deleted\",\n  \"_shards\": {\n    \"total\": 2,\n    \"successful\": 2,\n    \"failed\": 0\n  }\n}</code></pre>\n\n<p><a name=\"upsert操作\"></a>3、upsert操作</p>\n\n<pre>\n<code class=\"language-json\">POST e/p/4/_update\n{\n  \"doc\": {\n    \"num\": 1\n  }\n}\n因为上面已经把数据删除了，所以会报错\n{\n  \"error\": {\n    \"root_cause\": [\n      {\n        \"type\": \"document_missing_exception\",\n        \"reason\": \"[p][4]: document missing\",\n        \"index_uuid\": \"7Bg9e6aPSTOABbzuF5zafA\",\n        \"shard\": \"2\",\n        \"index\": \"e\"\n      }\n    ],\n    \"type\": \"document_missing_exception\",\n    \"reason\": \"[p][4]: document missing\",\n    \"index_uuid\": \"7Bg9e6aPSTOABbzuF5zafA\",\n    \"shard\": \"2\",\n    \"index\": \"e\"\n  },\n  \"status\": 404\n}\n=======引出upsert=====\nPOST e/p/4/_update\n{\n  \"script\" : \"ctx._source.num+=1\",\n  \"upsert\": {\n    \"num\":0,\n    \"tags\":[]\n  }\n}\n如果指定的document不存在，就执行upsert中的初始化操作；\n如果指定的document存在，就执行doc或者script指定的partial update操作</code></pre>\n\n<p>&nbsp;</p>\n', 2, '5f199b8885e24fc8b28672b872edb606', 5, '2017-07-12 18:04:24', '2018-09-06 16:12:58');
INSERT INTO `logcontent` VALUES ('7a10427649214396878fd961caa8c362', -1, 'f29612168904487aadb4043df110361c', '资料收集', '好文', '好文收集', '<p><a id=\"win上运行linux常用命令\" name=\"win上运行linux常用命令\"></a>一：UnxUtils.zip</p>\n\n<p>&nbsp; &nbsp; 说明：zip中包含可以在windows上运行的linux的常用命令，解压后配置环境变量即可使用</p>\n\n<p>&nbsp;&nbsp;&nbsp;&nbsp;下载地址：<a href=\"http://download.csdn.net/detail/eternal_m/6730137\">https://sourceforge.net/projects/unxutils/?source=typ_redirect</a></p>\n\n<p><a name=\"Chrome缓存清理\"></a>二：一键清理缓存插件</p>\n\n<p>&nbsp; &nbsp; 说明：在点击后自动清理缓存并刷新页面的一款插件，对于调试页面等很方便</p>\n\n<p>&nbsp; &nbsp; 下载地址：<a href=\"http://chromecj.com/productivity/2015-03/390/download.html\">http://chromecj.com/productivity/2015-03/390/download.html</a></p>\n\n<p><a name=\"远程控制\"></a>三：TeamViewer</p>\n\n<p>&nbsp; &nbsp; 说明：比较不错的远程操作软件</p>\n\n<p>&nbsp; &nbsp; 下载地址：<a href=\"https://www.teamviewer.com/zhCN/\">https://www.teamviewer.com/zhCN/</a></p>\n\n<p><a name=\"Java反编译\"></a>四：JD-GUI</p>\n\n<p>&nbsp; &nbsp; 说明：很简单的一个Java反编译工具，简单实用</p>\n\n<p>&nbsp; &nbsp; 下载地址：<a href=\"http://www.softpedia.com/get/Programming/Debuggers-Decompilers-Dissasemblers/JD-GUI.shtml#download\">http://www.softpedia.com/get/Programming/Debuggers-Decompilers-Dissasemblers/JD-GUI.shtml#download</a></p>\n\n<p><a id=\"软件开发文档模板\" name=\"软件开发文档模板\"></a>五：软件开发文档模板</p>\n\n<p>&nbsp; &nbsp; 说明：包括需求分析文档、概要设计文档、详细设计文档、软件测试文档、总结汇报文档</p>\n\n<p>&nbsp; &nbsp; 下载地址：http://ishare.iask.sina.com.cn/theme/RjKoHbt2RI.html</p>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n', 1, '5f199b8885e24fc8b28672b872edb606', 8, '2017-07-06 17:49:57', '2018-08-31 22:21:23');
INSERT INTO `logcontent` VALUES ('7ca387adaf76422c90b4587ae2eaa0a0', -1, '7338e53acd514defa1a17e47016f3f4a', '四、搜索——最基本的工具', '搜索', '搜索——最基本的工具', '<h2>搜索&mdash;&mdash;最基本的工具</h2>\n\n<p>现在，我们已经学会了如何使用 Elasticsearch 作为一个简单的 NoSQL 风格的分布式文档存储系统。我们可以将一个 JSON 文档扔到 Elasticsearch 里，然后根据 ID 检索。但 Elasticsearch 真正强大之处在于可以从无规律的数据中找出有意义的信息&mdash;&mdash;从&ldquo;大数据&rdquo;到&ldquo;大信息&rdquo;。</p>\n\n<p>Elasticsearch 不只会<em>存储（stores）</em>&nbsp;文档，为了能被搜索到也会为文档添加<em>索引（indexes）</em>&nbsp;，这也是为什么我们使用结构化的 JSON 文档，而不是无结构的二进制数据。</p>\n\n<p><em>文档中的每个字段都将被索引并且可以被查询</em>&nbsp;。不仅如此，在简单查询时，Elasticsearch 可以使用&nbsp;<em>所有（all）</em>&nbsp;这些索引字段，以惊人的速度返回结果。这是你永远不会考虑用传统数据库去做的一些事情。</p>\n\n<p><em>搜索（search）</em>&nbsp;可以做到：</p>\n\n<ul style=\"list-style-type:disc\">\n	<li>在类似于&nbsp;<code>gender</code>&nbsp;或者&nbsp;<code>age</code>&nbsp;这样的字段&nbsp;上使用结构化查询，<code>join_date</code>&nbsp;这样的字段上使用排序，就像SQL的结构化查询一样。</li>\n	<li>全文检索，找出所有匹配关键字的文档并按照<em>相关性（relevance）</em>&nbsp;排序后返回结果。</li>\n	<li>以上二者兼而有之。</li>\n</ul>\n\n<p>很多搜索都是开箱即用的，为了充分挖掘 Elasticsearch 的潜力，你需要理解以下三个概念：</p>\n\n<p><em>映射（Mapping）</em></p>\n\n<p>描述数据在每个字段内如何存储</p>\n\n<p><em>分析（Analysis）</em></p>\n\n<p>全文是如何处理使之可以被搜索的</p>\n\n<p><em>领域特定查询语言（Query DSL）</em></p>\n\n<p>Elasticsearch 中强大灵活的查询语言</p>\n\n<h2>空搜索</h2>\n\n<p>搜索API的最基础的形式是没有指定任何查询的空搜索&nbsp;，它简单地返回集群中所有索引下的所有文档：</p>\n\n<pre>\nGET /_search</pre>\n\n<p>返回的结果（为了界面简洁编辑过的）像这样：</p>\n\n<pre>\n<code class=\"language-json\">{\n   \"hits\" : {\n      \"total\" :       14,\n      \"hits\" : [\n        {\n          \"_index\":   \"us\",\n          \"_type\":    \"tweet\",\n          \"_id\":      \"7\",\n          \"_score\":   1,\n          \"_source\": {\n             \"date\":    \"2014-09-17\",\n             \"name\":    \"John Smith\",\n             \"tweet\":   \"The Query DSL is really powerful and flexible\",\n             \"user_id\": 2\n          }\n       },\n        ... 9 RESULTS REMOVED ...\n      ],\n      \"max_score\" :   1\n   },\n   \"took\" :           4,\n   \"_shards\" : {\n      \"failed\" :      0,\n      \"successful\" :  10,\n      \"total\" :       10\n   },\n   \"timed_out\" :      false\n}</code></pre>\n\n<h3>hits</h3>\n\n<p>返回结果中最重要的部分是&nbsp;<code>hits</code>&nbsp;，它&nbsp;包含&nbsp;<code>total</code>&nbsp;字段来表示匹配到的文档总数，并且一个&nbsp;<code>hits</code>&nbsp;数组包含所查询结果的前十个文档。</p>\n\n<p>在&nbsp;<code>hits</code>&nbsp;数组中每个结果包含文档的&nbsp;<code>_index</code>&nbsp;、&nbsp;<code>_type</code>&nbsp;、&nbsp;<code>_id</code>&nbsp;，加上&nbsp;<code>_source</code>&nbsp;字段。这意味着我们可以直接从返回的搜索结果中使用整个文档。这不像其他的搜索引擎，仅仅返回文档的ID，需要你单独去获取文档。</p>\n\n<p>每个结果还有一个&nbsp;<code>_score</code>&nbsp;，它衡量了文档与查询的匹配程度。默认情况下，首先返回最相关的文档结果，就是说，返回的文档是按照&nbsp;<code>_score</code>&nbsp;降序排列的。在这个例子中，我们没有指定任何查询，故所有的文档具有相同的相关性，因此对所有的结果而言&nbsp;<code>1</code>&nbsp;是中性的&nbsp;<code>_score</code>&nbsp;。</p>\n\n<p><code>max_score</code>&nbsp;值是与查询所匹配文档的&nbsp;<code>_score</code>&nbsp;的最大值。</p>\n\n<h3>took</h3>\n\n<p><code>took</code>&nbsp;值告诉我们执行整个搜索请求耗费了多少毫秒。</p>\n\n<h3>shards</h3>\n\n<p><code>_shards</code>&nbsp;部分&nbsp;告诉我们在查询中参与分片的总数，以及这些分片成功了多少个失败了多少个。正常情况下我们不希望分片失败，但是分片失败是可能发生的。如果我们遭遇到一种灾难级别的故障，在这个故障中丢失了相同分片的原始数据和副本，那么对这个分片将没有可用副本来对搜索请求作出响应。假若这样，Elasticsearch 将报告这个分片是失败的，但是会继续返回剩余分片的结果。</p>\n\n<h3>timeout</h3>\n\n<p><code>timed_out</code>&nbsp;值告诉我们查询是否超时。默认情况下，搜索请求不会超时。&nbsp;如果低响应时间比完成结果更重要，你可以指定&nbsp;<code>timeout</code>&nbsp;为 10 或者 10ms（10毫秒），或者 1s（1秒）：</p>\n\n<pre>\nGET /_search?timeout=10ms</pre>\n\n<p>在请求超时之前，Elasticsearch 将会返回已经成功从每个分片获取的结果。</p>\n\n<p><img alt=\"警告\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/warning.png\" /></p>\n\n<p>应当注意的是&nbsp;<code>timeout</code>&nbsp;不是停止执行查询，它仅仅是告知正在协调的节点返回到目前为止收集的结果并且关闭连接。在后台，其他的分片可能仍在执行查询即使是结果已经被发送了。</p>\n\n<p>使用超时是因为 SLA(服务等级协议)对你是很重要的，而不是因为想去中止长时间运行的查询。</p>\n\n<h2>多索引，多类型</h2>\n\n<p>你有没有注意到之前的&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/empty-search.html\" title=\"空搜索\">empty search</a>&nbsp;的结果，不同类型的文档&nbsp;&mdash;&nbsp;<code>user</code>&nbsp;和&nbsp;<code>tweet</code>&nbsp;来自不同的索引&mdash;&nbsp;<code>us</code>和&nbsp;<code>gb</code>&nbsp;？</p>\n\n<p>如果不对某一特殊的索引或者类型做限制，就会搜索集群中的所有文档。Elasticsearch 转发搜索请求到每一个主分片或者副本分片，汇集查询出的前10个结果，并且返回给我们。</p>\n\n<p>然而，经常的情况下，你&nbsp;想在一个或多个特殊的索引并且在一个或者多个特殊的类型中进行搜索。我们可以通过在URL中指定特殊的索引和类型达到这种效果，如下所示：</p>\n\n<p><code>/_search</code></p>\n\n<p>在所有的索引中搜索所有的类型</p>\n\n<p><code>/gb/_search</code></p>\n\n<p>在&nbsp;<code>gb</code>&nbsp;索引中搜索所有的类型</p>\n\n<p><code>/gb,us/_search</code></p>\n\n<p>在&nbsp;<code>gb</code>&nbsp;和&nbsp;<code>us</code>&nbsp;索引中搜索所有的文档</p>\n\n<p><code>/g*,u*/_search</code></p>\n\n<p>在任何以&nbsp;<code>g</code>&nbsp;或者&nbsp;<code>u</code>&nbsp;开头的索引中搜索所有的类型</p>\n\n<p><code>/gb/user/_search</code></p>\n\n<p>在&nbsp;<code>gb</code>&nbsp;索引中搜索&nbsp;<code>user</code>&nbsp;类型</p>\n\n<p><code>/gb,us/user,tweet/_search</code></p>\n\n<p>在&nbsp;<code>gb</code>&nbsp;和&nbsp;<code>us</code>&nbsp;索引中搜索&nbsp;<code>user</code>&nbsp;和&nbsp;<code>tweet</code>&nbsp;类型</p>\n\n<p><code>/_all/user,tweet/_search</code></p>\n\n<p>在所有的索引中搜索&nbsp;<code>user</code>&nbsp;和&nbsp;<code>tweet</code>&nbsp;类型</p>\n\n<p>当在单一的索引下进行搜索的时候，Elasticsearch 转发请求到索引的每个分片中，可以是主分片也可以是副本分片，然后从每个分片中收集结果。多索引搜索恰好也是用相同的方式工作的--只是会涉及到更多的分片。</p>\n\n<p><img alt=\"提示\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/tip.png\" /></p>\n\n<p>搜索一个索引有五个主分片和搜索五个索引各有一个分片准确来所说是等价的。</p>\n\n<p>接下来，你将明白这种简单的方式如何灵活的根据需求的变化让扩容变得简单。</p>\n\n<h2>分页</h2>\n\n<p>在之前的&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/empty-search.html\" title=\"空搜索\">空搜索</a>&nbsp;中说明了集群中有&nbsp;14 个文档匹配了（empty）query 。 但是在&nbsp;<code>hits</code>&nbsp;数组中只有 10 个文档。如何才能看到其他的文档？</p>\n\n<p>和 SQL 使用&nbsp;<code>LIMIT</code>&nbsp;关键字返回单个&nbsp;<code>page</code>&nbsp;结果的方法相同，Elasticsearch 接受&nbsp;<code>from</code>&nbsp;和&nbsp;<code>size</code>&nbsp;参数：</p>\n\n<p><code>size</code></p>\n\n<p>显示应该返回的结果数量，默认是&nbsp;<code>10</code></p>\n\n<p><code>from</code></p>\n\n<p>显示应该跳过的初始结果数量，默认是&nbsp;<code>0</code></p>\n\n<p>如果每页展示 5 条结果，可以用下面方式请求得到 1 到 3 页的结果：</p>\n\n<pre>\nGET /_search?size=5\nGET /_search?size=5&amp;from=5\nGET /_search?size=5&amp;from=10</pre>\n\n<p>考虑到分页过深以及一次请求太多结果的情况，结果集在返回之前先进行排序。 但请记住一个请求经常跨越多个分片，每个分片都产生自己的排序结果，这些结果需要进行集中排序以保证整体顺序是正确的。</p>\n\n<p><strong>在分布式系统中深度分页</strong></p>\n\n<p>理解为什么深度分页是有问题的，我们可以假设在一个有 5 个主分片的索引中搜索。 当我们请求结果的第一页（结果从 1 到 10 ），每一个分片产生前 10 的结果，并且返回给&nbsp;<em>协调节点</em>&nbsp;，协调节点对 50 个结果排序得到全部结果的前 10 个。</p>\n\n<p>现在假设我们请求第 1000 页--结果从 10001 到 10010 。所有都以相同的方式工作除了每个分片不得不产生前10010个结果以外。 然后协调节点对全部 50050 个结果排序最后丢弃掉这些结果中的 50040 个结果。</p>\n\n<p>可以看到，在分布式系统中，对结果排序的成本随分页的深度成指数上升。这就是 web 搜索引擎对任何查询都不要返回超过 1000 个结果的原因。</p>\n\n<p><img alt=\"提示\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/tip.png\" /></p>\n\n<p>在&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/reindex.html\" title=\"重新索引你的数据\">重新索引你的数据</a>&nbsp;中解释了如何&nbsp;<em>能够</em>&nbsp;有效获取大量的文档。</p>\n\n<h2><em>轻量</em>&nbsp;搜索</h2>\n\n<p>有两种形式的&nbsp;<code>搜索</code>&nbsp;API：一种是 &ldquo;轻量的&rdquo;&nbsp;<em>查询字符串</em>&nbsp;版本，要求在查询字符串中传递所有的&nbsp;参数，另一种是更完整的&nbsp;<em>请求体</em>&nbsp;版本，要求使用 JSON 格式和更丰富的查询表达式作为搜索语言。</p>\n\n<p>查询字符串搜索非常适用于通过命令行做即席查询。例如，查询在&nbsp;<code>tweet</code>&nbsp;类型中&nbsp;<code>tweet</code>&nbsp;字段包含<code>elasticsearch</code>&nbsp;单词的所有文档：</p>\n\n<pre>\nGET /_all/tweet/_search?q=tweet:elasticsearch</pre>\n\n<p>下一个查询在&nbsp;<code>name</code>&nbsp;字段中包含&nbsp;<code>john</code>&nbsp;并且在&nbsp;<code>tweet</code>&nbsp;字段中包含&nbsp;<code>mary</code>&nbsp;的文档。实际的查询就是这样</p>\n\n<pre>\n+name:john +tweet:mary</pre>\n\n<p>但是查询字符串参数所需要的&nbsp;<em>百分比编码</em>&nbsp;（译者注：URL编码）实际上更加难懂：</p>\n\n<pre>\nGET /_search?q=%2Bname%3Ajohn+%2Btweet%3Amary</pre>\n\n<p><code>+</code>&nbsp;前缀表示必须与查询条件匹配。类似地，&nbsp;<code>-</code>&nbsp;前缀表示一定不与查询条件匹配。没有&nbsp;<code>+</code>&nbsp;或者&nbsp;<code>-</code>&nbsp;的所有其他条件都是可选的&mdash;&mdash;匹配的越多，文档就越相关。</p>\n\n<h3>_all 字段</h3>\n\n<p>这个简单搜索返回包含&nbsp;<code>mary</code>&nbsp;的所有文档：</p>\n\n<pre>\nGET /_search?q=mary</pre>\n\n<p>之前的例子中，我们在&nbsp;<code>tweet</code>&nbsp;和&nbsp;<code>name</code>&nbsp;字段中搜索内容。然而，这个查询的结果在三个地方提到了&nbsp;<code>mary</code>：</p>\n\n<ul style=\"list-style-type:disc\">\n	<li>有一个用户叫做 Mary</li>\n	<li>6条微博发自 Mary</li>\n	<li>一条微博直接 @mary</li>\n</ul>\n\n<p>Elasticsearch 是如何在三个不同的字段中查找到结果的呢？</p>\n\n<p>当索引一个文档的时候，Elasticsearch 取出所有字段的值拼接成一个大的字符串，作为&nbsp;<code>_all</code>&nbsp;字段进行索引。例如，当索引这个文档时：</p>\n\n<pre>\n{\n    &quot;tweet&quot;:    &quot;However did I manage before Elasticsearch?&quot;,\n    &quot;date&quot;:     &quot;2014-09-14&quot;,\n    &quot;name&quot;:     &quot;Mary Jones&quot;,\n    &quot;user_id&quot;:  1\n}</pre>\n\n<p>这就好似增加了一个名叫&nbsp;<code>_all</code>&nbsp;的额外字段：</p>\n\n<pre>\n&quot;However did I manage before Elasticsearch? 2014-09-14 Mary Jones 1&quot;</pre>\n\n<p>除非设置特定字段，否则查询字符串就使用&nbsp;<code>_all</code>&nbsp;字段进行搜索。</p>\n\n<p><img alt=\"提示\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/tip.png\" /></p>\n\n<p>在刚开始开发一个应用时，<code>_all</code>&nbsp;字段是一个很实用的特性。之后，你会发现如果搜索时用指定字段来代替&nbsp;<code>_all</code>&nbsp;字段，将会更好控制搜索结果。当&nbsp;<code>_all</code>&nbsp;字段不再有用的时候，可以将它置为失效，正如在&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/root-object.html#all-field\" title=\"元数据: _all 字段\">元数据: _all 字段</a>&nbsp;中所解释的。</p>\n\n<h3>更复杂的查询</h3>\n\n<p>下面的查询针对tweents类型，并使用以下的条件：</p>\n\n<ul style=\"list-style-type:disc\">\n	<li><code>name</code>&nbsp;字段中包含&nbsp;<code>mary</code>&nbsp;或者&nbsp;<code>john</code></li>\n	<li><code>date</code>&nbsp;值大于&nbsp;<code>2014-09-10</code></li>\n	<li><code>_all</code>&nbsp;字段包含&nbsp;<code>aggregations</code>&nbsp;或者&nbsp;<code>geo</code></li>\n</ul>\n\n<pre>\n+name:(mary john) +date:&gt;2014-09-10 +(aggregations geo)</pre>\n\n<p>查询字符串在做了适当的编码后，可读性很差：</p>\n\n<pre>\n?q=%2Bname%3A(mary+john)+%2Bdate%3A%3E2014-09-10+%2B(aggregations+geo)</pre>\n\n<p>从之前的例子中可以看出，这种&nbsp;<em>轻量</em>&nbsp;的查询字符串搜索效果还是挺让人惊喜的。&nbsp;它的查询语法在相关参考文档中有详细解释，以便简洁的表达很复杂的查询。对于通过命令做一次性查询，或者是在开发阶段，都非常方便。</p>\n\n<p>但同时也可以看到，这种精简让调试更加晦涩和困难。而且很脆弱，一些查询字符串中很小的语法错误，像&nbsp;<code>-</code>&nbsp;，&nbsp;<code>:</code>&nbsp;，&nbsp;<code>/</code>&nbsp;或者&nbsp;<code>&quot;</code>&nbsp;不匹配等，将会返回错误而不是搜索结果。</p>\n\n<p>最后，查询字符串搜索允许任何用户在索引的任意字段上执行可能较慢且重量级的查询，这可能会暴露隐私信息，甚至将集群拖垮。</p>\n\n<p><img alt=\"提示\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/tip.png\" /></p>\n\n<p>因为这些原因，不推荐直接向用户暴露查询字符串搜索功能，除非对于集群和数据来说非常信任他们。</p>\n\n<p>相反，我们经常在生产环境中更多地使用功能全面的&nbsp;<em>request body</em>&nbsp;查询API，除了能完成以上所有功能，还有一些附加功能。</p>\n', 1, '5f199b8885e24fc8b28672b872edb606', 2, '2018-06-14 10:40:48', '2018-09-06 12:11:09');
INSERT INTO `logcontent` VALUES ('7dc8826ae9154da19b18af042b6748dc', -1, 'd7caeba238f0466d87db109b2b9724da', '非线性回归笔记', 'Logistic Regression', '初学', '<p><strong><a id=\"概率简述\" name=\"概率简述\"></a>概率：</strong></p>\n\n<p>定义 &nbsp; 概率(P)robability: 对一件事情发生的可能性的衡量<br />\n范围 &nbsp; 0 &lt;= P &lt;= 1<br />\n计算方法：&nbsp;根据个人置信、根据历史数据、根据模拟数据<br />\n条件概率：在B发生的前提下A发生的概率，即</p>\n\n<p><sub><img src=\"http://localhost:8080/myLog/images/logistic_regression_files/1.png\" style=\"height:25px; width:93px\" /></sub></p>\n\n<p><strong><a id=\"Logistic Regression (逻辑回归)\" name=\"Logistic Regression (逻辑回归)\"></a>Logistic Regression (逻辑回归)</strong></p>\n\n<p><sub><img src=\"http://localhost:8080/myLog/images/logistic_regression_files/2.png\" style=\"height:100px; width:343px\" /></sub></p>\n\n<p><sub>引出逻辑回归：上图1为恶性肿瘤，2为良性肿瘤，如果根据线性回归方程，那么当X值非常大的时候，Y值可能出现在线性方程下方，造成Y值错误分类，即右侧的红X</sub></p>\n\n<p><strong><a id=\"基本模型\" name=\"基本模型\"></a>基本模型</strong><br />\n测试数据为X（x<sub>0</sub>，x<sub>1</sub>，x<sub>2</sub>&middot;&middot;&middot;x<sub>n</sub>）<br />\n要学习的参数为： &Theta;（&theta;<sub>0</sub>，&theta;<sub>1</sub>，&theta;<sub>2</sub>，&middot;&middot;&middot;&theta;<sub>n</sub>）</p>\n\n<p>向量表示：<br />\n<img src=\"http://localhost:8080/myLog/images/logistic_regression_files/3.jpg\" style=\"height:15px; width:56px\" /></p>\n\n<p>引入Sigmoid函数时曲线平滑化&nbsp;<br />\n<img src=\"http://localhost:8080/myLog/images/logistic_regression_files/4.jpg\" style=\"height:30px; width:79px\" /><br />\n<img src=\"http://localhost:8080/myLog/images/logistic_regression_files/5.jpg\" style=\"height:100px; width:134px\" /></p>\n\n<p>预测函数：<br />\n<img src=\"http://localhost:8080/myLog/images/logistic_regression_files/6.jpg\" style=\"height:35px; width:157px\" /></p>\n\n<p>用概率表示：</p>\n\n<p>正例(y=1)：<br />\n<img src=\"http://localhost:8080/myLog/images/logistic_regression_files/7.jpg\" style=\"height:20px; width:159px\" /></p>\n\n<p>反例(y=0)：<br />\n<img src=\"http://localhost:8080/myLog/images/logistic_regression_files/8.jpg\" style=\"height:20px; width:187px\" /></p>\n\n<p>Cost函数<br />\n<img src=\"http://localhost:8080/myLog/images/logistic_regression_files/9.jpg\" style=\"height:100px; width:500px\" /><br />\n目标：找到合适的 &theta;<sub>0</sub>，&theta;<sub>1</sub>使上式最小</p>\n\n<p>梯度下降（gradient decent)<br />\n通过梯度下降来实现找到合适的 &theta;<sub>0</sub>，&theta;<sub>1</sub><br />\n<img src=\"http://localhost:8080/myLog/images/logistic_regression_files/10.jpg\" style=\"height:120px; width:164px\" /><img src=\"http://localhost:8080/myLog/images/logistic_regression_files/11.jpg\" style=\"height:120px; width:112px\" /><br />\n<img src=\"http://localhost:8080/myLog/images/logistic_regression_files/12.jpg\" style=\"height:35px; width:166px\" /><br />\n更新法则：<br />\n<img src=\"http://localhost:8080/myLog/images/logistic_regression_files/13.jpg\" style=\"height:35px; width:243px\" /><br />\n设置学习率<br />\n同时对所有的&theta;进行更新<br />\n重复更新直到收敛 &nbsp;</p>\n', 1, '5f199b8885e24fc8b28672b872edb606', 3, '2017-11-26 22:25:23', '2018-08-31 22:21:23');
INSERT INTO `logcontent` VALUES ('84b147cb9b44456a9a95ed1feae37c5a', 1, 'd7caeba238f0466d87db109b2b9724da', '最邻近规则分类KNN算法笔记', '距离衡量', '初学', '<p><a id=\"综述\" name=\"综述\"></a>综述</p>\n\n<pre>\n<code>Cover和Hart在1968年提出了最初的邻近算法\n分类(classification)算法\n输入基于实例的学习(instance-based learning), 懒惰学习(lazy learning)</code></pre>\n\n<p><img src=\"https://gss2.bdstatic.com/9fo3dSag_xI4khGkpoWK1HF6hhy/baike/c0%3Dbaike80%2C5%2C5%2C80%2C26/sign=8fabe2c8d5c8a786aa27425c0660a258/03087bf40ad162d95867202e15dfa9ec8a13cd73.jpg\" style=\"height:174px; width:200px\" /></p>\n\n<p><a id=\"算法详述\" name=\"算法详述\"></a>算法详述</p>\n\n<pre>\n<code>步骤：\n为了判断未知实例的类别，以所有已知类别的实例作为参照\n选择参数K，K的值一般不会太大，需要不断去优化\n计算未知实例与所有已知实例的距离\n选择最近K个已知实例\n根据少数服从多数的投票法则(majority-voting)，让未知实例归类为K个最邻近样本中最多数的类别</code></pre>\n\n<p>关于距离的衡量方法，最常用的是Euclidean distance:&nbsp;</p>\n\n<p><img alt=\"这里写图片描述\" src=\"http://img.blog.csdn.net/20161012163552322\" /></p>\n\n<p>二维的下距离计算公式如上图简单的数学公式,对于n维下P1、P2两点的距离公式为</p>\n\n<p><img alt=\"这里写图片描述\" src=\"http://img.blog.csdn.net/20161012163659532\" style=\"height:51px; width:150px\" /></p>\n\n<p>其他距离衡量：余弦值（cos）, 相关度 （correlation）, 曼哈顿距离 （Manhattan distance）</p>\n\n<table border=\"1\" cellpadding=\"1\" cellspacing=\"1\" style=\"width:500px\">\n	<tbody>\n		<tr>\n			<td>电影名称</td>\n			<td>打斗次数</td>\n			<td>接吻次数</td>\n			<td>电影类型</td>\n		</tr>\n		<tr>\n			<td>A</td>\n			<td>3</td>\n			<td>104</td>\n			<td>Romance</td>\n		</tr>\n		<tr>\n			<td>B</td>\n			<td>2</td>\n			<td>100</td>\n			<td>Romance</td>\n		</tr>\n		<tr>\n			<td>C</td>\n			<td>1</td>\n			<td>81</td>\n			<td>Romance</td>\n		</tr>\n		<tr>\n			<td>D</td>\n			<td>101</td>\n			<td>10</td>\n			<td>Action</td>\n		</tr>\n		<tr>\n			<td>E</td>\n			<td>99</td>\n			<td>5</td>\n			<td>Action</td>\n		</tr>\n		<tr>\n			<td>F</td>\n			<td>98</td>\n			<td>2</td>\n			<td>Action</td>\n		</tr>\n		<tr>\n			<td>未知</td>\n			<td>18</td>\n			<td>90</td>\n			<td>UnKnown</td>\n		</tr>\n	</tbody>\n</table>\n\n<p>分别计算A-F点于未知点的距离</p>\n\n<pre>\n<code>import math\ndef ComputeEuclideanDistance(x1, y1, x2, y2):\n    d = math.sqrt(math.pow((x1-x2), 2) + math.pow((y1-y2), 2))\n    return d\n\nd_ag = ComputeEuclideanDistance(3, 104, 18, 90)\nd_bg = ComputeEuclideanDistance(2, 100, 18, 90)\nd_cg = ComputeEuclideanDistance(1, 81, 18, 90)\nd_dg = ComputeEuclideanDistance(101, 10, 18, 90)\nd_eg = ComputeEuclideanDistance(99, 5, 18, 90)\nd_fg = ComputeEuclideanDistance(98, 2, 18, 90)\nprint(\'d_ag\',d_ag)\nprint(\'d_bg\',d_bg)\nprint(\'d_cg\',d_cg)\nprint(\'d_dg\',d_dg)\nprint(\'d_eg\',d_eg)\nprint(\'d_fg\',d_fg)</code></pre>\n\n<p>d_ag 20.518284528683193<br />\nd_bg 18.867962264113206<br />\nd_cg 19.235384061671343<br />\nd_dg 115.27792503337315<br />\nd_eg 117.41379816699569<br />\nd_fg 118.92854997854805</p>\n\n<p>由于ABC三点的距离较小，而且ABC三点的类型均为Romance，所以判断未知影片的类型也为Romance</p>\n\n<p><a id=\"算法优缺点\" name=\"算法优缺点\"></a>算法优缺点：</p>\n\n<pre>\n<code>算法优点\n\n简单、易于理解、容易实现、通过对K的选择可具备丢噪音数据的健壮性\n\n算法缺点\n\n需要大量空间储存所有已知实例\n\n算法复杂度高（需要比较所有已知实例与要分类的实例）\n\n当其样本分布不平衡时，比如其中一类样本过大（实例数量过多）占主导的时候，新的未知实例容易被归类为这个主导样本，因为这类样本实例的数量过大，但这个新的未知实例实际并木接近目标样本</code></pre>\n\n<p><a id=\"改进\" name=\"改进\"></a>改进</p>\n\n<p>考虑距离，根据距离加上权重</p>\n', 1, '5f199b8885e24fc8b28672b872edb606', 12, '2017-10-28 22:50:06', '2018-08-31 22:21:23');
INSERT INTO `logcontent` VALUES ('8dfa9e99b3d5438182e12b407d599d1c', -1, '7338e53acd514defa1a17e47016f3f4a', 'Elasticsearch（五）', '透明隐藏特性,垂直扩容与水平扩容,数据rebalance,master节点,节点平等的分布式架构,横向扩容,超出扩容极限,提升容错性容错机制过程', 'Elasticsearch对复杂分布式机制的透明隐藏特性/垂直扩容与水平扩容/增加或减少节点时的数据rebalance/master节点/节点平等的分布式架构/横向扩容过程，如何超出扩容极限，以及如何提升容错性/Elasticsearch容错机制：master选举，replica容错，数据恢复', '<p><a name=\"透明隐藏特性\"></a>1、Elasticsearch对复杂分布式机制的透明隐藏特性</p>\n\n<p>Elasticsearch是一套分布式的系统，分布式是为了应对大数据量</p>\n\n<p>Elasticsearch隐藏了复杂的分布式机制</p>\n\n<p>分片机制（我们之前随随便便就将一些document插入到es集群中去了，我们有没有care过数据怎么进行分片的，数据到哪个shard中去）</p>\n\n<p>cluster discovery（集群发现机制，我们之前在做那个集群status从yellow转green的实验里，直接启动了第二个es进程，那个进程作为一个node自动就发现了集群，并且加入了进去，还接受了部分数据，replica shard）</p>\n\n<p>shard负载均衡（举例，假设现在有3个节点，总共有25个shard要分配到3个节点上去，es会自动进行均匀分配，以保持每个节点的均衡的读写负载请求）</p>\n\n<p>shard副本，请求路由，集群扩容，shard重分配</p>\n\n<p><a name=\"垂直扩容与水平扩容\"></a>2、Elasticsearch的垂直扩容与水平扩容</p>\n\n<p>垂直扩容：采购更强大的服务器，成本非常高昂，而且会有瓶颈</p>\n\n<p>&nbsp;&nbsp;&nbsp;&nbsp;普通服务器：1T，1万</p>\n\n<p>&nbsp;&nbsp;&nbsp;&nbsp;强大服务器：10T，50万</p>\n\n<p>水平扩容：业界经常采用的方案，采购越来越多的普通服务器，性能比较一般，但是很多普通服务器组织在一起，就能构成强大的计算和存储能力</p>\n\n<p>&nbsp;</p>\n\n<p>扩容对应用程序的透明性</p>\n\n<p><a name=\"数据rebalance\"></a>3、增加或减少节点时的数据rebalance</p>\n\n<p>保持负载均衡</p>\n\n<p><a name=\"master节点\"></a>4、master节点</p>\n\n<p>master节点不承载请求，所以不会是一个单点瓶颈（不是请求先发送至master节点，然后再由master节点分配请求，不会导致程序受单台服务器的瓶颈限制）</p>\n\n<p>（1）管理es集群的元数据，比如说索引的创建和删除，维护索引的元数据，节点的增加和删除，维护节点的元数据</p>\n\n<p>（2）默认情况下，会自动选择出一个节点作为master节点</p>\n\n<p><a name=\"节点平等的分布式架构\"></a>5、节点平等的分布式架构</p>\n\n<p>（1）节点对等，每个节点都能接收所有的请求</p>\n\n<p>（2）任何节点接收到请求，都会自动请求路由到有相关数据的节点上去处理请求</p>\n\n<p>（3）原始接收到请求的节点会从处理请求的节点收集响应数据，返回响应</p>\n\n<p><a name=\"横向扩容|超出扩容极限|提升容错性\"></a>6、横向扩容过程，如何超出扩容极限，以及如何提升容错性</p>\n\n<p>（1）primary&amp;replica自动负载均衡，6个shard，3 primary，3 replica</p>\n\n<p>（2）扩容之后，每个node有更少的shard，IO/CPU/Memory资源给每个shard分配更多，每个shard性能更好</p>\n\n<p>（3）扩容的极限，6个shard（3 primary，3 replica），最多扩容到6台机器，每个shard可以占用单台服务器的所有资源，性能最好</p>\n\n<p>（4）超出扩容极限，动态修改replica数量，9个shard（3primary，6 replica），扩容到9台机器，比3台机器时，拥有3倍的读吞吐量</p>\n\n<p>（5）3台机器下，9个shard（3 primary，6 replica），资源更少，但是容错性更好，最多容纳2台机器宕机，6个shard只能容纳1台机器宕机</p>\n\n<p>（6）这里的这些知识点，你综合起来看，就是说，一方面告诉你扩容的原理，怎么扩容，怎么提升系统整体吞吐量；另一方面要考虑到系统的容错性，怎么保证提高容错性，让尽可能多的服务器宕机，保证数据不丢失</p>\n\n<p><a name=\"容错机制过程\"></a>7、Elasticsearch容错机制：master选举，replica容错，数据恢复</p>\n\n<p>前提是9 shard(3个primary、6个replica)，3 node</p>\n\n<p>master node宕机（primary shard宕机）</p>\n\n<p>宕机的一瞬间集群状态red，开始容错，</p>\n\n<p>容错第一步：master选举，选举另外一个node成为新的master，承担起master的责任</p>\n\n<p>容错第二步：replica容错：新master将replica提升为primary shard，集群状态yellow</p>\n\n<p>容错第三步：重启宕机node，master 拷贝replica到该node，使用原有的shard并同步宕机后的修改，集群状态green</p>\n\n<p>&nbsp;</p>\n', 2, '5f199b8885e24fc8b28672b872edb606', 5, '2017-07-12 18:03:58', '2018-09-06 16:12:58');
INSERT INTO `logcontent` VALUES ('8e5f9a105c1e482486e5535e4ac56993', -1, '7338e53acd514defa1a17e47016f3f4a', 'Elasticsearch（十六）', '​倒排索引,倒排索引不可变,召回率,内置分词器,分词设置定制,正排索引,filter执行原理,bitset', '倒排索引/分词器/内置分词器/doc values（正排索引）/filter执行原理', '<p><a name=\"倒排索引\"></a>1、倒排索引</p>\n\n<p>（1）倒排索引的结构</p>\n\n<pre>\n包含这个关键词的document&nbsp;list\n包含这个关键词的所有document的数量：IDF（inverse&nbsp;document&nbsp;frequency）\n这个关键词在每个document中出现的次数：TF（term&nbsp;frequency）\n这个关键词在这个document中的次序\n每个document的长度：length&nbsp;norm\n包含这个关键词的所有document的平均长度</pre>\n\n<p>&nbsp;</p>\n\n<p>（2）倒排索引不可变的原因</p>\n\n<pre>\n倒排索引不可变的好处\n不需要锁，提升并发能力，避免锁的问题\n数据不变，一直保存在os&nbsp;cache中，只要cache内存足够\nfilter&nbsp;cache一直驻留在内存，因为数据不变\n可以压缩，节省cpu和io开销\n\n倒排索引不可变的坏处：每次都要重新构建整个索引</pre>\n\n<p>&nbsp;</p>\n\n<pre>\n示例\ndoc1：I&nbsp;liked&nbsp;small&nbsp;dogs,&nbsp;my&nbsp;mom&nbsp;also&nbsp;liked&nbsp;them.\ndoc2：He&nbsp;never&nbsp;liked&nbsp;dogs,&nbsp;so&nbsp;I&nbsp;hope&nbsp;my&nbsp;mom&nbsp;will&nbsp;not&nbsp;liked&nbsp;him.\n\n分词，初步的倒排索引的建立\nI		doc1			doc2\n--------------------------------------------\nI		*			*\nliked		*			*\nsmall		*	\ndogs		*\nmy		*			*\nmom		*			*\nalso		*\nthem		*	\nHe					*\nnever					*\nso					*\nhope					*\nwill					*\nnot					*\nhim					*\n--------------------------------------------\n倒排索引最简单的建立的一个过程\n\n但是搜索mother&nbsp;like&nbsp;little&nbsp;dog，没有任何结果（没有任何匹配），这肯定不是我们想要的结果\n\nnormalization，建立倒排索引的时候，会执行一个操作，也就是说对拆分出的各个单词进行相应的处理（时态的转换，单复数的转换，\n同义词的转换，大小写的转换），以提升后面搜索的时候能够搜索到相关联的文档的概率\nmom&nbsp;&mdash;&gt;&nbsp;mother\nliked&nbsp;&mdash;&gt;&nbsp;like\nsmall&nbsp;&mdash;&gt;&nbsp;little\ndogs&nbsp;&mdash;&gt;&nbsp;dog\n\n加入normalization，再次用mother&nbsp;liked&nbsp;little&nbsp;dog搜索，就可以搜索到了</pre>\n\n<p>&nbsp;</p>\n\n<p><a name=\"分词器\"></a>2、分词器</p>\n\n<p>（1）什么是分词器</p>\n\n<p>切分词语，normalization（提升recall召回率）</p>\n\n<p>&nbsp;</p>\n\n<p>character filter：在一段文本进行分词之前，先进行预处理，常见有，过滤html标签（&lt;span&gt;hello&lt;span&gt; --&gt; hello），&amp; --&gt; and（I&amp;you --&gt; I and you）等</p>\n\n<p>tokenizer：分词，hello you --&gt; hello, you</p>\n\n<p>token filter：lowercase，stop word，synonymom，dogs --&gt; dog，liked --&gt; like，Tom --&gt; tom，a/the/an --&gt; 干掉（无意义），mother --&gt; mom</p>\n\n<p>&nbsp;</p>\n\n<p>将句子拆分成单个的单词，同时对每个单词进行normalization（时态转换，单复数转换），分词器</p>\n\n<p>recall，召回率：搜索的时候，增加能够搜索到的结果的数量</p>\n\n<p>&nbsp;</p>\n\n<p>（2）内置分词器的介绍</p>\n\n<pre>\n<code class=\"language-json\">Set the shape to semi-transparent by calling set_trans(5)\n下面四种分词器对上句进行分词\n    &lt;1&gt;standard analyzer：set, the, shape, to, semi, transparent, by, calling, set_trans, 5\n        （默认的是standard）\n        standard tokenizer：以单词边界进行切分\n        standard token filter：什么都不做\n        lowercase token filter：将所有字母转换为小写\n        stop token filer（默认被禁用）：移除停用词，比如a the it等等\n    &lt;2&gt;simple analyzer：set, the, shape, to, semi, transparent, by, calling, set, trans\n    &lt;3&gt;whitespace analyzer：Set, the, shape, to, semi-transparent, by, calling, set_trans(5)\n    &lt;4&gt;language analyzer（特定的语言的分词器，比如说，english，英语分词器）：set, shape, semi, transpar, call, set_tran, 5\n\n修改分词器的设置（简单举例）\n    启用english停用词token filter\n    方式：新建一个叫es_std的分词器\n    PUT /my_index\n    {\n      \"settings\": {\n        \"analysis\": {\n          \"analyzer\": {\n            \"es_std\": {\n              \"type\": \"standard\",\n              \"stopwords\": \"_english_\"\n            }\n          }\n        }\n      }\n    }\n\n定制化自己的分词器（简单举例）\nPUT /my_index\n{\n  \"settings\": {\n    \"analysis\": {\n      \"char_filter\": {\n        \"&amp;_to_and\": {\n          \"type\": \"mapping\",\n          \"mappings\": [\"&amp;=&gt; and\"]\n        }\n      },\n      \"filter\": {\n        \"my_stopwords\": {\n          \"type\": \"stop\",\n          \"stopwords\": [\"the\", \"a\"]\n        }\n      },\n      \"analyzer\": {\n        \"my_analyzer\": {\n          \"type\": \"custom\",\n          \"char_filter\": [\"html_strip\", \"&amp;_to_and\"],\n          \"tokenizer\": \"standard\",\n          \"filter\": [\"lowercase\", \"my_stopwords\"]\n        }\n      }\n    }\n  }\n}\n=====测试=====\nGET /my_index/_analyze\n{\n  \"text\": \"tom&amp;jerry are a friend in the house, &lt;a&gt;, HAHA!!\",\n  \"analyzer\": \"my_analyzer\"\n}\n=====结果=====\n{\n  \"tokens\": [\n    {\n      \"token\": \"tomandjerry\",\n      \"start_offset\": 0,\n      \"end_offset\": 9,\n      \"type\": \"&lt;ALPHANUM&gt;\",\n      \"position\": 0\n    },\n    {\n      \"token\": \"are\",\n      \"start_offset\": 10,\n      \"end_offset\": 13,\n      \"type\": \"&lt;ALPHANUM&gt;\",\n      \"position\": 1\n    },\n    {\n      \"token\": \"friend\",\n      \"start_offset\": 16,\n      \"end_offset\": 22,\n      \"type\": \"&lt;ALPHANUM&gt;\",\n      \"position\": 3\n    },\n    {\n      \"token\": \"in\",\n      \"start_offset\": 23,\n      \"end_offset\": 25,\n      \"type\": \"&lt;ALPHANUM&gt;\",\n      \"position\": 4\n    },\n    {\n      \"token\": \"house\",\n      \"start_offset\": 30,\n      \"end_offset\": 35,\n      \"type\": \"&lt;ALPHANUM&gt;\",\n      \"position\": 6\n    },\n    {\n      \"token\": \"haha\",\n      \"start_offset\": 42,\n      \"end_offset\": 46,\n      \"type\": \"&lt;ALPHANUM&gt;\",\n      \"position\": 7\n    }\n  ]\n}</code></pre>\n\n<p><a name=\"doc values（正排索引）\"></a>2、doc values（正排索引）</p>\n\n<p>在建立索引的时候，一方面会建立倒排索引，以供搜索用；一方面会建立正排索引，也就是doc values，以供排序，聚合，过滤等操作使用</p>\n\n<p>doc values是被保存在磁盘上的，此时如果内存足够，os会自动将其缓存在内存中，性能很高；如果内存不足够，os会将其写入磁盘上</p>\n\n<pre>\n示例\ndoc1:&nbsp;{&nbsp;&quot;name&quot;:&nbsp;&quot;jack&quot;,&nbsp;&quot;age&quot;:&nbsp;27&nbsp;}\ndoc2:&nbsp;{&nbsp;&quot;name&quot;:&nbsp;&quot;tom&quot;,&nbsp;&quot;age&quot;:&nbsp;30&nbsp;}\n\n分词，初步的正排索引的建立\ndocument	name		age\n--------------------------------------------\ndoc1		jack		27\ndoc2		tom		30	\n--------------------------------------------\n正排索引最简单的建立的一个过程</pre>\n\n<p>&nbsp;</p>\n\n<p><a name=\"filter执行原理\"></a>3.filter执行原理</p>\n\n<p>filter，仅仅只是按照搜索条件过滤出需要的数据而已，不计算任何相关度分数，对相关度没有任何影响</p>\n\n<p>&lt;1&gt;使用filter在倒排索引中查找搜索串</p>\n\n<p>为每个在倒排索引中搜索到的结果，构建一个bitset，[0, 0, 0, 1, 0, 1]，就是一个二进制的数组，数组每个元素都是0或1，用来标识一个doc对一个filter条件是否匹配，如果匹配就是1，不匹配就是0（用简单的数据结构去实现复杂的功能，可以节省内存空间，提升性能）</p>\n\n<p>遍历每个过滤条件对应的bitset，优先从最稀疏的开始搜索，查找满足所有条件的document</p>\n\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[0, 0, 0, 1, 0, 0]：比较稀疏</p>\n\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[0, 1, 0, 1, 0, 1]</p>\n\n<p>先遍历比较稀疏的bitset，就可以先过滤掉尽可能多的数据,遍历所有的bitset，找到匹配所有filter条件的doc，就可以将document作为结果返回给client了</p>\n\n<p>caching bitset，跟踪query，在最近256个query中超过一定次数的过滤条件，缓存其bitset。对于小segment（&lt;1000，或&lt;3%），不缓存bitset。这样下次如果再有这个条件过来的时候，就不用重新扫描倒排索引，反复生成bitset，可以大幅度提升性能。segment数据量很小，此时哪怕是扫描也很快；segment会在后台自动合并，小segment很快就会跟其他小segment合并成大segment。</p>\n\n<p>filter大部分情况下来说，在query之前执行，先尽量过滤掉尽可能多的数据</p>\n\n<p>如果document有新增或修改，那么cached bitset会被自动更新</p>\n\n<p>以后只要是有相同的filter条件的，会直接来使用这个过滤条件对应的cached bitset</p>\n\n<p>&nbsp;</p>\n\n<p>query，会去计算每个document相对于搜索条件的相关度，并按照相关度进行排序</p>\n\n<p>&nbsp;&nbsp;&nbsp;&nbsp;如果你希望将最匹配搜索条件的数据先返回，那么用query；</p>\n\n<p>&nbsp;&nbsp;&nbsp;&nbsp;如果你不希望一些搜索条件来影响你的document排序，那么使用filter；</p>\n\n<p>filter与query性能</p>\n\n<p>&nbsp;&nbsp;&nbsp;&nbsp;filter，不需要计算相关度分数，不需要按照相关度分数进行排序，同时还有内置的自动cache最常使用filter的数据</p>\n\n<p>&nbsp;&nbsp;&nbsp;&nbsp;query，相反，要计算相关度分数，按照分数进行排序，而且无法cache结果</p>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n', 2, '5f199b8885e24fc8b28672b872edb606', 12, '2017-07-12 18:04:43', '2018-09-06 16:12:58');
INSERT INTO `logcontent` VALUES ('8e8fe8a9aa204345b1067367b4c9f12e', -1, '7338e53acd514defa1a17e47016f3f4a', 'Elasticsearch（十四）', 'multi-index,multi-type', '一次性搜索多个index和多个type下的数据/multi-index和multi-type搜索模式', '<p><a name=\"multi-index和multi-type\"></a>multi-index和multi-type搜索模式</p>\n\n<p>如何一次性搜索多个index和多个type下的数据</p>\n\n<p>&nbsp;</p>\n\n<p>/_search：所有索引，所有index下的所有数据都搜索出来</p>\n\n<p>/index1/_search：指定一个index，搜索其下所有type的数据</p>\n\n<p>/index1,index2/_search：同时搜索两个index下的数据</p>\n\n<p>/*1,*2/_search：按照通配符去匹配多个索引</p>\n\n<p>/index1/type1/_search：搜索指定index下指定type的数据</p>\n\n<p>/index1/type1,type2/_search：可以搜索指定index下多个type的数据</p>\n\n<p>/index1,index2/type1,type2/_search：搜索多个index下的多个type的数据</p>\n\n<p>/_all/type1,type2/_search：_all，可以代表搜索所有index下的指定type的数据</p>\n\n<p>&nbsp;</p>\n', 2, '5f199b8885e24fc8b28672b872edb606', 4, '2017-07-10 18:04:37', '2018-09-06 16:12:58');
INSERT INTO `logcontent` VALUES ('946bffaab47b49a89f7e55e2ee2641b7', -1, 'd7caeba238f0466d87db109b2b9724da', '机器学习中的基本数学知识', '数学', '初学', '<p><a href=\"https://www.cnblogs.com/steven-yang/p/6348112.html\">https://www.cnblogs.com/steven-yang/p/6348112.html</a></p>\n', 1, '5f199b8885e24fc8b28672b872edb606', 6, '2017-12-02 21:35:32', '2018-08-31 22:21:23');
INSERT INTO `logcontent` VALUES ('a03c68ab2ee548e684bec3f95a1e9751', -1, '4eca926aa543420baea2f03f506d042e', 'Mysql误删除root用户', 'mysql,误删除', '空', '<p><a id=\"Linux环境\" name=\"Linux环境\"></a>Linux环境：</p>\n\n<pre>\n<code class=\"language-bash\">Ubuntu 12.04.5 LTS \\n \\l</code></pre>\n\n<p><a id=\"Mysql版本\" name=\"Mysql版本\"></a>Mysql版本：</p>\n\n<pre>\n<code class=\"language-bash\">Ver 14.14 Distrib 5.5.54, for debian-linux-gnu (x86_64) using readline 6.2</code></pre>\n\n<p><a id=\"原因\" name=\"原因\"></a>原因：</p>\n\n<pre>\n<code class=\"language-markdown\">对mysql库中的user表进行了delete from user（并未Drop表），造成所有用户的丢失</code></pre>\n\n<p><a id=\"影响\" name=\"影响\"></a>问题：</p>\n\n<pre>\n<code class=\"language-markdown\">登录不了数据库，数据库里面的数据很重要</code></pre>\n\n<p><a id=\"救援措施\" name=\"救援措施\"></a>救援措施：</p>\n\n<pre>\n<code class=\"language-markdown\">（1）停止mysql服务\n/etc/init.d/mysqld stop\n\n（2）进入mysql的安装目录下的bin目录\n如果找不到目录使用下面的命令\nfind / -name mysqld_safe\n======结果=======\n/alidata/server/mysql-5.5.37/bin/mysqld_safe\n注：如果没得到类似这个的结果，下面就不用看了，因为下面的命令都依赖于mysqld_safe文件，mysql的版本不同，不晓得文件是否相同，\n可以搜索mysql安全模式\n（3）进入bin目录(路径复制上面的结果路径)\ncd /alidata/server/mysql-5.5.37/bin/\n（4）进入mysql安全模式\nmysqld_safe --skip-grant-tables &amp;\n======结果=======\n命令会卡在那里，不会执行完\n（5）重新开启一个shell窗口，使用下列命令登陆mysql，无需密码\nmysql\n（6）操作\n查看所有数据库（应该能看到mysql，否则失败）\nshow databases;\n切换数据库到mysql\nuse mysql;\n插入root用户\ninsert into user values(\'localhost\',\'root\',\'123\',\'Y\',\'Y\',\'Y\',\'Y\',\'Y\',\'Y\',\'Y\',\'Y\',\'Y\',\'Y\',\'Y\',\'Y\',\'Y\',\'Y\',\'Y\',\'Y\',\n\'Y\',\'Y\',\'Y\',\'Y\',\'Y\',\'Y\',\'Y\',\'Y\',\'Y\',\'Y\',\'Y\',\'Y\',\'Y\',\'\',\'\',\'\',\'\',0,0,0,0,\'\',\'Y\');\n修改密码（密码是加密的，使用下面方式进行加密）\nupdate user set password=passworD(\"test\") where user=\'root\';\n更新设置\nflush privileges;\n退出数据库\nquit\n完事儿</code></pre>\n\n<p>&nbsp;</p>\n', 1, '5f199b8885e24fc8b28672b872edb606', 7, '2017-07-12 19:01:15', '2018-08-31 22:21:23');
INSERT INTO `logcontent` VALUES ('a38e990cc46f4adbbba1c8e327314b91', -1, '7338e53acd514defa1a17e47016f3f4a', 'Elasticsearch（九）', '并发冲突剖析,悲观锁,乐观锁,Elasticsearch乐观锁', 'Elasticsearch并发冲突问题/悲观锁与乐观锁两种并发控制方案/Elasticsearch内部如何基于_version进行乐观锁并发控制', '<p><a name=\"并发冲突剖析\"></a>1.剖析Elasticsearch并发冲突问题</p>\n\n<p>普通的Es操作流程</p>\n\n<p>（1）先get document数据，显示到网页上，同时在内存中缓存该document的数据</p>\n\n<p>（2）当网页发生修改请求时，会直接基于内存中的数据进行计算和操作</p>\n\n<p>（3）将计算结果写回Es</p>\n\n<p>这个流程会导致数据不准确，并发操作Es的线程越多，或者并发请求越多，不准确的可能性就越大</p>\n\n<p>&nbsp;</p>\n\n<p><a name=\"悲观锁与乐观锁\"></a>2.悲观锁与乐观锁两种并发控制方案</p>\n\n<p>（1）悲观锁并发控制方案</p>\n\n<p>常见于关系型数据库中</p>\n\n<p>每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会block直到它拿到锁。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。</p>\n\n<p>（2）乐观锁并发控制方案</p>\n\n<p>Es中</p>\n\n<p>每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号等机制。乐观锁适用于多读的应用类型，这样可以提高吞吐量。</p>\n\n<p>&nbsp;</p>\n\n<p><a name=\"Elasticsearch乐观锁\"></a>3.Elasticsearch内部如何基于_version进行乐观锁并发控制</p>\n\n<p>&nbsp;</p>\n\n<p>（1）_version元数据</p>\n\n<pre>\n<code class=\"language-json\">PUT /test_index/test_type/6\n{\n    \"test_field\":\"test test\"\n}\n\n{\n    \"_index\":\"test_index\",\n    \"_type\":\"test_type\",\n    \"_id\":\"6\",\n    \"_version\":1,\n    \"result\":\"created\",\n    \"_shards\":{\n        \"total\":2,\n        \"successful\":1,\n        \"failed\":0\n    },\n    \"created\":true\n}</code></pre>\n\n<p>第一次创建一个document的时候，它的_version内部版本号就是1；以后，每次对这个document执行修改或者删除操作，都会对这个_version版本号自动加1；哪怕是删除，也会对这条数据的版本号加1，可以从一个侧面证明，它不是立即物理删除掉的，因为它的一些版本号等信息还是保留着的。先删除一条document，再重新创建这条document，其实会在delete version基础之上，再把version号加1</p>\n\n<p>&nbsp;</p>\n\n<p>（2）修改时版本号不同报错</p>\n\n<pre>\n<code class=\"language-json\">{\n    \"error\":{\n        \"root_cause\":[\n            {\n                \"type\":\"version_conflict_engine_exception\",\n                \"reason\":\"[test_type][7]: version conflict, current version [2] is different than the one provided [1]\",\n                \"index_uuid\":\"6m0G7yx7R1KECWWGnfH1sw\",\n                \"shard\":\"3\",\n                \"index\":\"test_index\"\n            }\n        ],\n        \"type\":\"version_conflict_engine_exception\",\n        \"reason\":\"[test_type][7]: version conflict, current version [2] is different than the one provided [1]\",\n        \"index_uuid\":\"6m0G7yx7R1KECWWGnfH1sw\",\n        \"shard\":\"3\",\n        \"index\":\"test_index\"\n    },\n    \"status\":409\n}\n</code></pre>\n\n<p>（3）基于external version进行乐观锁并发控制</p>\n\n<p>Es提供了一个feature，你可以不用它提供的内部_version版本号来进行并发控制，可以基于你自己维护的一个版本号来进行并发控制。这个时候，你进行乐观锁并发控制的时候，可能并不是想要用es内部的_version来进行控制，而是用你自己维护的那个version来进行控制。</p>\n\n<p>使用内部_version来进行控制时为&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp;?version=1</p>\n\n<p>使用非内部_version来进行控制时为&nbsp;&nbsp;&nbsp;&nbsp;?version=1&amp;version_type=external</p>\n\n<p>唯一的区别在于，_version，只有当你提供的version与es中的_version相同的时候，才可以进行修改，只要不一样，就报错；当version_type=external的时候，只有当你提供的version比es中的_version大的时候，才能完成修改</p>\n\n<p>es，_version=1，?version=1，更新成功</p>\n\n<p>es，_version=1，?version&gt;1&amp;version_type=external，才能成功，比如说?version=2&amp;version_type=external</p>\n\n<pre>\n<code class=\"language-json\">（1）先构造一条数据\n\nPUT /test_index/test_type/8\n{\n  \"test_field\": \"test\"\n}\n\n{\n  \"_index\": \"test_index\",\n  \"_type\": \"test_type\",\n  \"_id\": \"8\",\n  \"_version\": 1,\n  \"result\": \"created\",\n  \"_shards\": {\n    \"total\": 2,\n    \"successful\": 1,\n    \"failed\": 0\n  },\n  \"created\": true\n}\n\n（2）进行修改\n\nPUT /test_index/test_type/8?version=2&amp;version_type=external\n{\n  \"test_field\": \"test client 1\"\n}\n\n{\n  \"_index\": \"test_index\",\n  \"_type\": \"test_type\",\n  \"_id\": \"8\",\n  \"_version\": 2,\n  \"result\": \"updated\",\n  \"_shards\": {\n    \"total\": 2,\n    \"successful\": 1,\n    \"failed\": 0\n  },\n  \"created\": false\n}\n修改成功--&gt;_version=1，?version&gt;1&amp;version_type=external，才能成功</code></pre>\n\n<p>&nbsp;</p>\n', 2, '5f199b8885e24fc8b28672b872edb606', 6, '2017-07-12 18:04:21', '2018-09-06 16:12:58');
INSERT INTO `logcontent` VALUES ('a3d40fcfb2714fe493e362e82721fdcc', -1, '7338e53acd514defa1a17e47016f3f4a', '九、索引管理', '索引管理', '索引管理', '<h2>索引管理</h2>\n\n<p>我们已经看到 Elasticsearch 让开发一个新的应用变得简单，不需要任何预先计划或设置。 不过，要不了多久你就会开始想要优化索引和搜索过程，以便更好地适合您的特定用例。 这些定制几乎围绕着索引和类型的方方面面，在本章，我们将介绍管理索引和类型映射的 API 以及一些最重要的设置。</p>\n\n<h2>创建一个索引</h2>\n\n<p>到目前为止, 我们已经通过索引一篇文档创建了一个新的索引&nbsp;。这个索引采用的是默认的配置，新的字段通过动态映射的方式被添加到类型映射。现在我们需要对这个建立索引的过程做更多的控制：我们想要确保这个索引有数量适中的主分片，并且在我们索引任何数据&nbsp;<em>之前</em>&nbsp;，分析器和映射已经被建立好。</p>\n\n<p>为了达到这个目的，我们需要手动创建索引，在请求体里面传入设置或类型映射，如下所示：</p>\n\n<pre>\nPUT /my_index\n{\n    &quot;settings&quot;: { ... any settings ... },\n    &quot;mappings&quot;: {\n        &quot;type_one&quot;: { ... any mappings ... },\n        &quot;type_two&quot;: { ... any mappings ... },\n        ...\n    }\n}</pre>\n\n<p>如果你想禁止自动创建索引，你&nbsp;可以通过在&nbsp;<code>config/elasticsearch.yml</code>&nbsp;的每个节点下添加下面的配置：</p>\n\n<pre>\naction.auto_create_index: false</pre>\n\n<p><img alt=\"注意\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/note.png\" /></p>\n\n<p>我们会在之后讨论你怎么用&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/index-templates.html\" title=\"索引模板\">索引模板</a>&nbsp;来预配置开启自动创建索引。这在索引日志数据的时候尤其有用：你将日志数据索引在一个以日期结尾命名的索引上，子夜时分，一个预配置的新索引将会自动进行创建。</p>\n\n<h2>删除一个索引</h2>\n\n<p>用以下的请求来&nbsp;删除索引:</p>\n\n<pre>\nDELETE /my_index</pre>\n\n<p>你也可以这样删除多个索引：</p>\n\n<pre>\nDELETE /index_one,index_two\nDELETE /index_*</pre>\n\n<p>你甚至可以这样删除&nbsp;<em>全部</em>&nbsp;索引：</p>\n\n<pre>\nDELETE /_all\nDELETE /*</pre>\n\n<p><img alt=\"注意\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/note.png\" /></p>\n\n<p>对一些人来说，能够用单个命令来删除所有数据可能会导致可怕的后果。如果你想要避免意外的大量删除, 你可以在你的&nbsp;<code>elasticsearch.yml</code>&nbsp;做如下配置：</p>\n\n<p><code>action.destructive_requires_name: true</code></p>\n\n<p>这个设置使删除只限于特定名称指向的数据, 而不允许通过指定&nbsp;<code>_all</code>&nbsp;或通配符来删除指定索引库。你同样可以通过&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/_changing_settings_dynamically.html\" title=\"动态变更设置\">Cluster State API</a>&nbsp;动态的更新这个设置。</p>\n\n<h2>索引设置</h2>\n\n<p>你可以通过修改配置来自定义索引行为，详细配置参照&nbsp;<a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/5.6/index-modules.html\" target=\"_top\">索引模块</a></p>\n\n<p><img alt=\"提示\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/tip.png\" /></p>\n\n<p>Elasticsearch 提供了优化好的默认配置。 除非你理解这些配置的作用并且知道为什么要去修改，否则不要随意修改。</p>\n\n<p>下面是两个&nbsp;最重要的设置：</p>\n\n<p><code>number_of_shards</code></p>\n\n<p>每个索引的主分片数，默认值是&nbsp;<code>5</code>&nbsp;。这个配置在索引创建后不能修改。</p>\n\n<p><code>number_of_replicas</code></p>\n\n<p>每个主分片的副本数，默认值是&nbsp;<code>1</code>&nbsp;。对于活动的索引库，这个配置可以随时修改。</p>\n\n<p>例如，我们可以创建只有&nbsp;一个主分片，没有副本的小索引：</p>\n\n<pre>\nPUT /my_temp_index\n{\n    &quot;settings&quot;: {\n        &quot;number_of_shards&quot; :   1,\n        &quot;number_of_replicas&quot; : 0\n    }\n}\n</pre>\n\n<p>然后，我们可以用&nbsp;<code>update-index-settings</code>&nbsp;API&nbsp;动态修改副本数：</p>\n\n<pre>\nPUT /my_temp_index/_settings\n{\n    &quot;number_of_replicas&quot;: 1\n}</pre>\n\n<h2>配置分析器</h2>\n\n<p>第三个重要的索引设置是&nbsp;<code>analysis</code>&nbsp;部分，&nbsp;用来配置已存在的分析器或针对你的索引创建新的自定义分析器。</p>\n\n<p>在&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/analysis-intro.html\" title=\"分析与分析器\">分析与分析器</a>&nbsp;，我们介绍了一些内置的&nbsp;分析器，用于将全文字符串转换为适合搜索的倒排索引。</p>\n\n<p><code>standard</code>&nbsp;分析器是用于全文字段的默认分析器，&nbsp;对于大部分西方语系来说是一个不错的选择。&nbsp;它包括了以下几点：</p>\n\n<ul style=\"list-style-type:disc\">\n	<li><code>standard</code>&nbsp;分词器，通过单词边界分割输入的文本。</li>\n	<li><code>standard</code>&nbsp;语汇单元过滤器，目的是整理分词器触发的语汇单元（但是目前什么都没做）。</li>\n	<li><code>lowercase</code>&nbsp;语汇单元过滤器，转换所有的语汇单元为小写。</li>\n	<li><code>stop</code>&nbsp;语汇单元过滤器，删除停用词--对搜索相关性影响不大的常用词，如&nbsp;<code>a</code>&nbsp;，&nbsp;<code>the</code>&nbsp;，&nbsp;<code>and</code>&nbsp;，&nbsp;<code>is</code>。</li>\n</ul>\n\n<p>默认情况下，停用词过滤器是被禁用的。如需启用它，你可以通过创建一个基于&nbsp;<code>standard</code>&nbsp;分析器的自定义分析器并设置&nbsp;<code>stopwords</code>&nbsp;参数。&nbsp;可以给分析器提供一个停用词列表，或者告知使用一个基于特定语言的预定义停用词列表。</p>\n\n<p>在下面的例子中，我们创建了一个新的分析器，叫做&nbsp;<code>es_std</code>&nbsp;， 并使用预定义的&nbsp;西班牙语停用词列表：</p>\n\n<pre>\nPUT /spanish_docs\n{\n    &quot;settings&quot;: {\n        &quot;analysis&quot;: {\n            &quot;analyzer&quot;: {\n                &quot;es_std&quot;: {\n                    &quot;type&quot;:      &quot;standard&quot;,\n                    &quot;stopwords&quot;: &quot;_spanish_&quot;\n                }\n            }\n        }\n    }\n}\n</pre>\n\n<p><code>es_std</code>&nbsp;分析器不是全局的--它仅仅存在于我们定义的&nbsp;<code>spanish_docs</code>&nbsp;索引中。 为了使用&nbsp;<code>analyze</code>&nbsp;API来对它进行测试，我们必须使用特定的索引名：</p>\n\n<pre>\nGET /spanish_docs/_analyze?analyzer=es_std\nEl veloz zorro marr&oacute;n\n</pre>\n\n<p>简化的结果显示西班牙语停用词&nbsp;<code>El</code>&nbsp;已被正确的移除：</p>\n\n<pre>\n{\n  &quot;tokens&quot; : [\n    { &quot;token&quot; :    &quot;veloz&quot;,   &quot;position&quot; : 2 },\n    { &quot;token&quot; :    &quot;zorro&quot;,   &quot;position&quot; : 3 },\n    { &quot;token&quot; :    &quot;marr&oacute;n&quot;,  &quot;position&quot; : 4 }\n  ]\n}</pre>\n\n<h2>自定义分析器</h2>\n\n<p>虽然Elasticsearch带有一些现成的分析器，然而在分析器上Elasticsearch真正的强大之处在于，你可以通过在一个适合你的特定数据的设置之中组合字符过滤器、分词器、词汇单元过滤器来创建自定义的分析器。</p>\n\n<p>在&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/analysis-intro.html\" title=\"分析与分析器\">分析与分析器</a>&nbsp;我们说过，一个&nbsp;<em>分析器</em>&nbsp;就是在一个包里面组合了三种函数的一个包装器，&nbsp;三种函数按照顺序被执行:</p>\n\n<p>字符过滤器</p>\n\n<p>字符过滤器&nbsp;用来&nbsp;<code>整理</code>&nbsp;一个尚未被分词的字符串。例如，如果我们的文本是HTML格式的，它会包含像<code>&lt;p&gt;</code>&nbsp;或者&nbsp;<code>&lt;div&gt;</code>&nbsp;这样的HTML标签，这些标签是我们不想索引的。我们可以使用&nbsp;<a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/5.6/analysis-htmlstrip-charfilter.html\" target=\"_top\"><code>html清除</code>&nbsp;字符过滤器</a>来移除掉所有的HTML标签，并且像把&nbsp;<code>&amp;Aacute;</code>&nbsp;转换为相对应的Unicode字符&nbsp;<code>&Aacute;</code>&nbsp;这样，转换HTML实体。</p>\n\n<p>一个分析器可能有0个或者多个字符过滤器。</p>\n\n<p>分词器</p>\n\n<p>一个分析器&nbsp;<em>必须</em>&nbsp;有一个唯一的分词器。&nbsp;分词器把字符串分解成单个词条或者词汇单元。&nbsp;<code>标准</code>&nbsp;分析器里使用的&nbsp;<a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/5.6/analysis-standard-tokenizer.html\" target=\"_top\"><code>标准</code>&nbsp;分词器</a>&nbsp;把一个字符串根据单词边界分解成单个词条，并且移除掉大部分的标点符号，然而还有其他不同行为的分词器存在。</p>\n\n<p>例如，&nbsp;<a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/5.6/analysis-keyword-tokenizer.html\" target=\"_top\"><code>关键词</code>&nbsp;分词器</a>&nbsp;完整地输出&nbsp;接收到的同样的字符串，并不做任何分词。&nbsp;<a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/5.6/analysis-whitespace-tokenizer.html\" target=\"_top\"><code>空格</code>&nbsp;分词器</a>&nbsp;只根据空格分割文本&nbsp;。&nbsp;<a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/5.6/analysis-pattern-tokenizer.html\" target=\"_top\"><code>正则</code>&nbsp;分词器</a>&nbsp;根据匹配正则表达式来分割文本&nbsp;。</p>\n\n<p>词单元过滤器</p>\n\n<p>经过分词，作为结果的&nbsp;<em>词单元流</em>&nbsp;会按照指定的顺序通过指定的词单元过滤器&nbsp;。</p>\n\n<p>词单元过滤器可以修改、添加或者移除词单元。我们已经提到过&nbsp;<a href=\"http://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lowercase-tokenizer.html\" target=\"_top\"><code>lowercase</code>&nbsp;</a>和&nbsp;<a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/5.6/analysis-stop-tokenfilter.html\" target=\"_top\"><code>stop</code>&nbsp;词过滤器</a>&nbsp;，但是在 Elasticsearch 里面还有很多可供选择的词单元过滤器。&nbsp;<a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/5.6/analysis-stemmer-tokenfilter.html\" target=\"_top\">词干过滤器</a>&nbsp;把单词&nbsp;<code>遏制</code>&nbsp;为&nbsp;词干。<a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/5.6/analysis-asciifolding-tokenfilter.html\" target=\"_top\"><code>ascii_folding</code>&nbsp;过滤器</a>移除变音符，把一个像&nbsp;<code>&quot;tr&egrave;s&quot;</code>&nbsp;这样的词转换为&nbsp;<code>&quot;tres&quot;</code>&nbsp;。&nbsp;<a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/5.6/analysis-ngram-tokenfilter.html\" target=\"_top\"><code>ngram</code></a>&nbsp;和<a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/5.6/analysis-edgengram-tokenfilter.html\" target=\"_top\"><code>edge_ngram</code>&nbsp;词单元过滤器</a>&nbsp;可以产生&nbsp;适合用于部分匹配或者自动补全的词单元。</p>\n\n<p>在&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/search-in-depth.html\" title=\"深入搜索\">深入搜索</a>，我们讨论了在哪里使用，以及怎样使用分词器和过滤器。但是首先，我们需要解释一下怎样创建自定义的分析器。</p>\n\n<h3>创建一个自定义分析器</h3>\n\n<p>和我们之前配置&nbsp;<code>es_std</code>&nbsp;分析器一样，我们可以在&nbsp;<code>analysis</code>&nbsp;下的相应位置设置字符过滤器、分词器和词单元过滤器:</p>\n\n<pre>\nPUT /my_index\n{\n    &quot;settings&quot;: {\n        &quot;analysis&quot;: {\n            &quot;char_filter&quot;: { ... custom character filters ... },\n            &quot;tokenizer&quot;:   { ...    custom tokenizers     ... },\n            &quot;filter&quot;:      { ...   custom token filters   ... },\n            &quot;analyzer&quot;:    { ...    custom analyzers      ... }\n        }\n    }\n}</pre>\n\n<p>作为示范，让我们一起来创建一个自定义分析器吧，这个分析器可以做到下面的这些事:</p>\n\n<ol style=\"list-style-type:decimal\">\n	<li>使用&nbsp;<code>html清除</code>&nbsp;字符过滤器移除HTML部分。</li>\n	<li>\n	<p>使用一个自定义的&nbsp;<code>映射</code>&nbsp;字符过滤器把&nbsp;<code>&amp;</code>&nbsp;替换为&nbsp;<code>&quot; and &quot;</code>&nbsp;：</p>\n\n	<pre>\n&quot;char_filter&quot;: {\n    &quot;&amp;_to_and&quot;: {\n        &quot;type&quot;:       &quot;mapping&quot;,\n        &quot;mappings&quot;: [ &quot;&amp;=&gt; and &quot;]\n    }\n}</pre>\n	</li>\n	<li>使用&nbsp;<code>标准</code>&nbsp;分词器分词。</li>\n	<li>小写词条，使用&nbsp;<code>小写</code>&nbsp;词过滤器处理。</li>\n	<li>\n	<p>使用自定义&nbsp;<code>停止</code>&nbsp;词过滤器移除自定义的停止词列表中包含的词：</p>\n\n	<pre>\n&quot;filter&quot;: {\n    &quot;my_stopwords&quot;: {\n        &quot;type&quot;:        &quot;stop&quot;,\n        &quot;stopwords&quot;: [ &quot;the&quot;, &quot;a&quot; ]\n    }\n}</pre>\n	</li>\n</ol>\n\n<p>我们的分析器定义用我们之前已经设置好的自定义过滤器组合了已经定义好的分词器和过滤器：</p>\n\n<pre>\n&quot;analyzer&quot;: {\n    &quot;my_analyzer&quot;: {\n        &quot;type&quot;:           &quot;custom&quot;,\n        &quot;char_filter&quot;:  [ &quot;html_strip&quot;, &quot;&amp;_to_and&quot; ],\n        &quot;tokenizer&quot;:      &quot;standard&quot;,\n        &quot;filter&quot;:       [ &quot;lowercase&quot;, &quot;my_stopwords&quot; ]\n    }\n}</pre>\n\n<p>汇总起来，完整的&nbsp;<code>创建索引</code>&nbsp;请求&nbsp;看起来应该像这样：</p>\n\n<pre>\nPUT /my_index\n{\n    &quot;settings&quot;: {\n        &quot;analysis&quot;: {\n            &quot;char_filter&quot;: {\n                &quot;&amp;_to_and&quot;: {\n                    &quot;type&quot;:       &quot;mapping&quot;,\n                    &quot;mappings&quot;: [ &quot;&amp;=&gt; and &quot;]\n            }},\n            &quot;filter&quot;: {\n                &quot;my_stopwords&quot;: {\n                    &quot;type&quot;:       &quot;stop&quot;,\n                    &quot;stopwords&quot;: [ &quot;the&quot;, &quot;a&quot; ]\n            }},\n            &quot;analyzer&quot;: {\n                &quot;my_analyzer&quot;: {\n                    &quot;type&quot;:         &quot;custom&quot;,\n                    &quot;char_filter&quot;:  [ &quot;html_strip&quot;, &quot;&amp;_to_and&quot; ],\n                    &quot;tokenizer&quot;:    &quot;standard&quot;,\n                    &quot;filter&quot;:       [ &quot;lowercase&quot;, &quot;my_stopwords&quot; ]\n            }}\n}}}\n</pre>\n\n<p>索引被创建以后，使用&nbsp;<code>analyze</code>&nbsp;API 来&nbsp;测试这个新的分析器：</p>\n\n<pre>\nGET /my_index/_analyze?analyzer=my_analyzer\nThe quick &amp; brown fox\n</pre>\n\n<p>下面的缩略结果展示出我们的分析器正在正确地运行：</p>\n\n<pre>\n{\n  &quot;tokens&quot; : [\n      { &quot;token&quot; :   &quot;quick&quot;,    &quot;position&quot; : 2 },\n      { &quot;token&quot; :   &quot;and&quot;,      &quot;position&quot; : 3 },\n      { &quot;token&quot; :   &quot;brown&quot;,    &quot;position&quot; : 4 },\n      { &quot;token&quot; :   &quot;fox&quot;,      &quot;position&quot; : 5 }\n    ]\n}</pre>\n\n<p>这个分析器现在是没有多大用处的，除非我们告诉&nbsp;Elasticsearch在哪里用上它。我们可以像下面这样把这个分析器应用在一个&nbsp;<code>string</code>&nbsp;字段上：</p>\n\n<pre>\nPUT /my_index/_mapping/my_type\n{\n    &quot;properties&quot;: {\n        &quot;title&quot;: {\n            &quot;type&quot;:      &quot;string&quot;,\n            &quot;analyzer&quot;:  &quot;my_analyzer&quot;\n        }\n    }\n}</pre>\n\n<h2>类型和映射</h2>\n\n<p><em>类型</em>&nbsp;在 Elasticsearch 中表示一类相似的文档。&nbsp;类型由&nbsp;<em>名称</em>&nbsp;&mdash;比如&nbsp;<code>user</code>&nbsp;或&nbsp;<code>blogpost</code>&nbsp;&mdash;和&nbsp;<em>映射</em>&nbsp;组成。</p>\n\n<p><em>映射</em>,&nbsp;就像数据库中的 schema ，描述了文档可能具有的字段或&nbsp;<em>属性</em>&nbsp;、&nbsp;每个字段的数据类型&mdash;比如<code>string</code>,&nbsp;<code>integer</code>&nbsp;或&nbsp;<code>date</code>&nbsp;&mdash;以及Lucene是如何索引和存储这些字段的。</p>\n\n<p>类型可以很好的抽象划分相似但不相同的数据。但由于 Lucene 的处理方式，类型的使用有些限制。</p>\n\n<h3>Lucene 如何处理文档</h3>\n\n<p>在 Lucene 中，一个文档由一组简单的键值对组成。&nbsp;每个字段都可以有多个值，但至少要有一个值。 类似的，一个字符串可以通过分析过程转化为多个值。Lucene 不关心这些值是字符串、数字或日期--所有的值都被当做&nbsp;<em>不透明字节</em>&nbsp;。</p>\n\n<p>当我们在 Lucene 中索引一个文档时，每个字段的值都被添加到相关字段的倒排索引中。你也可以将未处理的原始数据&nbsp;<em>存储</em>&nbsp;起来，以便这些原始数据在之后也可以被检索到。</p>\n\n<h3>类型是如何实现的</h3>\n\n<p>Elasticsearch 类型是&nbsp;以 Lucene 处理文档的这个方式为基础来实现的。一个索引可以有多个类型，这些类型的文档可以存储在相同的索引中。</p>\n\n<p>Lucene 没有文档类型的概念，每个文档的类型名被存储在一个叫&nbsp;<code>_type</code>&nbsp;的元数据字段上。&nbsp;当我们要检索某个类型的文档时, Elasticsearch 通过在&nbsp;<code>_type</code>&nbsp;字段上使用过滤器限制只返回这个类型的文档。</p>\n\n<p>Lucene 也没有映射的概念。&nbsp;映射是 Elasticsearch 将复杂 JSON 文档&nbsp;<em>映射</em>&nbsp;成 Lucene 需要的扁平化数据的方式。</p>\n\n<p>例如，在&nbsp;<code>user</code>&nbsp;类型中，&nbsp;<code>name</code>&nbsp;字段的映射可以声明这个字段是&nbsp;<code>string</code>&nbsp;类型，并且它的值被索引到名叫<code>name</code>&nbsp;的倒排索引之前，需要通过&nbsp;<code>whitespace</code>&nbsp;分词器分析：</p>\n\n<pre>\n&quot;name&quot;: {\n    &quot;type&quot;:     &quot;string&quot;,\n    &quot;analyzer&quot;: &quot;whitespace&quot;\n}</pre>\n\n<h3>避免类型陷阱</h3>\n\n<p>这导致了一个有趣的思想实验： 如果有两个不同的类型，每个类型都有同名的字段，但映射不同（例如：一个是字符串一个是数字），将会出现什么情况？</p>\n\n<p>简单回答是，Elasticsearch 不会允许你定义这个映射。当你配置这个映射时，将会出现异常。</p>\n\n<p>详细回答是，每个 Lucene 索引中的所有字段都包含一个单一的、扁平的模式。一个特定字段可以映射成 string 类型也可以是 number 类型，但是不能两者兼具。因为类型是 Elasticsearch 添加的&nbsp;<em>优于</em>&nbsp;Lucene 的额外机制（以元数据&nbsp;<code>_type</code>&nbsp;字段的形式），在 Elasticsearch 中的所有类型最终都共享相同的映射。</p>\n\n<p>以&nbsp;<code>data</code>&nbsp;索引中两种类型的映射为例：</p>\n\n<pre>\n{\n   &quot;data&quot;: {\n      &quot;mappings&quot;: {\n         &quot;people&quot;: {\n            &quot;properties&quot;: {\n               &quot;name&quot;: {\n                  &quot;type&quot;: &quot;string&quot;,\n               },\n               &quot;address&quot;: {\n                  &quot;type&quot;: &quot;string&quot;\n               }\n            }\n         },\n         &quot;transactions&quot;: {\n            &quot;properties&quot;: {\n               &quot;timestamp&quot;: {\n                  &quot;type&quot;: &quot;date&quot;,\n                  &quot;format&quot;: &quot;strict_date_optional_time&quot;\n               },\n               &quot;message&quot;: {\n                  &quot;type&quot;: &quot;string&quot;\n               }\n            }\n         }\n      }\n   }\n}</pre>\n\n<p>每个类型定义两个字段 (分别是&nbsp;<code>&quot;name&quot;</code>/<code>&quot;address&quot;</code>&nbsp;和&nbsp;<code>&quot;timestamp&quot;</code>/<code>&quot;message&quot;</code>&nbsp;)。它们看起来是相互独立的，但在后台 Lucene 将创建一个映射，如:</p>\n\n<pre>\n{\n   &quot;data&quot;: {\n      &quot;mappings&quot;: {\n        &quot;_type&quot;: {\n          &quot;type&quot;: &quot;string&quot;,\n          &quot;index&quot;: &quot;not_analyzed&quot;\n        },\n        &quot;name&quot;: {\n          &quot;type&quot;: &quot;string&quot;\n        }\n        &quot;address&quot;: {\n          &quot;type&quot;: &quot;string&quot;\n        }\n        &quot;timestamp&quot;: {\n          &quot;type&quot;: &quot;long&quot;\n        }\n        &quot;message&quot;: {\n          &quot;type&quot;: &quot;string&quot;\n        }\n      }\n   }\n}</pre>\n\n<p><em>注: 这不是真实有效的映射语法，只是用于演示</em></p>\n\n<p>对于整个索引，映射在本质上被&nbsp;<em>扁平化</em>&nbsp;成一个单一的、全局的模式。这就是为什么两个类型不能定义冲突的字段：当映射被扁平化时，Lucene 不知道如何去处理。</p>\n\n<h3>类型结论</h3>\n\n<p>那么，这个讨论的结论是什么？技术上讲，多个类型可以在相同的索引中存在，只要它们的字段不冲突（要么因为字段是互为独占模式，要么因为它们共享相同的字段）。</p>\n\n<p>重要的一点是: 类型可以很好的区分同一个集合中的不同细分。在不同的细分中数据的整体模式是相同的（或相似的）。</p>\n\n<p>类型不适合&nbsp;<em>完全不同类型的数据</em>&nbsp;。如果两个类型的字段集是互不相同的，这就意味着索引中将有一半的数据是空的（字段将是&nbsp;<em>稀疏的</em>&nbsp;），最终将导致性能问题。在这种情况下，最好是使用两个单独的索引。</p>\n\n<p>总结：</p>\n\n<ul style=\"list-style-type:disc\">\n	<li><strong>正确:</strong>&nbsp;将&nbsp;<code>kitchen</code>&nbsp;和&nbsp;<code>lawn-care</code>&nbsp;类型放在&nbsp;<code>products</code>&nbsp;索引中, 因为这两种类型基本上是相同的模式</li>\n	<li><strong>错误:</strong>&nbsp;将&nbsp;<code>products</code>&nbsp;和&nbsp;<code>logs</code>&nbsp;类型放在&nbsp;<code>data</code>&nbsp;索引中, 因为这两种类型互不相同。应该将它们放在不同的索引中。</li>\n</ul>\n\n<h2>根对象</h2>\n\n<p>映射的最高一层被称为&nbsp;<em>根对象</em>&nbsp;，它可能包含下面几项：</p>\n\n<ul style=\"list-style-type:disc\">\n	<li>一个&nbsp;<em>properties</em>&nbsp;节点，列出了文档中可能包含的每个字段的映射</li>\n	<li>各种元数据字段，它们都以一个下划线开头，例如&nbsp;<code>_type</code>&nbsp;、&nbsp;<code>_id</code>&nbsp;和&nbsp;<code>_source</code></li>\n	<li>设置项，控制如何动态处理新的字段，例如&nbsp;<code>analyzer</code>&nbsp;、&nbsp;<code>dynamic_date_formats</code>&nbsp;和<code>dynamic_templates</code></li>\n	<li>其他设置，可以同时应用在根对象和其他&nbsp;<code>object</code>&nbsp;类型的字段上，例如&nbsp;<code>enabled</code>&nbsp;、&nbsp;<code>dynamic</code>&nbsp;和<code>include_in_all</code></li>\n</ul>\n\n<h3>属性</h3>\n\n<p>我们已经介绍过文档字段和属性的三个&nbsp;最重要的设置：</p>\n\n<p><code>type</code></p>\n\n<p>字段的数据类型，例如&nbsp;<code>string</code>&nbsp;或&nbsp;<code>date</code></p>\n\n<p><code>index</code></p>\n\n<p>字段是否应当被当成全文来搜索（&nbsp;<code>analyzed</code>&nbsp;），或被当成一个准确的值（&nbsp;<code>not_analyzed</code>&nbsp;），还是完全不可被搜索（&nbsp;<code>no</code>&nbsp;）</p>\n\n<p><code>analyzer</code></p>\n\n<p>确定在索引和搜索时全文字段使用的&nbsp;<code>analyzer</code></p>\n\n<p>我们将在本书的后续部分讨论其他字段类型，例如&nbsp;<code>ip</code>&nbsp;、&nbsp;<code>geo_point</code>&nbsp;和&nbsp;<code>geo_shape</code>&nbsp;。</p>\n\n<h3>元数据: _source 字段</h3>\n\n<p>默认地，Elasticsearch&nbsp;在&nbsp;<code>_source</code>&nbsp;字段存储代表文档体的JSON字符串。和所有被存储的字段一样，<code>_source</code>&nbsp;字段在被写入磁盘之前先会被压缩。</p>\n\n<p>这个字段的存储几乎总是我们想要的，因为它意味着下面的这些：</p>\n\n<ul style=\"list-style-type:disc\">\n	<li>搜索结果包括了整个可用的文档&mdash;&mdash;不需要额外的从另一个的数据仓库来取文档。</li>\n	<li>如果没有&nbsp;<code>_source</code>&nbsp;字段，部分&nbsp;<code>update</code>&nbsp;请求不会生效。</li>\n	<li>当你的映射改变时，你需要重新索引你的数据，有了_source字段你可以直接从Elasticsearch这样做，而不必从另一个（通常是速度更慢的）数据仓库取回你的所有文档。</li>\n	<li>当你不需要看到整个文档时，单个字段可以从&nbsp;<code>_source</code>&nbsp;字段提取和通过&nbsp;<code>get</code>&nbsp;或者&nbsp;<code>search</code>&nbsp;请求返回。</li>\n	<li>调试查询语句更加简单，因为你可以直接看到每个文档包括什么，而不是从一列id猜测它们的内容。</li>\n</ul>\n\n<p>然而，存储&nbsp;<code>_source</code>&nbsp;字段的确要使用磁盘空间。如果上面的原因对你来说没有一个是重要的，你可以用下面的映射禁用&nbsp;<code>_source</code>&nbsp;字段：</p>\n\n<pre>\nPUT /my_index\n{\n    &quot;mappings&quot;: {\n        &quot;my_type&quot;: {\n            &quot;_source&quot;: {\n                &quot;enabled&quot;:  false\n            }\n        }\n    }\n}</pre>\n\n<p>在一个搜索请求里，你可以通过在请求体中指定&nbsp;<code>_source</code>&nbsp;参数，来达到只获取特定的字段的效果：</p>\n\n<pre>\nGET /_search\n{\n    &quot;query&quot;:   { &quot;match_all&quot;: {}},\n    &quot;_source&quot;: [ &quot;title&quot;, &quot;created&quot; ]\n}\n</pre>\n\n<p>这些字段的值会从&nbsp;<code>_source</code>&nbsp;字段被提取和返回，而不是返回整个&nbsp;<code>_source</code>&nbsp;。</p>\n\n<p><strong>Stored Fields 被存储字段</strong></p>\n\n<p>为了之后的检索，除了索引一个字段的值，你&nbsp;还可以选择&nbsp;<code>存储</code>&nbsp;原始字段值。有 Lucene 使用背景的用户使用被存储字段来选择他们想要在搜索结果里面返回的字段。事实上，&nbsp;<code>_source</code>&nbsp;字段就是一个被存储的字段。</p>\n\n<p>在Elasticsearch中，对文档的个别字段设置存储的做法通常不是最优的。整个文档已经被存储为<code>_source</code>&nbsp;字段。使用&nbsp;<code>_source</code>&nbsp;参数提取你需要的字段总是更好的。</p>\n\n<h3>元数据: _all 字段</h3>\n\n<p>在&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/search-lite.html\" title=\"轻量 搜索\"><em>轻量</em>&nbsp;搜索</a>&nbsp;中，我们介绍了&nbsp;<code>_all</code>&nbsp;字段：一个把其它字段值&nbsp;当作一个大字符串来索引的特殊字段。<code>query_string</code>&nbsp;查询子句(搜索&nbsp;<code>?q=john</code>&nbsp;)在没有指定字段时默认使用&nbsp;<code>_all</code>&nbsp;字段。</p>\n\n<p><code>_all</code>&nbsp;字段在新应用的探索阶段，当你还不清楚文档的最终结构时是比较有用的。你可以使用这个字段来做任何查询，并且有很大可能找到需要的文档：</p>\n\n<pre>\nGET /_search\n{\n    &quot;match&quot;: {\n        &quot;_all&quot;: &quot;john smith marketing&quot;\n    }\n}</pre>\n\n<p>随着应用的发展，搜索需求变得更加明确，你会发现自己越来越少使用&nbsp;<code>_all</code>&nbsp;字段。&nbsp;<code>_all</code>&nbsp;字段是搜索的应急之策。通过查询指定字段，你的查询更加灵活、强大，你也可以对相关性最高的搜索结果进行更细粒度的控制。</p>\n\n<p><img alt=\"注意\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/note.png\" /></p>\n\n<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/relevance-intro.html\" title=\"什么是相关性?\">relevance algorithm</a>&nbsp;考虑的一个最重要的原则是字段的长度：字段越短越重要。 在较短的<code>title</code>&nbsp;字段中出现的短语可能比在较长的&nbsp;<code>content</code>&nbsp;字段中出现的短语更加重要。字段长度的区别在&nbsp;<code>_all</code>&nbsp;字段中不会出现。</p>\n\n<p>如果你不再需要&nbsp;<code>_all</code>&nbsp;字段，你可以通过下面的映射来禁用：</p>\n\n<pre>\nPUT /my_index/_mapping/my_type\n{\n    &quot;my_type&quot;: {\n        &quot;_all&quot;: { &quot;enabled&quot;: false }\n    }\n}</pre>\n\n<p>通过&nbsp;<code>include_in_all</code>&nbsp;设置来逐个控制字段是否要包含在&nbsp;<code>_all</code>&nbsp;字段中，默认值是&nbsp;<code>true`。在一个对象(或根对象)上设置 `include_in_all</code>&nbsp;可以修改这个对象中的所有字段的默认行为。</p>\n\n<p>你可能想要保留&nbsp;<code>_all</code>&nbsp;字段作为一个只包含某些特定字段的全文字段，例如只包含&nbsp;<code>title`，`overview`，`summary</code>&nbsp;和&nbsp;<code>tags`。 相对于完全禁用 `_all</code>&nbsp;字段，你可以为所有字段默认禁用&nbsp;<code>include_in_all</code>&nbsp;选项，仅在你选择的字段上启用：</p>\n\n<pre>\nPUT /my_index/my_type/_mapping\n{\n    &quot;my_type&quot;: {\n        &quot;include_in_all&quot;: false,\n        &quot;properties&quot;: {\n            &quot;title&quot;: {\n                &quot;type&quot;:           &quot;string&quot;,\n                &quot;include_in_all&quot;: true\n            },\n            ...\n        }\n    }\n}</pre>\n\n<p>记住，<code>_all</code>&nbsp;字段仅仅是一个&nbsp;经过分词的&nbsp;<code>string</code>&nbsp;字段。它使用默认分词器来分析它的值，不管这个值原本所在字段指定的分词器。就像所有&nbsp;<code>string</code>&nbsp;字段，你可以配置&nbsp;<code>_all</code>&nbsp;字段使用的分词器：</p>\n\n<pre>\nPUT /my_index/my_type/_mapping\n{\n    &quot;my_type&quot;: {\n        &quot;_all&quot;: { &quot;analyzer&quot;: &quot;whitespace&quot; }\n    }\n}</pre>\n\n<h3>元数据：文档标识</h3>\n\n<p>文档标识与四个元数据字段&nbsp;相关：</p>\n\n<p><code>_id</code></p>\n\n<p>文档的 ID 字符串</p>\n\n<p><code>_type</code></p>\n\n<p>文档的类型名</p>\n\n<p><code>_index</code></p>\n\n<p>文档所在的索引</p>\n\n<p><code>_uid</code></p>\n\n<p><code>_type</code>&nbsp;和&nbsp;<code>_id</code>&nbsp;连接在一起构造成&nbsp;<code>type#id</code></p>\n\n<p>默认情况下，&nbsp;<code>_uid</code>&nbsp;字段是被存储（可取回）和索引（可搜索）的。&nbsp;<code>_type</code>&nbsp;字段被索引但是没有存储，<code>_id</code>&nbsp;和&nbsp;<code>_index</code>&nbsp;字段则既没有被索引也没有被存储，这意味着它们并不是真实存在的。</p>\n\n<p>尽管如此，你仍然可以像真实字段一样查询&nbsp;<code>_id</code>&nbsp;字段。Elasticsearch 使用&nbsp;<code>_uid</code>&nbsp;字段来派生出&nbsp;<code>_id</code>&nbsp;。 虽然你可以修改这些字段的&nbsp;<code>index</code>&nbsp;和&nbsp;<code>store</code>&nbsp;设置，但是基本上不需要这么做。</p>\n\n<h2>动态映射</h2>\n\n<p>当 Elasticsearch 遇到文档中以前&nbsp;未遇到的字段，它用&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/mapping-intro.html\" title=\"映射\"><em>dynamic mapping</em></a>&nbsp;来确定字段的数据类型并自动把新的字段添加到类型映射。</p>\n\n<p>有时这是想要的行为有时又不希望这样。通常没有人知道以后会有什么新字段加到文档，但是又希望这些字段被自动的索引。也许你只想忽略它们。如果Elasticsearch是作为重要的数据存储，可能就会期望遇到新字段就会抛出异常，这样能及时发现问题。</p>\n\n<p>幸运的是可以用&nbsp;<code>dynamic</code>&nbsp;配置来控制这种行为&nbsp;，可接受的选项如下：</p>\n\n<p><code>true</code></p>\n\n<p>动态添加新的字段--缺省</p>\n\n<p><code>false</code></p>\n\n<p>忽略新的字段</p>\n\n<p><code>strict</code></p>\n\n<p>如果遇到新字段抛出异常</p>\n\n<p>配置参数&nbsp;<code>dynamic</code>&nbsp;可以用在根&nbsp;<code>object</code>&nbsp;或任何&nbsp;<code>object</code>&nbsp;类型的字段上。你可以将&nbsp;<code>dynamic</code>&nbsp;的默认值设置为<code>strict</code>&nbsp;, 而只在指定的内部对象中开启它, 例如：</p>\n\n<pre>\nPUT /my_index\n{\n    &quot;mappings&quot;: {\n        &quot;my_type&quot;: {\n            &quot;dynamic&quot;:      &quot;strict&quot;, <img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" />\n            &quot;properties&quot;: {\n                &quot;title&quot;:  { &quot;type&quot;: &quot;string&quot;},\n                &quot;stash&quot;:  {\n                    &quot;type&quot;:     &quot;object&quot;,\n                    &quot;dynamic&quot;:  true <img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/2.png\" />\n                }\n            }\n        }\n    }\n}\n</pre>\n\n<table border=\"0\" summary=\"Callout list\">\n	<tbody>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/dynamic-mapping.html#CO34-1\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p>如果遇到新字段，对象&nbsp;<code>my_type</code>&nbsp;就会抛出异常。</p>\n			</td>\n		</tr>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/dynamic-mapping.html#CO34-2\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/2.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p>而内部对象&nbsp;<code>stash</code>&nbsp;遇到新字段就会动态创建新字段。</p>\n			</td>\n		</tr>\n	</tbody>\n</table>\n\n<p>使用上述动态映射， 你可以给&nbsp;<code>stash</code>&nbsp;对象添加新的可检索的字段：</p>\n\n<pre>\nPUT /my_index/my_type/1\n{\n    &quot;title&quot;:   &quot;This doc adds a new field&quot;,\n    &quot;stash&quot;: { &quot;new_field&quot;: &quot;Success!&quot; }\n}\n</pre>\n\n<p>但是对根节点对象&nbsp;<code>my_type</code>&nbsp;进行同样的操作会失败：</p>\n\n<pre>\nPUT /my_index/my_type/1\n{\n    &quot;title&quot;:     &quot;This throws a StrictDynamicMappingException&quot;,\n    &quot;new_field&quot;: &quot;Fail!&quot;\n}\n</pre>\n\n<p><img alt=\"注意\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/note.png\" /></p>\n\n<p>把&nbsp;<code>dynamic</code>&nbsp;设置为&nbsp;<code>false</code>&nbsp;一点儿也不会改变&nbsp;<code>_source</code>&nbsp;的字段内容。&nbsp;<code>_source</code>&nbsp;仍然包含被索引的整个JSON文档。只是新的字段不会被加到映射中也不可搜索。</p>\n\n<h2>自定义动态映射</h2>\n\n<p>如果你想在运行时增加新的字段，你可能会启用动态映射。&nbsp;然而，有时候，动态映射&nbsp;<code>规则</code>&nbsp;可能不太智能。幸运的是，我们可以通过设置去自定义这些规则，以便更好的适用于你的数据。</p>\n\n<h3>日期检测</h3>\n\n<p>当 Elasticsearch 遇到一个新的字符串字段时，它会检测这个字段是否包含一个可识别的日期，比如&nbsp;<code>2014-01-01</code>&nbsp;。&nbsp;如果它像日期，这个字段就会被作为&nbsp;<code>date</code>&nbsp;类型添加。否则，它会被作为&nbsp;<code>string</code>&nbsp;类型添加。</p>\n\n<p>有些时候这个行为可能导致一些问题。想象下，你有如下这样的一个文档：</p>\n\n<pre>\n{ &quot;note&quot;: &quot;2014-01-01&quot; }</pre>\n\n<p>假设这是第一次识别&nbsp;<code>note</code>&nbsp;字段，它会被添加为&nbsp;<code>date</code>&nbsp;字段。但是如果下一个文档像这样：</p>\n\n<pre>\n{ &quot;note&quot;: &quot;Logged out&quot; }</pre>\n\n<p>这显然不是一个日期，但为时已晚。这个字段已经是一个日期类型，这个&nbsp;<code>不合法的日期</code>&nbsp;将会造成一个异常。</p>\n\n<p>日期检测可以通过在根对象上设置&nbsp;<code>date_detection</code>&nbsp;为&nbsp;<code>false</code>&nbsp;来关闭：</p>\n\n<pre>\nPUT /my_index\n{\n    &quot;mappings&quot;: {\n        &quot;my_type&quot;: {\n            &quot;date_detection&quot;: false\n        }\n    }\n}</pre>\n\n<p>使用这个映射，字符串将始终作为&nbsp;<code>string</code>&nbsp;类型。如果你需要一个&nbsp;<code>date</code>&nbsp;字段，你必须手动添加。</p>\n\n<p><img alt=\"注意\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/note.png\" /></p>\n\n<p>Elasticsearch 判断字符串为日期的规则可以通过&nbsp;<a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/5.6/dynamic-field-mapping.html#date-detection\" target=\"_top\"><code>dynamic_date_formats</code>&nbsp;setting</a>&nbsp;来设置。</p>\n\n<h3>动态模板</h3>\n\n<p>使用&nbsp;<code>dynamic_templates</code>&nbsp;，你可以完全控制&nbsp;新检测生成字段的映射。你甚至可以通过字段名称或数据类型来应用不同的映射。</p>\n\n<p>每个模板都有一个名称，&nbsp;你可以用来描述这个模板的用途， 一个&nbsp;<code>mapping</code>&nbsp;来指定映射应该怎样使用，以及至少一个参数 (如&nbsp;<code>match</code>) 来定义这个模板适用于哪个字段。</p>\n\n<p>模板按照顺序来检测；第一个匹配的模板会被启用。例如，我们给&nbsp;<code>string</code>&nbsp;类型字段定义两个模板：</p>\n\n<ul style=\"list-style-type:disc\">\n	<li><code>es</code>&nbsp;：以&nbsp;<code>_es</code>&nbsp;结尾的字段名需要使用&nbsp;<code>spanish</code>&nbsp;分词器。</li>\n	<li><code>en</code>&nbsp;：所有其他字段使用&nbsp;<code>english</code>&nbsp;分词器。</li>\n</ul>\n\n<p>我们将&nbsp;<code>es</code>&nbsp;模板放在第一位，因为它比匹配所有字符串字段的&nbsp;<code>en</code>&nbsp;模板更特殊：</p>\n\n<pre>\nPUT /my_index\n{\n    &quot;mappings&quot;: {\n        &quot;my_type&quot;: {\n            &quot;dynamic_templates&quot;: [\n                { &quot;es&quot;: {\n                      &quot;match&quot;:              &quot;*_es&quot;, <img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" />\n                      &quot;match_mapping_type&quot;: &quot;string&quot;,\n                      &quot;mapping&quot;: {\n                          &quot;type&quot;:           &quot;string&quot;,\n                          &quot;analyzer&quot;:       &quot;spanish&quot;\n                      }\n                }},\n                { &quot;en&quot;: {\n                      &quot;match&quot;:              &quot;*&quot;, <img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/2.png\" />\n                      &quot;match_mapping_type&quot;: &quot;string&quot;,\n                      &quot;mapping&quot;: {\n                          &quot;type&quot;:           &quot;string&quot;,\n                          &quot;analyzer&quot;:       &quot;english&quot;\n                      }\n                }}\n            ]\n}}}&nbsp;</pre>\n\n<table border=\"0\" summary=\"Callout list\">\n	<tbody>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/custom-dynamic-mapping.html#CO35-1\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p>匹配字段名以&nbsp;<code>_es</code>&nbsp;结尾的字段。</p>\n			</td>\n		</tr>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/custom-dynamic-mapping.html#CO35-2\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/2.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p>匹配其他所有字符串类型字段。</p>\n			</td>\n		</tr>\n	</tbody>\n</table>\n\n<p><code>match_mapping_type</code>&nbsp;允许你应用模板到特定类型的字段上，就像有标准动态映射规则检测的一样， (例如<code>string</code>&nbsp;或&nbsp;<code>long</code>)。</p>\n\n<p><code>match</code>&nbsp;参数只匹配字段名称，&nbsp;<code>path_match</code>&nbsp;参数匹配字段在对象上的完整路径，所以&nbsp;<code>address.*.name</code>&nbsp;将匹配这样的字段：</p>\n\n<pre>\n{\n    &quot;address&quot;: {\n        &quot;city&quot;: {\n            &quot;name&quot;: &quot;New York&quot;\n        }\n    }\n}</pre>\n\n<p><code>unmatch</code>&nbsp;和&nbsp;<code>path_unmatch</code>将被用于未被匹配的字段。</p>\n\n<h2>缺省映射</h2>\n\n<p>通常，一个索引中的所有类型共享相同的字段和设置。&nbsp;<code>_default_</code>&nbsp;映射更加方便地指定通用设置，而不是每次创建新类型时都要重复设置。&nbsp;<code>_default_</code>&nbsp;映射是新类型的模板。在设置&nbsp;<code>_default_</code>&nbsp;映射之后创建的所有类型都将应用这些缺省的设置，除非类型在自己的映射中明确覆盖这些设置。</p>\n\n<p>例如，我们可以使用&nbsp;<code>_default_</code>&nbsp;映射为所有的类型禁用&nbsp;<code>_all</code>&nbsp;字段，&nbsp;而只在&nbsp;<code>blog</code>&nbsp;类型启用：</p>\n\n<pre>\nPUT /my_index\n{\n    &quot;mappings&quot;: {\n        &quot;_default_&quot;: {\n            &quot;_all&quot;: { &quot;enabled&quot;:  false }\n        },\n        &quot;blog&quot;: {\n            &quot;_all&quot;: { &quot;enabled&quot;:  true  }\n        }\n    }\n}\n</pre>\n\n<p><code>_default_</code>&nbsp;映射也是一个指定索引&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/custom-dynamic-mapping.html#dynamic-templates\" title=\"动态模板\">dynamic templates</a>&nbsp;的好方法。</p>\n\n<h2>重新索引你的数据</h2>\n\n<p>尽管可以增加新的类型到索引中，或者增加新的字段到类型中，但是不能添加新的分析器或者对现有的字段做改动。&nbsp;如果你那么做的话，结果就是那些已经被索引的数据就不正确， 搜索也不能正常工作。</p>\n\n<p>对现有数据的这类改变最简单的办法就是重新索引：用新的设置创建新的索引并把文档从旧的索引复制到新的索引。</p>\n\n<p>字段&nbsp;<code>_source</code>&nbsp;的一个优点是在Elasticsearch中已经有整个文档。你不必从源数据中重建索引，而且那样通常比较慢。</p>\n\n<p>为了有效的重新索引所有在旧的索引中的文档，用&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/scroll.html\" title=\"游标查询 Scroll\"><em>scroll</em></a>&nbsp;从旧的索引检索批量文档&nbsp;， 然后用&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/bulk.html\" title=\"代价较小的批量操作\"><code>bulk</code>&nbsp;API</a>&nbsp;把文档推送到新的索引中。</p>\n\n<p>从Elasticsearch v2.3.0开始，&nbsp;<a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/5.6/docs-reindex.html\" target=\"_top\">Reindex API</a>&nbsp;被引入。它能够对文档重建索引而不需要任何插件或外部工具。</p>\n\n<p><strong>批量重新索引</strong></p>\n\n<p>同时并行运行多个重建索引任务，但是你显然不希望结果有重叠。正确的做法是按日期或者时间 这样的字段作为过滤条件把大的重建索引分成小的任务：</p>\n\n<pre>\nGET /old_index/_search?scroll=1m\n{\n    &quot;query&quot;: {\n        &quot;range&quot;: {\n            &quot;date&quot;: {\n                &quot;gte&quot;:  &quot;2014-01-01&quot;,\n                &quot;lt&quot;:   &quot;2014-02-01&quot;\n            }\n        }\n    },\n    &quot;sort&quot;: [&quot;_doc&quot;],\n    &quot;size&quot;:  1000\n}</pre>\n\n<p>如果旧的索引持续会有变化，你希望新的索引中也包括那些新加的文档。那就可以对新加的文档做重新索引， 但还是要用日期类字段过滤来匹配那些新加的文档。</p>\n\n<h2>索引别名和零停机</h2>\n\n<p>在前面提到的，重建索引的问题是必须更新应用中的索引名称。&nbsp;索引别名就是用来解决这个问题的！</p>\n\n<p>索引&nbsp;<em>别名</em>&nbsp;就像一个快捷方式或软连接，可以指向一个或多个索引，也可以给任何一个需要索引名的API来使用。别名&nbsp;带给我们极大的灵活性，允许我们做下面这些：</p>\n\n<ul style=\"list-style-type:disc\">\n	<li>在运行的集群中可以无缝的从一个索引切换到另一个索引</li>\n	<li>给多个索引分组 (例如，&nbsp;<code>last_three_months</code>)</li>\n	<li>给索引的一个子集创建&nbsp;<code>视图</code></li>\n</ul>\n\n<p>在后面我们会讨论更多关于别名的使用。现在，我们将解释怎样使用别名在零停机下从旧索引切换到新索引。</p>\n\n<p>有两种方式管理别名：&nbsp;<code>_alias</code>&nbsp;用于单个操作，&nbsp;<code>_aliases</code>&nbsp;用于执行多个原子级操作。</p>\n\n<p>在本章中，我们假设你的应用有一个叫&nbsp;<code>my_index</code>&nbsp;的索引。事实上，&nbsp;<code>my_index</code>&nbsp;是一个指向当前真实索引的别名。真实索引包含一个版本号：&nbsp;<code>my_index_v1</code>&nbsp;，&nbsp;<code>my_index_v2</code>&nbsp;等等。</p>\n\n<p>首先，创建索引&nbsp;<code>my_index_v1</code>&nbsp;，然后将别名&nbsp;<code>my_index</code>&nbsp;指向它：</p>\n\n<pre>\nPUT /my_index_v1 <img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" />\nPUT /my_index_v1/_alias/my_index <img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/2.png\" />\n</pre>\n\n<table border=\"0\" summary=\"Callout list\">\n	<tbody>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/index-aliases.html#CO36-1\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p>创建索引&nbsp;<code>my_index_v1</code>&nbsp;。</p>\n			</td>\n		</tr>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/index-aliases.html#CO36-2\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/2.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p>设置别名&nbsp;<code>my_index</code>&nbsp;指向&nbsp;<code>my_index_v1</code>&nbsp;。</p>\n			</td>\n		</tr>\n	</tbody>\n</table>\n\n<p>你可以检测这个别名指向哪一个索引：</p>\n\n<pre>\nGET /*/_alias/my_index\n</pre>\n\n<p>或哪些别名指向这个索引：</p>\n\n<pre>\nGET /my_index_v1/_alias/*\n</pre>\n\n<p>两者都会返回下面的结果：</p>\n\n<pre>\n{\n    &quot;my_index_v1&quot; : {\n        &quot;aliases&quot; : {\n            &quot;my_index&quot; : { }\n        }\n    }\n}</pre>\n\n<p>然后，我们决定修改索引中一个字段的映射。当然，我们不能修改现存的映射，所以我们必须重新索引数据。&nbsp;首先, 我们用新映射创建索引&nbsp;<code>my_index_v2</code>&nbsp;：</p>\n\n<pre>\nPUT /my_index_v2\n{\n    &quot;mappings&quot;: {\n        &quot;my_type&quot;: {\n            &quot;properties&quot;: {\n                &quot;tags&quot;: {\n                    &quot;type&quot;:   &quot;string&quot;,\n                    &quot;index&quot;:  &quot;not_analyzed&quot;\n                }\n            }\n        }\n    }\n}\n</pre>\n\n<p>然后我们将数据从&nbsp;<code>my_index_v1</code>&nbsp;索引到&nbsp;<code>my_index_v2</code>&nbsp;，下面的过程在&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/reindex.html\" title=\"重新索引你的数据\">重新索引你的数据</a>&nbsp;中已经描述过。一旦我们确定文档已经被正确地重索引了，我们就将别名指向新的索引。</p>\n\n<p>一个别名可以指向多个索引，所以我们在添加别名到新索引的同时必须从旧的索引中删除它。这个操作需要原子化，这意味着我们需要使用&nbsp;<code>_aliases</code>&nbsp;操作：</p>\n\n<pre>\nPOST /_aliases\n{\n    &quot;actions&quot;: [\n        { &quot;remove&quot;: { &quot;index&quot;: &quot;my_index_v1&quot;, &quot;alias&quot;: &quot;my_index&quot; }},\n        { &quot;add&quot;:    { &quot;index&quot;: &quot;my_index_v2&quot;, &quot;alias&quot;: &quot;my_index&quot; }}\n    ]\n}\n</pre>\n\n<p>你的应用已经在零停机的情况下从旧索引迁移到新索引了。</p>\n\n<p><img alt=\"提示\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/tip.png\" /></p>\n\n<p>即使你认为现在的索引设计已经很完美了，在生产环境中，还是有可能需要做一些修改的。</p>\n\n<p>做好准备：在你的应用中使用别名而不是索引名。然后你就可以在任何时候重建索引。别名的开销很小，应该广泛使用。</p>\n', 1, '5f199b8885e24fc8b28672b872edb606', 0, '2018-06-27 15:50:23', '2018-08-31 22:21:23');
INSERT INTO `logcontent` VALUES ('a576f4b74b26443f8b1b9b8a98906f03', -1, '7338e53acd514defa1a17e47016f3f4a', 'Elasticsearch（十八）', 'bool,must,should,must_not,terms,query', 'Es关键字与sql对比/例子', '<p><a name=\"Es关键字与sql对比\"></a>bool：组合多个过滤条件（must，must_not，should），SQL:SELECT 1 FROM TABLE WHERE (A=1) AND (B&lt;&gt;1 OR C=1);</p>\n\n<p>must：必须匹配，SQL:SELECT 1 FROM TABLE WHERE A=1;</p>\n\n<p>should：其中任意一个匹配即可，SQL:SELECT 1 FROM TABLE WHERE A=1 OR B=1;</p>\n\n<p>must_not：必须不匹配，SQL:SELECT 1 FROM TABLE WHERE A&lt;&gt;1;</p>\n\n<p>term: {&quot;field&quot;: &quot;value&quot;}，单个字段单个值完全匹配，SQL:SELECT 1 FROM TABLE WHERE A=1;</p>\n\n<p>terms: {&quot;field&quot;: [&quot;value1&quot;, &quot;value2&quot;]}单个字段多个值完全匹配，SQL:SELECT 1 FROM TABLE WHERE A IN (1,2);</p>\n\n<p>&quot;range&quot;: { &quot;field&quot;: {&quot;gt&quot;: value1,&quot;lt&quot;: value2}}与条件做比较，SQL:SELECT 1 FROM TABLE WHERE A &gt;1;</p>\n\n<p><a name=\"例子\"></a>1、搜索发帖日期为2017-01-01，或者帖子ID为XHDK-A-1293-#fJ3的帖子，同时要求帖子的发帖日期绝对不为2017-01-02</p>\n\n<pre>\n<code class=\"language-json\">SELECT *\nFROM FORUM.ARTICLE\nWHERE (POST_DATE=\'2017-01-01\' OR ARTICLE_ID=\'XHDK-A-1293-#fJ3\')\nAND POST_DATE!=\'2017-01-02\'\n相当于\nGET /forum/article/_search\n{\n  \"query\": {\n    \"constant_score\": {\n      \"filter\": {\n        \"bool\": {\n          \"should\": [\n            {\"term\": { \"postDate\": \"2017-01-01\" }},\n            {\"term\": {\"articleID\": \"XHDK-A-1293-#fJ3\"}}\n          ],\n          \"must_not\": {\n            \"term\": {\n              \"postDate\": \"2017-01-02\"\n            }\n          }\n        }\n      }\n    }\n  }\n}</code></pre>\n\n<p>2、搜索帖子ID为XHDK-A-1293-#fJ3，或者是帖子ID为JODL-X-1937-#pV7而且发帖日期为2017-01-01的帖子</p>\n\n<pre>\n<code class=\"language-json\">SELECT *\nFROM FORUM.ARTICLE\nWHERE ARTICLE_ID=\'XHDK-A-1293-#fJ3\'\nOR (ARTICLE_ID=\'JODL-X-1937-#pV7\' AND POST_DATE=\'2017-01-01\')\n\nGET /forum/article/_search \n{\n  \"query\": {\n    \"constant_score\": {\n      \"filter\": {\n        \"bool\": {\n          \"should\": [\n            {\n              \"term\": {\n                \"articleID\": \"XHDK-A-1293-#fJ3\"\n              }\n            },\n            {\n              \"bool\": {\n                \"must\": [\n                  {\n                    \"term\":{\n                      \"articleID\": \"JODL-X-1937-#pV7\"\n                    }\n                  },\n                  {\n                    \"term\": {\n                      \"postDate\": \"2017-01-01\"\n                    }\n                  }\n                ]\n              }\n            }\n          ]\n        }\n      }\n    }\n  }\n}</code></pre>\n\n<p>3、搜索articleID为KDKE-B-9947-#kL5或QQPX-R-3956-#aD8的帖子</p>\n\n<pre>\n<code class=\"language-json\">SELECT * FROM FORUM.ARTICLE WHERE ARTICLE_ID IN (\"KDKE-B-9947-#kL5\",\"QQPX-R-3956-#aD8\")\nGET /forum/article/_search \n{\n  \"query\": {\n    \"constant_score\": {\n      \"filter\": {\n        \"terms\": {\n          \"articleID\": [\n            \"KDKE-B-9947-#kL5\",\n            \"QQPX-R-3956-#aD8\"\n          ]\n        }\n      }\n    }\n  }\n}\n</code></pre>\n\n<p>4、搜索浏览量在30~60之间的帖子</p>\n\n<pre>\n<code class=\"language-json\">GET /forum/article/_search\n{\n  \"query\": {\n    \"constant_score\": {\n      \"filter\": {\n        \"range\": {\n          \"view_cnt\": {\n            \"gt\": 30,\n            \"lt\": 60\n          }\n        }\n      }\n    }\n  }\n}</code></pre>\n\n<p>5、搜索发帖日期在最近1个月的帖子</p>\n\n<pre>\n<code class=\"language-json\">POST /forum/article/_bulk\n{ \"index\": { \"_id\": 5 }}\n{ \"articleID\" : \"DHJK-B-1395-#Ky5\", \"userID\" : 3, \"hidden\": false, \"postDate\": \"2017-03-01\", \"tag\": [\"elasticsearch\"], \"tag_cnt\": 1, \"view_cnt\": 10 }\n\nGET /forum/article/_search \n{\n  \"query\": {\n    \"constant_score\": {\n      \"filter\": {\n        \"range\": {\n          \"postDate\": {\n            \"gt\": \"2017-03-10||-30d\"\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre>\n\n<p>&nbsp;</p>\n', 2, '5f199b8885e24fc8b28672b872edb606', 4, '2017-07-17 18:29:20', '2018-09-06 16:12:58');
INSERT INTO `logcontent` VALUES ('a7a12a4bb71d4ad8b46f0a0b592bb55e', -1, '7338e53acd514defa1a17e47016f3f4a', 'Elasticsearch（七）', '_index元数据,_type元数据,_id元数据,_source元数据', '_index元数据/_type元数据/_id元数据/_source元数据', '<pre>\n<code class=\"language-json\">{\n    \"_index\":\"招聘\",\n    \"_type\":\"58同城\",\n    \"_id\":\"1\",\n    \"_version\":1,\n    \"found\":true,\n    \"_source\":{\n        \"test_content\":\"Java开发\",\n        \"test_content2\":\"Java开发2\"\n    }\n}</code></pre>\n\n<p><a name=\"_index元数据\"></a>1、_index元数据</p>\n\n<p>（1）代表此document存放在哪个索引（index）</p>\n\n<p>（2）类似的数据放在一个索引，非类似的数据放不同索引。</p>\n\n<p>（3）index中包含了很多类似的document：类似指的是document的fields很大一部分是相同的，每个document的fields都完全不一样，就不太适合放到一个index里面去了。</p>\n\n<p>（4）索引名称必须是小写的，不能用下划线开头，不能包含逗号</p>\n\n<p>&nbsp;</p>\n\n<p><a name=\"_type元数据\"></a>2、_type元数据</p>\n\n<p>（1）代表此document属于index中的哪个类别（type）</p>\n\n<p>（2）一个索引通常会划分为多个type，逻辑上对index中有些许不同的几类数据进行分类</p>\n\n<p>（3）type名称可以是大写或者小写，但是同时不能用下划线开头，不能包含逗号</p>\n\n<p>（4）底层的存储</p>\n\n<pre>\n<code class=\"language-json\">{\n  \"name\": \"geli kongtiao\",\n  \"price\": 1999.0,\n  \"service_period\": \"one year\"\n}\n\n{\n  \"name\": \"aozhou dalongxia\",\n  \"price\": 199.0,\n  \"eat_period\": \"one week\"\n}\n这两条数据在底层会被存储为\n{\n  \"_type\": \"elactronic_goods\",\n  \"name\": \"geli kongtiao\",\n  \"price\": 1999.0,\n  \"service_period\": \"one year\",\n  \"eat_period\": \"\"\n}\n\n{\n  \"_type\": \"fresh_goods\",\n  \"name\": \"aozhou dalongxia\",\n  \"price\": 199.0,\n  \"service_period\": \"\",\n  \"eat_period\": \"one week\"\n}\n没有的字段会进行空值补足\n所以如果将两个field完全不同的type，放在一个index下，那么就每条数据都至少有一半的field在底层的lucene中是空值，\n会有严重的性能问题</code></pre>\n\n<p><a name=\"_id元数据\"></a>3、_id元数据</p>\n\n<p>（1）代表document的唯一标识，与index和type一起，可以唯一标识和定位一个document</p>\n\n<p>（2）我们可以手动指定document的id（put /index/type/id），也可以不指定，由es自动为我们创建一个id</p>\n\n<pre>\n<code class=\"language-json\">手动指定document id\npost /index/type/id\nPOST /test_index/test_type/1\n{\n    \"test_content\":\"my test\"\n}</code></pre>\n\n<p>某些情况下，从某些其他的系统中导入一些数据到es时，会采取这种方式，例如数据库中数据到es，数据库中唯一的primary key可以做为es的document id。</p>\n\n<pre>\n<code class=\"language-json\">自动生成document id\npost /index/type\nPOST /test_index/test_type\n{\n    \"test_content\":\"my test\"\n}</code></pre>\n\n<p>自动生成的id，长度为20个字符，URL安全，base64编码，GUID，分布式系统并行生成时不可能会发生冲突</p>\n\n<p><a name=\"_source元数据\"></a>4、_source元数据</p>\n\n<p>_source元数据：我们在创建一个document的时候，使用的那个放在request body中的json串，默认情况下，在get的时候，会原封不动的返回。</p>\n\n<p>定制返回结果</p>\n\n<p>定制返回的结果，指定_source中，返回哪些field</p>\n\n<pre>\n<code class=\"language-json\">GET /test_index/test_type/1?_source=test_content,test_content2\n{\n  \"_index\": \"招聘\",\n  \"_type\": \"58同城\",\n  \"_id\": \"1\",\n  \"_version\": 1,\n  \"found\": true,\n  \"_source\": {\n    \"test_content\": \"Java开发\",\n    \"test_content2\": \"Java开发2\"\n  }\n}\n</code></pre>\n\n<p>&nbsp;</p>\n', 2, '5f199b8885e24fc8b28672b872edb606', 8, '2017-07-12 18:04:13', '2018-09-06 16:12:58');
INSERT INTO `logcontent` VALUES ('a877c84fffc242fab83c46607612a7c3', -1, '3ba9a3aed3ce48f682974ebcf3552ce2', 'Excel公式记录', 'Excel', '使用过的Excel公式记录', '<p><a id=\"时间戳转年月日\" name=\"时间戳转年月日\"></a>Excel中将时间戳转换为年月日</p>\n\n<pre>\n<code class=\"language-markdown\">=(A2+8*3600)/86400+70*365+19</code></pre>\n\n<p>&nbsp;</p>\n', 1, '5f199b8885e24fc8b28672b872edb606', 2, '2018-03-23 17:03:54', '2018-08-31 22:21:23');
INSERT INTO `logcontent` VALUES ('a8f0a06f7f914f88beb49e9094b0c018', -1, 'f29612168904487aadb4043df110361c', 'Varnish基本配置文件', 'varnish,配置文件', 'Varnish基本配置文件', '<p>Varnish配置文件</p>\n\n<pre>\n<code class=\"language-nginx\"># This is a basic VCL configuration file forvarnish. See the vcl(7)\n# man page for details on VCL syntax andsemantics.\n#\n# Default backend definition. Set this to point to your content\n# server.\n#定义后端服务器\nbackend server1 {\n	.host= \"100.11.6.202\";\n	.port= \"8080\";\n}\nbackend server2 {\n	.host= \"100.11.6.204\";\n	.port= \"8080\";\n}\n#定义调度器，调度方法为轮询，调度器名称为server\ndirector server round-robin {\n	{ .backend = server1; }\n	{ .backend = server2; }\n}\n#如果请求的服务器地址是100.11.6.202（也可以是域名）则调用server这个调度器\nsub vcl_recv {\n	if(req.http.host ~ \"100.11.6.202\") {\n		set req.backend=server;\n	}\n}\n\n#定义acl，用于管理缓存,允许acl中定义的网段去管理varnish\nacl purge {\n	\"localhost\";\n	\"127.0.0.1\";\n	\"172.30.0.0\"/16;\n	\"10.100.6.0\"/8;\n}\n# Below is a commented-out copy of thedefault VCL logic. If you\n# redefine any of these subroutines, thebuilt-in logic will be\n# appended to your code.\nsubvcl_recv {\n	if(req.restarts == 0) {\n		if (req.http.x-forwarded-for) {\n			set req.http.X-Forwarded-For = req.http.X-Forwarded-For +\", \" + client.ip;\n		} else {\n			set req.http.X-Forwarded-For = client.ip;\n		}\n	}\n\n#压缩设置，图片不压缩\nif(req.http.Accept-Encoding) {\n	if (req.url ~ \".(jpg|png|gif|jpeg|flv)$\") {\n		remove req.http.Accept-Encoding;\n		remove req.http.Cookie;\n	} else if (req.http.Accept-Encoding ~ \"gzip\") {\n		set req.http.Accept-Encoding = \"gzip\";\n	} else if (req.http.Accept-Encoding ~ \"deflate\") {\n		set req.http.Accept-Encoding = \"deflate\";\n	} else {\n		remove req.http.Accept-Encoding;\n	}\n}\n#不是get、head 、put、 post、 trace、 options、 delete 操作的返回pipe不缓存\nif(req.request != \"GET\" &amp;&amp;\n	req.request != \"HEAD\" &amp;&amp;\n	req.request != \"PUT\" &amp;&amp;\n	req.request != \"POST\" &amp;&amp;\n	req.request != \"TRACE\" &amp;&amp;\n	req.request != \"OPTIONS\"&amp;&amp;\n	req.request != \"DELETE\") {\n	return(pipe);\n}\n\nif(req.request == \"purge\") {\n	if (!client.ip ~ purge) {\n		error 405 \"Not allowed.\";\n	}\n	return(lookup);\n}\n#缓存方法是GET及HEAD的请求\nif(req.request != \"GET\" &amp;&amp; req.request != \"HEAD\") {\n	return(pass);\n}\n#不缓存认证及cookie信息\n# if (req.http.Authorization || req.http.Cookie) {\n# /* Not cacheable by default */\n# 	return (pass);\n# }\n#请求是GET且以jsp和do结尾或包含?的url直接访问后端服务器不缓存\n# if (req.request ==\"GET\"&amp;&amp;req.url~\"(?i)\\.(jsp|do)($|\\?)\") {\n# 	return (pass);\n# }\n# return (lookup);\n\n}\n\nsub vcl_pipe {\n# #Note that only the first request to the backend will have\n# #X-Forwarded-For set. If you useX-Forwarded-For and want to\n# #have it set for all requests, make sure to have:\n# #set bereq.http.connection = \"close\";\n# #here. It is not set by default as itmight break some broken web\n# #applications, like IIS with NTLM authentication.\n	return (pipe);\n}\n#\nsubvcl_pass {\n	return (pass);\n}\n#\nsubvcl_hash {\n	hash_data(req.url);\n	if(req.http.host) {\n		hash_data(req.http.host);\n	}else {\n		hash_data(server.ip);\n	}\n	return (hash);\n}\n#\nsubvcl_hit {\n	return (deliver);\n}\n#\nsubvcl_miss {\n	return (fetch);\n}\n#\nsubvcl_fetch {\n	if(beresp.ttl &lt;= 0s ||\n	beresp.http.Set-Cookie ||\n	beresp.http.Vary == \"*\") {\n	/*\n	* Mark as \"Hit-For-Pass\" for the next2 minutes\n	*/\n		set beresp.ttl = 120 s;\n		return (hit_for_pass);\n}\n#下为不缓存404的代码设置\n	if (beresp.status == 404){\n		set beresp.ttl = 0s;\n		return (hit_for_pass);\n	}\n#以css、js、html、htm结尾的文件缓存5分\n# if (req.request == \"GET\" &amp;&amp; req.url ~\"\\.(css|js|html|htm)$\") {\n# 	set beresp.do_gzip = true;\n# 	set beresp.ttl = 300s;\n# }\n#图片缓存7天\n	if(req.request == \"GET\" &amp;&amp; req.url ~\"\\.(gif|jpg|jpeg|bmp|png|tiff|tif|ico|img|bmp|wmf)$\") {\n		set beresp.ttl = 7d;\n	}\n	return(deliver);\n}\n#此函数用来统计请求是否在varnsih中命中\nsubvcl_deliver {\n	if(obj.hits &gt; 0) {\n		setresp.http.X-Cache = \"Hit from image.domain.com\";\n	} else {\n		set resp.http.X-Cache = \"Miss from image.domain.com\";\n	}\n	return (deliver);\n}\n#\n# sub vcl_error {\n# set obj.http.Content-Type = \"text/html; charset=utf-8\";\n# set obj.http.Retry-After = \"5\";\n# synthetic {\"\n# &lt;?xml version=\"1.0\"encoding=\"utf-8\"?&gt;\n# &lt;!DOCTYPE html PUBLIC \"-//W3C//DTDXHTML 1.0 Strict//EN\"\n# \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\"&gt;\n# &lt;html&gt;\n# &lt;head&gt;\n# &lt;title&gt;\"} + obj.status + \" \" + obj.response +{\"&lt;/title&gt;\n# &lt;/head&gt;\n# &lt;body&gt;\n# &lt;h1&gt;Error \"} + obj.status + \" \" + obj.response +{\"&lt;/h1&gt;\n# &lt;p&gt;\"} + obj.response + {\"&lt;/p&gt;\n# &lt;h3&gt;Guru Meditation:&lt;/h3&gt;\n# &lt;p&gt;XID: \"} + req.xid + {\"&lt;/p&gt;\n# &lt;hr&gt;\n# &lt;p&gt;Varnish cache server&lt;/p&gt;\n# &lt;/body&gt;\n# &lt;/html&gt;\n# \"};\n# return (deliver);\n# }\n#\nsubvcl_init {\n	return (ok);\n}\n#\nsubvcl_fini {\n	return (ok);\n}\n</code></pre>\n\n<p>&nbsp;</p>\n', 1, '5f199b8885e24fc8b28672b872edb606', 6, '2018-03-30 14:08:56', '2018-09-12 22:53:15');
INSERT INTO `logcontent` VALUES ('a9aeeb7b15d4487ca3eef23808f9b90f', -1, '7338e53acd514defa1a17e47016f3f4a', 'Elasticsearch（十九）', 'match query 底层转换为 bool + term,boost权重,多shard场景下relevance score不准确问题,best fields策略（dis_max）及tie_breaker,most-fields策略,区别,cross-fields搜索的弊端及解决', 'match query 底层转换为 bool + term/boost权重/多shard场景下relevance score不准确问题/best fields策略（dis_max）及tie_breaker/most-fields策略/best-fields与most-fields的区别/cross-fields搜索及使用most_fields策略进行cross-fields搜索的弊端及解决', '<p><a id=\"match query底层转换为bool+term\" name=\"match query底层转换为bool+term\"></a>1、match query 底层转换为&nbsp;bool + term。</p>\n\n<p>（1）普通match如何转换为term+should</p>\n\n<pre>\n<code>{\n    \"match\": { \"title\": \"java elasticsearch\"}\n}\n\n使用诸如上面的match query进行多值搜索的时候，es会在底层自动将这个match query转换为bool的语法\nbool should，指定多个搜索词，同时使用term query\n\n{\n  \"bool\": {\n    \"should\": [\n      { \"term\": { \"title\": \"java\" }},\n      { \"term\": { \"title\": \"elasticsearch\"   }}\n    ]\n  }\n}</code></pre>\n\n<p>（2）and match如何转换为term+must</p>\n\n<pre>\n<code>{\n    \"match\": {\n        \"title\": {\n            \"query\":\"java elasticsearch\",\n            \"operator\": \"and\"\n        }\n    }\n}\n\n{\n  \"bool\": {\n    \"must\": [\n      { \"term\": { \"title\": \"java\" }},\n      { \"term\": { \"title\": \"elasticsearch\" }}\n    ]\n  }\n}</code></pre>\n\n<p>（3）minimum_should_match如何转换</p>\n\n<pre>\n<code>{\n    \"match\": {\n        \"title\": {\n            \"query\":\"java elasticsearch hadoop spark\",\n            \"minimum_should_match\": \"75%\"\n        }\n    }\n}\n\n{\n  \"bool\": {\n    \"should\": [\n      { \"term\": { \"title\": \"java\" }},\n      { \"term\": { \"title\": \"elasticsearch\" }},\n      { \"term\": { \"title\": \"hadoop\" }},\n      { \"term\": { \"title\": \"spark\" }}\n    ],\n    \"minimum_should_match\": 3 \n  }\n}</code></pre>\n\n<p><a id=\"boost权重\" name=\"boost权重\"></a>2、搜索标题中包含java的帖子，如果标题中包含hadoop或elasticsearch就优先搜索出来，同时，如果一个帖子包含java hadoop，另一个帖子包含java elasticsearch，包含hadoop的帖子要比elasticsearch优先搜索出来</p>\n\n<pre>\n<code>boost:搜索条件的权重,可以将某个搜索条件的权重加大,此时当匹配这个搜索条件和匹配另一个搜索条件的document,计算relevance score时,匹配权重更大的搜索条件的document,relevance score会更高,当然也就会优先被返回回来,默认情况下，搜索条件的权重都是一样的，都是1\nGET /forum/article/_search \n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        {\"match\": {\"title\": \"blog\" }}\n      ],\n      \"should\": [\n        { \"match\": { \"title\": { \"query\": \"elasticsearch\" }}},\n        { \"match\": { \"title\": { \"query\": \"spark\",\"boost\": 5 }}}\n      ]\n    }\n  }\n}</code></pre>\n\n<p><a id=\"多shard场景下relevance score不准确问题\" name=\"多shard场景下relevance score不准确问题\"></a>3、多shard场景下relevance score不准确问题</p>\n\n<pre>\n<code>当一个搜索请求到某个shard时，会进行relevance score计算\n（1）在一个document中出现了几次，次数越多，分数越高\n（2）在所有document中出现了几次，次数越多，分数越低\n（3）匹配的document字段的长度，长度越短，分数越高\n\n这样匹配度高的document可能因为shard中出现的次数太多，而导致分数较低（根据2），默认在shard本地计算相关度分数，导致搜索结果不是你想要的，相关度很高的doc排在了后面，而相关度低的doc却排在了前面\n\n如何解决该问题？\n\n（1）生产环境下，数据量大，尽可能实现均匀分配\n数据量很大的话，其实一般情况下，在概率学的背景下，es都是在多个shard中均匀路由数据的，路由的时候根据_id，负载均衡\n比如说有10个document，title都包含java，一共有5个shard，那么在概率学的背景下，如果负载均衡的话，其实每个shard都应该有2个doc，title包含java\n如果说数据分布均匀的话，其实就没有刚才说的那个问题了\n\n（2）测试环境下，将索引的primary shard设置为1个，number_of_shards=1，index settings\n如果说只有一个shard，那么当然，所有的document都在这个shard里面，就没有这个问题了\n\n（3）测试环境下，搜索附带search_type=dfs_query_then_fetch参数，会将local IDF取出来计算global IDF\n计算一个doc的相关度分数的时候，就会将所有shard对的local IDF计算一下，获取出来，在本地进行global IDF分数的计算，会将所有shard的doc作为上下文来进行计算，也能确保准确性。但是production生产环境下，不推荐这个参数，因为性能很差。</code></pre>\n\n<p><a id=\"best fields策略（dis_max）及tie_breaker\" name=\"best fields策略（dis_max）及tie_breaker\"></a>4、best fields策略（dis_max）及tie_breaker</p>\n\n<pre>\n<code>计算每个document的relevance score：每个query的分数，乘以matched query数量，除以总query数量\n假设\ndoc4:{\"title\":\"java\",\"content\":\"solution\"}\ndoc5:{\"title\":\"hehe\",\"content\":\"java solution\"}\n\n搜索关键词java solution\n针对doc4\ntitle匹配度较低1.1\ncontent匹配度较低1.2\nmatched query数量 = 2\n总query数量 = 2\nscore = ( 1.1 + 1.2 ) * 2 / 2 = 2.3\n针对doc5\ntitle无匹配0\ncontent匹配度较高2.3\nmatched query数量 = 1\n总query数量 = 2\nscore = 2.3 * 1 / 2 = 1.15\n\n需要:应该是某一个field中匹配到了尽可能多的关键词，被排在前面；而不是尽可能多的field匹配到了少数的关键词，排在了前面\n\nbest fields策略\ndis_max语法，直接取多个query中，分数最高的那一个query的分数\nGET /forum/article/_search\n{\n    \"query\": {\n        \"dis_max\": {\n            \"queries\": [\n                { \"match\": { \"title\": \"java solution\" }},\n                { \"match\": { \"content\":  \"java solution\" }}\n            ]\n        }\n    }\n}\ndoc4的分数 = 1.2 &lt; doc5的分数 = 2.3，所以doc5就可以排在更前面的地方，符合我们的需要\n\n在实际场景中可能出现的一个情况:\nGET /e/q/_search\n{\n    \"query\": {\n        \"dis_max\": {\n            \"queries\": [\n                { \"match\": { \"title\": \"java beginner\" }}\n            ]\n        }\n    }\n}\ndoc1，title中包含java，content不包含java beginner任何一个关键词\ndoc2，content中包含beginner，title中不包含任何一个关键词\ndoc3，title中包含java，content中包含beginner\n最终搜索，因为dis_max只取最高的score，所以对于doc3中的两个匹配是分开算得，可能出来的结果是，doc1和doc2排在doc3的前面，而不是我们期望的doc3排在最前面\n\n使用tie_breaker将其他query的分数也考虑进去\n\ntie_breaker参数的意义，在于说，将其他query的分数，乘以tie_breaker，然后综合与最高分数的那个query的分数，综合在一起进行计算\n除了取最高分以外，还会考虑其他的query的分数\ntie_breaker的值，在0~1之间，是个小数\n\nGET /e/q/_search\n{\n    \"query\": {\n        \"dis_max\": {\n            \"queries\": [\n                { \"match\": { \"title\": \"java beginner\" }}\n            ],\n            \"tie_breaker\": 0.3\n        }\n    }\n}</code></pre>\n\n<p><a id=\"most-fields策略\" name=\"most-fields策略\"></a>5、most-fields策略</p>\n\n<pre>\n<code>best-fields策略，主要是说将某一个field匹配尽可能多的关键词的doc优先返回回来\nmost-fields策略，主要是说尽可能返回更多field匹配到某个关键词的doc，优先返回回来\n\n有的时候需求需要的就是most-fields策略\n\nGET /forum/article/_search\n{\n  \"query\": {\n    \"match\": { \"sub_title\": \"learning courses\" }\n  }\n}\n</code></pre>\n\n<p><a id=\"best-fields与most-fields的区别\" name=\"best-fields与most-fields的区别\"></a>6、best-fields与most-fields的区别</p>\n\n<pre>\n<code>（1）best_fields，是对多个field进行搜索，挑选某个field匹配度最高的那个分数，同时在多个query最高分相同的情况下，在一定程度上考虑其他query的分数。简单来说，你对多个field进行搜索，就想搜索到某一个field尽可能包含更多关键字的数据\n\n优点：通过best_fields策略，以及综合考虑其他field，还有minimum_should_match支持，可以尽可能精准地将匹配的结果推送到最前面\n缺点：除了那些精准匹配的结果，其他差不多大的结果，排序结果不是太均匀，没有什么区分度了\n\n实际的例子：百度之类的搜索引擎，最匹配的到最前面，但是其他的就没什么区分度了\n\n（2）most_fields，综合多个field一起进行搜索，尽可能多地让所有field的query参与到总分数的计算中来，此时就会是个大杂烩，出现类似best_fields案例最开始的那个结果，结果不一定精准，某一个document的一个field包含更多的关键字，但是因为其他document有更多field匹配到了，所以排在了前面；所以需要建立类似sub_title.std这样的field，尽可能让某一个field精准匹配query string，贡献更高的分数，将更精准匹配的数据排到前面\n\n优点：将尽可能匹配更多field的结果推送到最前面，整个排序结果是比较均匀的\n缺点：可能那些精准匹配的结果，无法推送到最前面\n\n实际的例子：wiki，明显的most_fields策略，搜索结果比较均匀，但是的确要翻好几页才能找到最匹配的结果</code></pre>\n\n<p><a id=\"cross-fields搜索弊端及解决\" name=\"cross-fields搜索弊端及解决\"></a>7、cross-fields搜索及使用most_fields策略进行cross-fields搜索的弊端及解决</p>\n\n<pre>\n<code>cross-fields搜索，一个唯一标识，跨了多个field。\n比如一个人的详细地址，可能会被拆分到country、province、city中，对于搜索结果需要多个field的匹配度分数。\n如果要实现，可能用most_fields比较合适。因为best_fields是优先单个field最匹配的结果，cross-fields本身就不是一个field的问题了。\n\n但是使用most_fields策略进行cross-fields search也会有一些弊端\n问题1：只是找到尽可能多的field匹配的doc，而不是某个field完全匹配的doc\n问题2：most_fields，没办法用minimum_should_match去掉长尾数据，就是匹配的特别少的结果\n问题3：对于TF/IDF算法，可能由于出现频率很低，得到的分数很高，这样匹配度高的反而会排到后面\n\n解决方法一:\n用copy_to，将多个field组合成一个field，避免了多个字段匹配的分数问题\nPUT /e/_mapping/article\n{\n  \"properties\": {\n      \"country\": { \"type\": \"string\",\"copy_to \": \"address\" },\n      \"province\": { \"type\": \"string\",\"copy_to\" : \"address\" },\n      \"address\": { \"type\" : \"string\" }\n  }\n}\n用了这个copy_to语法之后，就可以将多个字段的值拷贝到一个字段中，并建立倒排索引\n问题1：只是找到尽可能多的field匹配的doc，而不是某个field完全匹配的doc --&gt; 解决，最匹配的document被最先返回\n问题2：most_fields，没办法用minimum_should_match去掉长尾数据，就是匹配的特别少的结果 --&gt; 解决，可以使用minimum_should_match去掉长尾数据\n问题3：对于TF/IDF算法，可能由于出现频率很低，得到的分数很高，这样匹配度高的反而会排到后面 --&gt; 解决，组合为一个field，在所有document中出现的次数是均匀的，不会有极端的偏差</code></pre>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n', 2, '5f199b8885e24fc8b28672b872edb606', 12, '2017-07-26 23:41:21', '2018-09-06 16:12:58');
INSERT INTO `logcontent` VALUES ('aa23a267ba1540bc8c35440c883c7482', 1, '1dff2a5fdaeb4886938bb7c70b57acce', '重构之代码的坏味道', '重构-改善既有代码的设计', '重构-改善既有代码的设计', '<p>重复的代码</p>\n\n<p>同一个class内的两个函数含有相同表达式。这时候你需要做的就是采用Extract Method提炼出重复的代码，然后让这两个地点都调用被提炼出来的那一段代码。</p>\n\n<p>两个互为兄弟的subclasses内含相同表达式。要避免这种情况，只需对两个classes都使用Extract Method，然后再对被提炼出来的代码使用 Pull Up Field，将它推入superclass内。如果代码之 间只是类似，并非完全相同，那么就得运用Extract Method将相似部分和差异部分割开，构成单独一个函数。然后你可能发现或许可以运用Form Template Method获得一个Template Method设计模式。如果有些函数以不同的算法做相同的事，你可以择定其中较清晰的一个，并使用Substitute Algorithm将其他函数的算法替换掉。</p>\n\n<p>如果两个毫不相关的classes内出现[&hellip;]&rdquo;</p>\n\n<p>摘录来自: Martin Fowler，Kent Beck，John Brant ，William Opdyke ，Don Roberts. &ldquo;重构--改善既有代码的设计。&rdquo; iBooks.&nbsp;</p>', 1, '5f199b8885e24fc8b28672b872edb606', 1, '2018-09-23 15:36:41', '2018-09-23 17:40:39');
INSERT INTO `logcontent` VALUES ('ab0c9906880346a596a640403f52fb06', -1, '7338e53acd514defa1a17e47016f3f4a', 'Elasticsearch（十七）', 'mapping核心的数据类型,dynamic mapping,手动创建mapping_mapping,复杂数据类型查看,mapping', 'dynamic mapping/定制dynamic mapping策略', '<p><a name=\"dynamic mapping\"></a>1、dynamic mapping</p>\n\n<p>true：遇到陌生字段，就进行dynamic mapping</p>\n\n<p>false：遇到陌生字段，就忽略</p>\n\n<p>strict：遇到陌生字段，就报错</p>\n\n<pre>\n<code class=\"language-json\">PUT /my_index\n{\n  \"mappings\": {\n    \"my_type\": {\n      \"dynamic\": \"strict\",\n      \"properties\": {\n        \"title\": {\n          \"type\": \"text\"\n        },\n        \"address\": {\n          \"type\": \"object\",\n          \"dynamic\": \"true\"\n        }\n      }\n    }\n  }\n}</code></pre>\n\n<p><a name=\"定制dynamic mapping策略\"></a>2、定制dynamic mapping策略</p>\n\n<p>（1）date_detection</p>\n\n<p>默认会按照一定格式识别date，比如yyyy-MM-dd。但是如果某个field先过来一个2017-01-01的值，就会被自动dynamic mapping成date，后面如果再来一个&quot;hello world&quot;之类的值，就会报错。可以手动关闭某个type的date_detection，如果有需要，自己手动指定某个field为date类型。</p>\n\n<pre>\n<code class=\"language-json\">PUT /my_index/_mapping/my_type\n{\n    \"date_detection\": false\n}</code></pre>\n\n<p>（2）定制自己的dynamic mapping template（type level）</p>\n\n<pre>\n<code class=\"language-json\">PUT /my_index\n{\n    \"mappings\":{\n        \"my_type\":{\n            \"dynamic_templates\":[\n                {\n                    \"en\":{\n                        \"match\":\"*_en\",\n                        \"match_mapping_type\":\"string\",\n                        \"mapping\":{\n                            \"type\":\"string\",\n                            \"analyzer\":\"english\"\n                        }\n                    }\n                }\n            ]\n        }\n    }\n}\n=====操作=====\nPUT /my_index/my_type/1\n{\n  \"title\": \"this is my first article\"\n}\n\nPUT /my_index/my_type/2\n{\n  \"title_en\": \"this is my first article\"\n}\n=====结论=====\ntitle没有匹配到任何的dynamic模板，默认就是standard分词器，不会过滤停用词，is会进入倒排索引，用is来搜索是可以搜索到的\ntitle_en匹配到了dynamic模板，就是english分词器，会过滤停用词，is这种停用词就会被过滤掉，用is来搜索就搜索不到了</code></pre>\n\n<p>（3）定制自己的default mapping template（index level）</p>\n\n<pre>\n<code class=\"language-json\">PUT /my_index\n{\n    \"mappings\": {\n        \"_default_\": {\n            \"_all\": { \"enabled\":  false }\n        },\n        \"blog\": {\n            \"_all\": { \"enabled\":  true  }\n        }\n    }\n}\n</code></pre>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n', 2, '5f199b8885e24fc8b28672b872edb606', 7, '2017-07-10 18:04:47', '2018-09-06 16:12:58');
INSERT INTO `logcontent` VALUES ('af912d65ee2c4631bbb67e7af06639a0', 1, 'de93ceeed0464710af6dbb22e535d0a2', 'xlwt模块操作Excel笔记', '合并单元格,Excel写入,宽度,设置,日期格式,计算,链接,字体,对齐方式,边框,背景色,颜色图', '对python的xlwt进行简单总结', '<p><a id=\"Python环境和xlwt模块安装\" name=\"Python环境和xlwt模块安装\"></a>Python环境:</p>\n\n<pre>\n<code class=\"language-bash\">Windows版3.6.2</code></pre>\n\n<p>安装xlwt模块</p>\n\n<pre>\n<code class=\"language-python\">pip3 install xlwt</code></pre>\n\n<p><a id=\"导入模块\" name=\"导入模块\"></a>Head:设置编码和导入模块</p>\n\n<pre>\n<code class=\"language-python\"># -*- coding:utf8 -*-\nimport xlwt\nimport datetime</code></pre>\n\n<p><a id=\"创建Excel对象和Excel的sheet对象\" name=\"创建Excel对象和Excel的sheet对象\"></a>创建Excel对象和Excel的sheet对象</p>\n\n<pre>\n<code class=\"language-python\">wb=xlwt.Workbook(encoding = \'ascii\')#默认ascii可省略\nsheet=wb.add_sheet(\'sheetName\')</code></pre>\n\n<p><a id=\"xlwt操作Excel\" name=\"xlwt操作Excel\"></a>操作</p>\n\n<pre>\n<code class=\"language-python\">#插入数据i:行数0开始;j:列数0开始;value:插入值;style:Style对象;\nsheet.write(i,j,value,style);\n\n#合并单元格i:行数0开始;j:跨越行数0开始;k:列数0开始;l:跨越列数0开始;value:插入值;style:Style对象;\nsheet.write_merge(i, j, k, l, value,style)\n\n#设置单元格宽度\nsheet.col(0).width = 10000 # 数值10000约为Excel的38.29\n\n#设置日期格式\nstyle = xlwt.XFStyle()\nstyle.num_format_str = \'M/D/YY\' # （D-MMM-YY, D-MMM,MMM-YY, h:mm, h:mm:ss, h:mm, h:mm:ss, M/D/YY h:mm, mm:ss,[h]:mm:ss, mm:ss.0）\n\n#单元格计算\nsheet.write(0, 0, 5) # A1单元格值为5\nsheet.write(0, 1, 2) # B1单元格值为2\nsheet.write(1, 0, xlwt.Formula(\'A1*B1\')) # 输出10(A1[5] * A2[2])\nsheet.write(1, 1, xlwt.Formula(\'SUM(A1,B1)\')) # 输出\"7\" (A1[5] + A2[2])\n\n#链接\nsheet.write(0, 0,xlwt.Formula(\'HYPERLINK(\"http://www.google.com\";\"Google\")\')) #点击\"Google\"链至http://www.google.com\n\n#保存Excel\nwb.save(r\"D://NewCreateWorkbook.xls\")</code></pre>\n\n<p><a id=\"xlwt控制Excel样式\" name=\"xlwt控制Excel样式\"></a>样式</p>\n\n<pre>\n<code class=\"language-python\">style = xlwt.XFStyle() # 创建XFStyle对象\n\n#设置字体\nfont = xlwt.Font() # 创建Font对象\nfont.name = \'Times New Roman\' # 设置字体_简易版\nfont.bold = True # 设置粗体（True,False）\nfont.italic = True # 设置斜体（True,False）\nfont.struck_out = True # 删除线（True, False）\nfont.underline = xlwt.Font.UNDERLINE_SINGLE # 下划线_可配版（UNDERLINE_NONE, UNDERLINE_SINGLE, UNDERLINE_SINGLE_ACC,UNDERLINE_DOUBLE, UNDERLINE_DOUBLE_ACC）\nfont.escapement = xlwt.Font.ESCAPEMENT_SUPERSCRIPT # 上标（ESCAPEMENT_NONE, ESCAPEMENT_SUPERSCRIPT, ESCAPEMENT_SUBSCRIPT）\nfont.charset = xlwt.Font.CHARSET_ANSI_LATIN # 编码（CHARSET_ANSI_LATIN, CHARSET_SYS_DEFAULT, CHARSET_SYMBOL,CHARSET_APPLE_ROMAN, CHARSET_ANSI_JAP_SHIFT_JIS,CHARSET_ANSI_KOR_HANGUL, CHARSET_ANSI_KOR_JOHAB,CHARSET_ANSI_CHINESE_GBK, CHARSET_ANSI_CHINESE_BIG5,CHARSET_ANSI_GREEK, CHARSET_ANSI_TURKISH, CHARSET_ANSI_VIETNAMESE,CHARSET_ANSI_HEBREW, CHARSET_ANSI_ARABIC, CHARSET_ANSI_BALTIC,CHARSET_ANSI_CYRILLIC, CHARSET_ANSI_THAI, CHARSET_ANSI_LATIN_II,CHARSET_OEM_LATIN_I）\nfont.colour_index = 5 # 字体颜色,见颜色图\nfont.height = 1000 # Excel字号 = 数值/20\nstyle.font = font # 将Font对象赋给XFStyle对象\n\n#设置对齐方式\nalignment = xlwt.Alignment() # 创建Alignment对象\nalignment.horz = xlwt.Alignment.HORZ_CENTER # 水平对齐（HORZ_GENERAL,HORZ_LEFT, HORZ_CENTER, HORZ_RIGHT, HORZ_FILLED, HORZ_JUSTIFIED,HORZ_CENTER_ACROSS_SEL, HORZ_DISTRIBUTED）\nalignment.vert = xlwt.Alignment.VERT_CENTER # 垂直对齐（VERT_TOP,VERT_CENTER, VERT_BOTTOM, VERT_JUSTIFIED, VERT_DISTRIBUTED）\nstyle.alignment = alignment # 将Alignment对象赋给XFStyle对象\n\n#设置边框\nborders = xlwt.Borders() # 创建Borders对象\nborders.left = xlwt.Borders.DASHED # 左边框（NO_LINE, THIN, MEDIUM,DASHED, DOTTED, THICK, DOUBLE, HAIR, MEDIUM_DASHED,THIN_DASH_DOTTED, MEDIUM_DASH_DOTTED, THIN_DASH_DOT_DOTTED,MEDIUM_DASH_DOT_DOTTED, SLANTED_MEDIUM_DASH_DOTTED）\nborders.right = xlwt.Borders.DASHED # 右边框\nborders.top = xlwt.Borders.DASHED # 上边框\nborders.bottom = xlwt.Borders.DASHED # 下边框\nborders.left_colour = 5 # 左边框颜色,见颜色图\nborders.right_colour = 5 # 右边框颜色,见颜色图\nborders.top_colour = 5 # 上边框颜色,见颜色图\nborders.bottom_colour = 5 # 下边框颜色,见颜色图\nstyle.borders = borders # 将Borders对象赋给XFStyle对象\n\n#设置背景色\npattern = xlwt.Pattern() # 创建Pattern对象\npattern.pattern = xlwt.Pattern.SOLID_PATTERN # （NO_PATTERN,SOLID_PATTERN）\npattern.pattern_fore_colour = 5 # 背景色,见颜色图\nstyle.pattern = pattern # 将Pattern对象赋给XFStyle对象\n\n</code></pre>\n\n<p><a id=\"颜色图\" name=\"颜色图\"></a>颜色图</p>\n\n<table border=\"2\" cellspacing=\"0\" style=\"border-collapse:collapse; border:none\">\n	<tbody>\n		<tr>\n			<td style=\"background-color:black; height:13.5pt; vertical-align:bottom; white-space:nowrap; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap; width:15pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">0</span></span></span></td>\n			<td style=\"background-color:black; height:13.5pt; vertical-align:bottom; white-space:nowrap; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap; width:21pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">8</span></span></span></td>\n			<td style=\"background-color:maroon; height:13.5pt; vertical-align:bottom; white-space:nowrap; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap; width:21pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">16</span></span></span></td>\n			<td style=\"background-color:#9999ff; height:13.5pt; vertical-align:bottom; white-space:nowrap; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap; width:21pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">24</span></span></span></td>\n			<td style=\"background-color:navy; height:13.5pt; vertical-align:bottom; white-space:nowrap; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap; width:21pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">32</span></span></span></td>\n			<td style=\"background-color:#00ccff; height:13.5pt; vertical-align:bottom; white-space:nowrap; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap; width:21pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">40</span></span></span></td>\n			<td style=\"background-color:#3366ff; height:13.5pt; vertical-align:bottom; white-space:nowrap; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap; width:21pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">48</span></span></span></td>\n			<td style=\"background-color:#003366; height:13.5pt; vertical-align:bottom; white-space:nowrap; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap; width:21pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">56</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap; width:54pt\">&nbsp;</td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap; width:21pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">64</span></span></span></td>\n			<td style=\"background-color:black; height:13.5pt; vertical-align:bottom; white-space:nowrap; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap; width:21pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">72</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"background-color:white; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">1</span></span></span></td>\n			<td style=\"background-color:white; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">9</span></span></span></td>\n			<td style=\"background-color:green; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">17</span></span></span></td>\n			<td style=\"background-color:#993366; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">25</span></span></span></td>\n			<td style=\"background-color:fuchsia; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">33</span></span></span></td>\n			<td style=\"background-color:#ccffff; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">41</span></span></span></td>\n			<td style=\"background-color:#33cccc; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">49</span></span></span></td>\n			<td style=\"background-color:#339966; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">57</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\">&nbsp;</td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">65</span></span></span></td>\n			<td style=\"background-color:#c8c8c8; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">73</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"background-color:red; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">2</span></span></span></td>\n			<td style=\"background-color:red; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">10</span></span></span></td>\n			<td style=\"background-color:navy; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">18</span></span></span></td>\n			<td style=\"background-color:#ffffcc; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">26</span></span></span></td>\n			<td style=\"background-color:yellow; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">34</span></span></span></td>\n			<td style=\"background-color:#ccffcc; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">42</span></span></span></td>\n			<td style=\"background-color:#99cc00; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">50</span></span></span></td>\n			<td style=\"background-color:#003300; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">58</span></span></span></td>\n			<td style=\"background-color:#646464; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">66</span></span></span></td>\n			<td style=\"background-color:#373737; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">74</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"background-color:lime; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">3</span></span></span></td>\n			<td style=\"background-color:lime; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">11</span></span></span></td>\n			<td style=\"background-color:olive; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">19</span></span></span></td>\n			<td style=\"background-color:#ccffff; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">27</span></span></span></td>\n			<td style=\"background-color:aqua; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">35</span></span></span></td>\n			<td style=\"background-color:#ffff99; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">43</span></span></span></td>\n			<td style=\"background-color:#ffcc00; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">51</span></span></span></td>\n			<td style=\"background-color:#333300; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">59</span></span></span></td>\n			<td style=\"background-color:#f0f0f0; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">67</span></span></span></td>\n			<td style=\"background-color:#cce8cf; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">75</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"background-color:blue; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">4</span></span></span></td>\n			<td style=\"background-color:blue; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">12</span></span></span></td>\n			<td style=\"background-color:purple; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">20</span></span></span></td>\n			<td style=\"background-color:#660066; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">28</span></span></span></td>\n			<td style=\"background-color:purple; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">36</span></span></span></td>\n			<td style=\"background-color:#99ccff; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">44</span></span></span></td>\n			<td style=\"background-color:#ff9900; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">52</span></span></span></td>\n			<td style=\"background-color:#993300; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">60</span></span></span></td>\n			<td style=\"background-color:black; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">68</span></span></span></td>\n			<td style=\"background-color:#646464; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">76</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"background-color:yellow; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">5</span></span></span></td>\n			<td style=\"background-color:yellow; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">13</span></span></span></td>\n			<td style=\"background-color:teal; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">21</span></span></span></td>\n			<td style=\"background-color:#ff8080; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">29</span></span></span></td>\n			<td style=\"background-color:maroon; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">37</span></span></span></td>\n			<td style=\"background-color:#ff99cc; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">45</span></span></span></td>\n			<td style=\"background-color:#ff6600; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">53</span></span></span></td>\n			<td style=\"background-color:#993366; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">61</span></span></span></td>\n			<td style=\"background-color:white; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">69</span></span></span></td>\n			<td style=\"background-color:black; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">77</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"background-color:fuchsia; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">6</span></span></span></td>\n			<td style=\"background-color:fuchsia; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">14</span></span></span></td>\n			<td style=\"background-color:silver; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">22</span></span></span></td>\n			<td style=\"background-color:#0066cc; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">30</span></span></span></td>\n			<td style=\"background-color:teal; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">38</span></span></span></td>\n			<td style=\"background-color:#cc99ff; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">46</span></span></span></td>\n			<td style=\"background-color:#666699; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">54</span></span></span></td>\n			<td style=\"background-color:#333399; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">62</span></span></span></td>\n			<td style=\"background-color:#a0a0a0; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">70</span></span></span></td>\n			<td style=\"background-color:#cce8cf; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">78</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"background-color:aqua; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">7</span></span></span></td>\n			<td style=\"background-color:aqua; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">15</span></span></span></td>\n			<td style=\"background-color:gray; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">23</span></span></span></td>\n			<td style=\"background-color:#ccccff; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">31</span></span></span></td>\n			<td style=\"background-color:blue; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">39</span></span></span></td>\n			<td style=\"background-color:#ffcc99; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">47</span></span></span></td>\n			<td style=\"background-color:#969696; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">55</span></span></span></td>\n			<td style=\"background-color:#333333; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">63</span></span></span></td>\n			<td style=\"background-color:#0078d7; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">71</span></span></span></td>\n			<td style=\"background-color:black; height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">　</span></span></span></td>\n			<td style=\"height:13.5pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:宋体\">79</span></span></span></td>\n		</tr>\n	</tbody>\n</table>\n', 1, '5f199b8885e24fc8b28672b872edb606', 44, '2017-08-12 15:04:21', '2018-08-31 22:21:23');
INSERT INTO `logcontent` VALUES ('b011cfd4e9b44f31b168c0cfb56718ec', -1, 'e5e23ac742ca41a0a7e6f575fc710f4e', 'crontab详解及未执行原因', 'cron服务,crontab使用权限,crontab命令,crontab文件格式,crontab定时任务不执行的原因', 'Linux系统crontab定时器', '<p><a id=\"crontab\" name=\"crontab\"></a>crontab</p>\n\n<pre>\n<code class=\"language-makefile\">crontab命令常见于Unix和Linux的操作系统之中，用于设置周期性被执行的指令。该命令从标准输入设备读取指令，并将其存放于“crontab”文件中，以供之后读取和执行。通常，crontab储存的指令被守护进程激活。crond 常常在后台运行，每一分钟检查是否有预定的作业需要执行。这类作业一般称为cron jobs。</code></pre>\n\n<p><a id=\"crond服务\" name=\"crond服务\"></a>crond服务</p>\n\n<pre>\n<code class=\"language-makefile\">crontab不是Linux内核的功能，而是依赖crond服务\nservice crond start    //启动服务\nservice crond stop     //关闭服务\nservice crond restart  //重启服务\nservice crond reload   //重新载入配置\nservice crond status   //查看服务状态 </code></pre>\n\n<p><a id=\"使用权限\" name=\"使用权限\"></a>使用权限</p>\n\n<pre>\n<code class=\"language-makefile\">/etc/cron.deny文件中所列用户不允许使用crontab命令\n/etc/cron.allow文件中所列用户允许使用crontab命令\n/var/spool/cron/所有用户crontab文件存放的目录,以用户名命名</code></pre>\n\n<p><a id=\"crontab命令\" name=\"crontab命令\"></a>crontab命令</p>\n\n<pre>\n<code class=\"language-makefile\">crontab [-u user] file\ncrontab [-u user] [ -e | -l | -r ]\n参数：\n-u user：用来设定某个用户的crontab服务\nfile：file是命令文件的名字\n-e：编辑某个用户的crontab文件内容。如果不指定用户，则表示编辑当前用户的crontab文件。\n-l：显示某个用户的crontab文件内容，如果不指定用户，则表示显示当前用户的crontab文件内容。\n-r：从/var/spool/cron目录中删除某个用户的crontab文件，如果不指定用户，则默认删除当前用户的crontab文件。\n-i：在删除用户的crontab文件时给确认提示。</code></pre>\n\n<p><a id=\"crontab文件格式\" name=\"crontab文件格式\"></a>crontab文件格式</p>\n\n<pre>\n<code class=\"language-makefile\">格式：\n* * * * * command\n\n说明：\n第一个*：分，取值范围为0-59\n第二个*：时，取值范围为0-23\n第三个*：日，取值范围为1-31\n第四个*：月，取值范围为1-12\n第五个*：星期，取值范围为0-7\ncommand：需要执行的命令\n\neg:\n1、每分钟执行一次\n    * * * * * command\n2、每小时的第3和第15分钟执行\n    03,15 * * * * command\n3、在上午8点到11点的第3和第15分钟执行\n    03,15 08-11 * * * command\n4、每隔两天的上午8点到11点的第3和第15分钟执行\n    03,15 08-11 */2 * * command\n5、每个星期一的上午8点到11点的第3和第15分钟执行\n    03,15 08-11 * * 01 command\n6、每晚的21:30重启smb\n    30 21 * * * /etc/init.d/smb restart\n7、每月1、10、22日的4:45重启smb\n    00 11 04 * mon-wed /etc/init.d/smb restart\n8、一月一号的4点重启smb\n    00 04 01 jan * /etc/init.d/smb restart\n9、每小时执行/etc/cron.hourly目录内的脚本\n    01 * * * * root run-parts /etc/cron.hourly\n说明：\n如果去掉run-parts这个参数，后面就可以写要运行的脚本名，而不是目录名\n注意中间的间隔为空格</code></pre>\n\n<p><a id=\"任务不执行原因\" name=\"任务不执行原因\"></a>crontab定时任务不执行的原因</p>\n\n<pre>\n<code class=\"language-makefile\">1、crond命令不存在\n\n重新安装\n\nyum -y install vixie-cron\nyum -y install crontabs\n说明：\nvixie-cron 软件包是 cron 的主程序；\ncrontabs 软件包是用来安装、卸装、或列举用来驱动 cron 守护进程的表格的程序。\n\n在CentOS系统中加入开机自动启动: \n\nchkconfig --level 345 crond on\n\n2、crond服务是否启动\n\nservice crond status   //查看服务状态\n\n如果未启动可执行\n\nservice crond start\n\n3、脚本无执行权限\n\n对脚本添加可执行权限\n\nchmod +x fileName\n\n4、路径问题\n\ncrontab需要使用文件完整路径才能执行，可以在在脚本目录内使用pwd命令查看完整路径\n\n5、时差问题\n\n使用date命令查看服务器时间和本地时间是否一致\n\n6、环境变量问题\n\n执行任务调度时，是不会加载任何环境变量的，因此，需要在crontab文件中指定任务运行所需的所有环境变量\n\neg：export JAVA_HOME=/home/usr/local/java\n\n</code></pre>\n\n<p>&nbsp;</p>\n', 1, '5f199b8885e24fc8b28672b872edb606', 19, '2017-10-18 17:15:49', '2018-08-31 22:21:23');
INSERT INTO `logcontent` VALUES ('b2897ee40f974363bb95f6a14067fbf8', 1, 'd7caeba238f0466d87db109b2b9724da', '支持向量机(SVM)算法应用笔记一', 'svm应用', '初学', '<p><a id=\"Python环境\" name=\"Python环境\"></a>Python环境</p>\n\n<pre>\n<code>Windows版 Python 3.6.2</code></pre>\n\n<p><a id=\"Python代码实现\" name=\"Python代码实现\"></a>Python代码实现</p>\n\n<pre>\n<code># -*- coding:utf8 -*-\nimport numpy as np\nimport pylab as pl\nfrom sklearn import svm\n\n# 通过+-[2, 2],分别创造出20个在超平面上20个在超平面下的点\nX = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20, 2) + [2, 2]]\nY = [0]*20 +[1]*20\n\n#建立模型\nclf = svm.SVC(kernel=\'linear\')\nclf.fit(X, Y)\n\n#W0X + W1Y + W3 = 0 可以转化为Y = - ( W0 / W1 ) x + ( W3 / W1 )\nw = clf.coef_[0]                    #W的值即{W0,W1}\na = -w[0]/w[1]                      #求斜率，即上方转化公式中(W0/W1)\nxx = np.linspace(-5, 5)             #在-5至5间随机取一些X\nyy = a*xx - (clf.intercept_[0])/w[1]#clf.intercept_[0]为W3，带入方程，求得Y\n\n#clf.support_vectors_[0]为clf.support_vectors_的第一个即最小值带入，求得超平面下方截距\nb = clf.support_vectors_[0]\nyy_down = a*xx + (b[1] - a*b[0])\n#clf.support_vectors_[-1]为clf.support_vectors_的最后一个即最大值带入，求得超平面上方截距\nb = clf.support_vectors_[-1]\nyy_up = a*xx + (b[1] - a*b[0])\n\nprint(\"w: \", w)#w:  [ 0.57500546  0.89165015]\nprint(\"a: \", a)#a:  -0.644877881431\n\nprint(\"support_vectors_: \", clf.support_vectors_)   #support_vectors_:  [[-0.18935209 -0.76777052][ 0.83227026  0.81644003]]\nprint(\"clf.coef_: \", clf.coef_)                     #clf.coef_:  [[ 0.57500546  0.89165015]]\n\npl.plot(xx, yy, \'k-\')       #超平面\npl.plot(xx, yy_down, \'k--\') #超平面下方的切线\npl.plot(xx, yy_up, \'k--\')   #超平面上方的切线\n\npl.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n          s=80, facecolors=\'none\')\npl.scatter(X[:, 0], X[:, 1], c=Y, cmap=pl.cm.Paired)\n\npl.axis(\'tight\')\npl.show()</code></pre>\n\n<p>&nbsp;</p>\n', 1, '5f199b8885e24fc8b28672b872edb606', 6, '2017-11-19 21:35:39', '2018-08-31 22:21:23');
INSERT INTO `logcontent` VALUES ('b657bb887b0143a2a0f9226dce025f5c', 1, 'd7caeba238f0466d87db109b2b9724da', '监督学习之决策树算法笔记', '算法的评估', '初学', '<p><a id=\"分类和预测算法评估\" name=\"分类和预测算法评估\"></a>机器学习中分类和预测算法的评估：</p>\n\n<pre>\n<code>准确率：算法达到的准确率是多少。\n速度：算法复杂度高不高，速度快不快。\n强壮行：当数据有一些缺失或者有一些干扰时，算法的表现是不是非常好。\n可规模性：不仅在小规模范围使用在数量指数级增长的大规模范围是否依然适用。\n可解释性：是否容易观察解释</code></pre>\n\n<p><a id=\"决策树/判定树\" name=\"决策树/判定树\"></a>什么是决策树/判定树（decision tree)? &nbsp; &nbsp; &nbsp;<br />\n判定树是一个类似于流程图的树结构：其中，每个内部结点表示在一个属性上的测试，每个分支代表一个属性输出，而每个树叶结点代表类或类分布。树的最顶层是根结点。</p>\n\n<p><img src=\"https://gss0.bdstatic.com/-4o3dSag_xI4khGkpoWK1HF6hhy/baike/c0%3Dbaike80%2C5%2C5%2C80%2C26/sign=89c8d6461138534398c28f73f27adb1b/c2cec3fdfc0392456a6ac4258694a4c27d1e2538.jpg\" style=\"height:212px; width:300px\" /></p>\n\n<p><a id=\"熵（entropy）概念\" name=\"熵（entropy）概念\"></a>熵（entropy）概念：</p>\n\n<pre>\n<code>信息和抽象，如何度量？\n1948年，香农提出了 ”信息熵(entropy)“的概念，一条信息的信息量大小和它的不确定性有直接的关系，即要搞清楚的事情不确定性越大，我们需要了解的信息量越大。\n计算公式（比特(bit)来衡量信息的多少）\nH(x) = E[I(xi)] = E[ log(2,1/p(xi)) ] = -∑p(xi)log(2,p(xi)) (i=1,2,..n)\n变量的不确定性越大，熵值越大</code></pre>\n\n<p><a id=\"决策树归纳算法之ID3\" name=\"决策树归纳算法之ID3\"></a>决策树归纳算法之ID3</p>\n\n<pre>\n<code>选择属性判断结点（节点的选择）\n\n根据信息获取量来进行选择\n信息获取量(Information Gain)：\nGain(A) = Info(D) - Infor_A(D)\n通过A来作为节点分类获取了多少信息（原始获得的信息量减去加上该属性后获得的信息量，得到该属性的信息获取量）</code></pre>\n\n<p><a id=\"例子\" name=\"例子\"></a>例子：</p>\n\n<table border=\"1\" cellpadding=\"1\" cellspacing=\"1\" style=\"width:500px\">\n	<tbody>\n		<tr>\n			<td>RID</td>\n			<td>age</td>\n			<td>income</td>\n			<td>student</td>\n			<td>credit_rating</td>\n			<td>Class:buy_computer</td>\n		</tr>\n		<tr>\n			<td>1</td>\n			<td>youth</td>\n			<td>high</td>\n			<td>no</td>\n			<td>fair</td>\n			<td>no</td>\n		</tr>\n		<tr>\n			<td>2</td>\n			<td>youth</td>\n			<td>high</td>\n			<td>no</td>\n			<td>excellent</td>\n			<td>no</td>\n		</tr>\n		<tr>\n			<td>3</td>\n			<td>middle_aged</td>\n			<td>high</td>\n			<td>no</td>\n			<td>fair</td>\n			<td>yes</td>\n		</tr>\n		<tr>\n			<td>4</td>\n			<td>senior</td>\n			<td>medium</td>\n			<td>no</td>\n			<td>fair</td>\n			<td>yes</td>\n		</tr>\n		<tr>\n			<td>5</td>\n			<td>senior</td>\n			<td>low</td>\n			<td>yes</td>\n			<td>fair</td>\n			<td>yes</td>\n		</tr>\n		<tr>\n			<td>6</td>\n			<td>senior</td>\n			<td>low</td>\n			<td>yes</td>\n			<td>excellent</td>\n			<td>no</td>\n		</tr>\n		<tr>\n			<td>7</td>\n			<td>middle_aged</td>\n			<td>low</td>\n			<td>yes</td>\n			<td>excellent</td>\n			<td>yes</td>\n		</tr>\n		<tr>\n			<td>8</td>\n			<td>youth</td>\n			<td>medium</td>\n			<td>no</td>\n			<td>fair</td>\n			<td>no</td>\n		</tr>\n		<tr>\n			<td>9</td>\n			<td>youth</td>\n			<td>low</td>\n			<td>yes</td>\n			<td>fair</td>\n			<td>yes</td>\n		</tr>\n		<tr>\n			<td>10</td>\n			<td>senior</td>\n			<td>medium</td>\n			<td>yes</td>\n			<td>fair</td>\n			<td>yes</td>\n		</tr>\n		<tr>\n			<td>11</td>\n			<td>youth</td>\n			<td>medium</td>\n			<td>yes</td>\n			<td>excellent</td>\n			<td>yes</td>\n		</tr>\n		<tr>\n			<td>12</td>\n			<td>middle_aged</td>\n			<td>medium</td>\n			<td>no</td>\n			<td>excellent</td>\n			<td>yes</td>\n		</tr>\n		<tr>\n			<td>13</td>\n			<td>middle_aged</td>\n			<td>high</td>\n			<td>yes</td>\n			<td>fair</td>\n			<td>yes</td>\n		</tr>\n		<tr>\n			<td>14</td>\n			<td>senior</td>\n			<td>medium</td>\n			<td>no</td>\n			<td>excellent</td>\n			<td>no</td>\n		</tr>\n	</tbody>\n</table>\n\n<pre>\n<code>首先不按任何属性分，按目标函数（标记）即Class:buy_computer来分信息度是多少\n\n14人9个yes、5个no，即\nInfo(D)=-(9/14)log2(9/14)-(5/14)log2(5/14)=0.940bits\n\n如果以age来分\nage中14人中5个yonth(其中3个no、2个yes)；4个middle_aged(其中0个no、4个yes)；5个senior(其中2个no、3个yes)；\nInfo(D)=(5/14)×(-(2/5)log2(2/5)-(3/5)log2(3/5)) + (4/14)×(-(4/4)log2(4/4)-(0/4)log2(0/4)) + (5/14)×(-(3/5)log2(3/5)-(2/5)log2(2/5)) = 0.694\n\nGain(age) = Info(D)-Info_age(D) = 0.940 - 0.694 = 0.246 bits\n同理：Gain(income) = 0.029; Gain(student) = 0.151; Gain(credit_rating) = 0.048;\n\nGain(age)&gt;Gain(student)&gt;Gain(credit_rating)&gt;Gain(income)\n所以选择age作为第一个根节点\n\n重复。。。</code></pre>\n\n<pre>\n<code>树以代表训练样本的单个结点开始。\n\n如果样本都在同一个类，则该结点成为树叶，并用该类标号。\n\n否则，算法使用称为信息增益的基于熵的度量作为启发信息，选择能够最好地将样本分类的属性。该属性成为该结点的“测试”或“判定”属性。在算法的该版本中，\n\n所有的属性都是分类的，即离散值。连续属性必须离散化。\n\n对测试属性的每个已知的值，创建一个分枝，并据此划分样本。\n\n算法使用同样的过程，递归地形成每个划分上的样本判定树。一旦一个属性出现在一个结点上，就不必该结点的任何后代上考虑它。\n\n递归划分步骤仅当下列条件之一成立停止：\n(a) 给定结点的所有样本属于同一类。\n(b) 没有剩余属性可以用来进一步划分样本。在此情况下，使用多数表决。\n这涉及将给定的结点转换成树叶，并用样本中的多数所在的类标记它。替换地，可以存放结\n点样本的类分布。\n(c) 分枝\ntest_attribute = a i 没有样本。在这种情况下，以 samples 中的多数类\n创建一个树叶</code></pre>\n\n<p><a id=\"其他算法\" name=\"其他算法\"></a>其他算法</p>\n\n<pre>\n<code>C4.5、 Classification and Regression Trees (CART)\n共同点：都是贪心算法，自上而下(Top-down approach)\n区别：属性选择度量方法不同： C4.5 (gain ratio), CART(gini index), ID3 (Information Gain)</code></pre>\n\n<p><a id=\"树剪枝叶\" name=\"树剪枝叶\"></a>树剪枝叶 （避免overfitting)</p>\n\n<pre>\n<code>如果树的深度太大，可能导致在训练集上表现很好，但是在测试集上表现不是很理想，这时一般采用两种方式：\n先剪枝：分到一定程度就不在分了；\n后剪枝：先完全建好，然后再把叶子减掉；</code></pre>\n\n<p><a id=\"决策树的优缺点\" name=\"决策树的优缺点\"></a>决策树的优点：</p>\n\n<pre>\n<code>直观，便于理解，小规模数据集有效</code></pre>\n\n<p>决策树的缺点：</p>\n\n<pre>\n<code>处理连续变量不好（离散化的阈值对结果影响较大）\n类别较多时，错误增加的比较快\n可规模性一般（小规模的数据表现非常好，大规模的数据算法复杂度增加非常大，对每一个属性都需要去测试）</code></pre>\n\n<p>&nbsp;</p>\n', 1, '5f199b8885e24fc8b28672b872edb606', 9, '2017-10-26 22:08:22', '2018-08-31 22:21:23');
INSERT INTO `logcontent` VALUES ('b77ba96f04e147038483cee6010e546d', -1, '7338e53acd514defa1a17e47016f3f4a', 'Elasticsearch（四）', 'query string,searchquery,DSL查询所有,查询名称包含分页查询,指定要查询的字段,query filter搜索名称,full-text search（全文检索）,phrase search（短语搜索）,highlight search（高亮搜索结果）,timed_out,_shards,took,total,max_score,hits,精准度', '两种查询方式：query string search/query DSL', '<p>两种查询方式：query string search/query DSL</p>\n\n<p><a name=\"query string search\"></a>1、query string search</p>\n\n<p>（1）搜索全部</p>\n\n<pre>\n<code class=\"language-json\">GET /epb_jkqy/epb_jkqy/_search\n\ntook：耗费了几毫秒\n\ntimed_out：默认无timeout，latency平衡completeness，手动指定timeout\n    timeout=10ms，timeout=1s，timeout=1m\n    GET /_search?timeout=10m\n    timeout查询执行机制：在timeout时间范围内将数据返回给客户端，注意可能是全部数据也可能是部分数据，并不一定等到\n    所有数据都检索出来再返回\n\n_shards：数据拆成了5个分片，所以对于搜索请求，会打到所有的primary shard（或者是它的某个replica shard也可以）\n\nhits.total：查询结果的数量，3个document\n\nhits.max_score：\n    相关度评分TF&amp;IDF算法：Elasticsearch使用的是 term frequency/inverse document frequency算法，简称为TF/IDF算法，\n        规则：\n        Term frequency：搜索文本中的各个词条在field文本中出现了多少次，出现次数越多，相关度越高\n        Inverse document frequency：搜索文本中的各个词条在整个索引的所有文档中出现了多少次，出现的次数越多，\n            相关度越低\n        Field-length norm：field长度，field越长，相关度越低\n    _score是如何被计算出来的:添加?explain，结果复杂，暂不讨论\n        GET /epb_jkqy/epb_jkqy/_search?explain\n        {\n          \"query\": {\n            \"match\": {\n              \"pname\": \"永州 有限公司\"\n            }\n          }\n        }\n    分析一个document是如何被匹配上的:添加_explain，结果复杂，暂不讨论\n    GET /epb_jkqy/epb_jkqy/2/_explain\n    {\n      \"query\": {\n        \"match\": {\n          \"pname\": \"永州 有限公司\"\n        }\n      }\n    }\n\n   \nhits.hits：包含了匹配搜索的document的详细数据</code></pre>\n\n<p>（2）query string基础语法</p>\n\n<pre>\n<code class=\"language-json\">查询pname包含永州福嘉的数据\nGET /epb_jkqy/epb_jkqy/_search?q=pname:永州福嘉\n\n查询pname必须包含永州福嘉的数据（与上同）\nGET /epb_jkqy/epb_jkqy/_search?q=+pname:永州福嘉\n\n查询pname不包含永州福嘉的数据\nGET /epb_jkqy/epb_jkqy/_search?q=-pname:永州福嘉</code></pre>\n\n<p>（3）_all metadata的原理和作用</p>\n\n<pre>\n<code class=\"language-json\">GET /epb_jkqy/epb_jkqy/_search?q=永州福嘉\n直接可以搜索所有的field，任意一个field包含指定的关键字就可以搜索出来。\n\n我们在进行中搜索的时候，难道是对document中的每一个field都进行一次搜索吗？不是的\nes中的_all元数据，在建立索引的时候，我们插入一条document，它里面包含了多个field，此时，es会自动将多个field的值，\n全部用字符串的方式串联起来，变成一个长的字符串，作为_all field的值，同时建立索引\n\n后面如果在搜索的时候，没有对某个field指定搜索，就默认搜索_all field，其中是包含了所有field的值的\n\n举个例子\n\n{\n  \"name\": \"jack\",\n  \"age\": 26,\n  \"email\": \"jack@sina.com\",\n  \"address\": \"guamgzhou\"\n}\n\n\"jack 26 jack@sina.com guangzhou\"，作为这一条document的_all field的值，同时进行分词后建立对应的倒排索\n</code></pre>\n\n<pre>\n<code class=\"language-json\">搜索pname中包含永州福嘉，而且按照年份降序排序：GET /epb_jkqy/epb_jkqy/_search?q=pname:永州福嘉&amp;sort=years:desc</code></pre>\n\n<p>适用于临时的在命令行使用一些工具，比如curl，快速的发出请求，来检索想要的信息；但是如果查询请求很复杂，是很难去构建的</p>\n\n<p>在生产环境中，几乎很少使用query string search</p>\n\n<p>&nbsp;</p>\n\n<p><a name=\"query DSL\"></a>2、query DSL</p>\n\n<p>DSL：Domain Specified Language，Es专用语言</p>\n\n<p>http request body：请求体，可以用json的格式来构建查询语法，比较方便，可以构建各种复杂的语法，功能强大</p>\n\n<p>&nbsp;</p>\n\n<p>（1）full-text search（全文检索）</p>\n\n<p>不是单纯的只是匹配完整的一个值，而是可以对值进行拆分词语后（分词）进行匹配，也可以通过缩写、时态、大小写、同义词等进行匹配</p>\n\n<p>&lt;1&gt;缩写：cn china</p>\n\n<p>&lt;2&gt;时态：like liked likes</p>\n\n<p>&lt;3&gt;大小写：Tom tom</p>\n\n<p>&lt;4&gt;同义词：like love</p>\n\n<p>&lt;5&gt;等...</p>\n\n<pre>\n<code class=\"language-json\">GET /epb_jkqy/epb_jkqy/_search\n{\n    \"query\":{\n        \"match\":{\n            \"pname\":\"永州 福嘉\"\n        }\n    }\n}\n包含永州福嘉有色金属有限公司、嘉兴福鑫纸业有限公司、国电永福发电有限公司的检索结果，但是相关度（_score）不一样</code></pre>\n\n<p>&lt;1&gt;查询所有</p>\n\n<pre>\n<code class=\"language-json\">GET /epb_jkqy/epb_jkqy/_search\n{\n  \"query\": {\n     \"match_all\": {}\n  }\n}</code></pre>\n\n<p>&lt;2&gt;搜索标题中包含java或elasticsearch的数据</p>\n\n<pre>\n<code class=\"language-markdown\">GET /e/p/_search\n{\n    \"query\": {\n        \"match\": {\n            \"title\": \"java elasticsearch\"\n        }\n    }\n}</code></pre>\n\n<p>&lt;3&gt;搜索标题中包含java和elasticsearch的数据,那么就用and</p>\n\n<pre>\n<code class=\"language-markdown\">GET /e/p/_search\n{\n    \"query\":{\n        \"match\":{\n            \"title\":{\n                \"query\":\"java elasticsearch\",\n                \"operator\":\"and\"\n            }\n        }\n    }\n}</code></pre>\n\n<p>&lt;4&gt;搜索包含java，elasticsearch，spark，hadoop，4个关键字中，至少3个的数据</p>\n\n<pre>\n<code class=\"language-markdown\">GET /e/p/_search\n{\n  \"query\": {\n    \"match\": {\n      \"title\": {\n        \"query\": \"java elasticsearch spark hadoop\",\n        \"minimum_should_match\": \"75%\"\n      }\n    }\n  }\n}</code></pre>\n\n<p>&lt;5&gt;query为查询条件，_source为查询字段，from/size为分页，sort为排序</p>\n\n<pre>\n<code class=\"language-json\">GET /e/p/_search\n{\n   \"query\": {\n     \"match\": {\n       \"FIELD\": \"TEXT\"\n     }\n   },\n   \"_source\": [\"field\"],\n   \"from\": 0,\n   \"size\": 20,\n   \"sort\": [\n     {\n       \"FIELD\": {\n         \"order\": \"desc\"\n       }\n     }\n   ]\n}</code></pre>\n\n<p>&lt;6&gt;组合多个查询条件</p>\n\n<pre>\n<code class=\"language-json\">详细为Elasticsearch（十八）\nhttp://www.localhost:8080/myLog/detailContent.jsp?id=a576f4b74b26443f8b1b9b8a98906f03</code></pre>\n\n<p>（2）match_phrase</p>\n\n<p>必须完全匹配，部分匹配无效(pname为永州福嘉)</p>\n\n<pre>\n<code class=\"language-json\">{\n  \"query\": {\n    \"match_phrase\": {\n      \"pname\": {\n        \"query\": \"永州福嘉\"\n      }\n    }\n  }\n}</code></pre>\n\n<p>（3）proximity match</p>\n\n<p>有时我们会希望有个可调节因子，临近也满足slop，(pname为永州福嘉)</p>\n\n<pre>\n<code class=\"language-markdown\">query string搜索文本中的几个term,要经过几次移动才能与一个document匹配，这个移动的次数，就是slop\n举例：\nhello world, java is very good, spark is also very good.\njava spark搜不到\n如果我们指定了slop，那么就允许java spark进行移动，来尝试与doc进行匹配\n\njava		is		very		good		spark		is\njava		spark\njava		--&gt;		spark\njava				--&gt;		spark\njava						--&gt;		spark\n\n这里的slop，就是3，因为java spark这个短语，spark移动了3次，就可以跟一个doc匹配上了\n{\n  \"query\": {\n    \"match_phrase\": {\n      \"pname\": {\n        \"query\": \"java spark\",\n        \"slop\": 1\n      }\n    }\n  }\n}\n</code></pre>\n\n<p>match和phrase match(proximity match)区别</p>\n\n<pre>\n<code>match --&gt; 只要简单的匹配到了一个term，就可以理解将term对应的doc作为结果返回，扫描倒排索引，扫描到了就ok\nphrase match --&gt; 首先扫描到所有term的doc list; 找到包含所有term的doc list; 然后对每个doc都计算每个term的position，是否符合指定的范围; slop，需要进行复杂的运算，来判断能否通过slop移动，匹配一个doc\n\nmatch query的性能比phrase match和proximity match（有slop）要高很多。因为后两者都要计算position的距离。\nmatch query比phrase match的性能要高10倍，比proximity match的性能要高20倍。\n\n但是因为es的性能一般都在毫秒级别，match query一般几毫秒，或者几十毫秒，而phrase match和proximity match的性能在几十毫秒到几百毫秒之间，所以也是可以接受的。\n\n优化proximity match的性能，一般就是减少要进行proximity match搜索的document数量。主要思路就是，用match query先过滤出需要的数据，然后再用proximity match来根据term距离提高doc的分数，因为一般用户会分页查询，只会看到前几页的数据，所以不需要对所有结果进行proximity match操作，所以proximity match只针对每个shard的分数排名前n个doc起作用，来重新调整它们的分数，这个过程称之为rescoring，重计分。\n\nmatch + proximity match同时实现召回率和精准度\nmatch：1000个doc，其实这时候每个doc都有一个分数了; proximity match，前50个doc，进行rescore，重打分，即可; 让前50个doc，term举例越近的，排在越前面\nGET /e/p/_search \n{\n  \"query\": {\n    \"match\": { \"content\": \"java spark\" }\n  },\n  \"rescore\": {\n    \"window_size\": 50,\n    \"query\": {\n      \"rescore_query\": {\n        \"match_phrase\": {\n          \"content\": { \"query\": \"java spark\",\"slop\": 50 }\n        }\n      }\n    }\n  }\n}\n</code></pre>\n\n<p>（4）multi_phrase</p>\n\n<p>多个字段进行匹配，其中一个字段有这个文档就满足(pname/eventName/xzqhCode字段中检索昌邑)</p>\n\n<pre>\n<code class=\"language-json\">{\n  \"query\": {\n    \"multi_match\": {\n      \"query\": \"昌邑\",\n      \"fields\": [\n        \"pname\",\n        \"eventName\",\n        \"xzqhCode\"\n      ]\n    }\n  }\n}</code></pre>\n\n<p>（5）best_fields</p>\n\n<p>我们希望完全匹配的文档占的评分比较高，则需要使用</p>\n\n<pre>\n<code class=\"language-json\">{\n  \"query\": {\n    \"multi_match\": {\n      \"query\": \"昌邑\",\n      \"type\": \"best_fields\",\n      \"fields\": [\n        \"pname\",\n        \"eventName\",\n        \"xzqhCode\"\n      ],\n      \"tie_breaker\": 0.3\n    }\n  }\n}</code></pre>\n\n<p>（6）most_fields</p>\n\n<p>我们希望字段被匹配越多的文档评分越高，就要使用</p>\n\n<pre>\n<code class=\"language-json\">{\n  \"query\": {\n    \"multi_match\": {\n      \"query\": \"昌邑\",\n      \"type\": \"most_fields\",\n      \"fields\": [\n        \"pname\",\n        \"eventName\",\n        \"xzqhCode\"\n      ]\n    }\n  }\n}</code></pre>\n\n<p>（7）cross_fields</p>\n\n<p>我们会希望这个词条的分词词汇是分配到不同字段中的，那么就使用</p>\n\n<pre>\n<code class=\"language-json\">{\n  \"query\": {\n    \"multi_match\": {\n      \"query\": \"昌邑\",\n      \"type\": \"cross_fields\",\n      \"fields\": [\n        \"pname\",\n        \"eventName\",\n        \"xzqhCode\"\n      ]\n    }\n  }\n}</code></pre>\n\n<p>（8）scoll滚动搜索</p>\n\n<p>如果一次性查10万条数据，那么性能会很差，此时一般会采取用scoll滚动查询，一批一批的查，直到所有数据都查询处理完</p>\n\n<p>scoll搜索会在第一次搜索的时候，保存一个当时的视图快照，之后只会基于该旧的视图快照提供数据搜索，如果这个期间数据变更，是不会让用户看到的</p>\n\n<p>采用基于_doc进行排序的方式，性能较高</p>\n\n<p>每次发送scroll请求，我们还需要指定一个scoll参数，指定一个时间窗口，每次搜索请求只要在这个时间窗口内能完成就可以了</p>\n\n<pre>\n<code class=\"language-json\">GET /e/p/_search?scroll=1m\n{\n  \"query\": {\n    \"match_all\": {}\n  },\n  \"sort\": [\n    \"_doc\"\n  ],\n  \"size\": 2\n}\n=====结果======\n{\n  \"_scroll_id\": \"cXVlcnlUaGVuRmV0Y2g7NTs4NzIyNDE6dnFtQzdNWnBURi1EZVdWSEdscmI5ZzsxMDEzMTYzOjJYbFN0Q2NhUUEtUTFqblh\n  BYjNYR1E7NzEyOTA2OkhDQUxtME54VGVHSmpLX2dnc2U1MHc7NjIwNTQ2OkZqX203cHd2UzdxZ3ZESkYzdDhQdnc7Nzc2MTA0OmZzTkdVNkxZV\n  HFPUTFRQnZQWmZSdXc7MDs=\",\n  \"took\": 2,\n  \"timed_out\": false,\n  \"_shards\": {\n    \"total\": 5,\n    \"successful\": 5,\n    \"failed\": 0\n  },\n  \"hits\": {\n    \"total\": 109213,\n    \"max_score\": null,\n    \"hits\": [\n      {\n        \"_index\": \"epb_jkqy\",\n        \"_type\": \"epb_jkqy\",\n        \"_id\": \"epb_jkqy19996\",\n        \"_score\": null,\n        \"_source\": {\n          \"sortTime\": null,\n          \"body\": null\n        },\n        \"sort\": [\n          1060\n        ]\n      }\n    ]\n  }\n}\n=====再次查询=====\nGET /_search/scroll\n{\n    \"scroll\": \"1m\", \n    \"scroll_id\": \"cXVlcnlUaGVuRmV0Y2g7NTs4NzIyNDE6dnFtQzdNWnBURi1EZVdWSEdscmI5ZzsxMDEzMTYzOjJYbFN0Q2NhUUEtUTFqblh\n  BYjNYR1E7NzEyOTA2OkhDQUxtME54VGVHSmpLX2dnc2U1MHc7NjIwNTQ2OkZqX203cHd2UzdxZ3ZESkYzdDhQdnc7Nzc2MTA0OmZzTkdVNkxZV\n  HFPUTFRQnZQWmZSdXc7MDs=\"\n}\nscoll,看起来挺像分页的，但是其实使用场景不一样。分页主要是用来一页一页搜索，给用户看的；\nscoll主要是用来一批一批检索数据，让系统进行处理的</code></pre>\n\n<p>（9）定位不合法的搜索及其原因</p>\n\n<pre>\n<code class=\"language-json\">GET /epb_jkqy/epb_jkqy/_validate/query?explain\n{\n  \"quey\": {}\n}\n======结果======\n{\n  \"valid\": false,\n  \"error\": \"org.elasticsearch.common.ParsingException: request does not support [quey]\"\n}\n</code></pre>\n\n<p>（10）解决字符串排序问题</p>\n\n<p>如果对一个string field直接进行排序，结果往往不准确，因为分词后是多个单词，再排序可能不是我们想要的结果</p>\n\n<p>通常解决方案是，将一个string field建立两次索引，一个分词，用来进行搜索；一个不分词，保持整体，用来进行排序</p>\n\n<pre>\n<code class=\"language-json\">PUT /website \n{\n  \"mappings\": {\n    \"article\": {\n      \"properties\": {\n        \"title\": {\n          \"type\": \"text\",\n          \"fields\": {\n            \"raw\": {\n              \"type\": \"string\",\n              \"index\": \"not_analyzed\"\n            }\n          },\n          \"fielddata\": true\n        }\n      }\n    }\n  }\n}\n\n查询并排序\nGET /website/article/_search\n{\n  \"query\": {\n    \"match\": {\n        \"title\":\"ceshi\"\n    }\n  },\n  \"sort\": [\n    {\n      \"title.raw\": {\n        \"order\": \"desc\"\n      }\n    }\n  ]\n}</code></pre>\n\n<p>（11）highlight search（高亮搜索结果）</p>\n\n<pre>\n<code class=\"language-json\">GET /epb_jkqy/epb_jkqy/_search\n{\n  \"query\": {\n    \"match\": {\n      \"pname\": \"昌邑 公司\"\n    }\n  },\n  \"highlight\": {\n    \"fields\": {\n      \"pname\": {}\n    }\n  }\n}\n=====结果======\n{\n  \"took\": 8,\n  \"timed_out\": false,\n  \"_shards\": {\n    \"total\": 5,\n    \"successful\": 5,\n    \"failed\": 0\n  },\n  \"hits\": {\n    \"total\": 93449,\n    \"max_score\": 2.8541431,\n    \"hits\": [\n      {\n        \"_index\": \"epb_jkqy\",\n        \"_type\": \"epb_jkqy\",\n        \"_id\": \"epb_jkqy2147\",\n        \"_score\": 2.8541431,\n        \"_source\": {\n          \"sortTime\": null,\n          \"body\": null,\n          \"createTime\": 1496736169000,\n          \"dataType\": \"epb_jkqy\",\n          \"county\": \"\",\n          \"postTime\": null\n        },\n        \"highlight\": {\n          \"pname\": [\n            \"&lt;em&gt;昌&lt;/em&gt;&lt;em&gt;邑&lt;/em&gt;海洋水业有限&lt;em&gt;公&lt;/em&gt;&lt;em&gt;司&lt;/em&gt;\"\n          ]\n        }\n      }\n    ]\n  }\n}</code></pre>\n\n<p>&nbsp;</p>\n', 2, '5f199b8885e24fc8b28672b872edb606', 18, '2017-07-10 18:05:04', '2018-09-07 10:19:23');
INSERT INTO `logcontent` VALUES ('b983a5343e1a41ca9d415dd4ea2b285f', -1, '7338e53acd514defa1a17e47016f3f4a', 'Elasticsearch（十五）', '分页搜索,deep paging', '分页搜索/deep paging', '<p><a name=\"分页搜索\"></a>如何使用es进行分页搜索的语法</p>\n\n<pre>\n<code class=\"language-json\">size，from\n\nGET /_search?size=10\n\nGET /_search?size=10&amp;from=0\n\nGET /_search?size=10&amp;from=20\n\nGET /e/p/_search\n  \"hits\": {\n    \"total\": 4,\n    \"max_score\": 1,\n    ...\n    \n我们假设将这4条数据分成2页，每一页是2条数据，来实验一下这个分页搜索的效果\n\nGET /e/p/_search?from=0&amp;size=2\n\n{\n  \"took\": 3,\n  \"timed_out\": false,\n  \"_shards\": {\n    \"total\": 5,\n    \"successful\": 5,\n    \"failed\": 0\n  },\n  \"hits\": {\n    \"total\": 4,\n    \"max_score\": 1,\n    \"hits\": [\n      {\n        \"_index\": \"e\",\n        \"_type\": \"p\",\n        \"_id\": \"2\",\n        \"_score\": 1,\n        \"_source\": {\n          \"name\": \"jiajieshi yagao\",\n          \"desc\": \"youxiao fangzhu\",\n          \"price\": 25,\n          \"producer\": \"jiajieshi producer\",\n          \"tags\": [\n            \"fangzhu\"\n          ]\n        }\n      },\n      {\n        \"_index\": \"e\",\n        \"_type\": \"p\",\n        \"_id\": \"4\",\n        \"_score\": 1,\n        \"_source\": {\n          \"num\": 0,\n          \"tags\": []\n        }\n      }\n    ]\n  }\n}\n第一页：id=2,4\n\nGET /test_index/test_type/_search?from=2&amp;size=4\n第二页：id=1,3</code></pre>\n\n<p><a name=\"deep paging\"></a>2、什么是deep paging问题？为什么会产生这个问题，它的底层原理是什么？</p>\n\n<p>&nbsp;</p>\n\n<p>deep paging：简单来说就是搜索的特别深，假如共60000条数据，每个shard上分了20000条，每页为10条数据，这时，你要搜索到1000页。</p>\n\n<p>每个shard返回的都是最后10条数据么？错误！</p>\n\n<p>你要搜索第1000页，实际每个shard都要将内部的20000条数据中的0-10010条数据拿出来，并不是10条，3个shard每个shard都返回10010条数据给coordinate node，coordinate node会收到共30030条数据，然后进行数据排序，_score,相关度分数，然后取排位最高的10条数据</p>\n\n<p>&nbsp;</p>\n', 2, '5f199b8885e24fc8b28672b872edb606', 13, '2017-07-10 18:04:39', '2018-09-06 16:12:58');
INSERT INTO `logcontent` VALUES ('bcee76fca7ba43b1bbb61a0c63873052', 1, 'd7caeba238f0466d87db109b2b9724da', '监督学习之决策树算法应用笔记', 'scikit-learn安装,决策树代码实现,graphviz安装', '初学', '<p><a id=\"Python环境\" name=\"Python环境\"></a>Python环境</p>\n\n<pre>\n<code>Windows版 Python 3.6.2</code></pre>\n\n<p><a id=\"机器学习的库\" name=\"机器学习的库\"></a>Python机器学习的库：scikit-learn</p>\n\n<pre>\n<code>特性：\n简单高效的数据挖掘和机器学习分析\n对所有用户开放，根据不同需求高度可重用性\n基于Numpy, SciPy和matplotlib\n开源，商用级别：获得 BSD许可\n\n覆盖问题领域：\n分类（classification), 回归（regression), 聚类（clustering), 降维(dimensionality reduction)\n模型选择(model selection), 预处理(preprocessing)</code></pre>\n\n<p><a id=\"安装scikit-learn\" name=\"安装scikit-learn\"></a>安装scikit-learn</p>\n\n<p>参考<a href=\"http://www.cnblogs.com/LuffySir/p/6054238.html\" id=\"cb_post_title_url\">Python 安装scikit-learn</a></p>\n\n<p><a id=\"代码实现\" name=\"代码实现\"></a>决策树Python代码实现(买电脑的人群分析)</p>\n\n<table border=\"1\" cellpadding=\"1\" cellspacing=\"1\" style=\"width:500px\">\n	<tbody>\n		<tr>\n			<td>RID</td>\n			<td>age</td>\n			<td>income</td>\n			<td>student</td>\n			<td>credit_rating</td>\n			<td>Class:buy_computer</td>\n		</tr>\n		<tr>\n			<td>1</td>\n			<td>youth</td>\n			<td>high</td>\n			<td>no</td>\n			<td>fair</td>\n			<td>no</td>\n		</tr>\n		<tr>\n			<td>2</td>\n			<td>youth</td>\n			<td>high</td>\n			<td>no</td>\n			<td>excellent</td>\n			<td>no</td>\n		</tr>\n		<tr>\n			<td>3</td>\n			<td>middle_aged</td>\n			<td>high</td>\n			<td>no</td>\n			<td>fair</td>\n			<td>yes</td>\n		</tr>\n		<tr>\n			<td>4</td>\n			<td>senior</td>\n			<td>medium</td>\n			<td>no</td>\n			<td>fair</td>\n			<td>yes</td>\n		</tr>\n		<tr>\n			<td>5</td>\n			<td>senior</td>\n			<td>low</td>\n			<td>yes</td>\n			<td>fair</td>\n			<td>yes</td>\n		</tr>\n		<tr>\n			<td>6</td>\n			<td>senior</td>\n			<td>low</td>\n			<td>yes</td>\n			<td>excellent</td>\n			<td>no</td>\n		</tr>\n		<tr>\n			<td>7</td>\n			<td>middle_aged</td>\n			<td>low</td>\n			<td>yes</td>\n			<td>excellent</td>\n			<td>yes</td>\n		</tr>\n		<tr>\n			<td>8</td>\n			<td>youth</td>\n			<td>medium</td>\n			<td>no</td>\n			<td>fair</td>\n			<td>no</td>\n		</tr>\n		<tr>\n			<td>9</td>\n			<td>youth</td>\n			<td>low</td>\n			<td>yes</td>\n			<td>fair</td>\n			<td>yes</td>\n		</tr>\n		<tr>\n			<td>10</td>\n			<td>senior</td>\n			<td>medium</td>\n			<td>yes</td>\n			<td>fair</td>\n			<td>yes</td>\n		</tr>\n		<tr>\n			<td>11</td>\n			<td>youth</td>\n			<td>medium</td>\n			<td>yes</td>\n			<td>excellent</td>\n			<td>yes</td>\n		</tr>\n		<tr>\n			<td>12</td>\n			<td>middle_aged</td>\n			<td>medium</td>\n			<td>no</td>\n			<td>excellent</td>\n			<td>yes</td>\n		</tr>\n		<tr>\n			<td>13</td>\n			<td>middle_aged</td>\n			<td>high</td>\n			<td>yes</td>\n			<td>fair</td>\n			<td>yes</td>\n		</tr>\n		<tr>\n			<td>14</td>\n			<td>senior</td>\n			<td>medium</td>\n			<td>no</td>\n			<td>excellent</td>\n			<td>no</td>\n		</tr>\n	</tbody>\n</table>\n\n<p>图表即为AllElectronics.csv</p>\n\n<pre>\n<code class=\"language-python\"># -*- coding: utf-8 -*- \nfrom sklearn.feature_extraction import DictVectorizer\nimport csv\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn import preprocessing\nfrom sklearn.externals.six import StringIO\n\n# Read in the csv file and put features into list of dict and list of class label\nallElectronicsData = open(r\'D://deepLearning/AllElectronics.csv\', \'rt\')\nreader = csv.reader(allElectronicsData)\nheaders = next(reader)\n\n#[\'RID\', \'age\', \'income\', \'student\', \'credit_rating\', \'class_buys_computer\']\nprint(headers)\nfeatureList = []\nlabelList = []\n\nfor row in reader:\n    labelList.append(row[len(row)-1])\n    rowDict = {}\n    for i in range(1, len(row)-1):\n        rowDict[headers[i]] = row[i]\n    featureList.append(rowDict)\n#[{\'age\': \'youth\', \'income\': \'high\', \'student\': \'no\', \'credit_rating\': \'fair\'}, {\'age\': \'youth\', \'income\': \'high\', \'student\': \'no\', \'credit_rating\': \'excellent\'}, {\'age\': \'middle_aged\', \'income\': \'high\', \'student\': \'no\', \'credit_rating\': \'fair\'}, {\'age\': \'senior\', \'income\': \'medium\', \'student\': \'no\', \'credit_rating\': \'fair\'}, {\'age\': \'senior\', \'income\': \'low\', \'student\': \'yes\', \'credit_rating\': \'fair\'}, {\'age\': \'senior\', \'income\': \'low\', \'student\': \'yes\', \'credit_rating\': \'excellent\'}, {\'age\': \'middle_aged\', \'income\': \'low\', \'student\': \'yes\', \'credit_rating\': \'excellent\'}, {\'age\': \'youth\', \'income\': \'medium\', \'student\': \'no\', \'credit_rating\': \'fair\'}, {\'age\': \'youth\', \'income\': \'low\', \'student\': \'yes\', \'credit_rating\': \'fair\'}, {\'age\': \'senior\', \'income\': \'medium\', \'student\': \'yes\', \'credit_rating\': \'fair\'}, {\'age\': \'youth\', \'income\': \'medium\', \'student\': \'yes\', \'credit_rating\': \'excellent\'}, {\'age\': \'middle_aged\', \'income\': \'medium\', \'student\': \'no\', \'credit_rating\': \'excellent\'}, {\'age\': \'middle_aged\', \'income\': \'high\', \'student\': \'yes\', \'credit_rating\': \'fair\'}, {\'age\': \'senior\', \'income\': \'medium\', \'student\': \'no\', \'credit_rating\': \'excellent\'}]\nprint(featureList)\n#将文字转化为数字\nvec = DictVectorizer()\ndummyX = vec.fit_transform(featureList) .toarray()\n#dummyX: [[ 0.  0.  1.  0.  1.  1.  0.  0.  1.  0.]\n# [ 0.  0.  1.  1.  0.  1.  0.  0.  1.  0.]\n# [ 1.  0.  0.  0.  1.  1.  0.  0.  1.  0.]\n# [ 0.  1.  0.  0.  1.  0.  0.  1.  1.  0.]\n# [ 0.  1.  0.  0.  1.  0.  1.  0.  0.  1.]\n# [ 0.  1.  0.  1.  0.  0.  1.  0.  0.  1.]\n# [ 1.  0.  0.  1.  0.  0.  1.  0.  0.  1.]\n# [ 0.  0.  1.  0.  1.  0.  0.  1.  1.  0.]\n# [ 0.  0.  1.  0.  1.  0.  1.  0.  0.  1.]\n# [ 0.  1.  0.  0.  1.  0.  0.  1.  0.  1.]\n# [ 0.  0.  1.  1.  0.  0.  0.  1.  0.  1.]\n# [ 1.  0.  0.  1.  0.  0.  0.  1.  1.  0.]\n# [ 1.  0.  0.  0.  1.  1.  0.  0.  0.  1.]\n# [ 0.  1.  0.  1.  0.  0.  0.  1.  1.  0.]]\nprint(\"dummyX: \" + str(dummyX))\n#[\'age=middle_aged\', \'age=senior\', \'age=youth\', \'credit_rating=excellent\', \'credit_rating=fair\', \'income=high\', \'income=low\', \'income=medium\', \'student=no\', \'student=yes\']\nprint(vec.get_feature_names())\n#labelList: [\'no\', \'no\', \'yes\', \'yes\', \'yes\', \'no\', \'yes\', \'no\', \'yes\', \'yes\', \'yes\', \'yes\', \'yes\', \'no\']\nprint(\"labelList: \" + str(labelList))\nlb = preprocessing.LabelBinarizer()\ndummyY = lb.fit_transform(labelList)\n#dummyY: [[0]\n# [0]\n# [1]\n# [1]\n# [1]\n# [0]\n# [1]\n# [0]\n# [1]\n# [1]\n# [1]\n# [1]\n# [1]\n# [0]]\nprint(\"dummyY: \" + str(dummyY))\n\nclf = tree.DecisionTreeClassifier(criterion=\'entropy\')\nclf = clf.fit(dummyX, dummyY)\n#clf: DecisionTreeClassifier(class_weight=None, criterion=\'entropy\', max_depth=None,\n#            max_features=None, max_leaf_nodes=None,\n#            min_impurity_decrease=0.0, min_impurity_split=None,\n#            min_samples_leaf=1, min_samples_split=2,\n#            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n#            splitter=\'best\')\nprint(\"clf: \" + str(clf))\n\n\n#将决策树的信息保存为dot文件，之后通过graphviz工具可直接生成pdf格式图形化决策树\nwith open(\"D://deepLearning/allElectronicInformationGainOri.dot\", \'w\') as f:\n    f = tree.export_graphviz(clf, feature_names=vec.get_feature_names(), out_file=f)\n\n##拿出第一条数据测试结果\nnewRowX = [1,0,0,0,1,1,0,0,1,0]\npredictedY = clf.predict(np.array(newRowX).reshape(1, -1))\n#predictedY: [1]\nprint(\"predictedY: \" + str(predictedY))\n</code></pre>\n\n<p><a id=\"安装graphviz\" name=\"安装graphviz\"></a>安装graphviz</p>\n\n<p>点击超链接进行安装<a href=\"http://dlsw.baidu.com/sw-search-sp/soft/d7/19179/graphviz-2.38.1401953780.msi\" id=\"file-link\" tabindex=\"0\">graphviz</a>，如果安装失败可以在官网下载</p>\n\n<pre>\n<code>安装完成后，需要配置环境变量，在Path中加入\nC:\\Program Files (x86)\\Graphviz2.38\\bin\n\n在生成dot文件的目录下执行\ndot -Tpdf xxx.dot -o xxx.pdf</code></pre>\n\n<p>&nbsp;</p>\n', 1, '5f199b8885e24fc8b28672b872edb606', 13, '2017-10-28 13:13:59', '2018-08-31 22:21:23');
INSERT INTO `logcontent` VALUES ('bd0caf5b58c64a3da6520336a5e57dc8', -1, '3ba9a3aed3ce48f682974ebcf3552ce2', '消息摘要算法', '消息摘要算法,MD5', '简介消息摘要算法', '<p>消息摘要算法</p>\n\n<pre>\n<code>1、无论输入的消息有多长，计算出来的消息摘要的长度总是固定的。例如应用MD5算法摘要的消息有128个比特位，用SHA-1算法摘要的消息最终有160比特位的输出，SHA-1的变体可以产生192比特位和256比特位的消息摘要。一般认为，摘要的最终输出越长，该摘要算法就越安全。\n\n2、一般地，只要输入的消息不同，对其进行摘要以后产生的摘要消息也必不相同；但相同的输入必会产生相同的输出。这正是好的消息摘要算法所具有的性质：输入改变了，输出也就改变了；两条相似的消息的摘要确不相近，甚至会大相径庭。\n\n3、消息摘要函数是单向函数，即只能进行正向的信息摘要，而无法从摘要中恢复出任何的消息，甚至根本找不到任何与原信息相关的信息。当然，可以采用强力攻击的方法，即尝试每一个可能的信息，计算其摘要，看看是否与已有的摘要相同，如果这样做，最终肯定会恢复出摘要的消息。但实际上，要得到的信息可能是无穷个消息之一，所以这种强力攻击几乎是无效的。\n\n4、好的摘要算法，没有人能从中找到“碰撞”，虽然“碰撞”是肯定存在的。即对于给定的一个摘要，不可能找到一条信息使其摘要正好是给定的。或者说，无法找到两条消息，使它们的摘要相同。\n\n5、摘要算法由于加密过程不需要密钥，且经过加密的数据无法被解密，因此摘要算法并非加密算法，但由于摘要算法的输出唯一性（参考2），不可伪造性（参考4），不可逆性（参考3），所以消息摘要算法适合作为数字签名算法。</code></pre>\n\n<p>&nbsp;</p>\n', 1, '5f199b8885e24fc8b28672b872edb606', 5, '2018-07-04 14:38:29', '2018-08-31 22:21:23');
INSERT INTO `logcontent` VALUES ('c01356f400254d5fb42e1d4efa0d83b3', -1, '7338e53acd514defa1a17e47016f3f4a', 'Elasticsearch（二十一）', '对相关度评分进行调节和优化的常见的4种方法,query-time boost,重构查询结构,negative boost,constant_score,自定义相关度分数算法', '对相关度评分进行调节和优化的常见的4种方法/query-time boost/重构查询结构/negative boost/constant_score/自定义相关度分数算法', '<p>对相关度评分进行调节和优化的常见的4种方法</p>\n\n<p><a id=\"query-time boost\" name=\"query-time boost\"></a>1、query-time boost</p>\n\n<pre>\n<code class=\"language-json\">GET /e/p/_search {\n  \"query\": {\n    \"bool\": {\n      \"should\": [\n        {\n          \"match\": {\n            \"title\": { \"query\": \"java spark\",\"boost\": 2 }\n          }\n        },\n        {\n          \"match\": {\n            \"content\": \"java spark\"\n          }\n        }\n      ]\n    }\n  }\n}</code></pre>\n\n<p><a id=\"重构查询结构\" name=\"重构查询结构\"></a>2、重构查询结构</p>\n\n<p>重构查询结果，在es新版本中，影响越来越小了。一般情况下，没什么必要的话，大家不用也行。</p>\n\n<pre>\n<code class=\"language-json\">GET /e/p/_search {\n  \"query\": {\n    \"bool\": {\n      \"should\": [\n        {\n          \"match\": {\n            \"content\": \"java\"\n          }\n        }\n        {\n          \"bool\": {\n            \"should\": [\n              {\n                \"match\": {\n                  \"content\": \"solution\"\n                }\n              }\n            ]\n          }\n        }\n      ]\n    }\n  }\n}</code></pre>\n\n<p><a id=\"negative boost\" name=\"negative boost\"></a>3、negative boost</p>\n\n<pre>\n<code class=\"language-json\">搜索包含java，尽量不包含spark的doc，如果包含了spark，不会说排除掉这个doc，而是说将这个doc的分数降低\n包含了negative term的doc，分数乘以negative boost，分数降低\n\nGET /forum/article/_search {\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        {\n          \"match\": {\n            \"content\": \"java\"\n          }\n        }\n      ],\n      \"must_not\": [\n        {\n          \"match\": {\n            \"content\": \"spark\"\n          }\n        }\n      ]\n    }\n  }\n}\n\nGET /forum/article/_search {\n  \"query\": {\n    \"boosting\": {\n      \"positive\": {\n        \"match\": {\n          \"content\": \"java\"\n        }\n      },\n      \"negative\": {\n        \"match\": {\n          \"content\": \"spark\"\n        }\n      },\n      \"negative_boost\": 0.2\n    }\n  }\n}\n\nnegative的doc，会乘以negative_boost，降低分数</code></pre>\n\n<p><a id=\"constant_score\" name=\"constant_score\"></a>4、constant_score</p>\n\n<pre>\n<code class=\"language-json\">如果你压根儿不需要相关度评分，直接走constant_score加filter，所有的doc分数都是1，没有评分的概念了\n\nGET /forum/article/_search {\n  \"query\": {\n    \"bool\": {\n      \"should\": [\n        {\n          \"constant_score\": {\n            \"query\": {\n              \"match\": {\n                \"title\": \"java\"\n              }\n            }\n          }\n        },\n        {\n          \"constant_score\": {\n            \"query\": {\n              \"match\": {\n                \"title\": \"spark\"\n              }\n            }\n          }\n        }\n      ]\n    }\n  }\n}\n</code></pre>\n\n<p><a id=\"用function_score自定义相关度分数算法\" name=\"用function_score自定义相关度分数算法\"></a>用function_score自定义相关度分数算法</p>\n\n<pre>\n<code class=\"language-json\">例如：Blog数据中有Click字段，这样在关键字查询的时候，可以在关键字相关度分数的基础上乘以他的Click数量，得到Click数量越高的博客匹配度越高的效果\nGET /e/p/_search\n{\n  \"query\": {\n    \"function_score\": {\n      \"query\": {\n        \"multi_match\": {\n          \"query\": \"java spark\",\n          \"fields\": [\"tile\", \"content\"]\n        }\n      },\n      \"field_value_factor\": {\n        \"field\": \"Click\",\n        \"modifier\": \"log1p\",\n        \"factor\": 0.5\n      },\n      \"boost_mode\": \"sum\",\n      \"max_boost\": 2\n    }\n  }\n}\n问题一：如果Click为0，那么匹配度也会变为0，有的是100w，分数会变得很夸张，效果不好\n加个log1p函数，公式会变为，new_score = old_score * log(1 + number_of_votes)，这样出来的分数会比较合理\n再加个factor，添加权重进一步影响分数，new_score = old_score * log(1 + factor * number_of_votes)\nboost_mode，决定分数与指定字段的值如何计算，multiply，sum，min，max，replace\nmax_boost，限制计算出来的分数不要超过max_boost指定的值</code></pre>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n', 2, '5f199b8885e24fc8b28672b872edb606', 26, '2017-07-27 23:15:49', '2018-09-06 16:12:58');
INSERT INTO `logcontent` VALUES ('c57577787099438a9215f52334375f1a', 1, 'd7caeba238f0466d87db109b2b9724da', '支持向量机(SVM)算法笔记一', 'svm', '初学', '<p><strong><a id=\"背景\" name=\"背景\"></a>背景：</strong></p>\n\n<pre>\n<code>最早是由 Vladimir N. Vapnik 和 Alexey Ya. Chervonenkis 在1963年提出\n目前的版本(soft margin)是由Corinna Cortes 和 Vapnik在1993年提出，并在1995年发表\n深度学习（2012）出现之前，SVM被认为机器学习中近十几年来最成功，表现最好的算法</code></pre>\n\n<p><strong><a id=\"介绍\" name=\"介绍\"></a>介绍：</strong></p>\n\n<p><img src=\"http://localhost:8080/myLog/images/svm/1.png\" style=\"height:190px; width:220px\" />&nbsp;<span style=\"font-size:18px\">（图一：H3优于H2优于H1）</span></p>\n\n<p>&nbsp;<img src=\"http://localhost:8080/myLog/images/svm/2.png\" style=\"height:218px; width:220px\" />&nbsp;&nbsp;<img src=\"http://localhost:8080/myLog/images/svm/3.png\" style=\"height:217px; width:220px\" /><span style=\"font-size:18px\">（图二：2图优于1图）</span></p>\n\n<pre>\n<code>SVM寻找区分两类的超平面（hyper plane), 使边际(margin)最大,超平面到一侧最近点的距离等于到另一侧最近点的距离，两侧的两个超平面平行,如何选取使边际(margin)最大的超平面 (Max Margin Hyperplane)？</code></pre>\n\n<p><strong><a id=\"线性可区分和线性不可区分\" name=\"线性可区分和线性不可区分\"></a>两种情况</strong></p>\n\n<p>线性可区分(linear separable)和线性不可区分（linear inseparable)&nbsp;</p>\n\n<p><img src=\"http://localhost:8080/myLog/images/svm/4.png\" />&nbsp; <span style=\"font-size:72px\">|</span> &nbsp;&nbsp;<img src=\"http://localhost:8080/myLog/images/svm/5.png\" /></p>\n\n<p><strong><a id=\"超平面的定义与公式建立\" name=\"超平面的定义与公式建立\"></a>针对线性可区分超平面的定义与公式建立</strong></p>\n\n<p>首先超平面可以定义为 W&middot;X + b = 0&nbsp;<br />\nW: weight vectot,W = {w1,w2...wn},n是特征值的个数<br />\nX: 训练实例<br />\nb: bias</p>\n\n<p>假设为二维特征向量：X=（x<sub>1</sub>,x<sub>2</sub>）,把b想象成额外的wight</p>\n\n<p>超平面方程变成：<strong>w<sub>0</sub>+w<sub>1</sub>x<sub>1</sub>+w<sub>2</sub>x<sub>2</sub> = 0</strong></p>\n\n<p>那么图二2图中所有超平面右上方的点满足：<strong>w<sub>0</sub>+w<sub>1</sub>x<sub>1</sub>+w<sub>2</sub>x<sub>2</sub> &gt;&nbsp;0</strong></p>\n\n<p>同理图二2图中所有超平面左下方的点满足：<strong>w<sub>0</sub>+w<sub>1</sub>x<sub>1</sub>+w<sub>2</sub>x<sub>2</sub> &lt;&nbsp;0</strong></p>\n\n<p>如果假设超平面到两侧最近点的距离为1，那么</p>\n\n<p>H1:<strong>w<sub>0</sub>+w<sub>1</sub>x<sub>1</sub>+w<sub>2</sub>x<sub>2</sub>&ge; 1 for y<sub>i</sub><sub> </sub>&nbsp;= +1</strong></p>\n\n<p>H2:<strong>w<sub>0</sub>+w<sub>1</sub>x<sub>1</sub>+w<sub>2</sub>x<sub>2</sub> &le;&nbsp;1 for y<sub>i&nbsp;</sub>&nbsp;= -1</strong></p>\n\n<p>H2*-1 = H1，综合两式，得到：<strong>y<sub>i</sub>(w<sub>0</sub>+w<sub>1</sub>x<sub>1</sub>+w<sub>2</sub>x<sub>2</sub>)&ge; 1 ,&forall;<sub>i</sub></strong></p>\n\n<p>所有坐落在边际的两边的的超平面上的被称作&quot;支持向量(support vectors)&quot;</p>\n\n<p>分界的超平面和H1或H2上任意一点的距离为&nbsp;<img src=\"http://localhost:8080/myLog/images/svm/fanshu.png\" style=\"height:25px; width:18px\" />&nbsp; (其中||W||是向量的范数(norm))</p>\n\n<p>所以，最大边际距离为：<img src=\"http://localhost:8080/myLog/images/svm/fanshu2.png\" style=\"height:25px; width:18px\" /></p>\n\n<p><strong><a id=\"如何找出最大边际的超平面呢\" name=\"如何找出最大边际的超平面呢\"></a>SVM如何找出最大边际的超平面呢(MMH)？（简述）</strong></p>\n\n<p>利用一些数学推倒，以上公式 （1）可变为有限制的凸优化问题(convex quadratic optimization)</p>\n\n<p>利用 Karush-Kuhn-Tucker (KKT)条件和拉格朗日公式，可以推出MMH可以被表示为以下&ldquo;决定边界 (decision boundary)&rdquo;</p>\n\n<p><img src=\"http://localhost:8080/myLog/images/svm/juedingbianjie.png\" style=\"height:41px; width:175px\" /></p>\n\n<p>y<sub>i</sub>是支持向量点<br />\nX<sub>i</sub>（support vector)的类别标记（class label)<br />\nX<sub>T</sub>是要测试的实例<br />\n&alpha;<sub>i</sub> 和b<sub>0</sub>&nbsp;都是单一数值型参数，由以上提到的最有算法得出<br />\nl是支持向量点的个数</p>\n\n<p><strong><a id=\"找出最大边际的超平面呢(MMH)举例\" name=\"找出最大边际的超平面呢(MMH)举例\"></a>SVM找出最大边际的超平面呢(MMH)举例</strong></p>\n\n<p><img src=\"http://localhost:8080/myLog/images/svm/svm8.png\" style=\"height:250px; width:262px\" /></p>\n\n<p>W&middot;X + b = 0&nbsp;</p>\n\n<p>已知两个支持向量(1,1)、(2,3)</p>\n\n<p>式1：W = (2,3)-(1,1) = (a,2a)</p>\n\n<p>式2：g(1,1) = -1 = a+2a+w<sub>0&nbsp;</sub>=》w<sub>0&nbsp;</sub>= -3a-1</p>\n\n<p>式3：g(2,3) = 1 = 2a+6a+w<sub>0 </sub>=》w<sub>0 </sub>= 1-8a</p>\n\n<p>式2、3=》a=2/5 =》w<sub>0 </sub>= -11/5 =》W = (2/5,4/5)</p>\n\n<p>g(x) = 2/5x<sub>1</sub>+4/5x<sub>2</sub>-11/5 =&nbsp;x<sub>1</sub>+2x<sub>2</sub>-5.5</p>\n', 1, '5f199b8885e24fc8b28672b872edb606', 21, '2017-11-19 18:43:33', '2018-08-31 22:21:23');
INSERT INTO `logcontent` VALUES ('c64fc013e6da45c1b8bc0deb371a29cd', -1, '7338e53acd514defa1a17e47016f3f4a', 'Elasticsearch（三）', 'document数据格式,简单的集群管理,快速检查集群的健康状况,快速查看集群中索引,简单的索引操作,简单CRUD新增文档,建立索引,检索文档,替换文档（相同id覆盖,相当于删除再新增）,更新文档（如果没有会新增）,删除文档', 'document数据格式/简单的集群管理/简单CRUD', '<p><a name=\"document数据格式\"></a>1、document数据格式</p>\n\n<p>面向文档的搜索分析引擎</p>\n\n<p>（1）应用系统的数据结构都是面向对象的，对象数据存储到数据库中，只能拆解开来，变为扁平的多张表，每次查询的时候再还原回对象格式，降低性能</p>\n\n<p>（2）ES是面向文档的，文档中存储的数据结构，与面向对象的数据结构是一样的，基于这种文档数据结构，es可以提供复杂的索引，全文检索，分析聚合等功能</p>\n\n<p>（3）es的document用json数据格式来表达</p>\n\n<pre>\n<code class=\"language-json\">{\n    \"email\":\"zhangsan@sina.com\",\n    \"first_name\":\"san\",\n    \"last_name\":\"zhang\",\n    \"info\":{\n        \"bio\":\"curious and modest\",\n        \"age\":30,\n        \"interests\":[\n            \"bike\",\n            \"climb\"\n        ]\n    },\n    \"join_date\":\"2017/01/01\"\n}</code></pre>\n\n<p><a name=\"简单的集群管理\"></a>2、简单的集群管理</p>\n\n<p>（1）快速检查集群的健康状况</p>\n\n<pre>\n<code class=\"language-json\">GET /_cat/health?v\n=====结果======\nepoch      timestamp cluster       status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent\n1496305242 16:20:42  elasticsearch green           2         2      2   1    0    0        0             0                  -                100.0%</code></pre>\n\n<p>（2）快速查看集群中有哪些索引</p>\n\n<pre>\n<code class=\"language-json\">GET /_cat/indices?v\n=====结果======\nhealth status index   uuid                   pri rep docs.count docs.deleted store.size pri.store.size\ngreen  open   .kibana yiRf8tPdQI6x6Lh0Y7SQ3w   1   1          1            0      6.3kb          3.1kb</code></pre>\n\n<p>（3）简单的索引操作</p>\n\n<pre>\n<code class=\"language-json\">创建索引：PUT test\n=====结果======\n{\n  \"acknowledged\": true,\n  \"shards_acknowledged\": true\n}\n\n删除索引：DELETE /test_index?pretty</code></pre>\n\n<p><a name=\"简单CRUD\"></a>3、简单CRUD</p>\n\n<p>（1）新增文档，建立索引</p>\n\n<pre>\n<code class=\"language-json\">格式结构：\nPUT /index/type/id\n{\n  \"key\":\"value\"\n}\n例：\nPUT e/p/1\n{\n    \"name\":\"gaolujie yagao\",\n    \"desc\":\"gaoxiao meibai\",\n    \"price\":30,\n    \"producer\":\"gaolujie producer\",\n    \"tags\":[\n        \"meibai\",\n        \"fangzhu\"\n    ]\n}</code></pre>\n\n<p>（2）检索文档</p>\n\n<pre>\n<code class=\"language-json\">格式结构：\nGET /index/type/id</code></pre>\n\n<p>（3）替换文档（相同id覆盖,相当于删除再新增）</p>\n\n<pre>\n<code class=\"language-json\">PUT /e/p/1\n{\n    \"name\":\"jiaqiangban gaolujie yagao\",\n    \"desc\":\"gaoxiao meibai\",\n    \"price\":30,\n    \"producer\":\"gaolujie producer\",\n    \"tags\":[\n        \"meibai\",\n        \"fangzhu\"\n    ]\n}\n替换方式必须带上所有的field，才能去进行信息的修改</code></pre>\n\n<p>（4）更新文档（如果没有会新增）</p>\n\n<pre>\n<code class=\"language-json\">POST /e/p/1/_update\n{\n    \"doc\":{\n        \"name\":\"jiaqiangban gaolujie yagao\"\n    }\n}</code></pre>\n\n<p>（5）删除文档</p>\n\n<pre>\n<code class=\"language-json\">DELETE e/p/4\n{\n    \"found\":true,\n    \"_index\":\"e\",\n    \"_type\":\"p\",\n    \"_id\":\"4\",\n    \"_version\":3,\n    \"result\":\"deleted\",\n    \"_shards\":{\n        \"total\":2,\n        \"successful\":2,\n        \"failed\":0\n    }\n}</code></pre>\n\n<p>（6）聚合分析</p>\n\n<pre>\n<code class=\"language-json\">第一个分析需求：计算每个tag下的商品数量\n\n你可能需要将文本field的fielddata属性设置为true，如果不需要直接进行下一个\n\nPUT /e/_mapping/p\n{\n  \"properties\": {\n    \"tags\": {\n      \"type\": \"text\",\n      \"fielddata\": true\n    }\n  }\n}\n\nGET /e/p/_search\n{\n  \"aggs\": {\n    \"group_by_tags\": {\n      \"terms\": { \"field\": \"tags\" }\n    }\n  }\n}</code></pre>\n\n<pre>\n<code class=\"language-json\">第二个聚合分析的需求：对名称中包含yagao的商品，计算每个tag下的商品数量\n\nGET /e/p/_search\n{\n  \"size\": 0,\n  \"query\": {\n    \"match\": {\n      \"name\": \"yagao\"\n    }\n  },\n  \"aggs\": {\n    \"all_tags\": {\n      \"terms\": {\n        \"field\": \"tags\"\n      }\n    }\n  }\n}</code></pre>\n\n<pre>\n<code class=\"language-json\">第三个聚合分析的需求：先分组，再算每组的平均值，计算每个tag下的商品的平均价格\n\nGET /e/p/_search\n{\n    \"size\": 0,\n    \"aggs\" : {\n        \"group_by_tags\" : {\n            \"terms\" : { \"field\" : \"tags\" },\n            \"aggs\" : {\n                \"avg_price\" : {\n                    \"avg\" : { \"field\" : \"price\" }\n                }\n            }\n        }\n    }\n}</code></pre>\n\n<pre>\n<code class=\"language-json\">第四个数据分析需求：计算每个tag下的商品的平均价格，并且按照平均价格降序排序\n\nGET /e/p/_search\n{\n    \"size\": 0,\n    \"aggs\" : {\n        \"all_tags\" : {\n            \"terms\" : { \"field\" : \"tags\", \"order\": { \"avg_price\": \"desc\" } },\n            \"aggs\" : {\n                \"avg_price\" : {\n                    \"avg\" : { \"field\" : \"price\" }\n                }\n            }\n        }\n    }\n}</code></pre>\n\n<pre>\n<code class=\"language-json\">第五个数据分析需求：按照指定的价格范围区间进行分组，然后在每组内再按照tag进行分组，最后再计算每组的平均价格\n\nGET /e/p/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"group_by_price\": {\n      \"range\": {\n        \"field\": \"price\",\n        \"ranges\": [\n          {\n            \"from\": 0,\n            \"to\": 20\n          },\n          {\n            \"from\": 20,\n            \"to\": 40\n          },\n          {\n            \"from\": 40,\n            \"to\": 50\n          }\n        ]\n      },\n      \"aggs\": {\n        \"group_by_tags\": {\n          \"terms\": {\n            \"field\": \"tags\"\n          },\n          \"aggs\": {\n            \"average_price\": {\n              \"avg\": {\n                \"field\": \"price\"\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}</code></pre>\n\n<p>&nbsp;</p>\n', 2, '5f199b8885e24fc8b28672b872edb606', 8, '2017-07-12 18:04:59', '2018-09-06 16:12:58');
INSERT INTO `logcontent` VALUES ('ca668fbde5a8472f9c6d8b851c9bad28', -1, '7338e53acd514defa1a17e47016f3f4a', 'Elasticsearch（八）', 'document,全量替换,强制创建,写入原理,buffer缓冲,translog日志,segment,删除,deleted,删除原理', 'document的全量替换/document的修改原理/document的强制创建/document的写入原理/document的删除/document的删除原理', '<p><a name=\"document的全量替换\"></a>1、document的全量替换</p>\n\n<p>（1）语法与创建文档是一样的，如果document id不存在，那么就是创建；如果document id已经存在，那么就是全量替换操作，替换document的json串内容</p>\n\n<p>（2）document是不可变的，如果要修改document的内容，第一种方式就是全量替换，直接对document重新建立索引，替换里面所有的内容</p>\n\n<p>（3）es会将老的document标记为deleted但是并没有被删除，然后新增我们给定的一个document，当我们创建越来越多的document的时候，es会在适当的时机在后台自动物理删除标记为deleted的document来释放空间</p>\n\n<p>（4）document的修改原理</p>\n\n<pre>\n<code class=\"language-json\">每次commit point时，会有一个.del文件，标记了哪些segment中的哪些document被标记为deleted了\n搜索的时候，会依次查询所有的segment，从旧的到新的，被修改过的document，在旧的segment中，会标记为deleted，在新的\nsegment中会有其新的数据，被标记为deleted的数据会被过滤掉，不会做为搜索结果返回</code></pre>\n\n<p><a name=\"document的强制创建\"></a>2、document的强制创建</p>\n\n<p>（1）创建文档与全量替换的语法是一样的，有时我们只是想新建文档，不想替换文档，如果强制进行创建呢？</p>\n\n<p>（2）PUT /index/type/id?op_type=create，PUT /index/type/id/_create</p>\n\n<p>（3）document的写入原理</p>\n\n<pre>\n<code class=\"language-json\">&lt;1&gt;数据写入buffer缓冲和translog日志文件\n&lt;2&gt;每隔一秒钟，buffer中的数据被写入新的segment file，并立即进入os cache，此时segment被打开并供search使用\n&lt;3&gt;buffer被清空\n&lt;4&gt;重复1~3，新的segment不断添加，buffer不断被清空，而translog中的数据不断累加\n&lt;5&gt;当translog长度达到一定程度的时候，commit操作发生\n  buffer中的所有数据写入一个新的segment，并写入os cache，打开供使用\n  buffer被清空\n  一个commit point被写入磁盘，标明了所有的index segment\n  filesystem cache中的所有index segment file缓存数据，被fsync强行刷到磁盘上\n  现有的translog被清空，创建一个新的translog\n&lt;6&gt;如果宕机导致os cache数据丢失，当机器重启后，会根据translog中的变更记录进行回放，进行还原</code></pre>\n\n<p><a name=\"document的删除\"></a>3、document的删除</p>\n\n<p>（1）DELETE /index/type/id</p>\n\n<p>（2）不会进行物理删除，只会将其标记为deleted，当数据越来越多的时候，在后台自动删除</p>\n\n<p>（3）document的删除原理</p>\n\n<pre>\n<code class=\"language-json\">每次commit point时，会有一个.del文件，标记了哪些segment中的哪些document被标记为deleted了\n被标记为deleted的数据会被过滤掉，不会做为搜索结果返回</code></pre>\n\n<pre>\n<code class=\"language-json\">每秒一个segment file，文件过多，而且每次search都要搜索所有的segment，很耗时\n默认会在后台执行segment merge操作，在merge的时候，被标记为deleted的document也会被彻底物理删除\n每次merge操作的执行流程\n（1）选择一些有相似大小的segment，merge成一个大的segment\n（2）将新的segment flush到磁盘上去\n（3）写一个新的commit point，包括了新的segment，并且排除旧的那些segment\n（4）将新的segment打开供搜索\n（5）将旧的segment删除\nPOST /my_index/_optimize?max_num_segments=1，尽量不要手动执行，让它自动默认执行就可以了</code></pre>\n\n<p>&nbsp;</p>\n', 2, '5f199b8885e24fc8b28672b872edb606', 6, '2017-07-12 18:04:18', '2018-09-06 16:12:58');
INSERT INTO `logcontent` VALUES ('ca8b0f34f61f49589cebd35c153db9e0', -1, 'f29612168904487aadb4043df110361c', 'Nginx日志模块（Log）', 'nginx,linux,日志模块,Log', '日志模块（Log）', '<p>日志模块（Log）：控制nginx如何记录请求日志。</p>\n\n<p>access_log</p>\n\n<pre>\n<code>语法：access_log path [format [buffer=size | off]] \n默认值：access_log log/access.log combined \n使用字段：http, server, location \n参数为连接日志指定了路径，格式和缓冲区大小。使用“off”将在当前的字段中清除access_log的所有参数，如果没有指定日志格式，默认为“combined”。缓冲区大小不能超过写入磁盘文件的最小大小。\n日志文件路径可以包含变量（0.7.4以上版本），但是有一些限制：\n·nginx指定的用户必须有创建日志文件的权限。\n·缓冲区不会工作\n·每个到来的连接，日志文件将被打开并且在记录日志后迅速关闭，然而，频繁使用的文件描述符将被保存到open_log_file_cache中，关于日志的轮询记录，必须记住随着时间的过去（使用open_log_file_cache的valid参数设置），日志仍然在旧的文件中记录。\n\nnginx支持为每个location指定强大的日志记录。同样的连接可以在同一时间输出到不止一个的日志中，更多信息请查看Multiple access_log directives in different contexts</code></pre>\n\n<p>log_format</p>\n\n<pre>\n<code class=\"language-nginx\">语法：log_format name format [format ...] \n默认值：log_format combined \"...\" \n使用字段：http server \n描述记录日志的格式，格式中可以使用大多数变量，也包括一些在写入日志文件过程中定义的变量：\n·$body_bytes_sent，减去应答头后传送给客户端的字节数，这个变量兼容apache模块mod_log_config的%B参数（在0.3.10前这个变量为$apache_bytes_sent）。\n·$bytes_sent，传送给客户端的字节数。\n·$connection，连接数。\n·$msec，正在写入日志条目的当前时间（精确到百万分之一秒）\n·$pipe，如果请求为管道的。\n·$request_length，请求主体的长度。\n·$request_time，从一个请求发出到而使nginx工作的时间，单位为毫秒（0.5.19版本后可以使用秒为单位）。\n·$status，应答的状态（代码）。\n·$time_local，写入普通日志格式的当地时间（服务器时间）。\n\n传送到客户端的头中的变量以\"sent_http_\"标记开头，如：$sent_http_content_range。\n注意其他模块产生的变量同样可以写入日志，例如你可以记录前端负载均衡应答头使用“upstream_http_”开头的变量，具体请查看负载均衡模块。\nnginx有一个预定的日志格式称为combined：\nlog_format combined \'$remote_addr - $remote_user [$time_local]\'\n                    \'\"$request\" $status $body_bytes_sent\'\n                    \'\"$http_referer\" \"$http_user_agent\"\';</code></pre>\n\n<p>open_log_file_cache</p>\n\n<pre>\n<code class=\"language-nginx\">语法：open_log_file_cache max=N [inactive=time] [min_uses=N] [valid=time] | off \n默认值：open_log_file_cache off \n使用字段：http server location \n这个指令为频繁使用的日志文件描述符所在的路径变量设置缓存。\n指令选项：\n·max - 缓存中存储的最大文件描述符数。\n·inactive - 设置缓存中在某个时间段内没有使用的文件描述符将被移除，默认为10秒。\n·min_uses - 在一定时间内（inactive指定），一个文件描述符最少使用多少次后被放入缓存，默认为1。\n·valid - 设置检查同名文件存在的时间，默认是60秒。\n·off - 关闭缓存。</code></pre>\n', 1, '5f199b8885e24fc8b28672b872edb606', 4, '2018-04-14 14:45:48', '2018-08-31 22:27:29');
INSERT INTO `logcontent` VALUES ('d1669b0f184d480b94aa635bd479a33f', -1, '3ba9a3aed3ce48f682974ebcf3552ce2', 'Base64', 'base64,编码', 'Base64编码', '<p><strong><a id=\"简介\" name=\"简介\"></a>Base64编码简介</strong></p>\n\n<p>编码方法，不能用于加密，将二进制转换为可显示的字符。</p>\n\n<p><strong><a id=\"编码转换表\" name=\"编码转换表\"></a>Base64编码转换表</strong></p>\n\n<table border=\"1\" cellspacing=\"0\" style=\"border-collapse:collapse; border:none; width:432pt\">\n	<caption>base64转换表</caption>\n	<thead>\n		<tr>\n			<th scope=\"col\" style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">索引</span></span></span></th>\n			<th scope=\"col\" style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">对应字符</span></span></span></th>\n			<th scope=\"col\" style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">索引</span></span></span></th>\n			<th scope=\"col\" style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">对应字符</span></span></span></th>\n			<th scope=\"col\" style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">索引</span></span></span></th>\n			<th scope=\"col\" style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">对应字符</span></span></span></th>\n			<th scope=\"col\" style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">索引</span></span></span></th>\n			<th scope=\"col\" style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">对应字符</span></span></span></th>\n		</tr>\n	</thead>\n	<tbody>\n		<tr>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">A</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">16</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">Q</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">32</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">g</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">48</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">w</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">1</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">B</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">17</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">R</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">33</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">h</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">49</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">x</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">2</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">C</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">18</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">S</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">34</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">i</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">50</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">y</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">3</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">D</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">19</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">T</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">35</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">j</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">51</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">z</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">4</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">E</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">20</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">U</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">36</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">k</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">52</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">5</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">F</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">21</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">V</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">37</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">l</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">53</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">1</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">6</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">G</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">22</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">W</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">38</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">m</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">54</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">2</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">7</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">H</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">23</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">X</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">39</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">n</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">55</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">3</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">8</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">I</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">24</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">Y</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">40</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">o</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">56</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">4</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">9</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">J</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">25</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">Z</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">41</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">p</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">57</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">5</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">10</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">K</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">26</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">a</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">42</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">q</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">58</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">6</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">11</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">L</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">27</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">b</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">43</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">r</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">59</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">7</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">12</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">M</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">28</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">c</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">44</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">s</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">60</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">8</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">13</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">N</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">29</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">d</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">45</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">t</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">61</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">9</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">14</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">O</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">30</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">e</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">46</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">u</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">62</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">+</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">15</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">P</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">31</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">f</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">47</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">v</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">63</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:middle; white-space:normal; width:54pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">/</span></span></span></td>\n		</tr>\n	</tbody>\n</table>\n\n<p><strong><a id=\"编码过程\" name=\"编码过程\"></a>Base64编码</strong></p>\n\n<pre>\n<code>base64一共64个索引（0-63），而63转换为二进制为111111，64个字符最多使用6个bit即可表示，而一个字节8个bit。\n那么如何使用6个bit表示8个bit?\nbase64将3个传统字节转化为4个base64字符表示。</code></pre>\n\n<table cellspacing=\"0\" style=\"border-collapse:collapse; border:none; width:633pt\">\n	<tbody>\n		<tr>\n			<td style=\"background-color:#acb9ca; height:14.25pt; vertical-align:bottom; white-space:nowrap; width:105pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">文本</span></span></span></td>\n			<td colspan=\"8\" style=\"background-color:#ddebf7; height:14.25pt; text-align:center; vertical-align:bottom; white-space:nowrap; width:176pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">Y</span></span></span></td>\n			<td colspan=\"8\" style=\"background-color:#9bc2e6; height:14.25pt; text-align:center; vertical-align:bottom; white-space:nowrap; width:176pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">e</span></span></span></td>\n			<td colspan=\"8\" style=\"background-color:#ddebf7; height:14.25pt; text-align:center; vertical-align:bottom; white-space:nowrap; width:176pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">s</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"background-color:#acb9ca; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">ASCII编码</span></span></span></td>\n			<td colspan=\"8\" style=\"background-color:#ddebf7; height:14.25pt; text-align:center; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">89</span></span></span></td>\n			<td colspan=\"8\" style=\"background-color:#9bc2e6; height:14.25pt; text-align:center; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">101</span></span></span></td>\n			<td colspan=\"8\" style=\"background-color:#ddebf7; height:14.25pt; text-align:center; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">115</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"background-color:#acb9ca; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">二进制</span></span></span></td>\n			<td style=\"background-color:#ddebf7; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#ddebf7; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">1</span></span></span></td>\n			<td style=\"background-color:#ddebf7; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#ddebf7; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">1</span></span></span></td>\n			<td style=\"background-color:#ddebf7; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">1</span></span></span></td>\n			<td style=\"background-color:#ddebf7; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#ddebf7; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#ddebf7; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">1</span></span></span></td>\n			<td style=\"background-color:#9bc2e6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#9bc2e6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">1</span></span></span></td>\n			<td style=\"background-color:#9bc2e6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">1</span></span></span></td>\n			<td style=\"background-color:#9bc2e6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#9bc2e6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#9bc2e6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">1</span></span></span></td>\n			<td style=\"background-color:#9bc2e6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#9bc2e6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">1</span></span></span></td>\n			<td style=\"background-color:#ddebf7; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#ddebf7; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">1</span></span></span></td>\n			<td style=\"background-color:#ddebf7; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">1</span></span></span></td>\n			<td style=\"background-color:#ddebf7; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">1</span></span></span></td>\n			<td style=\"background-color:#ddebf7; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#ddebf7; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#ddebf7; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">1</span></span></span></td>\n			<td style=\"background-color:#ddebf7; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">1</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"background-color:#acb9ca; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">二进制</span></span></span></td>\n			<td style=\"background-color:#fce4d6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#fce4d6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">1</span></span></span></td>\n			<td style=\"background-color:#fce4d6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#fce4d6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">1</span></span></span></td>\n			<td style=\"background-color:#fce4d6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">1</span></span></span></td>\n			<td style=\"background-color:#fce4d6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#ffc000; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#ffc000; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">1</span></span></span></td>\n			<td style=\"background-color:#ffc000; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#ffc000; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">1</span></span></span></td>\n			<td style=\"background-color:#ffc000; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">1</span></span></span></td>\n			<td style=\"background-color:#ffc000; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#fce4d6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#fce4d6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">1</span></span></span></td>\n			<td style=\"background-color:#fce4d6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#fce4d6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">1</span></span></span></td>\n			<td style=\"background-color:#fce4d6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#fce4d6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">1</span></span></span></td>\n			<td style=\"background-color:#ffc000; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">1</span></span></span></td>\n			<td style=\"background-color:#ffc000; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">1</span></span></span></td>\n			<td style=\"background-color:#ffc000; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#ffc000; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#ffc000; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">1</span></span></span></td>\n			<td style=\"background-color:#ffc000; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">1</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"background-color:#acb9ca; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">base<span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">64十进制索引</span></span></span></span></span></span></td>\n			<td colspan=\"6\" style=\"background-color:#fce4d6; height:14.25pt; text-align:center; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">22</span></span></span></td>\n			<td colspan=\"6\" style=\"background-color:#ffc000; height:14.25pt; text-align:center; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">22</span></span></span></td>\n			<td colspan=\"6\" style=\"background-color:#fce4d6; height:14.25pt; text-align:center; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">21</span></span></span></td>\n			<td colspan=\"6\" style=\"background-color:#ffc000; height:14.25pt; text-align:center; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">51</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"background-color:#acb9ca; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">对应字符</span></span></span></td>\n			<td colspan=\"6\" style=\"background-color:#fce4d6; height:14.25pt; text-align:center; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">W</span></span></span></td>\n			<td colspan=\"6\" style=\"background-color:#ffc000; height:14.25pt; text-align:center; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">W</span></span></span></td>\n			<td colspan=\"6\" style=\"background-color:#fce4d6; height:14.25pt; text-align:center; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">V</span></span></span></td>\n			<td colspan=\"6\" style=\"background-color:#ffc000; height:14.25pt; text-align:center; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">z</span></span></span></td>\n		</tr>\n	</tbody>\n</table>\n\n<pre>\n<code>三个字符3*8/6=4，那如果字符数并非3的整数倍怎么办？</code></pre>\n\n<table cellspacing=\"0\" style=\"border-collapse:collapse; border:none; width:633pt\">\n	<tbody>\n		<tr>\n			<td style=\"background-color:#acb9ca; height:14.25pt; vertical-align:bottom; white-space:nowrap; width:105pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">文本</span></span></span></td>\n			<td colspan=\"8\" style=\"background-color:#ddebf7; height:14.25pt; text-align:center; vertical-align:bottom; white-space:nowrap; width:176pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">A</span></span></span></td>\n			<td colspan=\"8\" style=\"background-color:#9bc2e6; height:14.25pt; text-align:center; vertical-align:bottom; white-space:nowrap; width:176pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">　</span></span></span></td>\n			<td colspan=\"8\" style=\"background-color:#ddebf7; height:14.25pt; text-align:center; vertical-align:bottom; white-space:nowrap; width:176pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">　</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"background-color:#acb9ca; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">ASCII编码</span></span></span></td>\n			<td colspan=\"8\" style=\"background-color:#ddebf7; height:14.25pt; text-align:center; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">65</span></span></span></td>\n			<td colspan=\"8\" style=\"background-color:#9bc2e6; height:14.25pt; text-align:center; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">　</span></span></span></td>\n			<td colspan=\"8\" style=\"background-color:#ddebf7; height:14.25pt; text-align:center; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">　</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"background-color:#acb9ca; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">二进制</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#ddebf7; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">1</span></span></span></td>\n			<td style=\"background-color:#ddebf7; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#ddebf7; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#ddebf7; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#ddebf7; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#ddebf7; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#ddebf7; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">1</span></span></span></td>\n			<td style=\"background-color:#9bc2e6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:red\"><strong><span style=\"font-family:等线\">0</span></strong></span></span></td>\n			<td style=\"background-color:#9bc2e6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:red\"><strong><span style=\"font-family:等线\">0</span></strong></span></span></td>\n			<td style=\"background-color:#9bc2e6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:red\"><strong><span style=\"font-family:等线\">0</span></strong></span></span></td>\n			<td style=\"background-color:#9bc2e6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:red\"><strong><span style=\"font-family:等线\">0</span></strong></span></span></td>\n			<td style=\"background-color:#9bc2e6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">　</span></span></span></td>\n			<td style=\"background-color:#9bc2e6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">　</span></span></span></td>\n			<td style=\"background-color:#9bc2e6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">　</span></span></span></td>\n			<td style=\"background-color:#9bc2e6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">　</span></span></span></td>\n			<td style=\"background-color:#ddebf7; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">　</span></span></span></td>\n			<td style=\"background-color:#ddebf7; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">　</span></span></span></td>\n			<td style=\"background-color:#ddebf7; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">　</span></span></span></td>\n			<td style=\"background-color:#ddebf7; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">　</span></span></span></td>\n			<td style=\"background-color:#ddebf7; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">　</span></span></span></td>\n			<td style=\"background-color:#ddebf7; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">　</span></span></span></td>\n			<td style=\"background-color:#ddebf7; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">　</span></span></span></td>\n			<td style=\"background-color:#ddebf7; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">　</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"background-color:#acb9ca; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">二进制</span></span></span></td>\n			<td style=\"background-color:#fce4d6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#fce4d6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">1</span></span></span></td>\n			<td style=\"background-color:#fce4d6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#fce4d6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#fce4d6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#fce4d6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#ffc000; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#ffc000; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">1</span></span></span></td>\n			<td style=\"background-color:#ffc000; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:red\"><strong><span style=\"font-family:等线\">0</span></strong></span></span></td>\n			<td style=\"background-color:#ffc000; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:red\"><strong><span style=\"font-family:等线\">0</span></strong></span></span></td>\n			<td style=\"background-color:#ffc000; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:red\"><strong><span style=\"font-family:等线\">0</span></strong></span></span></td>\n			<td style=\"background-color:#ffc000; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:red\"><strong><span style=\"font-family:等线\">0</span></strong></span></span></td>\n			<td style=\"background-color:#fce4d6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">　</span></span></span></td>\n			<td style=\"background-color:#fce4d6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">　</span></span></span></td>\n			<td style=\"background-color:#fce4d6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">　</span></span></span></td>\n			<td style=\"background-color:#fce4d6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">　</span></span></span></td>\n			<td style=\"background-color:#fce4d6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">　</span></span></span></td>\n			<td style=\"background-color:#fce4d6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">　</span></span></span></td>\n			<td style=\"background-color:#ffc000; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">　</span></span></span></td>\n			<td style=\"background-color:#ffc000; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">　</span></span></span></td>\n			<td style=\"background-color:#ffc000; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">　</span></span></span></td>\n			<td style=\"background-color:#ffc000; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">　</span></span></span></td>\n			<td style=\"background-color:#ffc000; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">　</span></span></span></td>\n			<td style=\"background-color:#ffc000; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">　</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"background-color:#acb9ca; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">base<span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">64十进制索引</span></span></span></span></span></span></td>\n			<td colspan=\"6\" style=\"background-color:#fce4d6; height:14.25pt; text-align:center; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">16</span></span></span></td>\n			<td colspan=\"6\" style=\"background-color:#ffc000; height:14.25pt; text-align:center; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">16</span></span></span></td>\n			<td colspan=\"6\" style=\"background-color:#fce4d6; height:14.25pt; text-align:center; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">　</span></span></span></td>\n			<td colspan=\"6\" style=\"background-color:#ffc000; height:14.25pt; text-align:center; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">　</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"background-color:#acb9ca; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">对应字符</span></span></span></td>\n			<td colspan=\"6\" style=\"background-color:#fce4d6; height:14.25pt; text-align:center; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">Q</span></span></span></td>\n			<td colspan=\"6\" style=\"background-color:#ffc000; height:14.25pt; text-align:center; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">Q</span></span></span></td>\n			<td colspan=\"6\" style=\"background-color:#fce4d6; height:14.25pt; text-align:center; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">=</span></span></span></td>\n			<td colspan=\"6\" style=\"background-color:#ffc000; height:14.25pt; text-align:center; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">=</span></span></span></td>\n		</tr>\n	</tbody>\n</table>\n\n<table cellspacing=\"0\" style=\"border-collapse:collapse; border:none; width:633pt\">\n	<tbody>\n		<tr>\n			<td style=\"background-color:#acb9ca; height:14.25pt; vertical-align:bottom; white-space:nowrap; width:105pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">文本</span></span></span></td>\n			<td colspan=\"8\" style=\"background-color:#ddebf7; height:14.25pt; text-align:center; vertical-align:bottom; white-space:nowrap; width:176pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">A</span></span></span></td>\n			<td colspan=\"8\" style=\"background-color:#9bc2e6; height:14.25pt; text-align:center; vertical-align:bottom; white-space:nowrap; width:176pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">B</span></span></span></td>\n			<td colspan=\"8\" style=\"background-color:#ddebf7; height:14.25pt; text-align:center; vertical-align:bottom; white-space:nowrap; width:176pt\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">　</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"background-color:#acb9ca; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">ASCII编码</span></span></span></td>\n			<td colspan=\"8\" style=\"background-color:#ddebf7; height:14.25pt; text-align:center; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">65</span></span></span></td>\n			<td colspan=\"8\" style=\"background-color:#9bc2e6; height:14.25pt; text-align:center; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">66</span></span></span></td>\n			<td colspan=\"8\" style=\"background-color:#ddebf7; height:14.25pt; text-align:center; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">　</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"background-color:#acb9ca; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">二进制</span></span></span></td>\n			<td style=\"height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#ddebf7; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">1</span></span></span></td>\n			<td style=\"background-color:#ddebf7; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#ddebf7; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#ddebf7; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#ddebf7; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#ddebf7; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#ddebf7; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">1</span></span></span></td>\n			<td style=\"background-color:#9bc2e6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#9bc2e6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">1</span></span></span></td>\n			<td style=\"background-color:#9bc2e6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#9bc2e6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#9bc2e6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#9bc2e6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#9bc2e6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">1</span></span></span></td>\n			<td style=\"background-color:#9bc2e6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#ddebf7; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:red\"><strong><span style=\"font-family:等线\">0</span></strong></span></span></td>\n			<td style=\"background-color:#ddebf7; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:red\"><strong><span style=\"font-family:等线\">0</span></strong></span></span></td>\n			<td style=\"background-color:#ddebf7; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">　</span></span></span></td>\n			<td style=\"background-color:#ddebf7; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">　</span></span></span></td>\n			<td style=\"background-color:#ddebf7; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">　</span></span></span></td>\n			<td style=\"background-color:#ddebf7; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">　</span></span></span></td>\n			<td style=\"background-color:#ddebf7; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">　</span></span></span></td>\n			<td style=\"background-color:#ddebf7; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">　</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"background-color:#acb9ca; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">二进制</span></span></span></td>\n			<td style=\"background-color:#fce4d6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#fce4d6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">1</span></span></span></td>\n			<td style=\"background-color:#fce4d6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#fce4d6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#fce4d6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#fce4d6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#ffc000; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#ffc000; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">1</span></span></span></td>\n			<td style=\"background-color:#ffc000; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#ffc000; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">1</span></span></span></td>\n			<td style=\"background-color:#ffc000; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#ffc000; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#fce4d6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#fce4d6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#fce4d6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">1</span></span></span></td>\n			<td style=\"background-color:#fce4d6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">0</span></span></span></td>\n			<td style=\"background-color:#fce4d6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:red\"><strong><span style=\"font-family:等线\">0</span></strong></span></span></td>\n			<td style=\"background-color:#fce4d6; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:red\"><strong><span style=\"font-family:等线\">0</span></strong></span></span></td>\n			<td style=\"background-color:#ffc000; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">　</span></span></span></td>\n			<td style=\"background-color:#ffc000; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">　</span></span></span></td>\n			<td style=\"background-color:#ffc000; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">　</span></span></span></td>\n			<td style=\"background-color:#ffc000; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">　</span></span></span></td>\n			<td style=\"background-color:#ffc000; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">　</span></span></span></td>\n			<td style=\"background-color:#ffc000; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">　</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"background-color:#acb9ca; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">base<span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">64十进制索引</span></span></span></span></span></span></td>\n			<td colspan=\"6\" style=\"background-color:#fce4d6; height:14.25pt; text-align:center; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">16</span></span></span></td>\n			<td colspan=\"6\" style=\"background-color:#ffc000; height:14.25pt; text-align:center; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">20</span></span></span></td>\n			<td colspan=\"6\" style=\"background-color:#fce4d6; height:14.25pt; text-align:center; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">8</span></span></span></td>\n			<td colspan=\"6\" style=\"background-color:#ffc000; height:14.25pt; text-align:center; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">　</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"background-color:#acb9ca; height:14.25pt; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">对应字符</span></span></span></td>\n			<td colspan=\"6\" style=\"background-color:#fce4d6; height:14.25pt; text-align:center; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">Q</span></span></span></td>\n			<td colspan=\"6\" style=\"background-color:#ffc000; height:14.25pt; text-align:center; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">U</span></span></span></td>\n			<td colspan=\"6\" style=\"background-color:#fce4d6; height:14.25pt; text-align:center; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">I</span></span></span></td>\n			<td colspan=\"6\" style=\"background-color:#ffc000; height:14.25pt; text-align:center; vertical-align:bottom; white-space:nowrap\"><span style=\"font-size:11pt\"><span style=\"color:black\"><span style=\"font-family:等线\">=</span></span></span></td>\n		</tr>\n	</tbody>\n</table>\n\n<p><a id=\"Java示例\" name=\"Java示例\"></a>Java示例</p>\n\n<p>jar：commons-codec-1.9.jar</p>\n\n<p>import org.apache.commons.codec.binary.Base64;</p>\n\n<pre>\n<code class=\"language-java\">public String encode(String str) {\n	byte[] encodeBytes = Base64.encodeBase64(str.getBytes());\n	return new String(encodeBytes);\n}\n\npublic String decode(String str) {\n	byte[] decodeBytes = Base64.encodeBase64(str.getBytes());\n	return new String(decodeBytes);\n}</code></pre>\n\n<p>&nbsp;</p>\n', 1, '5f199b8885e24fc8b28672b872edb606', 13, '2018-02-11 10:47:18', '2018-08-31 22:21:23');
INSERT INTO `logcontent` VALUES ('d536f160dba34a70863266380bbfa806', 1, '4eca926aa543420baea2f03f506d042e', '高性能Mysql笔记六', '高性能Mysql（第三版）', '高性能Mysql（第三版）--优化服务器设置', '<h2>MySQL配置的工作原理</h2>\n\n<p>my.cnf</p>\n\n<pre>\n<code>[client]\nport = 3306 #端口号\nsocket = /tmp/mysql.sock #socket所在路径\n[mysqld]\n!include /home/mysql/etc/mysqld.cnf #包含的配置文件 ，把用户名，密码文件单独存放\nport = 3306\nsocket = /tmp/mysql.sock\npid-file = /home/mysql/var/mysql.pid#进程pid\nbasedir = /home/mysql/#mysql的安装路径\ndatadir = /home/mysql/var/ #数据文件所在路径\ntmpdir = /home/mysql/tmp/#临时文件保存路径\nslave-load-tmpdir=/home/mysql/tmp#当slave执行load data infile时用\nskip-name-resolve#grant时，必须使用ip不能使用主机名\nskip-symbolic-links#不能使用连接文件\nskip-external-locking#不指定系统锁定\nback_log = 50 #接受队列，对于没建立 tcp 连接的请求队列放入缓存中，队列大小为 back_log，受限制与 OS 参数\nmax_connections = 1000 #最大并发连接数 ，增大该值需要相应增加允许打开的文件描述符数\nmax_connect_errors = 10000 #如果某个用户发起的连接 error 超过该数值，则该用户的下次连接将被阻塞\nopen_files_limit = 10240#打开文件限制\nconnect-timeout = 10 #连接超时之前的最大秒数\nwait-timeout = 28800 #等待关闭连接的时间\ninteractive-timeout = 28800 #关闭连接之前，允许 interactive_timeout（取代了wait_timeout）秒的不活动时间。\nslave-net-timeout = 600#从服务器超过slave_net_timeout 秒没有从主服务器收到数据才通知网络中断\nnet_read_timeout = 30 #从服务器读取信息的超时\nnet_write_timeout = 60 #从服务器写入信息的超时\nnet_retry_count = 10 #如果某个通信端口的读操作中断了，在放弃前重试多次\nnet_buffer_length = 16384 #包消息缓冲区初始化字节\ntable_cache = 512 #所有线程打开的表的数目\nthread_stack = 192K #每个线程的堆栈大小\nthread_cache_size = 20 #线程缓存\nthread_concurrency = 8 #同时运行的线程的数据 此处最好为 CPU 个数两倍。\nquery_cache_size = 256M #查询缓存大小\nquery_cache_limit = 2M #不缓存查询大于该值的结果\nquery_cache_min_res_unit = 2K #查询缓存分配的最小块大小\ndefault_table_type = INNODB#默认表存储引擎\ndefault-time-zone = system #服务器时区\ncharacter-set-server = utf8 #server 级别字符集\ndefault-storage-engine = InnoDB #默认存储\ntmp_table_size = 512M #临时表大小\nlog-bin = mysql-bin #打开binlog\nlog-bin-index = mysql-bin.index\nrelay-log = relay-log\nrelay_log_index = relay-log.index\nlog-error = /home/mysql/log/mysql.err#错误文件路径\nlog_output = FILE #慢查询输出格式\nslow_query_log = 1\nlong-query-time = 1 #慢查询时间 超过 1 秒则为慢查询\nslow_query_log_file = /home/mysql/log/slow.log#慢查询存储路径\ngeneral_log = 1\ngeneral_log_file = /home/mysql/log/mysql.log#一般查询存储路径\nmax_binlog_size = 1G#最大binlog\nmax_relay_log_size = 1G#最大relaylog\nrelay-log-purge = 1 #当不用中继日志时，删除他们。这个操作有 SQL 线程完成\nexpire_logs_days = 30 #超过 30 天的 binlog 删除\nbinlog_cache_size = 1M #session 级别\nreplicate-wild-ignore-table = mysql.% #复制时忽略数据库及表\nreplicate-wild-ignore-table = test.% #复制时忽略数据库及表\nkey_buffer_size = 256M#查询排序时所能使用的缓冲区大小\nsort_buffer_size = 2M #排序 buffer 大小\nread_buffer_size = 2M #读查询操作所能使用的缓冲区大小\njoin_buffer_size = 8M # join buffer 大小\nquery_cache_size = 64M#指定 MySQL 查询缓冲区的大小\nread_rnd_buffer_size = 8M#随机读缓存大小\ninnodb_file_per_table#独立表空间\ninnodb_additional_mem_pool_size = 100M#附加的内存池\ninnodb_buffer_pool_size = 2G #缓冲池\ninnodb_data_file_path = ibdata1:1G:autoextend#表空间，自动递增\ninnodb_file_io_threads = 4 #io 线程数\ninnodb_thread_concurrency = 16 #并发线程数\ninnodb_flush_log_at_trx_commit = 1#刷新事务日志到磁盘\ninnodb_log_buffer_size = 8M #事物日志缓存\ninnodb_log_file_size = 500M #事物日志大小\ninnodb_log_files_in_group = 2 #两组事物日志\ninnodb_log_group_home_dir = /home/mysql/var/#日志组\ninnodb_max_dirty_pages_pct = 90 #innodb 主线程刷新缓存池中的数据，使脏数据比例小于 90%\ninnodb_lock_wait_timeout = 50 #InnoDB 事务在被回滚之前可以等待一个锁定的超时秒数\ninnodb_flush_method = O_DSYNC  # InnoDB 用来刷新日志的方法\ninnodb_force_recovery=1#导出表空间损坏的表\ninnodb_fast_shutdown#加速innodb关闭\nmax_allowed_packet = 64M#最大允许的包大小\n[mysql]\ndefault-character-set = utf8\nconnect-timeout = 3\n[mysqld_safe]\nopen-files-limit  = 8192#可打开文件数量</code></pre>\n\n<pre>\n<code>MySQL从命令行参数和配置文件中获得配置信息，在类UNIX系统中，配置文件的位置一般在/etc/my.cnf或者/etc/mysql/my.cnf（注：大部分变量和它们对应的命令行选项名称一样，但是有一些例外,不要简单的替换使用）。\nMySQL有大量的可以修改的参数,但不应该随便修改.应该将更多时间花在schema的优化,索引,查询设计上。\n任何打算长期使用的设置都应该写到全局配置文件，而不是在命令行特别指定。\n所有的配置文件放在同一个地方以方便检查。\n不建议动态修改变量,因为可能导致意外的副作用。\n设置变量并不是值越大就越好。\n\n配置内存使用\n确定可使用内存上限\n每个连接使用多少内存,如排序缓冲和临时表\n确定操作系统内存使用量\n把剩下的分配给缓存,如InnoDB缓存池\n\n配置MySQL的I/O行为\n有些配置项影响如何同步数据到磁盘及如何恢复操作,这对性能影响很大,而且表现了性能和数据安全之间的平衡\nInnoDB I/O配置\n重要配置: InnoDB日志文件大小,InnoDB怎样刷新日志缓冲,InnoDB怎样执行I/O\nInnoDB使用日志减少提交事务时开销,不用每个事务提交时把缓冲池的脏块刷到磁盘中\n事务日志可以把随机IO变成顺序IO,同时如果发生断电,InnoDB可以重放日志恢复已经提交的事务\nsync_binlog选项控制MySQL怎么刷新二进制日志到磁盘\n把二进制日志放到一个带有电池保护的写缓存的RAID卷可以极大的提升性能\nMyISAM的I/O配置\n因为MyISAM表每次写入都会将索引变更刷新到磁盘\n批量操作时,通过设置delay_key_write可以延迟索引写入,可以提升性能\n配置MyISAM怎样尝试从损坏中恢复\n\n配置MySQL并发\nInnoDB并发配置\n如果在InnoDB并发方面有问题,解决方案通常是升级服务器\ninnodb_thread_concurrency: 限制一次性可以有多少线程进入内核(根据实践取合适值)\ninnodb_thread_sleep_delay: 线程第一次进入内核失败等的时间,如果还不能进入则放入等待线程队列\ninnodb_commit_concurrency: 控制有多少线程可以在同一时间提交\n使用线程池限制并发: MariaDB已经实现\nMyISAM并发配置\nconcurrency_insert: 配置MyISAM打开并发插入\n\n其他\n基于工作负载的配置: 利用工具分析并调整配置\nmax_connections: 保证服务器不会因应用程序激增的连接而不堪重负\n安全和稳定的设置: 感兴趣者请自行google\n高级InnoDB设置: 感兴趣者请自行google\nInnoDB两个重要配置: innodb_buffer_pool_size和innodb_log_file_size</code></pre>\n\n<p>&nbsp;</p>', 1, '5f199b8885e24fc8b28672b872edb606', 2, '2018-09-02 11:55:33', '2018-09-11 09:08:11');
INSERT INTO `logcontent` VALUES ('d66ad77560cf44cfbc5f912193958021', 1, '4eca926aa543420baea2f03f506d042e', '高性能Mysql笔记四', '查询性能优化', '高性能Mysql（第三版）--查询性能优化', '<h2>EXPLAIN</h2>\n\n<pre>\n<code>1、id\n2、select_type：每个select子句的类型\n	SIMPLE(简单SELECT,不使用UNION或子查询等)\n	PRIMARY(查询中若包含任何复杂的子部分,最外层的select被标记为PRIMARY)\n	UNION(UNION中的第二个或后面的SELECT语句)\n	DEPENDENT UNION(UNION中的第二个或后面的SELECT语句，取决于外面的查询)\n	UNION RESULT(UNION的结果)\n	SUBQUERY(子查询中的第一个SELECT)\n	DEPENDENT SUBQUERY(子查询中的第一个SELECT，取决于外面的查询)\n	DERIVED(派生表的SELECT, FROM子句的子查询)\n	UNCACHEABLE SUBQUERY(一个子查询的结果不能被缓存，必须重新评估外链接的第一行)\n3、table\n4、type：访问类型\n	常用的类型有： ALL, index,  range, ref, eq_ref, const, system, NULL（从左到右，性能越来越好）\n	ALL：全表扫描\n	index：索引全扫描\n	range：索引范围扫描，常用语&lt;,&lt;=,&gt;=,between等操作\n	ref：使用非唯一索引扫描或唯一索引前缀扫描，返回单条记录，常出现在关联查询中\n	eq_ref：类似ref，区别在于使用的是唯一索引，使用主键的关联查询\n	const/system：单条记录，系统会把匹配行中的其他列作为常数处理，如主键或唯一索引查询\n	null：MySQL不访问任何表或索引，直接返回结果\n5、possible_keys：查询涉及到的字段上若存在索引，则该索引将被列出，但不一定被查询使用\n6、Key：MySQL实际决定使用的键（索引）\n	如果没有选择索引，键是NULL。要想强制MySQL使用或忽视possible_keys列中的索引，在查询中使用FORCE INDEX、USE INDEX或者IGNORE INDEX。\n7、key_len：索引中使用的字节数，可通过该列计算查询中使用的索引的长度（key_len显示的值为索引字段的最大可能长度，并非实际使用长度，即key_len是根据表定义计算而得，不是通过表内检索出的）\n	不损失精确性的情况下，长度越短越好 \n8、ref：上述表的连接匹配条件，即哪些列或常量被用于查找索引列上的值\n9、rows：根据表统计信息及索引选用情况，估算的找到所需的记录所需要读取的行数\n10、Extra：包含MySQL解决查询的详细信息\n	distinct: 一旦mysql找到了与行相联合匹配的行，就不再搜索了\n	Using index：这发生在对表的请求列都是同一索引的部分的时候，返回的列数据只使用了索引中的信息，而没有再去访问表中的行记录。是性能高的表现\n	Using where：mysql服务器将在存储引擎检索行后再进行过滤。就是先读取整行数据，再按 where 条件进行检查，符合就留下，不符合就丢弃\n	Using temporary：mysql需要创建一张临时表来处理查询。出现这种情况一般是要进行优化的，首先考虑使用索引来优化。\n	Using filesort：mysql会对结果使用一个外部索引排序，而不是按索引次序从表里读取行。此时mysql会根据联接类型浏览所有符合条件的记录，并保存排序关键字和行指针，然后排序关键字并按顺序检索行信息。考虑使用索引来优化\n总结：\n	• EXPLAIN不会告诉你关于触发器、存储过程的信息或用户自定义函数对查询的影响情况\n	• EXPLAIN不考虑各种Cache\n	• EXPLAIN不能显示MySQL在执行查询时所作的优化工作\n	• 部分统计信息是估算的，并非精确值\n	• EXPALIN只能解释SELECT操作，其他操作要重写为SELECT后查看执行计划。</code></pre>\n\n<h2>慢查询基础：优化数据访问</h2>\n\n<pre>\n<code>是否向数据库请求了不需要的数据\n1、查询不需要的记录\n2、多表关联时返回全部列\n3、总是取出全部列\n4、重复查询相同的数据\n\n一般MySQL能够使用如下三种方式应用WHERE条件，从好到坏依次为：\n1、在索引中使用WHERE条件来过滤不匹配的记录。这是在存储引擎层完成的。\n2、使用索引覆盖扫描（在Extra列中出现了Using index）来返回记录，直接从索引中过滤不需要的记录并返回命中的结果。这是在MySQL服务器层完成的，但无须再回表查询记录。\n3、从数据表中返回数据，然后过滤不满足条件的记录（在Extra列中出现Using Where）。这在MySQL服务器层完成，MySQL需要先从数据表读出记录然后过滤。\n\n发现查询需要扫描大量的数据但只返回少数的行，如何优化：\n1、使用索引覆盖扫描，把所有需要用的列都放到索引中，这样存储引擎无须回表获取对应行就可以返回结果了\n2、改变库表结构。例如使用单独的汇总表\n3、重写这个复杂的查询，让MySQL优化器能够以更优化的方式执行这个查询</code></pre>\n\n<h2>重构查询的方式</h2>\n\n<pre>\n<code>根据实际进行重构：\n一个复杂查询还是多个简单查询\n\n切分查询\n大查询切分成小查询\n\n分解关联查询\n1、让缓存的效率更高。许多应用程序可以方便地缓存单表查询对应的结果对象，如果关联中的某个表发生了变化，缓存就无法使用了，而拆分后，如果某个表很少改变，那么基于该表的查询就可以重复利用查询缓存结果了\n2、将查询分解后，执行单个查询可以减少锁的竞争。\n3、在应用层做关联，可以更容易对数据库进行拆分，更容易做到高性能和可扩展\n4、查询本身效率也可能会有所提升。例如使用IN()代替关联查询，可以让MySQL按照ID顺序进行查询，这可能比随机的关联要更高效。\n5、可以减少冗余记录的查询。在应用层做关联查询，意味着对于某条记录应用只需要查询一次，而在数据库中做关联查询，则可能需要重复地访问一部分数据。从这点看，这样的重构还可能会减少网络和内存的消耗\n6、更进一步，这样做相当于在应用中实现了哈希关联，而不是使用MySQL的嵌套循环关联。某些场景哈希关联的效率要高很多\n\n</code></pre>\n\n<h2>查询执行的基础</h2>\n\n<pre>\n<code>查询执行路径\n1、客户端发送一条查询给服务器\n2、服务器先检查查询缓存，如果命中了缓存，则立刻返回存储在缓存中的结果。否则进入下一阶段\n3、服务器端进行SQL解析、预处理，再由优化器生成对应的执行计划\n4、MySQL根据优化器生成的执行计划，调用存储引擎的API来执行查询\n5、将结果返回给客户端\n\nMySQL客户端/服务器通信协议\nMySQL客户端和服务器之间的通信协议是“半双工”的,任何一时刻要么是服务器向客户端发送数据,要么是客户端向服务端发送数据，即客户端从服务器取数据时，实际上是MySQL在向客户端推送数据的过程\n\n查询状态\n使用SHOW FULL PROCESSLIST命令（该命令返回结果中的Command列就表示当前的状态）\n1、Sleep：线程正在等待客户端发送新的请求\n2、Query：线程正在执行查询或者正在将结果发送给客户端\n3、Locked：在MySQL服务器层，该线程正在等待表锁。在存储引擎级别实现的锁。\n4、Analyzing and statistics：线程正在收集存储引擎的统计信息，并生成查询的执行计划\n5、Copying to tmp table [on disk]：线程正在执行查询，并且将其结果集都复制到一个临时表中，这种状态一般是在做GROUP BY或者文件排序或者UNION操作。如果这个状态后面还有“on disk”标记，那表示MySQL正在将一个内存临时表放到磁盘上。\n6、The thread is：线程正在对结果集进行排序\n7、Sending data：这表示多种情况：线程可能在多个状态之间传送数据，或者在生成结果集，或者在向客户端返回数据\n\n查询缓存\n在解析一个查询语句之前，如果查询缓存是打开的，那么MySQL会优先检查这个查询是否命中查询缓存中的数据，必须完全匹配。这个检查是通过一个对大小写敏感的哈希查找实现的。如果当前查询命中了查询缓存，那么在返回查询结果之前MySQL会检查一次用户权限，权限没有问题，MySQL会跳过所有其他阶段，直接从缓存中拿到结果并返回给客户端。这种情况下，查询不会被解析，不用生成执行计划，不会被执行。</code></pre>\n\n<p>&nbsp;</p>', 1, '5f199b8885e24fc8b28672b872edb606', 7, '2018-08-31 17:23:08', '2018-09-11 09:09:33');
INSERT INTO `logcontent` VALUES ('dd0230a37c2e4715a872390303af5f9e', -1, '7338e53acd514defa1a17e47016f3f4a', '三、分布式文档存储', '分布式文档存储', '分布式文档存储', '<h2>分布式文档存储</h2>\n\n<p>路由一个文档到一个分片中</p>\n\n<p>当索引一个文档的时候，文档会被存储到一个主分片中。&nbsp;Elasticsearch 如何知道一个文档应该存放到哪个分片中呢？当我们创建文档时，它如何决定这个文档应当被存储在分片&nbsp;<code>1</code>&nbsp;还是分片&nbsp;<code>2</code>&nbsp;中呢？</p>\n\n<p>首先这肯定不会是随机的，否则将来要获取文档的时候我们就不知道从何处寻找了。实际上，这个过程是根据下面这个公式决定的：</p>\n\n<pre>\nshard = hash(routing) % number_of_primary_shards</pre>\n\n<p><code>routing</code>&nbsp;是一个可变值，默认是文档的&nbsp;<code>_id</code>&nbsp;，也可以设置成一个自定义的值。&nbsp;<code>routing</code>&nbsp;通过 hash 函数生成一个数字，然后这个数字再除以&nbsp;<code>number_of_primary_shards</code>&nbsp;（主分片的数量）后得到&nbsp;<strong>余数</strong>&nbsp;。这个分布在&nbsp;<code>0</code>&nbsp;到&nbsp;<code>number_of_primary_shards-1</code>&nbsp;之间的余数，就是我们所寻求的文档所在分片的位置。</p>\n\n<p>这就解释了为什么我们要在创建索引的时候就确定好主分片的数量&nbsp;并且永远不会改变这个数量：因为如果数量变化了，那么所有之前路由的值都会无效，文档也再也找不到了。</p>\n\n<p><img alt=\"注意\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/note.png\" /></p>\n\n<p>所有的文档 API（&nbsp;<code>get</code>&nbsp;、&nbsp;<code>index</code>&nbsp;、&nbsp;<code>delete</code>&nbsp;、&nbsp;<code>bulk</code>&nbsp;、&nbsp;<code>update</code>&nbsp;以及&nbsp;<code>mget</code>&nbsp;）都接受一个叫做&nbsp;<code>routing</code>&nbsp;的路由参数&nbsp;，通过这个参数我们可以自定义文档到分片的映射。一个自定义的路由参数可以用来确保所有相关的文档&mdash;&mdash;例如所有属于同一个用户的文档&mdash;&mdash;都被存储到同一个分片中。</p>\n\n<p>主分片和副本分片如何交互</p>\n\n<p>为了说明目的, 我们&nbsp;假设有一个集群由三个节点组成。 它包含一个叫&nbsp;<code>blogs</code>&nbsp;的索引，有两个主分片，每个主分片有两个副本分片。相同分片的副本不会放在同一节点，所以我们的集群看起来像&nbsp;图&nbsp;8 &ldquo;有三个节点和一个索引的集群&rdquo;。</p>\n\n<p>&nbsp;</p>\n\n<p><strong>图&nbsp;8.&nbsp;有三个节点和一个索引的集群</strong></p>\n\n<p><img alt=\"有三个节点和一个索引的集群\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/elas_0401.png\" /></p>\n\n<p>&nbsp;</p>\n\n<p>我们可以发送请求到集群中的任一节点。&nbsp;每个节点都有能力处理任意请求。 每个节点都知道集群中任一文档位置，所以可以直接将请求转发到需要的节点上。 在下面的例子中，将所有的请求发送到&nbsp;<code>Node 1</code>&nbsp;，我们将其称为&nbsp;<em>协调节点(coordinating node)</em>&nbsp;。</p>\n\n<p><img alt=\"提示\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/tip.png\" /></p>\n\n<p>当发送请求的时候， 为了扩展负载，更好的做法是轮询集群中所有的节点。</p>\n\n<h2>新建、索引和删除文档</h2>\n\n<p>新建、索引和删除&nbsp;请求都是&nbsp;<em>写</em>&nbsp;操作，&nbsp;必须在主分片上面完成之后才能被复制到相关的副本分片，如下图所示&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/distrib-write.html#img-distrib-write\" title=\"图 9. 新建、索引和删除单个文档\">图&nbsp;9 &ldquo;新建、索引和删除单个文档&rdquo;</a>.</p>\n\n<p><strong>图&nbsp;9.&nbsp;新建、索引和删除单个文档</strong></p>\n\n<p><img alt=\"新建、索引和删除单个文档\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/elas_0402.png\" /></p>\n\n<p>&nbsp;</p>\n\n<p>以下是在主副分片和任何副本分片上面&nbsp;成功新建，索引和删除文档所需要的步骤顺序：</p>\n\n<ol style=\"list-style-type:decimal\">\n	<li>客户端向&nbsp;<code>Node 1</code>&nbsp;发送新建、索引或者删除请求。</li>\n	<li>节点使用文档的&nbsp;<code>_id</code>&nbsp;确定文档属于分片 0 。请求会被转发到&nbsp;<code>Node 3`，因为分片 0 的主分片目前被分配在 `Node 3</code>&nbsp;上。</li>\n	<li><code>Node 3</code>&nbsp;在主分片上面执行请求。如果成功了，它将请求并行转发到&nbsp;<code>Node 1</code>&nbsp;和&nbsp;<code>Node 2</code>&nbsp;的副本分片上。一旦所有的副本分片都报告成功,&nbsp;<code>Node 3</code>&nbsp;将向协调节点报告成功，协调节点向客户端报告成功。</li>\n</ol>\n\n<p>在客户端收到成功响应时，文档变更已经在主分片和所有副本分片执行完成，变更是安全的。</p>\n\n<p>有一些可选的请求参数允许您影响这个过程，可能以数据安全为代价提升性能。这些选项很少使用，因为Elasticsearch已经很快，但是为了完整起见，在这里阐述如下：</p>\n\n<p><code>consistency</code></p>\n\n<p>consistency，即一致性。在默认设置下，即使仅仅是在试图执行一个_写_操作之前，主分片都会要求必须要有 _规定数量(quorum)_（或者换种说法，也即必须要有大多数）的分片副本处于活跃可用状态，才会去执行_写_操作(其中分片副本可以是主分片或者副本分片)。这是为了避免在发生网络分区故障（network partition）的时候进行_写_操作，进而导致数据不一致。_规定数量_即：</p>\n\n<pre>\nint( (primary + number_of_replicas) / 2 ) + 1</pre>\n\n<p><code>consistency</code>&nbsp;参数的值可以设为&nbsp;<code>one</code>&nbsp;（只要主分片状态 ok 就允许执行_写_操作）,<code>all`（必须要主分片和所有副本分片的状态没问题才允许执行_写_操作）, 或 `quorum</code>&nbsp;。默认值为&nbsp;<code>quorum</code>&nbsp;, 即大多数的分片副本状态没问题就允许执行_写_操作。</p>\n\n<p>注意，<em>规定数量</em>&nbsp;的计算公式中&nbsp;<code>number_of_replicas</code>&nbsp;指的是在索引设置中的设定副本分片数，而不是指当前处理活动状态的副本分片数。如果你的索引设置中指定了当前索引拥有三个副本分片，那规定数量的计算结果即：</p>\n\n<pre>\nint( (primary + 3 replicas) / 2 ) + 1 = 3</pre>\n\n<p>如果此时你只启动两个节点，那么处于活跃状态的分片副本数量就达不到规定数量，也因此您将无法索引和删除任何文档。</p>\n\n<p><code>timeout</code></p>\n\n<p>如果没有足够的副本分片会发生什么？ Elasticsearch会等待，希望更多的分片出现。默认情况下，它最多等待1分钟。 如果你需要，你可以使用&nbsp;<code>timeout</code>&nbsp;参数&nbsp;使它更早终止：&nbsp;<code>100</code>&nbsp;100毫秒，<code>30s</code>&nbsp;是30秒。</p>\n\n<p><img alt=\"注意\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/note.png\" /></p>\n\n<p>新索引默认有&nbsp;<code>1</code>&nbsp;个副本分片，这意味着为满足&nbsp;<code>规定数量</code>&nbsp;<em>应该</em>&nbsp;需要两个活动的分片副本。 但是，这些默认的设置会阻止我们在单一节点上做任何事情。为了避免这个问题，要求只有当<code>number_of_replicas</code>&nbsp;大于1的时候，规定数量才会执行。</p>\n\n<h2>取回一个文档</h2>\n\n<p>可以从主分片或者从其它任意副本分片检索文档&nbsp;，如下图所示&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/distrib-read.html#img-distrib-read\" title=\"图 10. 取回单个文档\">图&nbsp;10 &ldquo;取回单个文档&rdquo;</a>.</p>\n\n<p><strong>图&nbsp;10.&nbsp;取回单个文档</strong></p>\n\n<p><img alt=\"取回单个文档\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/elas_0403.png\" /></p>\n\n<p>&nbsp;</p>\n\n<p>以下是从主分片或者副本分片检索文档的步骤顺序：</p>\n\n<p>1、客户端向&nbsp;<code>Node 1</code>&nbsp;发送获取请求。</p>\n\n<p>2、节点使用文档的&nbsp;<code>_id</code>&nbsp;来确定文档属于分片&nbsp;<code>0</code>&nbsp;。分片&nbsp;<code>0</code>&nbsp;的副本分片存在于所有的三个节点上。 在这种情况下，它将请求转发到&nbsp;<code>Node 2</code>&nbsp;。</p>\n\n<p>3、<code>Node 2</code>&nbsp;将文档返回给&nbsp;<code>Node 1</code>&nbsp;，然后将文档返回给客户端。</p>\n\n<p>在处理读取请求时，协调结点在每次请求的时候都会通过轮询所有的副本分片来达到负载均衡。</p>\n\n<p>在文档被检索时，已经被索引的文档可能已经存在于主分片上但是还没有复制到副本分片。 在这种情况下，副本分片可能会报告文档不存在，但是主分片可能成功返回文档。 一旦索引请求成功返回给用户，文档在主分片和副本分片都是可用的。</p>\n\n<h2>局部更新文档</h2>\n\n<p>如&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/_partial_updates_to_a_document.html#img-distrib-update\" title=\"图 11. 局部更新文档\">图&nbsp;11 &ldquo;局部更新文档&rdquo;</a>&nbsp;所示，<code>update</code>&nbsp;API 结合了先前说明的读取和写入模式&nbsp;。</p>\n\n<p><strong>图&nbsp;11.&nbsp;局部更新文档</strong></p>\n\n<p><img alt=\"局部更新文档\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/elas_0404.png\" /></p>\n\n<p>&nbsp;</p>\n\n<p>以下是部分更新一个文档的步骤：</p>\n\n<ol style=\"list-style-type:decimal\">\n	<li>客户端向&nbsp;<code>Node 1</code>&nbsp;发送更新请求。</li>\n	<li>它将请求转发到主分片所在的&nbsp;<code>Node 3</code>&nbsp;。</li>\n	<li><code>Node 3</code>&nbsp;从主分片检索文档，修改&nbsp;<code>_source</code>&nbsp;字段中的 JSON ，并且尝试重新索引主分片的文档。 如果文档已经被另一个进程修改，它会重试步骤 3 ，超过&nbsp;<code>retry_on_conflict</code>&nbsp;次后放弃。</li>\n	<li>如果&nbsp;<code>Node 3</code>&nbsp;成功地更新文档，它将新版本的文档并行转发到&nbsp;<code>Node 1</code>&nbsp;和&nbsp;<code>Node 2</code>&nbsp;上的副本分片，重新建立索引。 一旦所有副本分片都返回成功，&nbsp;<code>Node 3</code>&nbsp;向协调节点也返回成功，协调节点向客户端返回成功。</li>\n</ol>\n\n<p><code>update</code>&nbsp;API 还接受&nbsp;<code>routing</code>&nbsp;、&nbsp;<code>replication</code>&nbsp;、&nbsp;<code>consistency</code>&nbsp;和<code>timeout</code>&nbsp;参数。</p>\n\n<p><strong>基于文档的复制</strong></p>\n\n<p>当主分片把更改转发到副本分片时，&nbsp;它不会转发更新请求。 相反，它转发完整文档的新版本。请记住，这些更改将会异步转发到副本分片，并且不能保证它们以发送它们相同的顺序到达。 如果Elasticsearch仅转发更改请求，则可能以错误的顺序应用更改，导致得到损坏的文档。</p>\n\n<h2>多文档模式</h2>\n\n<p><code>mget</code>&nbsp;和&nbsp;<code>bulk</code>&nbsp;API 的&nbsp;模式类似于单文档模式。区别在于协调节点知道每个文档存在于哪个分片中。 它将整个多文档请求分解成&nbsp;<em>每个分片</em>&nbsp;的多文档请求，并且将这些请求并行转发到每个参与节点。</p>\n\n<p>协调节点一旦收到来自每个节点的应答，就将每个节点的响应收集整理成单个响应，返回给客户端，如<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/distrib-multi-doc.html#img-distrib-mget\" title=\"图 12. 使用 mget 取回多个文档\">图&nbsp;12 &ldquo;使用&nbsp;<code>mget</code>&nbsp;取回多个文档&rdquo;</a>&nbsp;所示。</p>\n\n<p><strong>图&nbsp;12.&nbsp;使用&nbsp;<code>mget</code>&nbsp;取回多个文档</strong></p>\n\n<p><img alt=\"“使用 `mget` 取回多个文档”\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/elas_0405.png\" /></p>\n\n<p>&nbsp;</p>\n\n<p>以下是使用单个&nbsp;<code>mget</code>&nbsp;请求取回多个文档所需的步骤顺序：</p>\n\n<ol style=\"list-style-type:decimal\">\n	<li>客户端向&nbsp;<code>Node 1</code>&nbsp;发送&nbsp;<code>mget</code>&nbsp;请求。</li>\n	<li><code>Node 1</code>&nbsp;为每个分片构建多文档获取请求，然后并行转发这些请求到托管在每个所需的主分片或者副本分片的节点上。一旦收到所有答复，&nbsp;<code>Node 1</code>&nbsp;构建响应并将其返回给客户端。</li>\n</ol>\n\n<p>可以对&nbsp;<code>docs</code>&nbsp;数组中每个文档设置&nbsp;<code>routing</code>&nbsp;参数。</p>\n\n<p>bulk API， 如&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/distrib-multi-doc.html#img-distrib-bulk\" title=\"图 13. 使用 bulk 修改多个文档\">图&nbsp;13 &ldquo;使用&nbsp;<code>bulk</code>&nbsp;修改多个文档&rdquo;</a>&nbsp;所示， 允许在单个批量请求中执行多个创建、索引、删除和更新请求。</p>\n\n<p><strong>图&nbsp;13.&nbsp;使用&nbsp;<code>bulk</code>&nbsp;修改多个文档</strong></p>\n\n<p><img alt=\"“使用 `bulk` 修改多个文档”\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/elas_0406.png\" /></p>\n\n<p>&nbsp;</p>\n\n<p><code>bulk</code>&nbsp;API&nbsp;按如下步骤顺序执行：</p>\n\n<ol style=\"list-style-type:decimal\">\n	<li>客户端向&nbsp;<code>Node 1</code>&nbsp;发送&nbsp;<code>bulk</code>&nbsp;请求。</li>\n	<li><code>Node 1</code>&nbsp;为每个节点创建一个批量请求，并将这些请求并行转发到每个包含主分片的节点主机。</li>\n	<li>主分片一个接一个按顺序执行每个操作。当每个操作成功时，主分片并行转发新文档（或删除）到副本分片，然后执行下一个操作。 一旦所有的副本分片报告所有操作成功，该节点将向协调节点报告成功，协调节点将这些响应收集整理并返回给客户端。</li>\n</ol>\n\n<p><code>bulk</code>&nbsp;API 还可以在整个批量请求的最顶层使用&nbsp;<code>consistency</code>&nbsp;参数，以及在每个请求中的元数据中使用<code>routing</code>&nbsp;参数。</p>\n\n<h3>为什么是有趣的格式？</h3>\n\n<p>当我们了解批量请求时，&nbsp;您可能会问自己， &quot;为什么&nbsp;<code>bulk</code>&nbsp;API 需要有换行符的有趣格式，而不是发送包装在 JSON 数组中的请求，例如&nbsp;<code>mget</code>&nbsp;API？&quot; 。</p>\n\n<p>为了回答这一点，我们需要解释一点背景：在批量请求中引用的每个文档可能属于不同的主分片， 每个文档可能被分配给集群中的任何节点。这意味着批量请求&nbsp;<code>bulk</code>&nbsp;中的每个&nbsp;<em>操作</em>&nbsp;都需要被转发到正确节点上的正确分片。</p>\n\n<p>如果单个请求被包装在 JSON 数组中，那就意味着我们需要执行以下操作：</p>\n\n<ul style=\"list-style-type:disc\">\n	<li>将 JSON 解析为数组（包括文档数据，可以非常大）</li>\n	<li>查看每个请求以确定应该去哪个分片</li>\n	<li>为每个分片创建一个请求数组</li>\n	<li>将这些数组序列化为内部传输格式</li>\n	<li>将请求发送到每个分片</li>\n</ul>\n\n<p>这是可行的，但需要大量的 RAM 来存储原本相同的数据的副本，并将创建更多的数据结构，Java虚拟机（JVM）将不得不花费时间进行垃圾回收。</p>\n\n<p>相反，Elasticsearch可以直接读取被网络缓冲区接收的原始数据。 它使用换行符字符来识别和解析小的<code>action/metadata</code>&nbsp;行来决定哪个分片应该处理每个请求。</p>\n\n<p>这些原始请求会被直接转发到正确的分片。没有冗余的数据复制，没有浪费的数据结构。整个请求尽可能在最小的内存中处理。</p>\n', 1, '5f199b8885e24fc8b28672b872edb606', 4, '2018-06-06 18:32:09', '2018-09-06 11:25:02');
INSERT INTO `logcontent` VALUES ('deecaaff07824608ad83f7e76e8c8ee3', 1, 'f29612168904487aadb4043df110361c', 'Nginx事件模块（Events Module）', 'linux,nginx,事件模块,Events Module', '事件模块（Events Module）', '<p>事件模块（Events Module）</p>\n\n<p>use</p>\n\n<pre>\n<code class=\"language-nginx\">语法：use [ kqueue | rtsig | epoll | /dev/poll | select | poll | eventport ] \n默认值：\n如果你在./configure的时候指定了不止一个事件模型，你可以通过这个参数告诉nginx你想使用哪一个事件模型，默认情况下nginx在编译时会检查最适合你系统的事件模型。\n你可以在这里看到所有可用的事件模型并且如果在./configure时激活它们。</code></pre>\n\n<p>worker_connections</p>\n\n<pre>\n<code class=\"language-nginx\">语法：worker_connections \n默认值：\nworker_connections和worker_proceses（见主模块）允许你计算理论最大连接数：\n最大连接数 = worker_processes * worker_connections \n在反向代理环境下：\n最大连接数 = worker_processes * worker_connections/4\n由于浏览器默认打开2个连接到服务器，nginx使用来自相同地址池的fds（文件描述符）与前后端相连接</code></pre>\n\n<p>&nbsp;</p>\n', 1, '5f199b8885e24fc8b28672b872edb606', 5, '2018-04-14 14:50:17', '2018-08-31 22:27:24');
INSERT INTO `logcontent` VALUES ('df0419ce311048dda0b85851c48766cd', -1, 'f29612168904487aadb4043df110361c', 'Varnish安装', 'Varnish安装', 'Varnish安装', '<p>Varnish安装</p>\n\n<pre>\n<code>Varnish依赖包\nyum install gcc\nyum install pcre*\nyum install libedit-dev*\n下载Varnish使用wget命令\nyum install wget\n下载Varnish安装包\nwget -c http://varnish-cache.org/_downloads/varnish-6.0.0.tgz\n解压\ntar zvxf varnish-6.0.0.tgz\n进入varnish-6.0.0进行配置\ncd varnish-6.0.0/\n设置路径\n./configure --prefix=/usr/common/varnish\n编译安装\nmake\nmake install</code></pre>\n', 1, '5f199b8885e24fc8b28672b872edb606', 1, '2018-03-28 11:55:23', '2018-08-31 22:21:23');
INSERT INTO `logcontent` VALUES ('e7d4dc38ba0241d9815eb978c2ee032e', -1, 'd7caeba238f0466d87db109b2b9724da', '从最大似然到EM算法', '最大似然,EM算法', '初学', '<p>http://blog.csdn.net/zouxy09/article/details/8537620</p>\n', 1, '5f199b8885e24fc8b28672b872edb606', 3, '2017-12-11 22:02:43', '2018-08-31 22:21:23');
INSERT INTO `logcontent` VALUES ('e85d7a54237b4e758459f244c89b8642', -1, '7338e53acd514defa1a17e47016f3f4a', '十、分片内部原理', '分片内部原理', '分片内部原理', '<h2>分片内部原理</h2>\n\n<p>在&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/distributed-cluster.html\" title=\"集群内的原理\"><em>集群内的原理</em></a>, 我们介绍了&nbsp;<em>分片</em>, 并将它&nbsp;描述成最小的&nbsp;<em>工作单元_。但是究竟什么 _是</em>&nbsp;一个分片，它是如何工作的？ 在这个章节，我们回答以下问题:</p>\n\n<ul style=\"list-style-type:disc\">\n	<li>为什么搜索是&nbsp;<em>近</em>&nbsp;实时的？</li>\n	<li>为什么文档的 CRUD (创建-读取-更新-删除) 操作是&nbsp;<em>实时</em>&nbsp;的?</li>\n	<li>Elasticsearch 是怎样保证更新被持久化在断电时也不丢失数据?</li>\n	<li>为什么删除文档不会立刻释放空间？</li>\n	<li><code>refresh</code>,&nbsp;<code>flush</code>, 和&nbsp;<code>optimize</code>&nbsp;API 都做了什么, 你什么情况下应该是用他们？</li>\n</ul>\n\n<p>最简单的理解一个分片如何工作的方式是上一堂历史课。 我们将要审视提供一个带近实时搜索和分析的 分布式持久化数据存储需要解决的问题。</p>\n\n<h2>使文本可被搜索</h2>\n\n<p>必须解决的第一个挑战是如何&nbsp;使文本可被搜索。 传统的数据库每个字段存储单个值，但这对全文检索并不够。文本字段中的每个单词需要被搜索，对数据库意味着需要单个字段有索引多值(这里指单词)的能力。</p>\n\n<p>最好的支持&nbsp;<em>一个字段多个值</em>&nbsp;需求的数据结构是我们在&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/inverted-index.html\" title=\"倒排索引\">倒排索引</a>&nbsp;章节中介绍过的&nbsp;<em>倒排索引</em>&nbsp;。 倒排索引包含一个有序列表，列表包含所有文档出现过的不重复个体，或称为&nbsp;<em>词项</em>&nbsp;，对于每一个词项，包含了它所有曾出现过文档的列表。</p>\n\n<pre>\nTerm  | Doc 1 | Doc 2 | Doc 3 | ...\n------------------------------------\nbrown |   X   |       |  X    | ...\nfox   |   X   |   X   |  X    | ...\nquick |   X   |   X   |       | ...\nthe   |   X   |       |  X    | ...</pre>\n\n<p><img alt=\"注意\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/note.png\" /></p>\n\n<p>当讨论倒排索引时，我们会谈到&nbsp;<em>文档</em>&nbsp;标引，因为历史原因，倒排索引被用来对整个非结构化文本文档进行标引。 Elasticsearch 中的&nbsp;<em>文档</em>&nbsp;是有字段和值的结构化 JSON 文档。事实上，在 JSON 文档中， 每个被索引的字段都有自己的倒排索引。</p>\n\n<p>这个倒排索引相比特定词项出现过的文档列表，会包含更多其它信息。它会保存每一个词项出现过的文档总数， 在对应的文档中一个具体词项出现的总次数，词项在文档中的顺序，每个文档的长度，所有文档的平均长度，等等。这些统计信息允许 Elasticsearch 决定哪些词比其它词更重要，哪些文档比其它文档更重要，这些内容在&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/relevance-intro.html\" title=\"什么是相关性?\">什么是相关性?</a>&nbsp;中有描述。</p>\n\n<p>为了能够实现预期功能，倒排索引需要知道集合中的&nbsp;<em>所有</em>&nbsp;文档，这是需要认识到的关键问题。</p>\n\n<p>早期的全文检索会为整个文档集合建立一个很大的倒排索引并将其写入到磁盘。 一旦新的索引就绪，旧的就会被其替换，这样最近的变化便可以被检索到。</p>\n\n<h3>不变性</h3>\n\n<p>倒排索引被写入磁盘后是&nbsp;<em>不可改变</em>&nbsp;的:它永远不会修改。&nbsp;不变性有重要的价值：</p>\n\n<ul style=\"list-style-type:disc\">\n	<li>不需要锁。如果你从来不更新索引，你就不需要担心多进程同时修改数据的问题。</li>\n	<li>一旦索引被读入内核的文件系统缓存，便会留在哪里，由于其不变性。只要文件系统缓存中还有足够的空间，那么大部分读请求会直接请求内存，而不会命中磁盘。这提供了很大的性能提升。</li>\n	<li>其它缓存(像filter缓存)，在索引的生命周期内始终有效。它们不需要在每次数据改变时被重建，因为数据不会变化。</li>\n	<li>写入单个大的倒排索引允许数据被压缩，减少磁盘 I/O 和 需要被缓存到内存的索引的使用量。</li>\n</ul>\n\n<p>当然，一个不变的索引也有不好的地方。主要事实是它是不可变的! 你不能修改它。如果你需要让一个新的文档 可被搜索，你需要重建整个索引。这要么对一个索引所能包含的数据量造成了很大的限制，要么对索引可被更新的频率造成了很大的限制。</p>\n\n<h2>动态更新索引</h2>\n\n<p>下一个需要被解决的问题是怎样在保留不变性的前提下实现倒排索引的更新？&nbsp;答案是: 用更多的索引。</p>\n\n<p>通过增加新的补充索引来反映新近的修改，而不是直接重写整个倒排索引。每一个倒排索引都会被轮流查询到--从最早的开始--查询完后再对结果进行合并。</p>\n\n<p>Elasticsearch 基于 Lucene, 这个 java 库引入了&nbsp;<em>按段搜索</em>&nbsp;的概念。&nbsp;每一&nbsp;<em>段</em>&nbsp;本身都是一个倒排索引， 但&nbsp;<em>索引</em>&nbsp;在 Lucene 中除表示所有&nbsp;<em>段</em>&nbsp;的集合外， 还增加了&nbsp;<em>提交点</em>&nbsp;的概念 &mdash; 一个列出了所有已知段的文件，就像在&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/dynamic-indices.html#img-index-segments\" title=\"图 16. 一个 Lucene 索引包含一个提交点和三个段\">图&nbsp;16 &ldquo;一个 Lucene 索引包含一个提交点和三个段&rdquo;</a>&nbsp;中描绘的那样。 如&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/dynamic-indices.html#img-memory-buffer\" title=\"图 17. 一个在内存缓存中包含新文档的 Lucene 索引\">图&nbsp;17 &ldquo;一个在内存缓存中包含新文档的 Lucene 索引&rdquo;</a>&nbsp;所示，新的文档首先被添加到内存索引缓存中，然后写入到一个基于磁盘的段，如<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/dynamic-indices.html#img-post-commit\" title=\"图 18. 在一次提交后，一个新的段被添加到提交点而且缓存被清空。\">图&nbsp;18 &ldquo;在一次提交后，一个新的段被添加到提交点而且缓存被清空。&rdquo;</a>&nbsp;所示。</p>\n\n<p>&nbsp;</p>\n\n<p><strong>图&nbsp;16.&nbsp;一个 Lucene 索引包含一个提交点和三个段</strong></p>\n\n<p><img alt=\"A Lucene index with a commit point and three segments\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/elas_1101.png\" /></p>\n\n<p>&nbsp;</p>\n\n<p><strong>索引与分片的比较</strong></p>\n\n<p>被混淆的概念是，一个&nbsp;<em>Lucene 索引</em>&nbsp;我们在 Elasticsearch 称作&nbsp;<em>分片</em>&nbsp;。 一个 Elasticsearch&nbsp;<em>索引</em>是分片的集合。 当 Elasticsearch 在索引中搜索的时候， 他发送查询到每一个属于索引的分片(Lucene 索引)，然后像&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/distributed-search.html\" title=\"执行分布式检索\"><em>执行分布式检索</em></a>&nbsp;提到的那样，合并每个分片的结果到一个全局的结果集。</p>\n\n<p>逐段搜索会以如下流程进行工作：</p>\n\n<ol style=\"list-style-type:decimal\">\n	<li>新文档被收集到内存索引缓存， 见&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/dynamic-indices.html#img-memory-buffer\" title=\"图 17. 一个在内存缓存中包含新文档的 Lucene 索引\">图&nbsp;17 &ldquo;一个在内存缓存中包含新文档的 Lucene 索引&rdquo;</a>&nbsp;。</li>\n	<li>\n	<p>不时地, 缓存被&nbsp;<em>提交</em>&nbsp;：</p>\n\n	<ul style=\"list-style-type:disc\">\n		<li>一个新的段--一个追加的倒排索引--被写入磁盘。</li>\n		<li>一个新的包含新段名字的&nbsp;<em>提交点</em>&nbsp;被写入磁盘。</li>\n		<li>磁盘进行&nbsp;<em>同步</em>&nbsp;&mdash; 所有在文件系统缓存中等待的写入都刷新到磁盘，以确保它们被写入物理文件。</li>\n	</ul>\n	</li>\n	<li>新的段被开启，让它包含的文档可见以被搜索。</li>\n	<li>内存缓存被清空，等待接收新的文档。</li>\n</ol>\n\n<p>&nbsp;</p>\n\n<p><strong>图&nbsp;17.&nbsp;一个在内存缓存中包含新文档的 Lucene 索引</strong></p>\n\n<p><img alt=\"A Lucene index with new documents in the in-memory buffer, ready to commit\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/elas_1102.png\" /></p>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n\n<p><strong>图&nbsp;18.&nbsp;在一次提交后，一个新的段被添加到提交点而且缓存被清空。</strong></p>\n\n<p><img alt=\"After a commit, a new segment is added to the index and the buffer is cleared\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/elas_1103.png\" /></p>\n\n<p>&nbsp;</p>\n\n<p>当一个查询被触发，所有已知的段按顺序被查询。词项统计会对所有段的结果进行聚合，以保证每个词和每个文档的关联都被准确计算。 这种方式可以用相对较低的成本将新文档添加到索引。</p>\n\n<h3>删除和更新</h3>\n\n<p>段是不可改变的，所以既不能从把文档从旧的段中移除，也不能修改旧的段来进行反映文档的更新。 取而代之的是，每个提交点会包含一个&nbsp;<code>.del</code>&nbsp;文件，文件中会列出这些被删除文档的段信息。</p>\n\n<p>当一个文档被 &ldquo;删除&rdquo; 时，它实际上只是在&nbsp;<code>.del</code>&nbsp;文件中被&nbsp;<em>标记</em>&nbsp;删除。一个被标记删除的文档仍然可以被查询匹配到， 但它会在最终结果被返回前从结果集中移除。</p>\n\n<p>文档更新也是类似的操作方式：当一个文档被更新时，旧版本文档被标记删除，文档的新版本被索引到一个新的段中。 可能两个版本的文档都会被一个查询匹配到，但被删除的那个旧版本文档在结果集返回前就已经被移除。</p>\n\n<p>在&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/merge-process.html\" title=\"段合并\">段合并</a>&nbsp;, 我们展示了一个被删除的文档是怎样被文件系统移除的。</p>\n\n<h2>近实时搜索</h2>\n\n<p>随着按段（per-segment）搜索的发展，&nbsp;一个新的文档从索引到可被搜索的延迟显著降低了。新文档在几分钟之内即可被检索，但这样还是不够快。</p>\n\n<p>磁盘在这里成为了瓶颈。&nbsp;提交（Commiting）一个新的段到磁盘需要一个&nbsp;<a href=\"http://en.wikipedia.org/wiki/Fsync\" target=\"_top\"><code>fsync</code></a>&nbsp;来确保段被物理性地写入磁盘，这样在断电的时候就不会丢失数据。 但是&nbsp;<code>fsync</code>&nbsp;操作代价很大; 如果每次索引一个文档都去执行一次的话会造成很大的性能问题。</p>\n\n<p>我们需要的是一个更轻量的方式来使一个文档可被搜索，这意味着&nbsp;<code>fsync</code>&nbsp;要从整个过程中被移除。</p>\n\n<p>在Elasticsearch和磁盘之间是文件系统缓存。&nbsp;像之前描述的一样， 在内存索引缓冲区（&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/near-real-time.html#img-pre-refresh\" title=\"图 19. 在内存缓冲区中包含了新文档的 Lucene 索引\">图&nbsp;19 &ldquo;在内存缓冲区中包含了新文档的 Lucene 索引&rdquo;</a>&nbsp;）中的文档会被写入到一个新的段中（&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/near-real-time.html#img-post-refresh\" title=\"图 20. 缓冲区的内容已经被写入一个可被搜索的段中，但还没有进行提交\">图&nbsp;20 &ldquo;缓冲区的内容已经被写入一个可被搜索的段中，但还没有进行提交&rdquo;</a>&nbsp;）。 但是这里新段会被先写入到文件系统缓存--这一步代价会比较低，稍后再被刷新到磁盘--这一步代价比较高。不过只要文件已经在缓存中， 就可以像其它文件一样被打开和读取了。</p>\n\n<p><strong>图&nbsp;19.&nbsp;在内存缓冲区中包含了新文档的 Lucene 索引</strong></p>\n\n<p><img alt=\"A Lucene index with new documents in the in-memory buffer\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/elas_1104.png\" /></p>\n\n<p>&nbsp;</p>\n\n<p>Lucene 允许新段被写入和打开--使其包含的文档在未进行一次完整提交时便对搜索可见。 这种方式比进行一次提交代价要小得多，并且在不影响性能的前提下可以被频繁地执行。</p>\n\n<p><strong>图&nbsp;20.&nbsp;缓冲区的内容已经被写入一个可被搜索的段中，但还没有进行提交</strong></p>\n\n<p><img alt=\"The buffer contents have been written to a segment, which is searchable, but is not yet commited\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/elas_1105.png\" /></p>\n\n<p>&nbsp;</p>\n\n<h3>refresh API</h3>\n\n<p>在 Elasticsearch 中，写入和打开一个新段的轻量的过程叫做&nbsp;<em>refresh</em>&nbsp;。&nbsp;默认情况下每个分片会每秒自动刷新一次。这就是为什么我们说 Elasticsearch 是&nbsp;<em>近</em>&nbsp;实时搜索: 文档的变化并不是立即对搜索可见，但会在一秒之内变为可见。</p>\n\n<p>这些行为可能会对新用户造成困惑: 他们索引了一个文档然后尝试搜索它，但却没有搜到。这个问题的解决办法是用&nbsp;<code>refresh</code>&nbsp;API 执行一次手动刷新:</p>\n\n<pre>\nPOST /_refresh <img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" />\nPOST /blogs/_refresh <img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/2.png\" /></pre>\n\n<table border=\"0\" summary=\"Callout list\">\n	<tbody>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/near-real-time.html#CO37-1\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p>刷新（Refresh）所有的索引。</p>\n			</td>\n		</tr>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/near-real-time.html#CO37-2\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/2.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p>只刷新（Refresh）&nbsp;<code>blogs</code>&nbsp;索引。</p>\n			</td>\n		</tr>\n	</tbody>\n</table>\n\n<p><img alt=\"提示\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/tip.png\" /></p>\n\n<p>尽管刷新是比提交轻量很多的操作，它还是会有性能开销。&nbsp;当写测试的时候， 手动刷新很有用，但是不要在生产环境下每次索引一个文档都去手动刷新。 相反，你的应用需要意识到 Elasticsearch 的近实时的性质，并接受它的不足。</p>\n\n<p>并不是所有的情况都需要每秒刷新。可能你正在使用 Elasticsearch 索引大量的日志文件， 你可能想优化索引速度而不是近实时搜索， 可以通过设置&nbsp;<code>refresh_interval</code>&nbsp;， 降低每个索引的刷新频率：</p>\n\n<pre>\nPUT /my_logs\n{\n  &quot;settings&quot;: {\n    &quot;refresh_interval&quot;: &quot;30s&quot; <img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" />\n  }\n}</pre>\n\n<table border=\"0\" summary=\"Callout list\">\n	<tbody>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/near-real-time.html#CO38-1\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p>每30秒刷新&nbsp;<code>my_logs</code>&nbsp;索引。</p>\n			</td>\n		</tr>\n	</tbody>\n</table>\n\n<p><code>refresh_interval</code>&nbsp;可以在既存索引上进行动态更新。 在生产环境中，当你正在建立一个大的新索引时，可以先关闭自动刷新，待开始使用该索引时，再把它们调回来：</p>\n\n<pre>\nPUT /my_logs/_settings\n{ &quot;refresh_interval&quot;: -1 } <img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" />\n\nPUT /my_logs/_settings\n{ &quot;refresh_interval&quot;: &quot;1s&quot; } <img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/2.png\" /></pre>\n\n<table border=\"0\" summary=\"Callout list\">\n	<tbody>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/near-real-time.html#CO39-1\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p>关闭自动刷新。</p>\n			</td>\n		</tr>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/near-real-time.html#CO39-2\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/2.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p>每秒自动刷新。</p>\n			</td>\n		</tr>\n	</tbody>\n</table>\n\n<p><img alt=\"小心\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/caution.png\" /></p>\n\n<p><code>refresh_interval</code>&nbsp;需要一个&nbsp;<em>持续时间</em>&nbsp;值， 例如&nbsp;<code>1s</code>&nbsp;（1 秒） 或&nbsp;<code>2m</code>&nbsp;（2 分钟）。 一个绝对值&nbsp;<em>1</em>&nbsp;表示的是&nbsp;<em>1毫秒</em>&nbsp;--无疑会使你的集群陷入瘫痪。</p>\n\n<h2>持久化变更</h2>\n\n<p>如果没有用&nbsp;<code>fsync</code>&nbsp;把数据从文件系统缓存刷（flush）到硬盘，我们不能保证数据在断电甚至是程序正常退出之后依然存在。为了保证 Elasticsearch 的可靠性，需要确保数据变化被持久化到磁盘。</p>\n\n<p>在&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/dynamic-indices.html\" title=\"动态更新索引\">动态更新索引</a>，我们说一次完整的提交会将段刷到磁盘，并写入一个包含所有段列表的提交点。Elasticsearch 在启动或重新打开一个索引的过程中使用这个提交点来判断哪些段隶属于当前分片。</p>\n\n<p>即使通过每秒刷新（refresh）实现了近实时搜索，我们仍然需要经常进行完整提交来确保能从失败中恢复。但在两次提交之间发生变化的文档怎么办？我们也不希望丢失掉这些数据。</p>\n\n<p>Elasticsearch 增加了一个&nbsp;<em>translog</em>&nbsp;，或者叫事务日志，在每一次对 Elasticsearch 进行操作时均进行了日志记录。通过 translog ，整个流程看起来是下面这样：</p>\n\n<ol style=\"list-style-type:decimal\">\n	<li>\n	<p>一个文档被索引之后，就会被添加到内存缓冲区，<em>并且</em>&nbsp;追加到了 translog ，正如&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/translog.html#img-xlog-pre-refresh\" title=\"图 21. 新的文档被添加到内存缓冲区并且被追加到了事务日志\">图&nbsp;21 &ldquo;新的文档被添加到内存缓冲区并且被追加到了事务日志&rdquo;</a>&nbsp;描述的一样。</p>\n\n	<p><strong>图&nbsp;21.&nbsp;新的文档被添加到内存缓冲区并且被追加到了事务日志</strong></p>\n\n	<p><img alt=\"New documents are added to the in-memory buffer and appended to the transaction log\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/elas_1106.png\" /></p>\n	</li>\n	<li>\n	<p>刷新（refresh）使分片处于&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/translog.html#img-xlog-post-refresh\" title=\"图 22. 刷新（refresh）完成后, 缓存被清空但是事务日志不会\">图&nbsp;22 &ldquo;刷新（refresh）完成后, 缓存被清空但是事务日志不会&rdquo;</a>&nbsp;描述的状态，分片每秒被刷新（refresh）一次：</p>\n\n	<ul style=\"list-style-type:disc\">\n		<li>这些在内存缓冲区的文档被写入到一个新的段中，且没有进行&nbsp;<code>fsync</code>&nbsp;操作。</li>\n		<li>这个段被打开，使其可被搜索。</li>\n		<li>内存缓冲区被清空。</li>\n	</ul>\n\n	<p><strong>图&nbsp;22.&nbsp;刷新（refresh）完成后, 缓存被清空但是事务日志不会</strong></p>\n\n	<p><img alt=\"After a refresh, the buffer is cleared but the transaction log is not\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/elas_1107.png\" /></p>\n	</li>\n	<li>\n	<p>这个进程继续工作，更多的文档被添加到内存缓冲区和追加到事务日志（见&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/translog.html#img-xlog-pre-flush\" title=\"图 23. 事务日志不断积累文档\">图&nbsp;23 &ldquo;事务日志不断积累文档&rdquo;</a>&nbsp;）。</p>\n\n	<p><strong>图&nbsp;23.&nbsp;事务日志不断积累文档</strong></p>\n\n	<p><img alt=\"The transaction log keeps accumulating documents\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/elas_1108.png\" /></p>\n	</li>\n	<li>\n	<p>每隔一段时间--例如 translog 变得越来越大--索引被刷新（flush）；一个新的 translog 被创建，并且一个全量提交被执行（见&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/translog.html#img-xlog-post-flush\" title=\"图 24. 在刷新（flush）之后，段被全量提交，并且事务日志被清空\">图&nbsp;24 &ldquo;在刷新（flush）之后，段被全量提交，并且事务日志被清空&rdquo;</a>）：</p>\n\n	<ul style=\"list-style-type:disc\">\n		<li>所有在内存缓冲区的文档都被写入一个新的段。</li>\n		<li>缓冲区被清空。</li>\n		<li>一个提交点被写入硬盘。</li>\n		<li>文件系统缓存通过&nbsp;<code>fsync</code>&nbsp;被刷新（flush）。</li>\n		<li>老的 translog 被删除。</li>\n	</ul>\n	</li>\n</ol>\n\n<p>translog 提供所有还没有被刷到磁盘的操作的一个持久化纪录。当 Elasticsearch 启动的时候， 它会从磁盘中使用最后一个提交点去恢复已知的段，并且会重放 translog 中所有在最后一次提交后发生的变更操作。</p>\n\n<p>translog 也被用来提供实时 CRUD 。当你试着通过ID查询、更新、删除一个文档，它会在尝试从相应的段中检索之前， 首先检查 translog 任何最近的变更。这意味着它总是能够实时地获取到文档的最新版本。</p>\n\n<p><strong>图&nbsp;24.&nbsp;在刷新（flush）之后，段被全量提交，并且事务日志被清空</strong></p>\n\n<p><img alt=\"After a flush, the segments are fully commited and the transaction log is cleared\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/elas_1109.png\" /></p>\n\n<p>&nbsp;</p>\n\n<h3>flush API</h3>\n\n<p>这个执行一个提交并且截断 translog 的行为在 Elasticsearch 被称作一次&nbsp;<em>flush</em>&nbsp;。&nbsp;分片每30分钟被自动刷新（flush），或者在 translog 太大的时候也会刷新。请查看&nbsp;<a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/2.4/index-modules-translog.html#_translog_settings\" target=\"_top\"><code>translog</code>&nbsp;文档</a>&nbsp;来设置，它可以用来&nbsp;控制这些阈值：</p>\n\n<p><a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/5.6/indices-flush.html\" target=\"_top\"><code>flush</code>&nbsp;API</a>&nbsp;可以&nbsp;被用来执行一个手工的刷新（flush）:</p>\n\n<pre>\nPOST /blogs/_flush <img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" />\n\nPOST /_flush?wait_for_ongoing <img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/2.png\" /></pre>\n\n<table border=\"0\" summary=\"Callout list\">\n	<tbody>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/translog.html#CO40-1\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p>刷新（flush）&nbsp;<code>blogs</code>&nbsp;索引。</p>\n			</td>\n		</tr>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/translog.html#CO40-2\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/2.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p>刷新（flush）所有的索引并且并且等待所有刷新在返回前完成。</p>\n			</td>\n		</tr>\n	</tbody>\n</table>\n\n<p>你很少需要自己手动执行一个的&nbsp;<code>flush</code>&nbsp;操作；通常情况下，自动刷新就足够了。</p>\n\n<p>这就是说，在重启节点或关闭索引之前执行&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/translog.html#flush-api\" title=\"flush API\">flush</a>&nbsp;有益于你的索引。当 Elasticsearch 尝试恢复或重新打开一个索引， 它需要重放 translog 中所有的操作，所以如果日志越短，恢复越快。</p>\n\n<p><strong>Translog 有多安全?</strong></p>\n\n<p>translog 的目的是保证操作不会丢失。这引出了这个问题： Translog 有多安全&nbsp;？</p>\n\n<p>在文件被&nbsp;<code>fsync</code>&nbsp;到磁盘前，被写入的文件在重启之后就会丢失。默认 translog 是每 5 秒被<code>fsync</code>&nbsp;刷新到硬盘， 或者在每次写请求完成之后执行(e.g. index, delete, update, bulk)。这个过程在主分片和复制分片都会发生。最终， 基本上，这意味着在整个请求被&nbsp;<code>fsync</code>&nbsp;到主分片和复制分片的translog之前，你的客户端不会得到一个 200 OK 响应。</p>\n\n<p>在每次请求后都执行一个 fsync 会带来一些性能损失，尽管实践表明这种损失相对较小（特别是bulk导入，它在一次请求中平摊了大量文档的开销）。</p>\n\n<p>但是对于一些大容量的偶尔丢失几秒数据问题也并不严重的集群，使用异步的 fsync 还是比较有益的。比如，写入的数据被缓存到内存中，再每5秒执行一次&nbsp;<code>fsync</code>&nbsp;。</p>\n\n<p>这个行为可以通过设置&nbsp;<code>durability</code>&nbsp;参数为&nbsp;<code>async</code>&nbsp;来启用：</p>\n\n<pre>\nPUT /my_index/_settings\n{\n    &quot;index.translog.durability&quot;: &quot;async&quot;,\n    &quot;index.translog.sync_interval&quot;: &quot;5s&quot;\n}</pre>\n\n<p>这个选项可以针对索引单独设置，并且可以动态进行修改。如果你决定使用异步 translog 的话，你需要&nbsp;<em>保证</em>&nbsp;在发生crash时，丢失掉&nbsp;<code>sync_interval</code>&nbsp;时间段的数据也无所谓。请在决定前知晓这个特性。</p>\n\n<p>如果你不确定这个行为的后果，最好是使用默认的参数（&nbsp;<code>&quot;index.translog.durability&quot;: &quot;request&quot;</code>&nbsp;）来避免数据丢失。</p>\n\n<h2>段合并</h2>\n\n<p>由于自动刷新流程每秒会创建一个新的段&nbsp;，这样会导致短时间内的段数量暴增。而段数目太多会带来较大的麻烦。 每一个段都会消耗文件句柄、内存和cpu运行周期。更重要的是，每个搜索请求都必须轮流检查每个段；所以段越多，搜索也就越慢。</p>\n\n<p>Elasticsearch通过在后台进行段合并来解决这个问题。小的段被合并到大的段，然后这些大的段再被合并到更大的段。</p>\n\n<p>段合并的时候会将那些旧的已删除文档&nbsp;从文件系统中清除。&nbsp;被删除的文档（或被更新文档的旧版本）不会被拷贝到新的大段中。</p>\n\n<p>启动段合并不需要你做任何事。进行索引和搜索时会自动进行。这个流程像在&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/merge-process.html#img-merge\" title=\"图 25. 两个提交了的段和一个未提交的段正在被合并到一个更大的段\">图&nbsp;25 &ldquo;两个提交了的段和一个未提交的段正在被合并到一个更大的段&rdquo;</a>&nbsp;中提到的一样工作：</p>\n\n<p>1、 当索引的时候，刷新（refresh）操作会创建新的段并将段打开以供搜索使用。</p>\n\n<p>2、 合并进程选择一小部分大小相似的段，并且在后台将它们合并到更大的段中。这并不会中断索引和搜索。</p>\n\n<p>&nbsp;</p>\n\n<p><strong>图&nbsp;25.&nbsp;两个提交了的段和一个未提交的段正在被合并到一个更大的段</strong></p>\n\n<p><img alt=\"Two commited segments and one uncommited segment in the process of being merged into a bigger segment\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/elas_1110.png\" /></p>\n\n<p>&nbsp;</p>\n\n<p>3、&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/merge-process.html#img-post-merge\" title=\"图 26. 一旦合并结束，老的段被删除\">图&nbsp;26 &ldquo;一旦合并结束，老的段被删除&rdquo;</a>&nbsp;说明合并完成时的活动：</p>\n\n<ul style=\"list-style-type:disc\">\n	<li>新的段被刷新（flush）到了磁盘。 &nbsp; ** 写入一个包含新段且排除旧的和较小的段的新提交点。</li>\n	<li>新的段被打开用来搜索。</li>\n	<li>老的段被删除。</li>\n</ul>\n\n<p>&nbsp;</p>\n\n<p><strong>图&nbsp;26.&nbsp;一旦合并结束，老的段被删除</strong></p>\n\n<p><img alt=\"一旦合并结束，老的段被删除\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/elas_1111.png\" /></p>\n\n<p>&nbsp;</p>\n\n<p>合并大的段需要消耗大量的I/O和CPU资源，如果任其发展会影响搜索性能。Elasticsearch在默认情况下会对合并流程进行资源限制，所以搜索仍然 有足够的资源很好地执行。</p>\n\n<p><img alt=\"提示\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/tip.png\" /></p>\n\n<p>查看&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/indexing-performance.html#segments-and-merging\" title=\"段和合并\">段和合并</a>&nbsp;来为你的实例获取关于合并调整的建议。</p>\n\n<h3>optimize API</h3>\n\n<p><code>optimize</code>&nbsp;API大可看做是&nbsp;<em>强制合并</em>&nbsp;API&nbsp;。它会将一个分片强制合并到&nbsp;<code>max_num_segments</code>&nbsp;参数指定大小的段数目。 这样做的意图是减少段的数量（通常减少到一个），来提升搜索性能。</p>\n\n<p><img alt=\"警告\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/warning.png\" /></p>\n\n<p><code>optimize</code>&nbsp;API&nbsp;<em>不应该</em>&nbsp;被用在一个动态索引&mdash;&mdash;&mdash;&mdash;一个正在被活跃更新的索引。后台合并流程已经可以很好地完成工作。 optimizing 会阻碍这个进程。不要干扰它！</p>\n\n<p>在特定情况下，使用&nbsp;<code>optimize</code>&nbsp;API 颇有益处。例如在日志这种用例下，每天、每周、每月的日志被存储在一个索引中。 老的索引实质上是只读的；它们也并不太可能会发生变化。</p>\n\n<p>在这种情况下，使用optimize优化老的索引，将每一个分片合并为一个单独的段就很有用了；这样既可以节省资源，也可以使搜索更加快速：</p>\n\n<pre>\nPOST /logstash-2014-10/_optimize?max_num_segments=1 <img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" /></pre>\n\n<table border=\"0\" summary=\"Callout list\">\n	<tbody>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/merge-process.html#CO41-1\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p>合并索引中的每个分片为一个单独的段</p>\n			</td>\n		</tr>\n	</tbody>\n</table>\n\n<p><img alt=\"警告\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/warning.png\" /></p>\n\n<p>请注意，使用&nbsp;<code>optimize</code>&nbsp;API 触发段合并的操作一点也不会受到任何资源上的限制。这可能会消耗掉你节点上全部的I/O资源, 使其没有余裕来处理搜索请求，从而有可能使集群失去响应。 如果你想要对索引执行 `optimize`，你需要先使用分片分配（查看&nbsp;<a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/retiring-data.html#migrate-indices\" title=\"迁移旧索引\">迁移旧索引</a>）把索引移到一个安全的节点，再执行。</p>\n', 1, '5f199b8885e24fc8b28672b872edb606', 2, '2018-06-27 16:21:17', '2018-08-31 22:21:23');
INSERT INTO `logcontent` VALUES ('ebf692bee53948f2a550ccdcc4980977', -1, 'e5e23ac742ca41a0a7e6f575fc710f4e', 'Linux命令', 'linux,命令', '常用命令', '<p><strong>系统信息&nbsp;</strong></p>\n\n<pre>\n<code>arch 显示机器的处理器架构(1) \nuname -m 显示机器的处理器架构(2) \nuname -r 显示正在使用的内核版本 \ndmidecode -q 显示硬件系统部件 - (SMBIOS / DMI) \nhdparm -i /dev/hda 罗列一个磁盘的架构特性 \nhdparm -tT /dev/sda 在磁盘上执行测试性读取操作 \ncat /proc/cpuinfo 显示CPU info的信息 \ncat /proc/interrupts 显示中断 \ncat /proc/meminfo 校验内存使用 \ncat /proc/swaps 显示哪些swap被使用 \ncat /proc/version 显示内核的版本 \ncat /proc/net/dev 显示网络适配器及统计 \ncat /proc/mounts 显示已加载的文件系统 \nlspci -tv 罗列 PCI 设备 \nlsusb -tv 显示 USB 设备 \ndate 显示系统日期 \ncal 2007 显示2007年的日历表 \ndate 041217002007.00 设置日期和时间 - 月日时分年.秒 \nclock -w 将时间修改保存到 BIOS </code></pre>\n\n<p><br />\n<strong>关机 (系统的关机、重启以及登出 )&nbsp;</strong></p>\n\n<pre>\n<code>shutdown -h now 关闭系统(1) \ninit 0 关闭系统(2) \ntelinit 0 关闭系统(3) \nshutdown -h hours:minutes &amp; 按预定时间关闭系统 \nshutdown -c 取消按预定时间关闭系统 \nshutdown -r now 重启(1) \nreboot 重启(2) \nlogout 注销 </code></pre>\n\n<p><br />\n<strong>文件和目录</strong>&nbsp;</p>\n\n<pre>\n<code>cd /home 进入 \'/ home\' 目录\' \ncd .. 返回上一级目录 \ncd ../.. 返回上两级目录 \ncd 进入个人的主目录 \ncd ~user1 进入个人的主目录 \ncd - 返回上次所在的目录 \npwd 显示工作路径 \nls 查看目录中的文件 \nls -F 查看目录中的文件 \nls -l 显示文件和目录的详细资料 \nls -a 显示隐藏文件 \nls *[0-9]* 显示包含数字的文件名和目录名 \ntree 显示文件和目录由根目录开始的树形结构(1) \nlstree 显示文件和目录由根目录开始的树形结构(2) \nmkdir dir1 创建一个叫做 \'dir1\' 的目录\' \nmkdir dir1 dir2 同时创建两个目录 \nmkdir -p /tmp/dir1/dir2 创建一个目录树 \nrm -f file1 删除一个叫做 \'file1\' 的文件\' \nrmdir dir1 删除一个叫做 \'dir1\' 的目录\' \nrm -rf dir1 删除一个叫做 \'dir1\' 的目录并同时删除其内容 \nrm -rf dir1 dir2 同时删除两个目录及它们的内容 \nmv dir1 new_dir 重命名/移动 一个目录 \ncp file1 file2 复制一个文件 \ncp dir/* . 复制一个目录下的所有文件到当前工作目录 \ncp -a /tmp/dir1 . 复制一个目录到当前工作目录 \ncp -a dir1 dir2 复制一个目录 \nln -s file1 lnk1 创建一个指向文件或目录的软链接 \nln file1 lnk1 创建一个指向文件或目录的物理链接 \ntouch -t 0712250000 file1 修改一个文件或目录的时间戳 - (YYMMDDhhmm) \nfile file1 outputs the mime type of the file as text \niconv -l 列出已知的编码 \niconv -f fromEncoding -t toEncoding inputFile &gt; outputFile creates a new from the given input file by assuming it is encoded in fromEncoding and converting it to toEncoding. \nfind . -maxdepth 1 -name *.jpg -print -exec convert \"{}\" -resize 80x60 \"thumbs/{}\" \\; batch resize files in the current directory and send them to a thumbnails directory (requires convert from Imagemagick) </code></pre>\n\n<p><br />\n<strong>文件搜索</strong>&nbsp;</p>\n\n<pre>\n<code>find / -name file1 从 \'/\' 开始进入根文件系统搜索文件和目录 \nfind / -user user1 搜索属于用户 \'user1\' 的文件和目录 \nfind /home/user1 -name \\*.bin 在目录 \'/ home/user1\' 中搜索带有\'.bin\' 结尾的文件 \nfind /usr/bin -type f -atime +100 搜索在过去100天内未被使用过的执行文件 \nfind /usr/bin -type f -mtime -10 搜索在10天内被创建或者修改过的文件 \nfind / -name \\*.rpm -exec chmod 755 \'{}\' \\; 搜索以 \'.rpm\' 结尾的文件并定义其权限 \nfind / -xdev -name \\*.rpm 搜索以 \'.rpm\' 结尾的文件，忽略光驱、捷盘等可移动设备 \nlocate \\*.ps 寻找以 \'.ps\' 结尾的文件 - 先运行 \'updatedb\' 命令 \nwhereis halt 显示一个二进制文件、源码或man的位置 \nwhich halt 显示一个二进制文件或可执行文件的完整路径 </code></pre>\n\n<p><br />\n<strong>挂载一个文件系统</strong>&nbsp;</p>\n\n<pre>\n<code>mount /dev/hda2 /mnt/hda2 挂载一个叫做hda2的盘 - 确定目录 \'/ mnt/hda2\' 已经存在 \numount /dev/hda2 卸载一个叫做hda2的盘 - 先从挂载点 \'/ mnt/hda2\' 退出 \nfuser -km /mnt/hda2 当设备繁忙时强制卸载 \numount -n /mnt/hda2 运行卸载操作而不写入 /etc/mtab 文件- 当文件为只读或当磁盘写满时非常有用 \nmount /dev/fd0 /mnt/floppy 挂载一个软盘 \nmount /dev/cdrom /mnt/cdrom 挂载一个cdrom或dvdrom \nmount /dev/hdc /mnt/cdrecorder 挂载一个cdrw或dvdrom \nmount /dev/hdb /mnt/cdrecorder 挂载一个cdrw或dvdrom \nmount -o loop file.iso /mnt/cdrom 挂载一个文件或ISO镜像文件 \nmount -t vfat /dev/hda5 /mnt/hda5 挂载一个Windows FAT32文件系统 \nmount /dev/sda1 /mnt/usbdisk 挂载一个usb 捷盘或闪存设备 \nmount -t smbfs -o username=user,password=pass //WinClient/share /mnt/share 挂载一个windows网络共享 </code></pre>\n\n<p><br />\n<strong>磁盘空间&nbsp;</strong></p>\n\n<pre>\n<code>df -h 显示已经挂载的分区列表 \nls -lSr |more 以尺寸大小排列文件和目录 \ndu -sh dir1 估算目录 \'dir1\' 已经使用的磁盘空间\' \ndu -sk * | sort -rn 以容量大小为依据依次显示文件和目录的大小 \nrpm -q -a --qf \'%10{SIZE}t%{NAME}n\' | sort -k1,1n 以大小为依据依次显示已安装的rpm包所使用的空间 (fedora, redhat类系统) \ndpkg-query -W -f=\'${Installed-Size;10}t${Package}n\' | sort -k1,1n 以大小为依据显示已安装的deb包所使用的空间 (ubuntu, debian类系统) </code></pre>\n\n<p><br />\n<strong>用户和群组&nbsp;</strong></p>\n\n<pre>\n<code>groupadd group_name 创建一个新用户组 \ngroupdel group_name 删除一个用户组 \ngroupmod -n new_group_name old_group_name 重命名一个用户组 \nuseradd -c \"Name Surname \" -g admin -d /home/user1 -s /bin/bash user1 创建一个属于 \"admin\" 用户组的用户 \nuseradd user1 创建一个新用户 \nuserdel -r user1 删除一个用户 ( \'-r\' 排除主目录) \nusermod -c \"User FTP\" -g system -d /ftp/user1 -s /bin/nologin user1 修改用户属性 \npasswd 修改口令 \npasswd user1 修改一个用户的口令 (只允许root执行) \nchage -E 2005-12-31 user1 设置用户口令的失效期限 \npwck 检查 \'/etc/passwd\' 的文件格式和语法修正以及存在的用户 \ngrpck 检查 \'/etc/passwd\' 的文件格式和语法修正以及存在的群组 \nnewgrp group_name 登陆进一个新的群组以改变新创建文件的预设群组 </code></pre>\n\n<p><br />\n<strong>文件的权限 - 使用 &quot;+&quot; 设置权限，使用 &quot;-&quot; 用于取消&nbsp;</strong></p>\n\n<pre>\n<code>ls -lh 显示权限 \nls /tmp | pr -T5 -W$COLUMNS 将终端划分成5栏显示 \nchmod ugo+rwx directory1 设置目录的所有人(u)、群组(g)以及其他人(o)以读（r ）、写(w)和执行(x)的权限 \nchmod go-rwx directory1 删除群组(g)与其他人(o)对目录的读写执行权限 \nchown user1 file1 改变一个文件的所有人属性 \nchown -R user1 directory1 改变一个目录的所有人属性并同时改变改目录下所有文件的属性 \nchgrp group1 file1 改变文件的群组 \nchown user1:group1 file1 改变一个文件的所有人和群组属性 \nfind / -perm -u+s 罗列一个系统中所有使用了SUID控制的文件 \nchmod u+s /bin/file1 设置一个二进制文件的 SUID 位 - 运行该文件的用户也被赋予和所有者同样的权限 \nchmod u-s /bin/file1 禁用一个二进制文件的 SUID位 \nchmod g+s /home/public 设置一个目录的SGID 位 - 类似SUID ，不过这是针对目录的 \nchmod g-s /home/public 禁用一个目录的 SGID 位 \nchmod o+t /home/public 设置一个文件的 STIKY 位 - 只允许合法所有人删除文件 \nchmod o-t /home/public 禁用一个目录的 STIKY 位 </code></pre>\n\n<p><br />\n<strong>文件的特殊属性 - 使用 &quot;+&quot; 设置权限，使用 &quot;-&quot; 用于取消&nbsp;</strong></p>\n\n<pre>\n<code>chattr +a file1 只允许以追加方式读写文件 \nchattr +c file1 允许这个文件能被内核自动压缩/解压 \nchattr +d file1 在进行文件系统备份时，dump程序将忽略这个文件 \nchattr +i file1 设置成不可变的文件，不能被删除、修改、重命名或者链接 \nchattr +s file1 允许一个文件被安全地删除 \nchattr +S file1 一旦应用程序对这个文件执行了写操作，使系统立刻把修改的结果写到磁盘 \nchattr +u file1 若文件被删除，系统会允许你在以后恢复这个被删除的文件 \nlsattr 显示特殊的属性 </code></pre>\n\n<p><br />\n<strong>打包和压缩文件&nbsp;</strong></p>\n\n<pre>\n<code>bunzip2 file1.bz2 解压一个叫做 \'file1.bz2\'的文件 \nbzip2 file1 压缩一个叫做 \'file1\' 的文件 \ngunzip file1.gz 解压一个叫做 \'file1.gz\'的文件 \ngzip file1 压缩一个叫做 \'file1\'的文件 \ngzip -9 file1 最大程度压缩 \nrar a file1.rar test_file 创建一个叫做 \'file1.rar\' 的包 \nrar a file1.rar file1 file2 dir1 同时压缩 \'file1\', \'file2\' 以及目录 \'dir1\' \nrar x file1.rar 解压rar包 \nunrar x file1.rar 解压rar包 \ntar -cvf archive.tar file1 创建一个非压缩的 tarball \ntar -cvf archive.tar file1 file2 dir1 创建一个包含了 \'file1\', \'file2\' 以及 \'dir1\'的档案文件 \ntar -tf archive.tar 显示一个包中的内容 \ntar -xvf archive.tar 释放一个包 \ntar -xvf archive.tar -C /tmp 将压缩包释放到 /tmp目录下 \ntar -cvfj archive.tar.bz2 dir1 创建一个bzip2格式的压缩包 \ntar -xvfj archive.tar.bz2 解压一个bzip2格式的压缩包 \ntar -cvfz archive.tar.gz dir1 创建一个gzip格式的压缩包 \ntar -xvfz archive.tar.gz 解压一个gzip格式的压缩包 \nzip file1.zip file1 创建一个zip格式的压缩包 \nzip -r file1.zip file1 file2 dir1 将几个文件和目录同时压缩成一个zip格式的压缩包 \nunzip file1.zip 解压一个zip格式压缩包 </code></pre>\n\n<p><br />\n<strong>RPM 包 - （Fedora, Redhat及类似系统）</strong>&nbsp;</p>\n\n<pre>\n<code>rpm -ivh package.rpm 安装一个rpm包 \nrpm -ivh --nodeeps package.rpm 安装一个rpm包而忽略依赖关系警告 \nrpm -U package.rpm 更新一个rpm包但不改变其配置文件 \nrpm -F package.rpm 更新一个确定已经安装的rpm包 \nrpm -e package_name.rpm 删除一个rpm包 \nrpm -qa 显示系统中所有已经安装的rpm包 \nrpm -qa | grep httpd 显示所有名称中包含 \"httpd\" 字样的rpm包 \nrpm -qi package_name 获取一个已安装包的特殊信息 \nrpm -qg \"System Environment/Daemons\" 显示一个组件的rpm包 \nrpm -ql package_name 显示一个已经安装的rpm包提供的文件列表 \nrpm -qc package_name 显示一个已经安装的rpm包提供的配置文件列表 \nrpm -q package_name --whatrequires 显示与一个rpm包存在依赖关系的列表 \nrpm -q package_name --whatprovides 显示一个rpm包所占的体积 \nrpm -q package_name --scripts 显示在安装/删除期间所执行的脚本l \nrpm -q package_name --changelog 显示一个rpm包的修改历史 \nrpm -qf /etc/httpd/conf/httpd.conf 确认所给的文件由哪个rpm包所提供 \nrpm -qp package.rpm -l 显示由一个尚未安装的rpm包提供的文件列表 \nrpm --import /media/cdrom/RPM-GPG-KEY 导入公钥数字证书 \nrpm --checksig package.rpm 确认一个rpm包的完整性 \nrpm -qa gpg-pubkey 确认已安装的所有rpm包的完整性 \nrpm -V package_name 检查文件尺寸、 许可、类型、所有者、群组、MD5检查以及最后修改时间 \nrpm -Va 检查系统中所有已安装的rpm包- 小心使用 \nrpm -Vp package.rpm 确认一个rpm包还未安装 \nrpm2cpio package.rpm | cpio --extract --make-directories *bin* 从一个rpm包运行可执行文件 \nrpm -ivh /usr/src/redhat/RPMS/`arch`/package.rpm 从一个rpm源码安装一个构建好的包 \nrpmbuild --rebuild package_name.src.rpm 从一个rpm源码构建一个 rpm 包 </code></pre>\n\n<p><br />\n<strong>YUM 软件包升级器 - （Fedora, RedHat及类似系统）&nbsp;</strong></p>\n\n<pre>\n<code>yum install package_name 下载并安装一个rpm包 \nyum localinstall package_name.rpm 将安装一个rpm包，使用你自己的软件仓库为你解决所有依赖关系 \nyum update package_name.rpm 更新当前系统中所有安装的rpm包 \nyum update package_name 更新一个rpm包 \nyum remove package_name 删除一个rpm包 \nyum list 列出当前系统中安装的所有包 \nyum search package_name 在rpm仓库中搜寻软件包 \nyum clean packages 清理rpm缓存删除下载的包 \nyum clean headers 删除所有头文件 \nyum clean all 删除所有缓存的包和头文件 </code></pre>\n\n<p><br />\n<strong>DEB 包 (Debian, Ubuntu 以及类似系统)&nbsp;</strong></p>\n\n<pre>\n<code>dpkg -i package.deb 安装/更新一个 deb 包 \ndpkg -r package_name 从系统删除一个 deb 包 \ndpkg -l 显示系统中所有已经安装的 deb 包 \ndpkg -l | grep httpd 显示所有名称中包含 \"httpd\" 字样的deb包 \ndpkg -s package_name 获得已经安装在系统中一个特殊包的信息 \ndpkg -L package_name 显示系统中已经安装的一个deb包所提供的文件列表 \ndpkg --contents package.deb 显示尚未安装的一个包所提供的文件列表 \ndpkg -S /bin/ping 确认所给的文件由哪个deb包提供 </code></pre>\n\n<p><br />\n<strong>APT 软件工具 (Debian, Ubuntu 以及类似系统)&nbsp;</strong></p>\n\n<pre>\n<code>apt-get install package_name 安装/更新一个 deb 包 \napt-cdrom install package_name 从光盘安装/更新一个 deb 包 \napt-get update 升级列表中的软件包 \napt-get upgrade 升级所有已安装的软件 \napt-get remove package_name 从系统删除一个deb包 \napt-get check 确认依赖的软件仓库正确 \napt-get clean 从下载的软件包中清理缓存 \napt-cache search searched-package 返回包含所要搜索字符串的软件包名称 </code></pre>\n\n<p><br />\n<strong>查看文件内容</strong>&nbsp;</p>\n\n<pre>\n<code>cat file1 从第一个字节开始正向查看文件的内容 \ntac file1 从最后一行开始反向查看一个文件的内容 \nmore file1 查看一个长文件的内容 \nless file1 类似于 \'more\' 命令，但是它允许在文件中和正向操作一样的反向操作 \nhead -2 file1 查看一个文件的前两行 \ntail -2 file1 查看一个文件的最后两行 \ntail -f /var/log/messages 实时查看被添加到一个文件中的内容 </code></pre>\n\n<p><br />\n<strong>文本处理&nbsp;</strong></p>\n\n<pre>\n<code>cat file1 file2 ... | command &lt;&gt; file1_in.txt_or_file1_out.txt general syntax for text manipulation using PIPE, STDIN and STDOUT \ncat file1 | command( sed, grep, awk, grep, etc...) &gt; result.txt 合并一个文件的详细说明文本，并将简介写入一个新文件中 \ncat file1 | command( sed, grep, awk, grep, etc...) &gt;&gt; result.txt 合并一个文件的详细说明文本，并将简介写入一个已有的文件中 \ngrep Aug /var/log/messages 在文件 \'/var/log/messages\'中查找关键词\"Aug\" \ngrep ^Aug /var/log/messages 在文件 \'/var/log/messages\'中查找以\"Aug\"开始的词汇 \ngrep [0-9] /var/log/messages 选择 \'/var/log/messages\' 文件中所有包含数字的行 \ngrep Aug -R /var/log/* 在目录 \'/var/log\' 及随后的目录中搜索字符串\"Aug\" \nsed \'s/stringa1/stringa2/g\' example.txt 将example.txt文件中的 \"string1\" 替换成 \"string2\" \nsed \'/^$/d\' example.txt 从example.txt文件中删除所有空白行 \nsed \'/ *#/d; /^$/d\' example.txt 从example.txt文件中删除所有注释和空白行 \necho \'esempio\' | tr \'[:lower:]\' \'[:upper:]\' 合并上下单元格内容 \nsed -e \'1d\' result.txt 从文件example.txt 中排除第一行 \nsed -n \'/stringa1/p\' 查看只包含词汇 \"string1\"的行 \nsed -e \'s/ *$//\' example.txt 删除每一行最后的空白字符 \nsed -e \'s/stringa1//g\' example.txt 从文档中只删除词汇 \"string1\" 并保留剩余全部 \nsed -n \'1,5p;5q\' example.txt 查看从第一行到第5行内容 \nsed -n \'5p;5q\' example.txt 查看第5行 \nsed -e \'s/00*/0/g\' example.txt 用单个零替换多个零 \ncat -n file1 标示文件的行数 \ncat example.txt | awk \'NR%2==1\' 删除example.txt文件中的所有偶数行 \necho a b c | awk \'{print $1}\' 查看一行第一栏 \necho a b c | awk \'{print $1,$3}\' 查看一行的第一和第三栏 \npaste file1 file2 合并两个文件或两栏的内容 \npaste -d \'+\' file1 file2 合并两个文件或两栏的内容，中间用\"+\"区分 \nsort file1 file2 排序两个文件的内容 \nsort file1 file2 | uniq 取出两个文件的并集(重复的行只保留一份) \nsort file1 file2 | uniq -u 删除交集，留下其他的行 \nsort file1 file2 | uniq -d 取出两个文件的交集(只留下同时存在于两个文件中的文件) \ncomm -1 file1 file2 比较两个文件的内容只删除 \'file1\' 所包含的内容 \ncomm -2 file1 file2 比较两个文件的内容只删除 \'file2\' 所包含的内容 \ncomm -3 file1 file2 比较两个文件的内容只删除两个文件共有的部分 </code></pre>\n\n<p><br />\n<strong>字符设置和文件格式转换&nbsp;</strong></p>\n\n<pre>\n<code>dos2unix filedos.txt fileunix.txt 将一个文本文件的格式从MSDOS转换成UNIX \nunix2dos fileunix.txt filedos.txt 将一个文本文件的格式从UNIX转换成MSDOS \nrecode ..HTML &lt; page.txt &gt; page.html 将一个文本文件转换成html \nrecode -l | more 显示所有允许的转换格式 </code></pre>\n\n<p><br />\n<strong>文件系统分析&nbsp;</strong></p>\n\n<pre>\n<code>badblocks -v /dev/hda1 检查磁盘hda1上的坏磁块 \nfsck /dev/hda1 修复/检查hda1磁盘上linux文件系统的完整性 \nfsck.ext2 /dev/hda1 修复/检查hda1磁盘上ext2文件系统的完整性 \ne2fsck /dev/hda1 修复/检查hda1磁盘上ext2文件系统的完整性 \ne2fsck -j /dev/hda1 修复/检查hda1磁盘上ext3文件系统的完整性 \nfsck.ext3 /dev/hda1 修复/检查hda1磁盘上ext3文件系统的完整性 \nfsck.vfat /dev/hda1 修复/检查hda1磁盘上fat文件系统的完整性 \nfsck.msdos /dev/hda1 修复/检查hda1磁盘上dos文件系统的完整性 \ndosfsck /dev/hda1 修复/检查hda1磁盘上dos文件系统的完整性 </code></pre>\n\n<p><br />\n<strong>初始化一个文件系统</strong>&nbsp;</p>\n\n<pre>\n<code>mkfs /dev/hda1 在hda1分区创建一个文件系统 \nmke2fs /dev/hda1 在hda1分区创建一个linux ext2的文件系统 \nmke2fs -j /dev/hda1 在hda1分区创建一个linux ext3(日志型)的文件系统 \nmkfs -t vfat 32 -F /dev/hda1 创建一个 FAT32 文件系统 \nfdformat -n /dev/fd0 格式化一个软盘 \nmkswap /dev/hda3 创建一个swap文件系统</code></pre>\n\n<p><br />\n<strong>SWAP文件系统</strong>&nbsp;</p>\n\n<pre>\n<code>mkswap /dev/hda3 创建一个swap文件系统 \nswapon /dev/hda3 启用一个新的swap文件系统 \nswapon /dev/hda2 /dev/hdb3 启用两个swap分区 </code></pre>\n\n<p><br />\n<strong>备份</strong>&nbsp;</p>\n\n<pre>\n<code>dump -0aj -f /tmp/home0.bak /home 制作一个 \'/home\' 目录的完整备份 \ndump -1aj -f /tmp/home0.bak /home 制作一个 \'/home\' 目录的交互式备份 \nrestore -if /tmp/home0.bak 还原一个交互式备份 \nrsync -rogpav --delete /home /tmp 同步两边的目录 \nrsync -rogpav -e ssh --delete /home ip_address:/tmp 通过SSH通道rsync \nrsync -az -e ssh --delete ip_addr:/home/public /home/local 通过ssh和压缩将一个远程目录同步到本地目录 \nrsync -az -e ssh --delete /home/local ip_addr:/home/public 通过ssh和压缩将本地目录同步到远程目录 \ndd bs=1M if=/dev/hda | gzip | ssh user@ip_addr \'dd of=hda.gz\' 通过ssh在远程主机上执行一次备份本地磁盘的操作 \ndd if=/dev/sda of=/tmp/file1 备份磁盘内容到一个文件 \ntar -Puf backup.tar /home/user 执行一次对 \'/home/user\' 目录的交互式备份操作 \n( cd /tmp/local/ &amp;&amp; tar c . ) | ssh -C user@ip_addr \'cd /home/share/ &amp;&amp; tar x -p\' 通过ssh在远程目录中复制一个目录内容 \n( tar c /home ) | ssh -C user@ip_addr \'cd /home/backup-home &amp;&amp; tar x -p\' 通过ssh在远程目录中复制一个本地目录 \ntar cf - . | (cd /tmp/backup ; tar xf - ) 本地将一个目录复制到另一个地方，保留原有权限及链接 \nfind /home/user1 -name \'*.txt\' | xargs cp -av --target-directory=/home/backup/ --parents 从一个目录查找并复制所有以 \'.txt\' 结尾的文件到另一个目录 \nfind /var/log -name \'*.log\' | tar cv --files-from=- | bzip2 &gt; log.tar.bz2 查找所有以 \'.log\' 结尾的文件并做成一个bzip包 \ndd if=/dev/hda of=/dev/fd0 bs=512 count=1 做一个将 MBR (Master Boot Record)内容复制到软盘的动作 \ndd if=/dev/fd0 of=/dev/hda bs=512 count=1 从已经保存到软盘的备份中恢复MBR内容 </code></pre>\n\n<p><br />\n<strong>光盘&nbsp;</strong></p>\n\n<pre>\n<code>cdrecord -v gracetime=2 dev=/dev/cdrom -eject blank=fast -force 清空一个可复写的光盘内容 \nmkisofs /dev/cdrom &gt; cd.iso 在磁盘上创建一个光盘的iso镜像文件 \nmkisofs /dev/cdrom | gzip &gt; cd_iso.gz 在磁盘上创建一个压缩了的光盘iso镜像文件 \nmkisofs -J -allow-leading-dots -R -V \"Label CD\" -iso-level 4 -o ./cd.iso data_cd 创建一个目录的iso镜像文件 \ncdrecord -v dev=/dev/cdrom cd.iso 刻录一个ISO镜像文件 \ngzip -dc cd_iso.gz | cdrecord dev=/dev/cdrom - 刻录一个压缩了的ISO镜像文件 \nmount -o loop cd.iso /mnt/iso 挂载一个ISO镜像文件 \ncd-paranoia -B 从一个CD光盘转录音轨到 wav 文件中 \ncd-paranoia -- \"-3\" 从一个CD光盘转录音轨到 wav 文件中（参数-3） \ncdrecord --scanbus 扫描总线以识别scsi通道 \ndd if=/dev/hdc | md5sum 校验一个设备的md5sum编码，例如一张 CD </code></pre>\n\n<p><br />\n<strong>网络 - （以太网和WIFI无线</strong>）&nbsp;</p>\n\n<pre>\n<code>ifconfig eth0 显示一个以太网卡的配置 \nifup eth0 启用一个 \'eth0\' 网络设备 \nifdown eth0 禁用一个 \'eth0\' 网络设备 \nifconfig eth0 192.168.1.1 netmask 255.255.255.0 控制IP地址 \nifconfig eth0 promisc 设置 \'eth0\' 成混杂模式以嗅探数据包 (sniffing) \ndhclient eth0 以dhcp模式启用 \'eth0\' \nroute -n show routing table \nroute add -net 0/0 gw IP_Gateway configura default gateway \nroute add -net 192.168.0.0 netmask 255.255.0.0 gw 192.168.1.1 configure static route to reach network \'192.168.0.0/16\' \nroute del 0/0 gw IP_gateway remove static route \necho \"1\" &gt; /proc/sys/net/ipv4/ip_forward activate ip routing \nhostname show hostname of system \nhost www.example.com lookup hostname to resolve name to ip address and viceversa(1) \nnslookup www.example.com lookup hostname to resolve name to ip address and viceversa(2) \nip link show show link status of all interfaces \nmii-tool eth0 show link status of \'eth0\' \nethtool eth0 show statistics of network card \'eth0\' \nnetstat -tup show all active network connections and their PID \nnetstat -tupl show all network services listening on the system and their PID \ntcpdump tcp port 80 show all HTTP traffic \niwlist scan show wireless networks \niwconfig eth1 show configuration of a wireless network card \nhostname show hostname \nhost www.example.com lookup hostname to resolve name to ip address and viceversa \nnslookup www.example.com lookup hostname to resolve name to ip address and viceversa \nwhois www.example.com lookup on Whois database </code></pre>\n\n<p>&nbsp;</p>\n\n<p><strong>JPS工具</strong></p>\n\n<pre>\n<code>jps(Java Virtual Machine Process Status Tool)是JDK 1.5提供的一个显示当前所有java进程pid的命令，简单实用，非常适合在linux/unix平台上简单察看当前java进程的一些简单情况。\n\n    我想很多人都是用过unix系统里的ps命令，这个命令主要是用来显示当前系统的进程情况，有哪些进程，及其 id。 jps 也是一样，它的作用是显示当前系统的java进程情况，及其id号。我们可以通过它来查看我们到底启动了几个java进程（因为每一个java程序都会独占一个java虚拟机实例），和他们的进程号（为下面几个程序做准备），并可通过opt来查看这些进程的详细启动参数。\n\n     使用方法：在当前命令行下打 jps(需要JAVA_HOME，没有的话，到改程序的目录下打) 。\n\njps存放在JAVA_HOME/bin/jps，使用时为了方便请将JAVA_HOME/bin/加入到Path.\n\n$&gt; jps\n23991 Jps\n23789 BossMain\n23651 Resin\n\n\n比较常用的参数：\n\n-q 只显示pid，不显示class名称,jar文件名和传递给main 方法的参数\n$&gt;  jps -q\n28680\n23789\n23651\n\n-m 输出传递给main 方法的参数，在嵌入式jvm上可能是null\n\n$&gt; jps -m\n28715 Jps -m\n23789 BossMain\n23651 Resin -socketwait 32768 -stdout /data/aoxj/resin/log/stdout.log -stderr /data/aoxj/resin/log/stderr.log\n\n-l 输出应用程序main class的完整package名 或者 应用程序的jar文件完整路径名\n\n$&gt; jps -l\n28729 sun.tools.jps.Jps\n23789 com.asiainfo.aimc.bossbi.BossMain\n23651 com.caucho.server.resin.Resin\n\n-v 输出传递给JVM的参数\n\n$&gt; jps -v\n23789 BossMain\n28802 Jps -Denv.class.path=/data/aoxj/bossbi/twsecurity/java/trustwork140.jar:/data/aoxj/bossbi/twsecurity/java/:/data/aoxj/bossbi/twsecurity/java/twcmcc.jar:/data/aoxj/jdk15/lib/rt.jar:/data/aoxj/jd\n\nk15/lib/tools.jar -Dapplication.home=/data/aoxj/jdk15 -Xms8m\n23651 Resin -Xss1m -Dresin.home=/data/aoxj/resin -Dserver.root=/data/aoxj/resin -Djava.util.logging.manager=com.caucho.log.LogManagerImpl -\n\nDjavax.management.builder.initial=com.caucho.jmx.MBeanServerBuilderImpl\n\nsudo jps看到的进程数量最全\n\njps 192.168.0.77\n\n列出远程服务器192.168.0.77机器所有的jvm实例，采用rmi协议，默认连接端口为1099\n\n（前提是远程服务器提供jstatd服务）\n\n注：jps命令有个地方很不好，似乎只能显示当前用户的java进程，要显示其他用户的还是只能用unix/linux的ps命令。</code></pre>\n\n<p>&nbsp;</p>\n\n<p>详细情况请参考sun官方文档。<br />\n<a href=\"http://java.sun.com/j2se/1.5.0/docs/tooldocs/share/jps.html\" target=\"_blank\">http://java.sun.com/j2se/1.7.0/docs/tooldocs/share/jps.html</a></p>\n', 1, '5f199b8885e24fc8b28672b872edb606', 5, '2018-05-24 10:34:39', '2018-09-17 08:01:53');
INSERT INTO `logcontent` VALUES ('ecd85f0d169940e49c02f53389a76acb', 1, '4eca926aa543420baea2f03f506d042e', '高性能Mysql笔记一', '高性能Mysql,Mysql逻辑架构', '高性能Mysql（第三版）--Mysql逻辑架构', '<h2><a id=\"Mysql逻辑架构\" name=\"Mysql逻辑架构\"></a>Mysql逻辑架构</h2>\n\n<h2><img alt=\"\" src=\"http://localhost:8080/myLog/images/mysql/20170801185146233.png\" style=\"height:276px; width:400px\" /></h2>', 1, '5f199b8885e24fc8b28672b872edb606', 27, '2018-08-30 09:25:14', '2018-09-21 20:08:39');
INSERT INTO `logcontent` VALUES ('ef29dbee3f9c46a3a1f5219274fe78df', -1, 'f29612168904487aadb4043df110361c', 'Nginx安装', 'nginx', 'Nginx安装记录', '<p><a id=\"安装Nginx\" name=\"安装Nginx\"></a>一、安装Nginx</p>\n\n<pre>\n<code>Nginx依赖包\nyum install gcc\nyum install pcre*\nyum install zlib zlib-devel\nyum install openssl openssl-devel\n下载Nginx使用wget命令\nyum install wget\n下载Nginx安装包\nwget -c http://nginx.org/download/nginx-1.13.9.tar.gz\n解压\ntar zvxf nginx-1.13.9.tar.gz\n进入nginx-1.13.9进行配置\ncd nginx-1.13.9\n./configure --prefix=/usr/common/nginx/ --with-http_stub_status_module\n编译安装\nmake\nmake install\n</code></pre>\n\n<p><a id=\"Nginx安装配置选项\" name=\"Nginx安装配置选项\"></a>二、Nginx安装配置选项（编译参数可能会根据版本的不同进行变化）<br />\n./configure --help查看编译参数列表，常见的选项如下：</p>\n\n<pre>\n<code class=\"language-nginx\">--prefix=&lt;path&gt; - 安装路径，如果没有指定，默认为/usr/local/nginx。\n--sbin-path=&lt;path&gt; - nginx可执行命令的文件，如果没有指定，默认为&lt;prefix&gt;/sbin/nginx。\n--conf-path=&lt;path&gt; - 在没有使用-c参数指定的情况下nginx.conf的默认位置，如果没有指定，默认为&lt;prefix&gt;/conf/nginx.conf。\n--pid-path=&lt;path&gt; - nginx.pid的路径，如果没有在nginx.conf中通过“pid”指令指定，默认为&lt;prefix&gt;/logs/nginx.pid。\n--lock-path=&lt;path&gt; - nginx.lock文件路径，如果没有指定，默认为&lt;prefix&gt;/logs/nginx.lock。\n--error-log-path=&lt;path&gt; - 当没有在nginx.conf中使用“error_log”指令指定时的错误日志位置，如果没有指定，默认为&lt;prefix&gt;/logs/error.log。\n--http-log-path=&lt;path&gt; - 当没有在nginx.conf中使用“access_log”指令指定时的访问日志位置，如果没有指定，默认为&lt;prefix&gt;/logs/access.log。\n--user=&lt;user&gt; - 当没有在nginx.conf中使用“user”指令指定时nginx运行的用户，如果没有指定，默认为“nobody”。\n--group=&lt;group&gt; - 当没有在nginx.conf中使用“user”指令指定时nginx运行的组，如果没有指定，默认为“nobody”。\n--builddir=DIR - 设置构建目录。\n--with-rtsig_module - 启用rtsig模块。\n--with-select_module –without-select_module - 如果在configure的时候没有发现kqueue, epoll,rtsig或/dev/poll其中之一，select模块始终为启用状态。\n--with-poll_module –without-poll_module - 如果在configure的时候没有发现kqueue, epoll,rtsig或/dev/poll其中之一，poll模块始终为启用状态。\n--with-http_ssl_module - 启用ngx_http_ssl_module，启用SSL支持并且能够处理HTTPS请求。需要OpenSSL，在Debian系统中，对应的包为libssl-dev。\n--with-http_realip_module - 启用ngx_http_realip_module\n--with-http_addition_module - 启用ngx_http_addition_module\n--with-http_sub_module - 启用ngx_http_sub_module\n--with-http_dav_module - 启用ngx_http_dav_module\n--with-http_flv_module - 启用ngx_http_flv_module\n--with-http_stub_status_module - 启用”server status”（服务状态）页\n--without-http_charset_module - 禁用ngx_http_charset_module\n--without-http_gzip_module - 禁用ngx_http_gzip_module，如果启用，需要zlib包。\n--without-http_ssi_module - 禁用ngx_http_ssi_module\n--without-http_userid_module - 禁用ngx_http_userid_module\n--without-http_access_module - 禁用ngx_http_access_module\n--without-http_auth_basic_module - 禁用ngx_http_auth_basic_module\n--without-http_autoindex_module - 禁用ngx_http_autoindex_module\n--without-http_geo_module - 禁用ngx_http_geo_module\n--without-http_map_module - 禁用ngx_http_map_module\n--without-http_referer_module - 禁用ngx_http_referer_module\n--without-http_rewrite_module - 禁用ngx_http_rewrite_module。如果启用，需要PCRE包。\n--without-http_proxy_module - 禁用ngx_http_proxy_module\n--without-http_fastcgi_module - 禁用ngx_http_fastcgi_module\n--without-http_memcached_module - 禁用ngx_http_memcached_module\n--without-http_limit_zone_module - 禁用ngx_http_limit_zone_module\n--without-http_empty_gif_module - 禁用ngx_http_empty_gif_module\n--without-http_browser_module - 禁用ngx_http_browser_module\n--without-http_upstream_ip_hash_module - 禁用ngx_http_upstream_ip_hash_module\n--with-http_perl_module - 启用ngx_http_perl_module\n--with-perl_modules_path=PATH - 为perl模块设置路径\n--with-perl=PATH - 为perl库设置路径\n--http-client-body-temp-path=PATH - 为http连接的请求实体临时文件设置路径，如果没有指定，默认为&lt;prefix&gt;/client_body_temp\n--http-proxy-temp-path=PATH - 为http代理临时文件设置路径，如果没有指定，默认为&lt;prefix&gt;/proxy_temp\n--http-fastcgi-temp-path=PATH - 为http fastcgi临时文件设置路径，如果没有指定，默认为&lt;prefix&gt;/fastcgi_temp\n--without-http - 禁用HTTP服务\n--with-mail - 启用IMAP4/POP3/SMTP代理模块\n--with-mail_ssl_module - 启用ngx_mail_ssl_module\n--with-cc=PATH - 设置C编译器路径\n--with-cpp=PATH - 设置C预处理器路径\n--with-cc-opt=OPTIONS - 变量CFLAGS中附加的参数，用于FreeBSD中的PCRE库，同样需要指定–withcc-opt=”-I /usr/local/include”，如果我们使用select()函数则需要同时增加文件描述符数量，可以通过–with-cc-opt=”-D FD_SETSIZE=2048”指定。\n--with-ld-opt=OPTIONS - 通过连接器的附加参数，用于FreeBSD中的PCRE库，同样需要指定–withld-opt=”-L /usr/local/lib”。\n--with-cpu-opt=CPU - 指定编译的CPU，可用的值为: pentium, pentiumpro, pentium3, pentium4,athlon, opteron, amd64, sparc32, sparc64, ppc64\n--without-pcre - 禁用PCRE库文件，同时将禁用HTTP rewrite 模块，如果要在”location”指令中使用正则表达式，同样需要PCRE库。\n--with-pcre=DIR - 设置PCRE库源文件路径。\n--with-pcre-opt=OPTIONS - 在编译时为PCRE设置附加参数。\n--with-md5=DIR - 设置md5库源文件路径。\n--with-md5-opt=OPTIONS - 在编译时为md5设置附加参数。\n--with-md5-asm - 使用md5汇编源。\n--with-sha1=DIR - 设置sha1库源文件路径。\n--with-sha1-opt=OPTIONS - 在编译时为sha1设置附加参数。\n--with-sha1-asm - 使用sha1汇编源。\n--with-zlib=DIR - 设置zlib库源文件路径。\n--with-zlib-opt=OPTIONS - 在编译时为zlib设置附加参数。\n--with-zlib-asm=CPU - 为指定的CPU使用zlib汇编源进行优化，可用值为: pentium, pentiumpro。\n--with-openssl=DIR - 设置openssl库源文件路径。\n--with-openssl-opt=OPTIONS - 在编译时为openssl设置附加参数。\n--with-debug - 启用debug记录。\n--add-module=PATH - 增加一个在PATH中的第三方模块。</code></pre>\n\n<p><a id=\"测试配置文件\" name=\"测试配置文件\"></a>三、测试配置文件：<br />\n安装路径下的/nginx/sbin/nginx -t<br />\n<a id=\"Nginx启动\" name=\"Nginx启动\"></a>Nginx启动：<br />\n安装路径下的/nginx/sbin/nginx<br />\n<a id=\"Nginx停止\" name=\"Nginx停止\"></a>Nginx停止：<br />\n安装路径下的/nginx/sbin/nginx -s stop<br />\n或者是：nginx -s quit<br />\n<a id=\"Nginx重启\" name=\"Nginx重启\"></a>Nginx重启：<br />\n安装路径下的/nginx/sbin/nginx -s reload</p>\n', 1, '5f199b8885e24fc8b28672b872edb606', 4, '2018-03-26 17:49:13', '2018-09-07 18:19:15');
INSERT INTO `logcontent` VALUES ('f14a0848811642d29bc7ec29c734e2ca', -1, '7338e53acd514defa1a17e47016f3f4a', 'Elasticsearch（六）', 'shard&replica机制再次梳理,单node环境下创建index,2个node环境下replica shard分配', 'shard&replica机制再次梳理/单node环境下创建index/2个node环境下创建index', '<p><a name=\"shard&amp;replica机制再次梳理\"></a>1、shard&amp;replica机制再次梳理</p>\n\n<p>（1）index包含多个shard</p>\n\n<p>（2）每个shard都是一个最小工作单元，承载部分数据，每个shard都是一个lucene实例，具备完整的建立索引和处理请求的能力</p>\n\n<p>（3）增减节点时，shard会自动在nodes中负载均衡</p>\n\n<p>（4）primary shard和replica shard，每个document肯定只存在于某一个primary shard以及其对应的replica shard中，不可能存在多个primary shard</p>\n\n<p>（5）replica shard是primary shard的副本，负责容错，以及承担读请求负载</p>\n\n<p>（6）primary shard的数量在创建索引的时候就固定了，replica shard的数量可以随时修改</p>\n\n<p>（7）primary shard的默认数量是5，replica默认是1，默认有10个shard，5个primary shard，5个replica shard</p>\n\n<p>（8）primary shard不能和自己的replica shard放在同一个节点上（否则节点宕机，primary shard和副本都丢失，起不到容错的作用），但是可以和其他primary shard的replica shard放在同一个节点上</p>\n\n<p><a name=\"单node环境下创建index\"></a>2、单node环境下创建index是什么样子的</p>\n\n<p>（1）单node环境下，创建一个index，有3个primary shard，3个replica shard</p>\n\n<p>（2）集群status是yellow</p>\n\n<p>（3）这个时候，只会将3个primary shard分配到仅有的一个node上去，另外3个replica shard是无法分配的</p>\n\n<p>（4）集群可以正常工作，但是一旦出现节点宕机，数据将全部丢失，集群将不可用，无法承接任何请求</p>\n\n<pre>\n<code class=\"language-json\">PUT /test_index\n{\n    \"settings\":{\n        \"number_of_shards\":3,\n        \"number_of_replicas\":1\n    }\n}</code></pre>\n\n<p><a name=\"2个node环境下replica shard分配\"></a>3、2个node环境下replica shard是如何分配的</p>\n\n<p>（1）replica shard分配：3个primary shard，3个replica shard，1 node</p>\n\n<p>（2）primary ---&gt; replica同步</p>\n\n<p>（3）读请求：primary/replica</p>\n', 2, '5f199b8885e24fc8b28672b872edb606', 6, '2017-07-12 18:04:10', '2018-09-06 16:12:58');
INSERT INTO `logcontent` VALUES ('f41411e085fc4199bce8ad5fb111a950', -1, '7338e53acd514defa1a17e47016f3f4a', '一、集群内的原理', '集群内的原理', '集群内的原理', '<h2>集群内的原理</h2>\n\n<p>ElasticSearch 的主旨是随时可用和按需扩容。 而扩容可以通过购买性能更强大（&nbsp;<em>垂直扩容</em>&nbsp;，或&nbsp;<em>纵向扩容</em>） 或者数量更多的服务器（&nbsp;<em>水平扩容</em>&nbsp;，或&nbsp;<em>横向扩容</em>&nbsp;）来实现。</p>\n\n<p>虽然 Elasticsearch 可以获益于更强大的硬件设备，但是垂直扩容是有极限的。 真正的扩容能力是来自于水平扩容--为集群添加更多的节点，并且将负载压力和稳定性分散到这些节点中。</p>\n\n<p>对于大多数的数据库而言，通常需要对应用程序进行非常大的改动，才能利用上横向扩容的新增资源。 与之相反的是，ElastiSearch天生就是&nbsp;<em>分布式的</em>&nbsp;，它知道如何通过管理多节点来提高扩容性和可用性。 这也意味着你的应用无需关注这个问题。</p>\n\n<p>本章将讲述如何按需配置集群、节点和分片，并在硬件故障时确保数据安全。</p>\n\n<h2>空集群</h2>\n\n<p>如果我们启动了一个单独的节点，里面不包含任何的数据和&nbsp;索引，如图&nbsp;1 &ldquo;包含空内容节点的集群&rdquo;。</p>\n\n<p><strong>图&nbsp;1.&nbsp;包含空内容节点的集群</strong></p>\n\n<p><img alt=\"包含空内容节点的集群\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/elas_0201.png\" /></p>\n\n<p>一个运行中的 Elasticsearch 实例称为一个&nbsp;节点，而集群是由一个或者多个拥有相同&nbsp;<code>cluster.name</code>&nbsp;配置的节点组成， 它们共同承担数据和负载的压力。当有节点加入集群中或者从集群中移除节点时，集群将会重新平均分布所有的数据。</p>\n\n<p>当一个节点被选举成为&nbsp;<em>主</em>&nbsp;节点时， 它将负责管理集群范围内的所有变更，例如增加、删除索引，或者增加、删除节点等。 而主节点并不需要涉及到文档级别的变更和搜索等操作，所以当集群只拥有一个主节点的情况下，即使流量的增加它也不会成为瓶颈。 任何节点都可以成为主节点。我们的示例集群就只有一个节点，所以它同时也成为了主节点。</p>\n\n<p>作为用户，我们可以将请求发送到&nbsp;<em>集群中的任何节点</em>&nbsp;，包括主节点。 每个节点都知道任意文档所处的位置，并且能够将我们的请求直接转发到存储我们所需文档的节点。 无论我们将请求发送到哪个节点，它都能负责从各个包含我们所需文档的节点收集回数据，并将最终结果返回給客户端。 Elasticsearch 对这一切的管理都是透明的。</p>\n\n<h2>集群健康</h2>\n\n<p>Elasticsearch 的集群监控信息中包含了许多的统计数据，其中最为重要的一项就是&nbsp;<em>集群健康</em>&nbsp;， 它在<code>status</code>&nbsp;字段中展示为&nbsp;<code>green</code>&nbsp;、&nbsp;<code>yellow</code>&nbsp;或者&nbsp;<code>red</code>&nbsp;。</p>\n\n<pre>\nGET /_cluster/health</pre>\n\n<p>在一个不包含任何索引的空集群中，它将会有一个类似于如下所示的返回内容：</p>\n\n<pre>\n{\n   &quot;cluster_name&quot;:          &quot;elasticsearch&quot;,\n   &quot;status&quot;:                &quot;green&quot;, <img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" />\n   &quot;timed_out&quot;:             false,\n   &quot;number_of_nodes&quot;:       1,\n   &quot;number_of_data_nodes&quot;:  1,\n   &quot;active_primary_shards&quot;: 0,\n   &quot;active_shards&quot;:         0,\n   &quot;relocating_shards&quot;:     0,\n   &quot;initializing_shards&quot;:   0,\n   &quot;unassigned_shards&quot;:     0\n}</pre>\n\n<table border=\"0\" summary=\"Callout list\">\n	<tbody>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/cluster-health.html#CO7-1\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p><code>status</code>&nbsp;字段是我们最关心的。</p>\n			</td>\n		</tr>\n	</tbody>\n</table>\n\n<p><code>status</code>&nbsp;字段指示着当前集群在总体上是否工作正常。它的三种颜色含义如下：</p>\n\n<p><code>green&nbsp;</code>所有的主分片和副本分片都正常运行。</p>\n\n<p><code>yellow&nbsp;</code>所有的主分片都正常运行，但不是所有的副本分片都正常运行。</p>\n\n<p><code>red&nbsp;</code>有主分片没能正常运行。</p>\n\n<p>添加索引</p>\n\n<p>我们往 Elasticsearch 添加数据时需要用到&nbsp;<em>索引</em>&nbsp;&mdash;&mdash; 保存相关数据的地方。&nbsp;索引实际上是指向一个或者多个物理&nbsp;<em>分片</em>&nbsp;的&nbsp;<em>逻辑命名空间</em>&nbsp;。</p>\n\n<p>一个&nbsp;<em>分片</em>&nbsp;是一个底层的&nbsp;<em>工作单元</em>&nbsp;，它仅保存了&nbsp;全部数据中的一部分。 一个分片是一个 Lucene 的实例，以及它本身就是一个完整的搜索引擎。 我们的文档被存储和索引到分片内，但是应用程序是直接与索引而不是与分片进行交互。</p>\n\n<p>Elasticsearch 是利用分片将数据分发到集群内各处的。分片是数据的容器，文档保存在分片内，分片又被分配到集群内的各个节点里。 当你的集群规模扩大或者缩小时， Elasticsearch 会自动的在各节点中迁移分片，使得数据仍然均匀分布在集群里。</p>\n\n<p>一个分片可以是&nbsp;<em>主</em>&nbsp;分片或者&nbsp;<em>副本</em>&nbsp;分片。&nbsp;索引内任意一个文档都归属于一个主分片，所以主分片的数目决定着索引能够保存的最大数据量。</p>\n\n<p><img alt=\"注意\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/note.png\" /></p>\n\n<p>技术上来说，一个主分片最大能够存储 Integer.MAX_VALUE - 128 个文档，但是实际最大值还需要参考你的使用场景：包括你使用的硬件， 文档的大小和复杂程度，索引和查询文档的方式以及你期望的响应时长。</p>\n\n<p>一个副本分片只是一个主分片的拷贝。&nbsp;副本分片作为硬件故障时保护数据不丢失的冗余备份，并为搜索和返回文档等读操作提供服务。</p>\n\n<p>在索引建立的时候就已经确定了主分片数，但是副本分片数可以随时修改。</p>\n\n<p>让我们在包含一个空节点的集群内创建名为&nbsp;<code>blogs</code>&nbsp;的索引。&nbsp;索引在默认情况下会被分配5个主分片，&nbsp;但是为了演示目的，我们将分配3个主分片和一份副本（每个主分片拥有一个副本分片）：</p>\n\n<pre>\nPUT /blogs\n{\n   &quot;settings&quot; : {\n      &quot;number_of_shards&quot; : 3,\n      &quot;number_of_replicas&quot; : 1\n   }\n}</pre>\n\n<p>如图&nbsp;2 &ldquo;拥有一个索引的单节点集群&rdquo;。所有3个主分片都被分配在&nbsp;<code>Node 1</code>&nbsp;。</p>\n\n<p><strong>图&nbsp;2.&nbsp;拥有一个索引的单节点集群</strong></p>\n\n<p><img alt=\"拥有一个索引的单节点集群\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/elas_0202.png\" /></p>\n\n<p>如果我们现在查看<code>集群健康</code>，&nbsp;我们将看到如下内容：</p>\n\n<pre>\n{\n  &quot;cluster_name&quot;: &quot;elasticsearch&quot;,\n  &quot;status&quot;: &quot;yellow&quot;, <img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" />\n  &quot;timed_out&quot;: false,\n  &quot;number_of_nodes&quot;: 1,\n  &quot;number_of_data_nodes&quot;: 1,\n  &quot;active_primary_shards&quot;: 3,\n  &quot;active_shards&quot;: 3,\n  &quot;relocating_shards&quot;: 0,\n  &quot;initializing_shards&quot;: 0,\n  &quot;unassigned_shards&quot;: 3, <img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/2.png\" />\n  &quot;delayed_unassigned_shards&quot;: 0,\n  &quot;number_of_pending_tasks&quot;: 0,\n  &quot;number_of_in_flight_fetch&quot;: 0,\n  &quot;task_max_waiting_in_queue_millis&quot;: 0,\n  &quot;active_shards_percent_as_number&quot;: 50\n}</pre>\n\n<table border=\"0\" summary=\"Callout list\">\n	<tbody>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/_add-an-index.html#CO8-1\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p>集群&nbsp;<code>status</code>&nbsp;值为&nbsp;<code>yellow</code>&nbsp;。</p>\n			</td>\n		</tr>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/_add-an-index.html#CO8-2\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/2.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p>没有被分配到任何节点的副本数。</p>\n			</td>\n		</tr>\n	</tbody>\n</table>\n\n<p>集群的健康状况为&nbsp;<code>yellow</code>&nbsp;则表示全部&nbsp;<em>主</em>&nbsp;分片都正常运行（集群可以正常服务所有请求），但是&nbsp;<em>副本</em>&nbsp;分片没有全部处在正常状态。 实际上，所有3个副本分片都是&nbsp;<code>unassigned</code>&nbsp;&mdash;&mdash; 它们都没有被分配到任何节点。 在同一个节点上既保存原始数据又保存副本是没有意义的，因为一旦失去了那个节点，我们也将丢失该节点上的所有副本数据。</p>\n\n<p>当前我们的集群是正常运行的，但是在硬件故障时有丢失数据的风险。</p>\n\n<h2>添加故障转移</h2>\n\n<p>当集群中只有一个节点在运行时，意味着会有一个单点故障问题&mdash;&mdash;没有冗余。 幸运的是，我们只需再启动一个节点即可防止数据丢失。</p>\n\n<p><strong>启动第二个节点</strong></p>\n\n<p>为了测试第二个节点启动后的情况，你可以在同一个目录内，完全依照启动第一个节点的方式来启动一个新节点（参考安装并运行 Elasticsearch）。多个节点可以共享同一个目录。</p>\n\n<p>当你在同一台机器上启动了第二个节点时，只要它和第一个节点有同样的&nbsp;<code>cluster.name</code>&nbsp;配置，它就会自动发现集群并加入到其中。 但是在不同机器上启动节点的时候，为了加入到同一集群，你需要配置一个可连接到的单播主机列表。</p>\n\n<p>如果启动了第二个节点，我们的集群将会如图&nbsp;3 &ldquo;拥有两个节点的集群&mdash;&mdash;所有主分片和副本分片都已被分配&rdquo;所示。</p>\n\n<p><strong>图&nbsp;3.&nbsp;拥有两个节点的集群&mdash;&mdash;所有主分片和副本分片都已被分配</strong></p>\n\n<p><img alt=\"拥有两个节点的集群\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/elas_0203.png\" /></p>\n\n<p>当第二个节点加入到集群后，3个&nbsp;<em>副本分片</em>&nbsp;将会分配到这个节点上&mdash;&mdash;每个主分片对应一个副本分片。 这意味着当集群内任何一个节点出现问题时，我们的数据都完好无损。</p>\n\n<p>所有新近被索引的文档都将会保存在主分片上，然后被并行的复制到对应的副本分片上。这就保证了我们既可以从主分片又可以从副本分片上获得文档。</p>\n\n<p><code>cluster-health</code>&nbsp;现在展示的状态为&nbsp;<code>green</code>&nbsp;，这表示所有6个分片（包括3个主分片和3个副本分片）都在正常运行。</p>\n\n<pre>\n{\n  &quot;cluster_name&quot;: &quot;elasticsearch&quot;,\n  &quot;status&quot;: &quot;green&quot;, <img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" />\n  &quot;timed_out&quot;: false,\n  &quot;number_of_nodes&quot;: 2,\n  &quot;number_of_data_nodes&quot;: 2,\n  &quot;active_primary_shards&quot;: 3,\n  &quot;active_shards&quot;: 6,\n  &quot;relocating_shards&quot;: 0,\n  &quot;initializing_shards&quot;: 0,\n  &quot;unassigned_shards&quot;: 0,\n  &quot;delayed_unassigned_shards&quot;: 0,\n  &quot;number_of_pending_tasks&quot;: 0,\n  &quot;number_of_in_flight_fetch&quot;: 0,\n  &quot;task_max_waiting_in_queue_millis&quot;: 0,\n  &quot;active_shards_percent_as_number&quot;: 100\n}</pre>\n\n<table border=\"0\" summary=\"Callout list\">\n	<tbody>\n		<tr>\n			<td style=\"vertical-align:top\">\n			<p><a href=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/_add_failover.html#CO9-1\"><img alt=\"\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/callouts/1.png\" /></a></p>\n			</td>\n			<td style=\"vertical-align:top\">\n			<p>集群&nbsp;<code>status</code>&nbsp;值为&nbsp;<code>green</code>&nbsp;。</p>\n			</td>\n		</tr>\n	</tbody>\n</table>\n\n<p>我们的集群现在不仅仅是正常运行的，并且还处于&nbsp;<em>始终可用</em>&nbsp;的状态。</p>\n\n<h2>水平扩容</h2>\n\n<p>怎样为我们的正在增长中的应用程序按需扩容呢？&nbsp;当启动了第三个节点，我们的集群将会看起来如图&nbsp;4 &ldquo;拥有三个节点的集群&mdash;&mdash;为了分散负载而对分片进行重新分配&rdquo;所示。</p>\n\n<p><strong>图&nbsp;4.&nbsp;拥有三个节点的集群&mdash;&mdash;为了分散负载而对分片进行重新分配</strong></p>\n\n<p><img alt=\"拥有三个节点的集群\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/elas_0204.png\" /></p>\n\n<p><code>Node 1</code>&nbsp;和&nbsp;<code>Node 2</code>&nbsp;上各有一个分片被迁移到了新的&nbsp;<code>Node 3</code>&nbsp;节点，现在每个节点上都拥有2个分片，而不是之前的3个。 这表示每个节点的硬件资源（CPU, RAM, I/O）将被更少的分片所共享，每个分片的性能将会得到提升。</p>\n\n<p>分片是一个功能完整的搜索引擎，它拥有使用一个节点上的所有资源的能力。 我们这个拥有6个分片（3个主分片和3个副本分片）的索引可以最大扩容到6个节点，每个节点上存在一个分片，并且每个分片拥有所在节点的全部资源。</p>\n\n<h3>更多的扩容</h3>\n\n<p>但是如果我们想要扩容超过6个节点怎么办呢？</p>\n\n<p>主分片的数目在索引创建时&nbsp;就已经确定了下来。实际上，这个数目定义了这个索引能够&nbsp;<em>存储</em>&nbsp;的最大数据量。（实际大小取决于你的数据、硬件和使用场景。） 但是，读操作&mdash;&mdash;搜索和返回数据&mdash;&mdash;可以同时被主分片&nbsp;<em>或</em>&nbsp;副本分片所处理，所以当你拥有越多的副本分片时，也将拥有越高的吞吐量。</p>\n\n<p>在运行中的集群上是可以动态调整副本分片数目的&nbsp;，我们可以按需伸缩集群。让我们把副本数从默认的&nbsp;<code>1</code>增加到&nbsp;<code>2</code>&nbsp;：</p>\n\n<pre>\nPUT /blogs/_settings\n{\n   &quot;number_of_replicas&quot; : 2\n}</pre>\n\n<p>如图&nbsp;5 &ldquo;将参数&nbsp;<code>number_of_replicas</code>&nbsp;调大到 2&rdquo;所示，&nbsp;<code>blogs</code>&nbsp;索引现在拥有9个分片：3个主分片和6个副本分片。 这意味着我们可以将集群扩容到9个节点，每个节点上一个分片。相比原来3个节点时，集群搜索性能可以提升&nbsp;<em>3</em>&nbsp;倍。</p>\n\n<p><strong>图&nbsp;5.&nbsp;将参数&nbsp;<code>number_of_replicas</code>&nbsp;调大到 2</strong></p>\n\n<p><img alt=\"拥有2份副本分片3个节点的集群\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/elas_0205.png\" /></p>\n\n<p><img alt=\"注意\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/icons/note.png\" /></p>\n\n<p>当然，如果只是在相同节点数目的集群上增加更多的副本分片并不能提高性能，因为每个分片从节点上获得的资源会变少。 你需要增加更多的硬件资源来提升吞吐量。</p>\n\n<p>但是更多的副本分片数提高了数据冗余量：按照上面的节点配置，我们可以在失去2个节点的情况下不丢失任何数据。</p>\n\n<h2>应对故障</h2>\n\n<p>我们之前说过 Elasticsearch 可以应对节点故障，接下来让我们尝试下这个功能。&nbsp;如果我们关闭第一个节点，这时集群的状态为图&nbsp;6 &ldquo;关闭了一个节点后的集群&rdquo;</p>\n\n<p><strong>图&nbsp;6.&nbsp;关闭了一个节点后的集群</strong></p>\n\n<p><img alt=\"关闭了一个节点后的集群\" src=\"https://www.elastic.co/guide/cn/elasticsearch/guide/current/images/elas_0206.png\" /></p>\n\n<p>我们关闭的节点是一个主节点。而集群必须拥有一个主节点来保证正常工作，所以发生的第一件事情就是选举一个新的主节点：&nbsp;<code>Node 2</code>&nbsp;。</p>\n\n<p>在我们关闭&nbsp;<code>Node 1</code>&nbsp;的同时也失去了主分片&nbsp;<code>1</code>&nbsp;和&nbsp;<code>2</code>&nbsp;，并且在缺失主分片的时候索引也不能正常工作。&nbsp;如果此时来检查集群的状况，我们看到的状态将会为&nbsp;<code>red</code>&nbsp;：不是所有主分片都在正常工作。</p>\n\n<p>幸运的是，在其它节点上存在着这两个主分片的完整副本， 所以新的主节点立即将这些分片在&nbsp;<code>Node 2</code>&nbsp;和<code>Node 3</code>&nbsp;上对应的副本分片提升为主分片， 此时集群的状态将会为&nbsp;<code>yellow</code>&nbsp;。 这个提升主分片的过程是瞬间发生的，如同按下一个开关一般。</p>\n\n<p>为什么我们集群状态是&nbsp;<code>yellow</code>&nbsp;而不是&nbsp;<code>green</code>&nbsp;呢？ 虽然我们拥有所有的三个主分片，但是同时设置了每个主分片需要对应2份副本分片，而此时只存在一份副本分片。 所以集群不能为&nbsp;<code>green</code>&nbsp;的状态，不过我们不必过于担心：如果我们同样关闭了&nbsp;<code>Node 2</code>&nbsp;，我们的程序&nbsp;<em>依然</em>&nbsp;可以保持在不丢任何数据的情况下运行，因为&nbsp;<code>Node 3</code>&nbsp;为每一个分片都保留着一份副本。</p>\n\n<p>如果我们重新启动&nbsp;<code>Node 1</code>&nbsp;，集群可以将缺失的副本分片再次进行分配，那么集群的状态也将如图&nbsp;5 &ldquo;将参数&nbsp;<code>number_of_replicas</code>&nbsp;调大到 2&rdquo;所示。 如果&nbsp;<code>Node 1</code>&nbsp;依然拥有着之前的分片，它将尝试去重用它们，同时仅从主分片复制发生了修改的数据文件。</p>\n', 1, '5f199b8885e24fc8b28672b872edb606', 9, '2018-06-06 18:29:17', '2018-09-06 16:14:25');
INSERT INTO `logcontent` VALUES ('f865ffa2d253488d93d271490db2075d', -1, '716b0d3ea6e04d03a9deb097be1e2cf1', '每周工作4小时笔记', '聪明的工作', '更聪明的工作而不是更辛苦的工作', '<p>更聪明的工作，而不是更辛苦的工作</p>\n\n<pre>\n<code class=\"language-makefile\">勤奋既可能是优点也可能是缺点，更多的工作时间有时并不是最佳的解决办法，重视结果而不是付出的多少；</code></pre>\n\n<p>&nbsp;</p>\n', 1, '5f199b8885e24fc8b28672b872edb606', 0, '2017-10-17 18:47:28', '2018-08-31 22:21:23');
INSERT INTO `logcontent` VALUES ('f8bb02ba31b8462488b63933818916c8', -1, '7338e53acd514defa1a17e47016f3f4a', 'Elasticsearch（一）', 'kibana安装', 'Elasticseach及Kibana的安装记录', '<p>-------------------注意Es520需要jdk8的支持，Kibana与Es版本需要对应，不同版本的Kibana窗口可能不一样-----------------------------</p>\n\n<p><a name=\"Elasticsearch5.2.0的安装\"></a>Elasticsearch5.2.0的安装</p>\n\n<p>下载地址:<a href=\"https://www.elastic.co/downloads/past-releases/elasticsearch-5-2-0\">https://www.elastic.co/downloads/past-releases</a></p>\n\n<p>选择ZIP下载</p>\n\n<p>下载解压之后为绿色免安装版本（解压即用，如果是中小型应用，数据量少，操作不是很复杂，无需修改配置直接启动就可使用）</p>\n\n<p>打开elasticsearch-5.2.0\\bin\\elasticsearch.bat</p>\n\n<p>浏览器中输入<a href=\"http://localhost:9200/?pretty\">http://localhost:9200</a></p>\n\n<pre>\n<code class=\"language-json\">{\n  \"name\" : \"WB-qUv1\",\n  \"cluster_name\" : \"elasticsearch\",\n  \"cluster_uuid\" : \"KfEocFsIQ3ur-aWG5zl7AQ\",\n  \"version\" : {\n    \"number\" : \"5.2.0\",\n    \"build_hash\" : \"24e05b9\",\n    \"build_date\" : \"2017-01-24T19:52:35.800Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"6.4.0\"\n  },\n  \"tagline\" : \"You Know, for Search\"\n}</code></pre>\n\n<p>安装成功。。。</p>\n\n<p>&nbsp;</p>\n\n<p><a name=\"Kibana5.2.0的安装\"></a>Kibana5.2.0的安装</p>\n\n<p>下载地址:<a href=\"https://www.elastic.co/downloads/past-releases/kibana-5-2-0\">https://www.elastic.co/downloads/past-releases</a></p>\n\n<p>选择WINDOWS下载</p>\n\n<p>打开kibana-5.2.0-windows-x86\\bin</p>\n\n<p>浏览器中输入<a href=\"http://localhost:5601\">http://localhost:5601</a></p>\n\n<p>进入Kibana界面，进入DEV TOOLS进入Elasticsearch命令行操作</p>\n', 2, '5f199b8885e24fc8b28672b872edb606', 11, '2017-07-11 15:50:01', '2018-09-06 16:12:58');
INSERT INTO `logcontent` VALUES ('f8f03cc878a14b2eb24c1381f3d570f0', 1, '4eca926aa543420baea2f03f506d042e', '高性能Mysql笔记八', '高性能Mysql（第三版）', '高性能Mysql（第三版）--可扩展的MySQL', '<p>可扩展性: 通过增加资源提升容量的能力</p>\n\n<h2>考虑负载</h2>\n\n<pre>\n<code>容量可以简单地认为是处理负载的能力,考虑负载可从以下几个角度\n数据量: 很多应用从不物理删除任何数据,应用所积累的数据量是可扩展的普遍挑战\n用户量: 更多的用户意味着更多的事务,更多的复杂查询\n用户活跃度\n相关数据集的大小</code></pre>\n\n<h2>规划可扩展性</h2>\n\n<pre>\n<code>估算需要承担的负载到底有多少\n大致正确地估计日程表\n应用的功能完成多少\n预期的最大负载是多少\n如果依赖系统的每个部分分担负载,某个部分失效时会发生什么</code></pre>\n\n<h2>向上扩展(垂直扩展)</h2>\n\n<pre>\n<code>性价比低，难度越来越高\n无法无限制向上扩展</code></pre>\n\n<h2>向外扩展</h2>\n\n<pre>\n<code>策略: 复制,拆分,数据分片\n按功能拆分: 常见做法,根据功能将应用部署在不同服务器,并使用专用的数据库服务器</code></pre>\n\n<ol>\n</ol>\n\n<h2>通过集群扩展</h2>\n\n<pre>\n<code>可以使用集群或数据库分布式技术根据场景适当解决一些问题\nNDB Cluster, Clustrix等技术</code></pre>\n\n<ol>\n</ol>\n\n<h2>向内扩展</h2>\n\n<pre>\n<code>对不再需要的数据进行归档和清理\n需要考虑对应用的影响\n需要考虑数据逻辑的一致性,例如清理A表历史数据时需要考虑所有关联数据的处理\n冷热数据分离</code></pre>\n\n<h2>负载均衡</h2>\n\n<pre>\n<code>目的\n可扩展性: 如读写分离时从备库读数据\n高效性: 把更多工作分配给更好的机器\n可用性: 使用时刻保持可用的服务器\n透明性: 客户端无需知道服务器\n一致性: 如果应用是有状态的,负载均衡器就应该将相关的查询指向同一个服务器\n\n直接连接\n\n复制上的读写分离\n基于查询分离: 将不能容忍脏数据的查询分配到主库,其他分配到备库\n基于脏数据分离: 让应用检查复制延迟,许多报表类应用使用这个策略\n基于会话分离: 可以在会话层做一个标记,如果用户修改了数据,则一段时间内总是指向主库\n基于版本分离: 给用户的操作增加版本号,检查版本号决定从主库还是备库读取数据\n\n修改DNS名\n通过变更DNS名指定的服务器实现\n缺点很多,不建议\n\n转移IP地址\n在服务器之间转移虚拟地址\n给服务器分配固定的ip地址,为每个逻辑上的服务使用一个虚拟ip地址\n\n引入中间件\n负载均衡器,如HAproxy\n负载均衡算法: 随机, 轮询,最少连接数,最快响应,哈希,权重\n服务器池中增加或移除服务器: 在配置连接池中的服务器时,要保证有足够多未使用的容量</code></pre>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>', 1, '5f199b8885e24fc8b28672b872edb606', 7, '2018-09-03 14:51:31', '2018-09-14 11:07:03');
INSERT INTO `logcontent` VALUES ('f9400bd9c4de4b478cd5a41c0e3db337', -1, 'd7caeba238f0466d87db109b2b9724da', 'K-means算法', 'K-means', '初学', '<p><strong><a id=\"K-means 算法简介\" name=\"K-means 算法简介\"></a>K-means 算法：</strong></p>\n\n<p>Clustering 中的经典算法，数据挖掘十大经典算法之一<br />\n算法接受参数 k ；然后将事先输入的n个数据对象划分为 k个聚类以便使得所获得的聚类满足：同一聚类中的对象相似度较高；而不同聚类中的对象相似度较小。<br />\n算法思想：以空间中k个点为中心进行聚类，对最靠近他们的对象归类。通过迭代的方法，逐次更新各聚类中心的值，直至得到最好的聚类结果<br />\n算法描述：</p>\n\n<ul>\n	<li>适当选择c个类的初始中心；</li>\n	<li>在第k次迭代中，对任意一个样本，求其到c各中心的距离，将该样本归到距离最短的中心所在的类；</li>\n	<li>利用均值等方法更新该类的中心值；</li>\n	<li>对于所有的c个聚类中心，如果利用（2）（3）的迭代法更新后，值保持不变，则迭代结束，否则继续迭代。</li>\n</ul>\n\n<p><strong><a id=\"K-means 算法举例\" name=\"K-means 算法举例\"></a>K-means 算法举例：</strong></p>\n\n<table border=\"1\" cellspacing=\"0\" style=\"border-collapse:collapse; border:0.5pt solid #000000; height:70px; width:211.5pt\">\n	<tbody>\n		<tr>\n			<td style=\"background-color:#d0cece; height:14.25pt; text-align:center; vertical-align:middle; width:79.50pt\">&nbsp;</td>\n			<td style=\"background-color:#d0cece; height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">weight</span></span></span></td>\n			<td style=\"background-color:#d0cece; height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">pH</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:79.50pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">MedicineA</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">1</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">1</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:79.50pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">MedicineB</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">2</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">1</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:79.50pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">MedicineC</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">4</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">3</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:79.50pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">MedicineD</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">5</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">4</span></span></span></td>\n		</tr>\n	</tbody>\n</table>\n\n<p><img src=\"http://localhost:8080/myLog/images/k_means/1.png\" style=\"height:245px; width:261px\" /><br />\n前提：左数依次为A/B/C/D点<br />\n首先假定A、B即(1,1)、(2,1)为初始中心，分为两组group-1/group-2<br />\nA到A点的距离为0,B到A点的距离为1,C到A点的距离为3.61,D到A点的距离为5<br />\nA到B点的距离为1,B到B点的距离为0,C到B点的距离为2.83,D到B点的距离为4.24<br />\n<img src=\"http://localhost:8080/myLog/images/k_means/2.png\" style=\"height:50px; width:302px\" /><br />\nAA&lt;AB,BA&gt;BB,CA&gt;CB,DA&gt;DB<br />\n<img src=\"http://localhost:8080/myLog/images/k_means/3.png\" style=\"height:50px; width:212px\" /><br />\ngroup-1（A）<br />\n<img src=\"http://localhost:8080/myLog/images/k_means/4.png\" style=\"height:50px; width:145px\" /><br />\ngroup-2（B、C、D）<br />\n<img src=\"http://localhost:8080/myLog/images/k_means/5.png\" style=\"height:50px; width:277px\" /><br />\n更新中心值为(1,1)、(11/3,8/3)<br />\n<img src=\"http://localhost:8080/myLog/images/k_means/6.png\" style=\"height:243px; width:260px\" /><br />\n重复计算<br />\n<img src=\"http://localhost:8080/myLog/images/k_means/7.png\" style=\"height:50px; width:289px\" /><br />\n<img src=\"http://localhost:8080/myLog/images/k_means/8.png\" style=\"height:50px; width:196px\" /><br />\ngroup-1（A、B）<br />\n<img src=\"http://localhost:8080/myLog/images/k_means/9.png\" style=\"height:50px; width:207px\" /><br />\ngroup-2（C、D）<br />\n<img src=\"http://localhost:8080/myLog/images/k_means/10.png\" style=\"height:50px; width:220px\" /><br />\n更新中心值为(3/2,1)、(9/2,7/2)<br />\n<img src=\"http://localhost:8080/myLog/images/k_means/11.png\" style=\"height:243px; width:260px\" /><br />\n<img src=\"http://localhost:8080/myLog/images/k_means/12.png\" style=\"height:60px; width:273px\" /><br />\n<img src=\"http://localhost:8080/myLog/images/k_means/8.png\" style=\"height:50px; width:196px\" /><br />\n分组无变化，停止</p>\n\n<table border=\"1\" cellspacing=\"0\" style=\"border-collapse:collapse; border:.5pt solid #000000; height:95px; width:277.50pt\">\n	<tbody>\n		<tr>\n			<td style=\"background-color:#d0cece; height:14.25pt; text-align:center; vertical-align:middle; width:79.50pt\">&nbsp;</td>\n			<td style=\"background-color:#d0cece; height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">weight</span></span></span></td>\n			<td style=\"background-color:#d0cece; height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">ph</span></span></span></td>\n			<td style=\"background-color:#d0cece; height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">Group</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:79.50pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">MedicineA</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">1</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">1</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">1</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:79.50pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">MedicineB</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">2</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">1</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">1</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:79.50pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">MedicineC</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">4</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">3</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">2</span></span></span></td>\n		</tr>\n		<tr>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:79.50pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">MedicineD</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">5</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">4</span></span></span></td>\n			<td style=\"height:14.25pt; text-align:center; vertical-align:middle; width:66.00pt\"><span style=\"font-size:12.0pt\"><span style=\"color:#000000\"><span style=\"font-family:宋体\">2</span></span></span></td>\n		</tr>\n	</tbody>\n</table>\n\n<p><strong><a id=\"算法优缺点\" name=\"算法优缺点\"></a>算法优缺点：</strong></p>\n\n<p>优点：速度快，简单<br />\n缺点：最终结果跟初始点选择相关，容易陷入局部最优，需直到k值</p>\n\n<p>&nbsp;</p>\n', 1, '5f199b8885e24fc8b28672b872edb606', 7, '2017-11-27 22:47:38', '2018-08-31 22:21:23');
INSERT INTO `logcontent` VALUES ('fce3cc467c804f87a6f78d81e07d90aa', -1, 'f29612168904487aadb4043df110361c', 'Nginx基本配置文件', 'nginx,配置文件', 'nginx/conf/nginx.conf', '<pre>\n<code class=\"language-nginx\">#运行用户\nuser root;\n#启动进程,通常设置成和cpu的数量相等\nworker_processes  1;\n \n#全局错误日志及PID文件\n#error_log  logs/error.log;\n#error_log  logs/error.log  notice;\n#error_log  logs/error.log  info;\n \n#pid        logs/nginx.pid;\n \n#工作模式及连接数上限\nevents {\n    #epoll是多路复用IO(I/O Multiplexing)中的一种方式,\n    #仅用于linux2.6以上内核,可以大大提高nginx的性能\n    use   epoll; \n \n    #单个后台worker process进程的最大并发链接数    \n    worker_connections  1024;\n \n    # 并发总数是 worker_processes 和 worker_connections 的乘积\n    # 即 max_clients = worker_processes * worker_connections\n    # 在设置了反向代理的情况下，max_clients = worker_processes * worker_connections / 4\n    # 为什么上面反向代理要除以4，应该说是一个经验值\n    # 根据以上条件，正常情况下的Nginx Server可以应付的最大连接数为：4 * 8000 = 32000\n    # worker_connections 值的设置跟物理内存大小有关\n    # 因为并发受IO约束，max_clients的值须小于系统可以打开的最大文件数\n    # 而系统可以打开的最大文件数和内存大小成正比，一般1GB内存的机器上可以打开的文件数大约是10万左右\n    # 并发连接总数小于系统可以打开的文件句柄总数，这样就在操作系统可以承受的范围之内\n    # 所以，worker_connections 的值需根据 worker_processes 进程数目和系统可以打开的最大文件总数进行适当地进行设置\n    # 使得并发总数小于操作系统可以打开的最大文件数目\n    # 其实质也就是根据主机的物理CPU和内存进行配置\n    # 当然，理论上的并发总数可能会和实际有所偏差，因为主机还有其他的工作进程需要消耗系统资源。\n    # ulimit -SHn 65535\n \n}\n \n \nhttp {\n    #设定mime类型,类型由mime.type文件定义\n    include    mime.types;\n    default_type  application/octet-stream;\n    #设定日志格式\n    log_format  main  \'$remote_addr - $remote_user [$time_local] \"$request\" \'\n                      \'$status $body_bytes_sent \"$http_referer\" \'\n                      \'\"$http_user_agent\" \"$http_x_forwarded_for\"\';\n \n    access_log  logs/access.log  main;\n \n    #sendfile 指令指定 nginx 是否调用 sendfile 函数（zero copy 方式）来输出文件，\n    #对于普通应用，必须设为 on,\n    #如果用来进行下载等应用磁盘IO重负载应用，可设置为 off，\n    #以平衡磁盘与网络I/O处理速度，降低系统的uptime.\n    sendfile     on;\n    #tcp_nopush     on;\n \n    #连接超时时间\n    #keepalive_timeout  0;\n    keepalive_timeout  65;\n    tcp_nodelay     on;\n \n    #开启gzip压缩\n    gzip  on;\n    gzip_disable \"MSIE [1-6].\";\n \n    #设定请求缓冲\n    client_header_buffer_size    128k;\n    large_client_header_buffers  4 128k;\n \n \n    #设定虚拟主机配置\n    server {\n        #监听80端口\n        listen    80;\n        #定义使用 www.nginx.cn访问\n        server_name  www.nginx.cn;\n \n        #定义服务器的默认网站根目录位置\n        root html;\n \n        #设定本虚拟主机的访问日志\n        access_log  logs/nginx.access.log  main;\n \n        #默认请求\n        location / {\n            \n            #定义首页索引文件的名称\n            index index.php index.html index.htm;   \n \n        }\n \n        # 定义错误提示页面\n        error_page   500 502 503 504 /50x.html;\n        location = /50x.html {\n        }\n \n        #静态文件，nginx自己处理\n        location ~ ^/(images|javascript|js|css|flash|media|static)/ {\n            \n            #过期30天，静态文件不怎么更新，过期可以设大一点，\n            #如果频繁更新，则可以设置得小一点。\n            expires 30d;\n        }\n \n        #PHP 脚本请求全部转发到 FastCGI处理. 使用FastCGI默认配置.\n        location ~ .php$ {\n            fastcgi_pass 127.0.0.1:9000;\n            fastcgi_index index.php;\n            fastcgi_param  SCRIPT_FILENAME  $document_root$fastcgi_script_name;\n            include fastcgi_params;\n        }\n \n        #禁止访问 .htxxx 文件\n        location ~ /.ht {\n            deny all;\n        }\n \n    }\n}</code></pre>\n\n<pre>\n<code>#全局设置\nmain \n# 运行用户\nuser root;    \n# 启动进程,通常设置成和cpu的数量相等\nworker_processes  1;\n\n# 全局错误日志及PID文件\nerror_log  /var/log/nginx/error.log;\npid        /var/run/nginx.pid;\n\n# 工作模式及连接数上限\nevents {\n    use epoll; #epoll是多路复用IO(I/O Multiplexing)中的一种方式,但是仅用于linux2.6以上内核,可以大大提高nginx的性能\n    worker_connections 1024; #单个后台worker process进程的最大并发链接数\n    # multi_accept on; \n}\n\n#设定http服务器，利用它的反向代理功能提供负载均衡支持\nhttp {\n    #设定mime类型,类型由mime.type文件定义\n    include       /etc/nginx/mime.types;\n    default_type  application/octet-stream;\n    #设定日志格式\n    access_log    /var/log/nginx/access.log;\n\n    #sendfile 指令指定 nginx 是否调用 sendfile 函数（zero copy 方式）来输出文件，对于普通应用，\n    #必须设为 on,如果用来进行下载等应用磁盘IO重负载应用，可设置为 off，以平衡磁盘与网络I/O处理速度，降低系统的uptime.\n    sendfile        on;\n    #将tcp_nopush和tcp_nodelay两个指令设置为on用于防止网络阻塞\n    tcp_nopush      on;\n    tcp_nodelay     on;\n    #连接超时时间\n    keepalive_timeout  65;\n\n    #开启gzip压缩\n    gzip  on;\n    gzip_disable \"MSIE [1-6]\\.(?!.*SV1)\";\n\n    #设定请求缓冲\n    client_header_buffer_size    1k;\n    large_client_header_buffers  4 4k;\n\n    include /etc/nginx/conf.d/*.conf;\n    include /etc/nginx/sites-enabled/*;\n\n    #设定负载均衡的服务器列表\n    upstream mysvr {\n        #weigth参数表示权值，权值越高被分配到的几率越大\n        #本机上的Squid开启3128端口\n        server 192.168.8.1:3128 weight=5;\n        server 192.168.8.2:80  weight=1;\n        server 192.168.8.3:80  weight=6;\n    }\n\n\n    server {\n        #侦听80端口\n        listen       80;\n        #定义使用www.xx.com访问\n        server_name  www.xx.com;\n\n        #设定本虚拟主机的访问日志\n        access_log  logs/www.xx.com.access.log  main;\n\n        #默认请求\n        location / {\n            root   /root;      #定义服务器的默认网站根目录位置\n            index index.php index.html index.htm;   #定义首页索引文件的名称\n\n            fastcgi_pass  www.xx.com;\n            fastcgi_param  SCRIPT_FILENAME  $document_root/$fastcgi_script_name; \n            include /etc/nginx/fastcgi_params;\n        }\n\n        # 定义错误提示页面\n        error_page   500 502 503 504 /50x.html;  \n            location = /50x.html {\n            root   /root;\n        }\n\n        #静态文件，nginx自己处理\n        location ~ ^/(images|javascript|js|css|flash|media|static)/ {\n            root /var/www/virtual/htdocs;\n            #过期30天，静态文件不怎么更新，过期可以设大一点，如果频繁更新，则可以设置得小一点。\n            expires 30d;\n        }\n        #PHP 脚本请求全部转发到 FastCGI处理. 使用FastCGI默认配置.\n        location ~ \\.php$ {\n            root /root;\n            fastcgi_pass 127.0.0.1:9000;\n            fastcgi_index index.php;\n            fastcgi_param SCRIPT_FILENAME /home/www/www$fastcgi_script_name;\n            include fastcgi_params;\n        }\n        #设定查看Nginx状态的地址\n        location /NginxStatus {\n            stub_status            on;\n            access_log              on;\n            auth_basic              \"NginxStatus\";\n            auth_basic_user_file  conf/htpasswd;\n        }\n        #禁止访问 .htxxx 文件\n        location ~ /\\.ht {\n            deny all;\n        }\n\n    }\n\n    #第一个虚拟服务器\n    server {\n        #侦听192.168.8.x的80端口\n        listen       80;\n        server_name  192.168.8.x;\n\n        #对aspx后缀的进行负载均衡请求\n        location ~ .*\\.aspx$ {\n            root   /root;#定义服务器的默认网站根目录位置\n            index index.php index.html index.htm;#定义首页索引文件的名称\n\n            proxy_pass  http://mysvr;#请求转向mysvr 定义的服务器列表\n\n            #以下是一些反向代理的配置可删除.\n            proxy_redirect off;\n\n            #后端的Web服务器可以通过X-Forwarded-For获取用户真实IP\n            proxy_set_header Host $host;\n            proxy_set_header X-Real-IP $remote_addr;\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n            client_max_body_size 10m;    #允许客户端请求的最大单文件字节数\n            client_body_buffer_size 128k;  #缓冲区代理缓冲用户端请求的最大字节数，\n            proxy_connect_timeout 90;  #nginx跟后端服务器连接超时时间(代理连接超时)\n            proxy_send_timeout 90;        #后端服务器数据回传时间(代理发送超时)\n            proxy_read_timeout 90;         #连接成功后，后端服务器响应时间(代理接收超时)\n            proxy_buffer_size 4k;             #设置代理服务器（nginx）保存用户头信息的缓冲区大小\n            proxy_buffers 4 32k;               #proxy_buffers缓冲区，网页平均在32k以下的话，这样设置\n            proxy_busy_buffers_size 64k;    #高负荷下缓冲大小（proxy_buffers*2）\n            proxy_temp_file_write_size 64k;  #设定缓存文件夹大小，大于这个值，将从upstream服务器传\n        }\n    }\n}</code></pre>\n\n<p>&nbsp;</p>\n', 1, '5f199b8885e24fc8b28672b872edb606', 14, '2018-03-20 09:03:43', '2018-09-12 22:53:44');
INSERT INTO `logcontent` VALUES ('fd20b2569bcd4b2f9524e6cc078da069', 1, 'd7caeba238f0466d87db109b2b9724da', '支持向量机(SVM)算法笔记二', 'svm', '初学', '<p><strong>SVM算法特性：</strong></p>\n\n<pre>\n<code>训练好的模型的算法复杂度是由支持向量的个数决定的，而不是由数据的维度决定的。所以SVM不太容易产生overfitting\nSVM训练出来的模型完全依赖于支持向量(Support Vectors), 即使训练集里面所有非支持向量的点都被去除，重复训练过程，结果仍然会得到完全一样的模型。\n一个SVM如果训练得出的支持向量个数比较小，SVM训练出的模型比较容易被泛化。</code></pre>\n\n<p><strong>线性不可分的情况（linearly inseparable case)：</strong></p>\n\n<p>即数据集在空间中对应的向量不可被一个超平面区分开；</p>\n\n<pre>\n<code>两个步骤来解决：\n利用一个非线性的映射把原数据集中的向量点转化到一个更高维度的空间中\n在这个高维度的空间中找一个线性的超平面来根据线性可分的情况处理</code></pre>\n\n<p><img src=\"http://localhost:8080/myLog/images/svm/6.png\" style=\"height:160px; width:389px\" /></p>\n\n<pre>\n<code>如何利用非线性映射把原始数据转化到高维中：</code></pre>\n\n<p>例子：</p>\n\n<p>3维输入向量：X = (x<sub>1</sub>,x<sub>2</sub>,x<sub>3</sub>)<br />\n转化到6维空间 Z 中去&phi;<sub>1</sub>(X) = x<sub>1</sub>，&phi;<sub>2</sub>(X) = x<sub>2</sub>，&phi;<sub>3</sub>(X) = x<sub>3</sub>，&phi;<sub>4</sub>(X) = (x<sub>1</sub>)<sup>2</sup>，&phi;<sub>5</sub>(X) = x<sub>1</sub>x<sub>2</sub>，&phi;<sub>6</sub>(X) = x<sub>1</sub>x<sub>3</sub><br />\n带入线性可区分超平面：d(Z) = WZ+b，其中W和Z是向量<br />\n得d(Z) = w<sub>1</sub>x<sub>1</sub>+&nbsp;w<sub>2</sub>x<sub>2</sub>+&nbsp;w<sub>3</sub>x<sub>3</sub>+&nbsp;w<sub>4</sub>(x<sub>1</sub>)<sup>2</sup>+&nbsp;w<sub>5</sub>x<sub>1</sub>x<sub>2</sub>&nbsp;+&nbsp;w<sub>6</sub>x<sub>1</sub>x<sub>3</sub> + b =&nbsp;w<sub>1</sub>z<sub>1</sub> +&nbsp;w<sub>2</sub>z<sub>2</sub> +&nbsp;w<sub>3</sub>z<sub>3</sub> +&nbsp;w<sub>4</sub>z<sub>4</sub> +&nbsp;w<sub>5</sub>z<sub>5</sub> +&nbsp;w<sub>6</sub>z<sub>6</sub> + b</p>\n\n<pre>\n<code>思考问题：\n如何选择合理的非线性转化把数据转到高纬度中？\n如何解决计算内积时算法复杂度非常高的问题？</code></pre>\n\n<p>使用核方法（kernel trick)</p>\n\n<p>常用核函数(kernel functions)<br />\nh度多项式核函数(polynomial kernel of degree h)：K( X<sub>i ，&nbsp;</sub>X<sub>j&nbsp;</sub>) = ( X<sub>i&nbsp;</sub>&middot; X<sub>j </sub>+ 1)<sup>h</sup><br />\n高斯径向基核函数(Gaussian radial basis function kernel)：K( X<sub>i ，&nbsp;</sub>X<sub>j&nbsp;</sub>) = <img src=\"http://localhost:8080/myLog/images/svm/7.png\" style=\"height:20px; width:94px\" /><br />\nS型核函数(Sigmoid function kernel):&nbsp;：K( X<sub>i ，&nbsp;</sub>X<sub>j&nbsp;</sub>) = tanh ( kX<span style=\"font-size:10.8333px\">i</span>&middot;X<sub>j&nbsp;</sub>- &delta;)</p>\n\n<p>根据先验知识，比如图像分类，通常使用RBF，文字不使用RBF，尝试不同的kernel，根据结果准确度而定</p>\n\n<p>核函数举例:<br />\n假设定义两个向量： x = (x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub>); y = (y<sub>1</sub>, y<sub>2</sub>, y<sub>3</sub>)<br />\n定义方程：f(x) = (x<sub>1</sub>x<sub>1</sub>, x<sub>1</sub>x<sub>2</sub>, x<sub>1</sub>x<sub>3</sub>, x<sub>2</sub>x<sub>1</sub>, x<sub>2</sub>x<sub>2</sub>, x<sub>2</sub>x<sub>3</sub>, x<sub>3</sub>x<sub>1</sub>, x<sub>3</sub>x<sub>2</sub>, x<sub>3</sub>x<sub>3</sub>)<br />\n核函数：K(x, y ) = (&lt;x, y&gt;)<sup>2</sup><br />\n假设x = ( 1 , 2 , 3 ); y = ( 4 , 5 , 6 )<br />\n普通算法：f ( x ) = ( 1 , 2 , 3 , 2&nbsp;, 4&nbsp;, 6 , 3&nbsp;, 6&nbsp;, 9&nbsp;)<br />\nf ( y ) = ( 16 , 20 , 24 , 20 , 25 , 30 , 24 , 30&nbsp;, 36&nbsp;)<br />\n&lt;f ( x ), f ( y )&gt; = 16 + 40 + 72 + 40 + 100+ 180 + 72 + 180 + 324 = 1024<br />\n核函数算法：K(x, y) = ( 4 &nbsp;+ 10 + 18 ) <sup>2</sup> = 322 = 1024<br />\n同样的结果，使用kernel方法计算容易很多</p>\n\n<pre>\n<code>SVM扩展可解决多个类别分类问题\n对于每个类，有一个当前类和其他类的二类分类器（one-vs-rest)</code></pre>\n\n<p>&nbsp;</p>', 1, '5f199b8885e24fc8b28672b872edb606', 6, '2017-11-20 00:00:00', '2018-08-31 22:21:23');
INSERT INTO `logcontent` VALUES ('ffad7610afc249c3ba80472330fe7f2b', 1, '1dff2a5fdaeb4886938bb7c70b57acce', 'Java基础', '面试题', 'Java基础', '<h2>1、面向对象的特征有哪些方面？</h2>\n\n<p>答：面向对象的特征主要有以下几个方面：<br />\n- 抽象：抽象是将一类对象的共同特征总结出来构造类的过程，包括数据抽象和行为抽象两方面。抽象只关注对象属性和行为，并不关注行为的细节。<br />\n- 继承：继承是从已有类得到继承信息创建新类的过程。提供继承信息的类被称为父类（超类、基类）；得到继承信息的类被称为子类（派生类）。继承让变化中的软件系统有了一定的延续性，同时继承也是封装程序中可变因素的重要手段<br />\n- 封装：通常认为封装是把数据和操作数据的方法绑定起来，对数据的访问只能通过已定义的接口。封装就是隐藏一切细节，只向外界提供最简单的编程接口<br />\n- 多态性：多态性是指允许不同子类型的对象对同一消息作出不同的响应。多态性分为编译时多态性（方法重载）和运行时多态性（方法重写）。运行时的多态是面向对象最精髓的东西，1. 方法重写：子类继承父类并重写父类中已有的或抽象的方法。2. 对象造型：用父类型引用引用子类型对象，这样同样的引用调用同样的方法就会根据子类对象的不同而表现出不同的行为。</p>\n\n<h2><strong>2、访问修饰符public,private,protected,以及不写（默认）时的区别？</strong></h2>\n\n<table style=\"width:500px\">\n	<thead>\n		<tr>\n			<th style=\"text-align:left\">修饰符</th>\n			<th style=\"text-align:left\">当前类</th>\n			<th style=\"text-align:left\">同&nbsp;包</th>\n			<th style=\"text-align:left\">子&nbsp;类</th>\n			<th style=\"text-align:left\">其他包</th>\n		</tr>\n	</thead>\n	<tbody>\n		<tr>\n			<td style=\"text-align:left\">public</td>\n			<td style=\"text-align:left\">&radic;</td>\n			<td style=\"text-align:left\">&radic;</td>\n			<td style=\"text-align:left\">&radic;</td>\n			<td style=\"text-align:left\">&radic;</td>\n		</tr>\n		<tr>\n			<td style=\"text-align:left\">protected</td>\n			<td style=\"text-align:left\">&radic;</td>\n			<td style=\"text-align:left\">&radic;</td>\n			<td style=\"text-align:left\">&radic;</td>\n			<td style=\"text-align:left\">&times;</td>\n		</tr>\n		<tr>\n			<td style=\"text-align:left\">default</td>\n			<td style=\"text-align:left\">&radic;</td>\n			<td style=\"text-align:left\">&radic;</td>\n			<td style=\"text-align:left\">&times;</td>\n			<td style=\"text-align:left\">&times;</td>\n		</tr>\n		<tr>\n			<td style=\"text-align:left\">private</td>\n			<td style=\"text-align:left\">&radic;</td>\n			<td style=\"text-align:left\">&times;</td>\n			<td style=\"text-align:left\">&times;</td>\n			<td style=\"text-align:left\">&times;</td>\n		</tr>\n	</tbody>\n</table>\n\n<p><strong>3、String 是最基本的数据类型吗？</strong><br />\n答：不是。Java中的基本数据类型只有8个：byte、short、int、long、float、double、char、boolean；除了基本类型和枚举类型，剩下的都是引用类型。</p>\n\n<p><strong>4、float f=3.4;是否正确？</strong><br />\n答:不正确。3.4是双精度数，将双精度型（double）赋值给浮点型（float）属于下转型会造成精度损失，因此需要强制类型转换float f =(float)3.4; 或者写成float f =3.4F;。</p>\n\n<p><strong>5、short s1 = 1; s1 = s1 + 1;有错吗?short s1 = 1; s1 += 1;有错吗？</strong><br />\n答：对于short s1 = 1; s1 = s1 + 1;由于1是int类型，因此s1+1运算结果也是int 型，需要强制转换类型才能赋值给short型。而short s1 = 1; s1 += 1;可以正确编译，因为s1+= 1;相当于s1 = (short)(s1 + 1);其中有隐含的强制类型转换。</p>\n\n<p><strong>6、Java有没有goto？</strong><br />\n答：goto 是Java中的保留字，在目前版本的Java中没有使用。《The Java Programming Language》一书的附录中有goto和const，但是这两个是目前无法使用的关键字，因此有些地方将其称之为保留字，其实保留字这个词应该有更广泛的意义，因为熟悉C语言的程序员都知道，在系统类库中使用过的有特殊意义的单词或单词的组合都被视为保留字）</p>\n\n<p><strong>7、int和Integer有什么区别？</strong><br />\n答：Java是一个近乎纯洁的面向对象编程语言，但是为了编程的方便还是引入了基本数据类型，但是为了能够将这些基本数据类型当成对象操作，Java为每一个基本数据类型都引入了对应的包装类型，int的包装类就是Integer，从Java 5开始引入了自动装箱/拆箱机制，使得二者可以相互转换。<br />\nJava 为每个原始类型提供了包装类型：<br />\n- 原始类型: boolean，char，byte，short，int，long，float，double<br />\n- 包装类型：Boolean，Character，Byte，Short，Integer，Long，Float，Double</p>\n\n<pre>\n<code>class AutoUnboxingTest {\n \n    public static void main(String[] args) {\n        Integer a = new Integer(3);\n        Integer b = 3;                  // 将3自动装箱成Integer类型\n        int c = 3;\n        System.out.println(a == b);     // false 两个引用没有引用同一对象\n        System.out.println(a == c);     // true a自动拆箱成int类型再和c比较\n        System.out.println(b == c);     // true b自动拆箱成int类型再和c比较\n    }\n}</code></pre>\n\n<p>最近还遇到一个面试题，也是和自动装箱和拆箱有点关系的，代码如下所示：</p>\n\n<pre>\n<code>public class Test03 {\n    public static void main(String[] args) {\n        Integer f1 = 100, f2 = 100, f3 = 150, f4 = 150;\n        System.out.println(f1 == f2);\n        System.out.println(f3 == f4);\n    }\n}</code></pre>\n\n<p>如果不明就里很容易认为两个输出要么都是true要么都是false。首先需要注意的是f1、f2、f3、f4四个变量都是Integer对象引用，所以下面的==运算比较的不是值而是引用。装箱的本质是什么呢？当我们给一个Integer对象赋一个int值的时候，会调用Integer类的静态方法valueOf，如果看看valueOf的源代码就知道发生了什么。</p>\n\n<pre>\n<code>public static Integer valueOf(int i) {\n    if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high)\n        return IntegerCache.cache[i + (-IntegerCache.low)];\n    return new Integer(i);\n}</code></pre>\n\n<p>IntegerCache是Integer的内部类，其代码如下所示：</p>\n\n<pre>\n<code>private static class IntegerCache {\n    static final int low = -128;\n    static final int high;\n    static final Integer cache[];\n\n    static {\n        // high value may be configured by property\n        int h = 127;\n        String integerCacheHighPropValue =\n            sun.misc.VM.getSavedProperty(\"java.lang.Integer.IntegerCache.high\");\n        if (integerCacheHighPropValue != null) {\n            try {\n                int i = parseInt(integerCacheHighPropValue);\n                i = Math.max(i, 127);\n                // Maximum array size is Integer.MAX_VALUE\n                h = Math.min(i, Integer.MAX_VALUE - (-low) -1);\n            } catch( NumberFormatException nfe) {\n                // If the property cannot be parsed into an int, ignore it.\n            }\n        }\n        high = h;\n        cache = new Integer[(high - low) + 1];\n        int j = low;\n        for(int k = 0; k &lt; cache.length; k++)\n            cache[k] = new Integer(j++);\n        // range [-128, 127] must be interned (JLS7 5.1.7)\n        assert IntegerCache.high &gt;= 127;\n    }\n    private IntegerCache() {}\n}</code></pre>\n\n<p>简单的说，如果整型字面量的值在-128到127之间，那么不会new新的Integer对象，而是直接引用常量池中的Integer对象，所以上面的面试题中f1==f2的结果是true，而f3==f4的结果是false。</p>\n\n<p><strong>8、&amp;和&amp;&amp;的区别？</strong><br />\n答：&amp;运算符有两种用法：(1)按位与；(2)逻辑与。&amp;&amp;运算符是短路与运算。虽然二者都要求运算符左右两端的布尔值都是true整个表达式的值才是true。&amp;&amp;之所以称为短路运算是因为如果&amp;&amp;左边的表达式的值是false，右边的表达式不会进行运算。注意：逻辑或运算符（|）和短路或运算符（||）的差别也是如此。</p>\n\n<p><strong>9、解释内存中的栈(stack)、堆(heap)和静态区(static area)的用法。</strong><br />\n答：通常我们定义一个基本数据类型的变量，一个对象的引用，还有就是函数调用的现场保存都使用内存中的栈空间；而通过new关键字和构造器创建的对象放在堆空间；程序中的字面量（literal）如直接书写的100、&rdquo;hello&rdquo;和常量都是放在静态区中。栈空间操作起来最快但是栈很小，通常大量的对象都是放在堆空间，理论上整个内存没有被其他进程使用的空间甚至硬盘上的虚拟内存都可以被当成堆空间来使用。</p>\n\n<pre>\n<code>String str = new String(\"hello\");</code></pre>\n\n<p>上面的语句中变量str放在栈上，用new创建出来的字符串对象放在堆上，而&rdquo;hello&rdquo;这个字面量放在静态区。</p>\n\n<blockquote>\n<p><strong>补充：</strong>较新版本的Java（从Java 6的某个更新开始）中使用了一项叫&rdquo;逃逸分析&rdquo;的技术，可以将一些局部对象放在栈上以提升对象的操作性能。</p>\n</blockquote>\n\n<p><strong>10、Math.round(11.5) 等于多少？Math.round(-11.5)等于多少？</strong><br />\n答：Math.round(11.5)的返回值是12，Math.round(-11.5)的返回值是-11。四舍五入的原理是在参数上加0.5然后进行下取整。</p>\n\n<p><strong>11、switch 是否能作用在byte 上，是否能作用在long 上，是否能作用在String上？</strong><br />\n答：在Java 5以前，switch(expr)中，expr只能是byte、short、char、int。从Java 5开始，Java中引入了枚举类型，expr也可以是enum类型，从Java 7开始，expr还可以是字符串（String），但是长整型（long）在目前所有的版本中都是不可以的。</p>\n\n<p><strong>12、用最有效率的方法计算2乘以8？</strong><br />\n答： 2 &lt;&lt; 3（左移3位相当于乘以2的3次方，右移3位相当于除以2的3次方）。</p>\n\n<p><strong>13、数组有没有length()方法？String有没有length()方法？</strong><br />\n答：数组没有length()方法，有length 的属性。String 有length()方法。</p>\n\n<p><strong>14、在Java中，如何跳出当前的多重嵌套循环？</strong><br />\n答：在最外层循环前加一个标记如A，然后用break A;可以跳出多重循环。（Java中支持带标签的break和continue语句，作用有点类似于C和C++中的goto语句，但是就像要避免使用goto一样，应该避免使用带标签的break和continue，因为它不会让你的程序变得更优雅）</p>\n\n<p><strong>15、构造器（constructor）是否可被重写（override）？</strong><br />\n答：构造器不能被继承，因此不能被重写，但可以被重载。</p>\n\n<p><strong>16、两个对象值相同(x.equals(y) == true)，但却可有不同的hash code，这句话对不对？</strong><br />\n答：不对，如果两个对象x和y满足x.equals(y) == true，它们的哈希码（hash code）应当相同。Java对于eqauls方法和hashCode方法是这样规定的：(1)如果两个对象相同（equals方法返回true），那么它们的hashCode值一定要相同；(2)如果两个对象的hashCode相同，它们并不一定相同。当然，你未必要按照要求去做，但是如果你违背了上述原则就会发现在使用容器时，相同的对象可以出现在Set集合中，同时增加新元素的效率会大大下降（对于使用哈希存储的系统，如果哈希码频繁的冲突将会造成存取性能急剧下降）。</p>\n\n<blockquote>\n<p><strong>补充：</strong>关于equals和hashCode方法，《<a href=\"http://www.amazon.com/gp/product/B000WJOUPA/ref=as_li_qf_sp_asin_il_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=B000WJOUPA&amp;linkCode=as2&amp;tag=job0ae-20\" rel=\"nofollow\" target=\"_blank\" title=\"Effective Java\">Effective Java</a>》中是这样介绍equals方法的：首先equals方法必须满足自反性（x.equals(x)必须返回true）、对称性（x.equals(y)返回true时，y.equals(x)也必须返回true）、传递性（x.equals(y)和y.equals(z)都返回true时，x.equals(z)也必须返回true）和一致性（当x和y引用的对象信息没有被修改时，多次调用x.equals(y)应该得到同样的返回值），而且对于任何非null值的引用x，x.equals(null)必须返回false。实现高质量的equals方法的诀窍包括：1. 使用==操作符检查&rdquo;参数是否为这个对象的引用&rdquo;；2. 使用instanceof操作符检查&rdquo;参数是否为正确的类型&rdquo;；3. 对于类中的关键属性，检查参数传入对象的属性是否与之相匹配；4. 编写完equals方法后，问自己它是否满足对称性、传递性、一致性；5. 重写equals时总是要重写hashCode；6. 不要将equals方法参数中的Object对象替换为其他的类型，在重写时不要忘掉@Override注解。</p>\n\n<p>问题二：equals和==的区别，==比较两个变量指向的对象是否相同（基本类型为值），equals比较两个变量指向对象的值是否相同，比==要求低；</p>\n</blockquote>\n\n<pre>\n<code>public static void main(String[] args) {\n	String i = new String(\"1\");\n	String ii = new String(\"1\");\n	String iii = \"1\";\n	String iiii = \"1\";\n	System.out.println(i==ii);//false\n	System.out.println(i.equals(ii));//true\n	System.out.println(iii==iiii);//true\n	System.out.println(iii.equals(iiii));//true\n	System.out.println(i==iiii);//false\n	System.out.println(i.equals(iiii));//true\n}</code></pre>\n\n<p><strong>17、是否可以继承String类？</strong><br />\n答：String 类是final类，不可以被继承。</p>\n\n<blockquote>\n<p><strong>补充：</strong>继承String本身就是一个错误的行为，对String类型最好的重用方式是关联关系（Has-A）和依赖关系（Use-A）而不是继承关系（Is-A）。</p>\n</blockquote>\n\n<p><strong>18、当一个对象被当作参数传递到一个方法后，此方法可改变这个对象的属性，并可返回变化后的结果，那么这里到底是值传递还是引用传递？</strong><br />\n答：值传递。Java语言的方法调用只支持参数的值传递。当一个对象实例作为一个参数被传递到方法中时，参数的值就是对该对象的引用。对象的属性可以在被调用过程中被改变，但对对象引用的改变是不会影响到调用者的。C++和C#中可以通过传引用或传输出参数来改变传入的参数的值。</p>\n\n<p><strong>19、String和StringBuilder、StringBuffer的区别？</strong><br />\n答：Java平台提供了两种类型的字符串：String和StringBuffer/StringBuilder，它们可以储存和操作字符串。其中String是只读字符串，也就意味着String引用的字符串内容是不能被改变的。而StringBuffer/StringBuilder类表示的字符串对象可以直接进行修改。StringBuilder是Java 5中引入的，它和StringBuffer的方法完全相同，区别在于它是在单线程环境下使用的，因为它的所有方面都没有被synchronized修饰，因此它的效率也比StringBuffer要高。</p>\n\n<blockquote>\n<p><strong>面试题1</strong>&nbsp;- 什么情况下用+运算符进行字符串连接比调用StringBuffer/StringBuilder对象的append方法连接字符串性能更好？</p>\n\n<p><strong>面试题2</strong>&nbsp;- 请说出下面程序的输出。</p>\n</blockquote>\n\n<pre>\n<code>class StringEqualTest {\n \n    public static void main(String[] args) {\n    	String s1 = \"Programming\";\n        String s2 = new String(\"Programming\");\n        String s3 = \"Program\" + \"ming\";//内部相当于springbuffer.toString()\n        String s4 = \"Program\" + new String(\"ming\");\n        System.out.println(s1 == s2);//false\n        System.out.println(s1 == s3);//true\n        System.out.println(s1 == s4);//false\n        System.out.println(s1 == s1.intern());//true\n        System.out.println(s2 == s2.intern());//false\n    }\n}</code></pre>\n\n<blockquote>\n<p>补充：String对象的intern方法会得到字符串对象在常量池中对应的版本的引用（如果常量池中有一个字符串与String对象的equals结果是true），如果常量池中没有对应的字符串，则该字符串将被添加到常量池中，然后返回常量池中字符串的引用。</p>\n</blockquote>\n\n<p><strong>20、重载（Overload）和重写（Override）的区别。重载的方法能否根据返回类型进行区分？</strong><br />\n答：方法的重载和重写都是实现多态的方式，区别在于前者实现的是编译时的多态性，而后者实现的是运行时的多态性。重载发生在一个类中，同名的方法如果有不同的参数列表（参数类型不同、参数个数不同或者二者都不同）则视为重载；重写发生在子类与父类之间，重写要求子类被重写方法与父类被重写方法有相同的返回类型，比父类被重写方法更好访问，不能比父类被重写方法声明更多的异常（里氏代换原则）。重载对返回类型没有特殊的要求。</p>\n\n<p><strong>21、描述一下JVM加载class文件的原理机制？</strong><br />\n答：JVM中类的装载是由类加载器（ClassLoader）和它的子类来实现的，Java中的类加载器是一个重要的Java运行时系统组件，它负责在运行时查找和装入类文件中的类。<br />\n由于Java的跨平台性，经过编译的Java源程序并不是一个可执行程序，而是一个或多个类文件。当Java程序需要使用某个类时，JVM会确保这个类已经被加载、连接（验证、准备和解析）和初始化。类的加载是指把类的.class文件中的数据读入到内存中，通常是创建一个字节数组读入.class文件，然后产生与所加载类对应的Class对象。加载完成后，Class对象还不完整，所以此时的类还不可用。当类被加载后就进入连接阶段，这一阶段包括验证、准备（为静态变量分配内存并设置默认的初始值）和解析（将符号引用替换为直接引用）三个步骤。最后JVM对类进行初始化，包括：1)如果类存在直接的父类并且这个类还没有被初始化，那么就先初始化父类；2)如果类中存在初始化语句，就依次执行这些初始化语句。<br />\n类的加载是由类加载器完成的，类加载器包括：根加载器（BootStrap）、扩展加载器（Extension）、系统加载器（System）和用户自定义类加载器（java.lang.ClassLoader的子类）。从Java 2（JDK 1.2）开始，类加载过程采取了父亲委托机制（PDM）。PDM更好的保证了Java平台的安全性，在该机制中，JVM自带的Bootstrap是根加载器，其他的加载器都有且仅有一个父类加载器。类的加载首先请求父类加载器加载，父类加载器无能为力时才由其子类加载器自行加载。JVM不会向Java程序提供对Bootstrap的引用。下面是关于几个类加载器的说明：</p>\n\n<blockquote>\n<ul>\n	<li>Bootstrap：一般用本地代码实现，负责加载JVM基础核心类库（rt.jar）；</li>\n	<li>Extension：从java.ext.dirs系统属性所指定的目录中加载类库，它的父加载器是Bootstrap；</li>\n	<li>System：又叫应用类加载器，其父类是Extension。它是应用最广泛的类加载器。它从环境变量classpath或者系统属性java.class.path所指定的目录中记载类，是用户自定义加载器的默认父加载器。</li>\n</ul>\n</blockquote>\n\n<p><strong>22、char 型变量中能不能存贮一个中文汉字，为什么？</strong><br />\n答：char类型可以存储一个中文汉字，因为Java中使用的编码是Unicode（不选择任何特定的编码，直接使用字符在字符集中的编号，这是统一的唯一方法），一个char类型占2个字节（16比特），所以放一个中文是没问题的。</p>\n\n<p><strong>23、抽象类（abstract class）和接口（interface）有什么异同？</strong><br />\n答：抽象类和接口都不能够实例化，但可以定义抽象类和接口类型的引用。一个类如果继承了某个抽象类或者实现了某个接口都需要对其中的抽象方法全部进行实现，否则该类仍然需要被声明为抽象类。接口比抽象类更加抽象，因为抽象类中可以定义构造器，可以有抽象方法和具体方法，而接口中不能定义构造器而且其中的方法全部都是抽象方法。抽象类中的成员可以是private、默认、protected、public的，而接口中的成员全都是public的。抽象类中可以定义成员变量，而接口中定义的成员变量实际上都是常量。有抽象方法的类必须被声明为抽象类，而抽象类未必要有抽象方法。</p>\n\n<p><strong>24、静态嵌套类(Static Nested Class)和内部类（Inner Class）的不同？</strong><br />\n答：Static Nested Class是被声明为静态的内部类，它可以不依赖于外部类实例被实例化。而通常的内部类需要在外部类实例化后才能实例化，其语法看起来挺诡异的，如下所示。</p>\n\n<p>面试题 &ndash; 下面的代码哪些地方会产生编译错误？</p>\n\n<pre>\n<code>class Outer {\n    class Inner {}\n    public static void foo() { new Inner(); }\n    public void bar() { new Inner(); }\n    public static void main(String[] args) {\n        new Inner();\n    }\n}</code></pre>\n\n<p>注意：Java中非静态内部类对象的创建要依赖其外部类对象，上面的面试题中foo和main方法都是静态方法，静态方法中没有this，也就是说没有所谓的外部类对象，因此无法创建内部类对象，如果要在静态方法中创建内部类对象，可以这样做：</p>\n\n<pre>\n<code>new Outer().new Inner();</code></pre>\n\n<p><strong>25、Java 中会存在内存泄漏吗，请简单描述。</strong><br />\n答：理论上Java因为有垃圾回收机制（GC）不会存在内存泄露问题（这也是Java被广泛使用于服务器端编程的一个重要原因）；然而在实际开发中，可能会存在无用但可达的对象，这些对象不能被GC回收，因此也会导致内存泄露的发生。例如hibernate的Session（一级缓存）中的对象属于持久态，垃圾回收器是不会回收这些对象的，然而这些对象中可能存在无用的垃圾对象，如果不及时关闭（close）或清空（flush）一级缓存就可能导致内存泄露。下面例子中的代码也会导致内存泄露。</p>\n\n<pre>\n<code>import java.util.Arrays;\nimport java.util.EmptyStackException;\n \npublic class MyStack&lt;T&gt; {\n    private T[] elements;\n    private int size = 0;\n \n    private static final int INIT_CAPACITY = 16;\n \n    public MyStack() {\n        elements = (T[]) new Object[INIT_CAPACITY];\n    }\n \n    public T pop() {\n        if(size == 0) \n            throw new EmptyStackException();\n        return elements[--size];\n    }\n}</code></pre>\n\n<p>上面的代码实现了一个栈（先进后出（FILO））结构，然而其中的pop方法却存在内存泄露的问题，当我们用pop方法弹出栈中的对象时，该对象不会被当作垃圾回收，即使使用栈的程序不再引用这些对象，因为栈内部维护着对这些对象的过期引用。在支持垃圾回收的语言中，内存泄露是很隐蔽的，这种内存泄露其实就是无意识的对象保持。如果一个对象引用被无意识的保留起来了，那么垃圾回收器不会处理这个对象，也不会处理该对象引用的其他对象，即使这样的对象只有少数几个，也可能会导致很多的对象被排除在垃圾回收之外，从而对性能造成重大影响，极端情况下会引发Disk Paging（物理内存与硬盘的虚拟内存交换数据），甚至造成OutOfMemoryError。</p>\n\n<p><strong>26、抽象的（abstract）方法是否可同时是静态的（static）,是否可同时是本地方法（native），是否可同时被synchronized修饰？</strong><br />\n答：都不能。抽象方法需要子类重写，而静态的方法是无法被重写的，因此二者是矛盾的。本地方法是由本地代码（如C代码）实现的方法，而抽象方法是没有实现的，也是矛盾的。synchronized和方法的实现细节有关，抽象方法不涉及实现细节，因此也是相互矛盾的。</p>\n\n<p><strong>27、阐述静态变量和实例变量的区别。</strong><br />\n答：静态变量是被static修饰符修饰的变量，它属于类，也称为类变量，一个类不管创建多少个对象，静态变量在内存中有且仅有一个拷贝；实例变量必须依存于某一实例，需要先创建对象然后通过对象才能访问到它。静态变量可以实现让多个对象共享内存。</p>\n\n<p><strong>28、是否可以从一个静态（static）方法内部发出对非静态（non-static）方法的调用？</strong><br />\n答：不可以，静态方法只能访问静态成员，因为非静态方法的调用要先创建对象，在调用静态方法时可能对象并没有被初始化。</p>\n\n<p><strong>29、如何实现对象克隆？</strong><br />\n答：有两种方式：<br />\n1). 实现Cloneable接口并重写Object类中的clone()方法；<br />\n2). 实现Serializable接口，通过对象的序列化和反序列化实现克隆，可以实现真正的深度克隆，代码如下。</p>\n\n<pre>\n<code>import java.io.ByteArrayInputStream;\nimport java.io.ByteArrayOutputStream;\nimport java.io.ObjectInputStream;\nimport java.io.ObjectOutputStream;\n \npublic class MyUtil {\n    public static &lt;T&gt; T clone(T obj) throws Exception {\n        ByteArrayOutputStream bout = new ByteArrayOutputStream();\n        ObjectOutputStream oos = new ObjectOutputStream(bout);\n        oos.writeObject(obj);\n \n        ByteArrayInputStream bin = new ByteArrayInputStream(bout.toByteArray());\n        ObjectInputStream ois = new ObjectInputStream(bin);\n        return (T) ois.readObject();\n    }\n}</code></pre>\n\n<p>下面是测试代码：</p>\n\n<pre>\n<code>import java.io.Serializable;\n \n//人类\nclass Person implements Serializable {\n    private static final long serialVersionUID = -9102017020286042305L;\n \n    private String name;    // 姓名\n    private int age;        // 年龄\n    private Car car;        // 座驾\n \n    public Person(String name, int age, Car car) {\n        this.name = name;\n        this.age = age;\n        this.car = car;\n    }\n \n    public String getName() {\n        return name;\n    }\n \n    public void setName(String name) {\n        this.name = name;\n    }\n \n    public int getAge() {\n        return age;\n    }\n \n    public void setAge(int age) {\n        this.age = age;\n    }\n \n    public Car getCar() {\n        return car;\n    }\n \n    public void setCar(Car car) {\n        this.car = car;\n    }\n \n    @Override\n    public String toString() {\n        return \"Person [name=\" + name + \", age=\" + age + \", car=\" + car + \"]\";\n    }\n \n}</code></pre>\n\n<pre>\n<code>//小汽车类\nclass Car implements Serializable {\n    private static final long serialVersionUID = -5713945027627603702L;\n \n    private String brand;       // 品牌\n    private int maxSpeed;       // 最高时速\n \n    public Car(String brand, int maxSpeed) {\n        this.brand = brand;\n        this.maxSpeed = maxSpeed;\n    }\n \n    public String getBrand() {\n        return brand;\n    }\n \n    public void setBrand(String brand) {\n        this.brand = brand;\n    }\n \n    public int getMaxSpeed() {\n        return maxSpeed;\n    }\n \n    public void setMaxSpeed(int maxSpeed) {\n        this.maxSpeed = maxSpeed;\n    }\n \n    @Override\n    public String toString() {\n        return \"Car [brand=\" + brand + \", maxSpeed=\" + maxSpeed + \"]\";\n    }\n \n}</code></pre>\n\n<pre>\n<code>class CloneTest {\n \n    public static void main(String[] args) {\n        try {\n            Person p1 = new Person(\"Hao LUO\", 33, new Car(\"Benz\", 300));\n            Person p2 = MyUtil.clone(p1);   // 深度克隆\n            p2.getCar().setBrand(\"BYD\");\n            // 修改克隆的Person对象p2关联的汽车对象的品牌属性\n            // 原来的Person对象p1关联的汽车不会受到任何影响\n            // 因为在克隆Person对象时其关联的汽车对象也被克隆了\n            System.out.println(p1);\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n    }\n}</code></pre>\n\n<blockquote>\n<p><strong>注意：</strong>基于序列化和反序列化实现的克隆不仅仅是深度克隆，更重要的是通过泛型限定，可以检查出要克隆的对象是否支持序列化，这项检查是编译器完成的，不是在运行时抛出异常，这种是方案明显优于使用Object类的clone方法克隆对象。让问题在编译的时候暴露出来总是优于把问题留到运行时。</p>\n</blockquote>\n\n<p>30、GC是什么？为什么要有GC？<br />\n答：GC是垃圾收集的意思，内存处理是编程人员容易出现问题的地方，忘记或者错误的内存回收会导致程序或系统的不稳定甚至崩溃，Java提供的GC功能可以自动监测对象是否超过作用域从而达到自动回收内存的目的，Java语言没有提供释放已分配内存的显示操作方法。Java程序员不用担心内存管理，因为垃圾收集器会自动进行管理。要请求垃圾收集，可以调用下面的方法之一：System.gc() 或Runtime.getRuntime().gc() ，但JVM可以屏蔽掉显示的垃圾回收调用。<br />\n垃圾回收可以有效的防止内存泄露，有效的使用可以使用的内存。垃圾回收器通常是作为一个单独的低优先级的线程运行，不可预知的情况下对内存堆中已经死亡的或者长时间没有使用的对象进行清除和回收，程序员不能实时的调用垃圾回收器对某个对象或所有对象进行垃圾回收。在Java诞生初期，垃圾回收是Java最大的亮点之一，因为服务器端的编程需要有效的防止内存泄露问题，然而时过境迁，如今Java的垃圾回收机制已经成为被诟病的东西。移动智能终端用户通常觉得iOS的系统比Android系统有更好的用户体验，其中一个深层次的原因就在于Android系统中垃圾回收的不可预知性。</p>\n\n<blockquote>\n<p><strong>补充：</strong>垃圾回收机制有很多种，包括：分代复制垃圾回收、标记垃圾回收、增量垃圾回收等方式。标准的Java进程既有栈又有堆。栈保存了原始型局部变量，堆保存了要创建的对象。Java平台对堆内存回收和再利用的基本算法被称为标记和清除，但是Java对其进行了改进，采用&ldquo;分代式垃圾收集&rdquo;。这种方法会跟Java对象的生命周期将堆内存划分为不同的区域，在垃圾收集过程中，可能会将对象移动到不同区域：<br />\n- 伊甸园（Eden）：这是对象最初诞生的区域，并且对大多数对象来说，这里是它们唯一存在过的区域。<br />\n- 幸存者乐园（Survivor）：从伊甸园幸存下来的对象会被挪到这里。<br />\n- 终身颐养园（Tenured）：这是足够老的幸存对象的归宿。年轻代收集（Minor-GC）过程是不会触及这个地方的。当年轻代收集不能把对象放进终身颐养园时，就会触发一次完全收集（Major-GC），这里可能还会牵扯到压缩，以便为大对象腾出足够的空间。</p>\n</blockquote>\n\n<p>与垃圾回收相关的JVM参数：</p>\n\n<blockquote>\n<ul>\n	<li>-Xms / -Xmx &mdash; 堆的初始大小 / 堆的最大大小</li>\n	<li>-Xmn &mdash; 堆中年轻代的大小</li>\n	<li>-XX:-DisableExplicitGC &mdash; 让System.gc()不产生任何作用</li>\n	<li>-XX:+PrintGCDetails &mdash; 打印GC的细节</li>\n	<li>-XX:+PrintGCDateStamps &mdash; 打印GC操作的时间戳</li>\n	<li>-XX:NewSize / XX:MaxNewSize &mdash; 设置新生代大小/新生代最大大小</li>\n	<li>-XX:NewRatio &mdash; 可以设置老生代和新生代的比例</li>\n	<li>-XX:PrintTenuringDistribution &mdash; 设置每次新生代GC后输出幸存者乐园中对象年龄的分布</li>\n	<li>-XX:InitialTenuringThreshold / -XX:MaxTenuringThreshold：设置老年代阀值的初始值和最大值</li>\n	<li>-XX:TargetSurvivorRatio：设置幸存区的目标使用率</li>\n</ul>\n</blockquote>\n\n<p><strong>31、String s = new String(&ldquo;xyz&rdquo;);创建了几个字符串对象？</strong><br />\n答：两个对象，一个是静态区的&rdquo;xyz&rdquo;，一个是用new创建在堆上的对象。</p>\n\n<p><strong>32、接口是否可继承（extends）接口？抽象类是否可实现（implements）接口？抽象类是否可继承具体类（concrete class）？</strong><br />\n答：接口可以继承接口，而且支持多重继承。抽象类可以实现接口，抽象类可继承具体类也可以继承抽象类。</p>\n\n<p><strong>33、一个&rdquo;.java&rdquo;源文件中是否可以包含多个类（不是内部类）？有什么限制？</strong><br />\n答：可以，但一个源文件中最多只能有一个公开类而且公开类的类名必须和文件名完全保持一致。</p>\n\n<p><strong>34、Anonymous Inner Class(匿名内部类)是否可以继承其它类？是否可以实现接口？</strong><br />\n答：可以继承其他类或实现其他接口，在Swing编程和Android开发中常用此方式来实现事件监听和回调。</p>\n\n<p><strong>35、内部类可以引用它的包含类（外部类）的成员吗？有没有什么限制？</strong><br />\n答：没有限制，一个内部类对象可以访问创建它的外部类对象的成员，包括私有成员。</p>\n\n<p><strong>36、Java 中的final关键字有哪些用法？</strong><br />\n答：1、修饰类：类不能被继承；2、修饰方法：方法不能被重写；3、修饰变量：变量一次赋值后值不能被修改（常量）。</p>\n\n<p><strong>37、指出下面程序的运行结果。</strong></p>\n\n<pre>\n<code>public class t1 {\n	public static void main(String[] args) {\n        A ab = new B();\n        ab = new B();\n    }	\n}\n\nclass A {\n    static {\n        System.out.print(\"1\");\n    }\n    {\n    	System.out.println(\"3\");\n    }\n    public A() {\n        System.out.print(\"2\");\n    }\n}\n \nclass B extends A{ \n    static {\n        System.out.print(\"a\");\n    }\n    {\n    	System.out.println(\"c\");\n    }\n    public B() {\n        System.out.print(\"b\");\n    }\n}</code></pre>\n\n<p>答：执行结果：1a32cb32cb。父类静态代码块&gt;子类静态代码块&gt;父类代码块&gt;父类构造器&gt;子类代码块&gt;子类构造器</p>\n\n<p><strong>38、数据类型之间的转换：</strong><br />\n<strong>- 如何将字符串转换为基本数据类型？</strong><br />\n<strong>- 如何将基本数据类型转换为字符串？</strong><br />\n答：<br />\n- 调用基本数据类型对应的包装类中的方法parseXXX(String)或valueOf(String)即可返回相应基本类型；<br />\n- 一种方法是将基本数据类型与空字符串（&rdquo;&quot;）连接（+）即可获得其所对应的字符串；另一种方法是调用String 类中的valueOf()方法返回相应字符串</p>\n\n<p><strong>39、如何实现字符串的反转及替换？</strong><br />\n答：方法很多，可以自己写实现也可以使用String或StringBuffer/StringBuilder中的方法。有一道很常见的面试题是用递归实现字符串反转，代码如下所示：</p>\n\n<pre>\n<code>public static String reverse(String originStr) {\n      if(originStr == null || originStr.length() &lt;= 1) \n          return originStr;\n      return reverse(originStr.substring(1)) + originStr.charAt(0);\n}</code></pre>\n\n<p><strong>40、怎样将GB2312编码的字符串转换为ISO-8859-1编码的字符串？</strong><br />\n答：代码如下所示：</p>\n\n<pre>\n<code>String s1 = \"你好\";\nString s2 = new String(s1.getBytes(\"GB2312\"), \"ISO-8859-1\");</code></pre>\n\n<p><strong>41、日期和时间：</strong><br />\n<strong>- 如何取得年月日、小时分钟秒？</strong><br />\n<strong>- 如何取得从1970年1月1日0时0分0秒到现在的毫秒数？</strong><br />\n<strong>- 如何取得某月的最后一天？</strong><br />\n<strong>- 如何格式化日期？</strong><br />\n答：<br />\n问题1：创建java.util.Calendar 实例，调用其get()方法传入不同的参数即可获得参数所对应的值。Java 8中可以使用java.time.LocalDateTimel来获取，代码如下所示。</p>\n\n<pre>\n<code>public class DateTimeTest {\n    public static void main(String[] args) {\n        Calendar cal = Calendar.getInstance();\n        System.out.println(cal.get(Calendar.YEAR));\n        System.out.println(cal.get(Calendar.MONTH));    // 0 - 11\n        System.out.println(cal.get(Calendar.DATE));\n        System.out.println(cal.get(Calendar.HOUR_OF_DAY));\n        System.out.println(cal.get(Calendar.MINUTE));\n        System.out.println(cal.get(Calendar.SECOND));\n \n        // Java 8\n        LocalDateTime dt = LocalDateTime.now();\n        System.out.println(dt.getYear());\n        System.out.println(dt.getMonthValue());     // 1 - 12\n        System.out.println(dt.getDayOfMonth());\n        System.out.println(dt.getHour());\n        System.out.println(dt.getMinute());\n        System.out.println(dt.getSecond());\n    }\n}</code></pre>\n\n<p>问题2：以下方法均可获得该毫秒数。</p>\n\n<pre>\n<code>Calendar.getInstance().getTimeInMillis();\nSystem.currentTimeMillis();\nClock.systemDefaultZone().millis(); // Java 8</code></pre>\n\n<p>问题3：代码如下所示。</p>\n\n<pre>\n<code>Calendar time = Calendar.getInstance();\ntime.getActualMaximum(Calendar.DAY_OF_MONTH);</code></pre>\n\n<p>问题4：利用java.text.DataFormat 的子类（如SimpleDateFormat类）中的format(Date)方法可将日期格式化。Java 8中可以用java.time.format.DateTimeFormatter来格式化时间日期，代码如下所示。</p>\n\n<pre>\n<code>import java.text.SimpleDateFormat;\nimport java.time.LocalDate;\nimport java.time.format.DateTimeFormatter;\nimport java.util.Date;\n \nclass DateFormatTest {\n \n    public static void main(String[] args) {\n        SimpleDateFormat oldFormatter = new SimpleDateFormat(\"yyyy/MM/dd\");\n        Date date1 = new Date();\n        System.out.println(oldFormatter.format(date1));\n \n        // Java 8\n        DateTimeFormatter newFormatter = DateTimeFormatter.ofPattern(\"yyyy/MM/dd\");\n        LocalDate date2 = LocalDate.now();\n        System.out.println(date2.format(newFormatter));\n    }\n}</code></pre>\n\n<p>补充：Java的时间日期API一直以来都是被诟病的东西，为了解决这一问题，Java 8中引入了新的时间日期API，其中包括LocalDate、LocalTime、LocalDateTime、Clock、Instant等类，这些的类的设计都使用了不变模式，因此是线程安全的设计。如果不理解这些内容</p>\n\n<p><strong>42、打印昨天的当前时刻。</strong></p>\n\n<pre>\n<code>import java.util.Calendar;\n \nclass YesterdayCurrent {\n    public static void main(String[] args){\n        Calendar cal = Calendar.getInstance();\n        cal.add(Calendar.DATE, -1);\n        System.out.println(cal.getTime());\n    }\n}</code></pre>\n\n<p>在Java 8中，可以用下面的代码实现相同的功能。</p>\n\n<pre>\n<code>import java.time.LocalDateTime;\n \nclass YesterdayCurrent {\n \n    public static void main(String[] args) {\n        LocalDateTime today = LocalDateTime.now();\n        LocalDateTime yesterday = today.minusDays(1);\n \n        System.out.println(yesterday);\n    }\n}</code></pre>\n\n<p><strong>43、比较一下Java和JavaSciprt。</strong><br />\n答：JavaScript 与Java是两个公司开发的不同的两个产品。Java 是原Sun Microsystems公司推出的面向对象的程序设计语言，特别适合于互联网应用程序开发；而JavaScript是Netscape公司的产品，为了扩展Netscape浏览器的功能而开发的一种可以嵌入Web页面中运行的基于对象和事件驱动的解释性语言。JavaScript的前身是LiveScript；而Java的前身是Oak语言。<br />\n下面对两种语言间的异同作如下比较：<br />\n- 基于对象和面向对象：Java是一种真正的面向对象的语言，即使是开发简单的程序，必须设计对象；JavaScript是种脚本语言，它可以用来制作与网络无关的，与用户交互作用的复杂软件。它是一种基于对象（Object-Based）和事件驱动（Event-Driven）的编程语言，因而它本身提供了非常丰富的内部对象供设计人员使用。<br />\n- 解释和编译：Java的源代码在执行之前，必须经过编译。JavaScript是一种解释性编程语言，其源代码不需经过编译，由浏览器解释执行。（目前的浏览器几乎都使用了JIT（即时编译）技术来提升JavaScript的运行效率）<br />\n- 强类型变量和类型弱变量：Java采用强类型变量检查，即所有变量在编译之前必须作声明；JavaScript中变量是弱类型的，甚至在使用变量前可以不作声明，JavaScript的解释器在运行时检查推断其数据类型。<br />\n- 代码格式不一样。</p>\n\n<blockquote>\n<p><strong>补充：</strong>上面列出的四点是网上流传的所谓的标准答案。其实Java和JavaScript最重要的区别是一个是静态语言，一个是动态语言。目前的编程语言的发展趋势是函数式语言和动态语言。在Java中类（class）是一等公民，而JavaScript中函数（function）是一等公民，因此JavaScript支持函数式编程，可以使用Lambda函数和闭包（closure），当然Java 8也开始支持函数式编程，提供了对Lambda表达式以及函数式接口的支持。对于这类问题，在面试的时候最好还是用自己的语言回答会更加靠谱，不要背网上所谓的标准答案。</p>\n</blockquote>\n\n<p><strong>44、什么时候用断言（assert）？</strong><br />\n答：断言在软件开发中是一种常用的调试方式，很多开发语言中都支持这种机制。一般来说，断言用于保证程序最基本、关键的正确性。断言检查通常在开发和测试时开启。为了保证程序的执行效率，在软件发布后断言检查通常是关闭的。断言是一个包含布尔表达式的语句，在执行这个语句时假定该表达式为true；如果表达式的值为false，那么系统会报告一个AssertionError。断言的使用如下面的代码所示：</p>\n\n<pre>\n<code>assert(a &gt; 0); // throws an AssertionError if a &lt;= 0</code></pre>\n\n<p>断言可以有两种形式：<br />\nassert Expression1;<br />\nassert Expression1 : Expression2 ;<br />\nExpression1 应该总是产生一个布尔值。<br />\nExpression2 可以是得出一个值的任意表达式；这个值用于生成显示更多调试信息的字符串消息。</p>\n\n<p>要在运行时启用断言，可以在启动JVM时使用-enableassertions或者-ea标记。要在运行时选择禁用断言，可以在启动JVM时使用-da或者-disableassertions标记。要在系统类中启用或禁用断言，可使用-esa或-dsa标记。还可以在包的基础上启用或者禁用断言。</p>\n\n<blockquote>\n<p><strong>注意：</strong>断言不应该以任何方式改变程序的状态。简单的说，如果希望在不满足某些条件时阻止代码的执行，就可以考虑用断言来阻止它。</p>\n</blockquote>\n\n<p><strong>45、Error和Exception有什么区别？</strong><br />\n答：Error表示系统级的错误和程序不必处理的异常，是恢复不是不可能但很困难的情况下的一种严重问题；比如内存溢出，不可能指望程序能处理这样的情况；Exception表示需要捕捉或者需要程序进行处理的异常，是一种设计或实现问题；也就是说，它表示如果程序运行正常，从不会发生的情况。</p>\n\n<blockquote>\n<p>面试题：2005年摩托罗拉的面试中曾经问过这么一个问题&ldquo;If a process reports a stack overflow run-time error, what&rsquo;s the most possible cause?&rdquo;，给了四个选项a. lack of memory; b. write on an invalid memory space; c. recursive function calling; d. array index out of boundary. Java程序在运行时也可能会遭遇StackOverflowError，这是一个无法恢复的错误，只能重新修改代码了，这个面试题的答案是c。如果写了不能迅速收敛的递归，则很有可能引发栈溢出的错误，如下所示：</p>\n</blockquote>\n\n<pre>\n<code>class StackOverflowErrorTest {\n \n    public static void main(String[] args) {\n        main(null);\n    }\n}</code></pre>\n\n<blockquote>\n<p><strong>提示：</strong>用递归编写程序时一定要牢记两点：1. 递归公式；2. 收敛条件（什么时候就不再继续递归）。</p>\n</blockquote>\n\n<p><strong>46、try{}里有一个return语句，那么紧跟在这个try后的finally{}里的代码会不会被执行，什么时候被执行，在return前还是后?</strong><br />\n答：会执行，在方法返回调用者前执行。</p>\n\n<blockquote>\n<p><strong>注意：</strong>在finally中改变返回值的做法是不好的，因为如果存在finally代码块，try中的return语句不会立马返回调用者，而是记录下返回值待finally代码块执行完毕之后再向调用者返回其值，然后如果在finally中修改了返回值，就会返回修改后的值。显然，在finally中返回或者修改返回值会对程序造成很大的困扰，C#中直接用编译错误的方式来阻止程序员干这种龌龊的事情。</p>\n</blockquote>\n\n<p><strong>47、Java语言如何进行异常处理，关键字：throws、throw、try、catch、finally分别如何使用？</strong><br />\n答：Java通过面向对象的方法进行异常处理，把各种不同的异常进行分类，并提供了良好的接口。在Java中，每个异常都是一个对象，它是Throwable类或其子类的实例。当一个方法出现异常后便抛出一个异常对象，该对象中包含有异常信息，调用这个对象的方法可以捕获到这个异常并可以对其进行处理。Java的异常处理是通过5个关键词来实现的：try、catch、throw、throws和finally。一般情况下是用try来执行一段程序，如果系统会抛出（throw）一个异常对象，可以通过它的类型来捕获（catch）它，或通过总是执行代码块（finally）来处理；try用来指定一块预防所有异常的程序；catch子句紧跟在try块后面，用来指定你想要捕获的异常的类型；throw语句用来明确地抛出一个异常；throws用来声明一个方法可能抛出的各种异常（当然声明异常时允许无病呻吟）；finally为确保一段代码不管发生什么异常状况都要被执行；try语句可以嵌套，每当遇到一个try语句，异常的结构就会被放入异常栈中，直到所有的try语句都完成。如果下一级的try语句没有对某种异常进行处理，异常栈就会执行出栈操作，直到遇到有处理这种异常的try语句或者最终将异常抛给JVM。</p>\n\n<p><strong>48、运行时异常与受检异常有何异同？</strong><br />\n答：异常表示程序运行过程中可能出现的非正常状态，运行时异常表示虚拟机的通常操作中可能遇到的异常，是一种常见运行错误，只要程序设计得没有问题通常就不会发生。受检异常跟程序运行的上下文环境有关，即使程序设计无误，仍然可能因使用的问题而引发。Java编译器要求方法必须声明抛出可能发生的受检异常，但是并不要求必须声明抛出未被捕获的运行时异常。异常和继承一样，是面向对象程序设计中经常被滥用的东西，在<em>Effective Java</em>中对异常的使用给出了以下指导原则：<br />\n- 不要将异常处理用于正常的控制流（设计良好的API不应该强迫它的调用者为了正常的控制流而使用异常）<br />\n- 对可以恢复的情况使用受检异常，对编程错误使用运行时异常<br />\n- 避免不必要的使用受检异常（可以通过一些状态检测手段来避免异常的发生）<br />\n- 优先使用标准的异常<br />\n- 每个方法抛出的异常都要有文档<br />\n- 保持异常的原子性<br />\n- 不要在catch中忽略掉捕获到的异常</p>\n\n<p><strong>49、列出一些你常见的运行时异常？</strong><br />\n答：<br />\n- ArithmeticException（算术异常）<br />\n- ClassCastException （类转换异常）<br />\n- IllegalArgumentException （非法参数异常）<br />\n- IndexOutOfBoundsException （下标越界异常）<br />\n- NullPointerException （空指针异常）<br />\n- SecurityException （安全异常）</p>\n\n<p><strong>50、阐述final、finally、finalize的区别。</strong><br />\n答：<br />\n- final：修饰符（关键字）有三种用法：如果一个类被声明为final，意味着它不能再派生出新的子类，即不能被继承，因此它和abstract是反义词。将变量声明为final，可以保证它们在使用中不被改变，被声明为final的变量必须在声明时给定初值，而在以后的引用中只能读取不可修改。被声明为final的方法也同样只能使用，不能在子类中被重写。<br />\n- finally：通常放在try&hellip;catch&hellip;的后面构造总是执行代码块，这就意味着程序无论正常执行还是发生异常，这里的代码只要JVM不关闭都能执行，可以将释放外部资源的代码写在finally块中。<br />\n- finalize：Object类中定义的方法，Java中允许使用finalize()方法在垃圾收集器将对象从内存中清除出去之前做必要的清理工作。这个方法是由垃圾收集器在销毁对象时调用的，通过重写finalize()方法可以整理系统资源或者执行其他清理工作。</p>\n\n<p><strong>51、</strong>说出下面代码的运行结果</p>\n\n<pre>\n<code>class Annoyance extends Exception {}\nclass Sneeze extends Annoyance {}\nclass Human {\n//Caught Annoyance\n//Caught Sneeze\n//Hello World!\n    public static void main(String[] args) \n        throws Exception {\n        try {\n            try {\n                throw new Sneeze();\n            } catch ( Annoyance a ) {\n                System.out.println(\"Caught Annoyance\");\n                throw a;\n            }\n        } catch ( Sneeze s ) {\n            System.out.println(\"Caught Sneeze\");\n            return ;\n        } finally {\n            System.out.println(\"Hello World!\");\n        }\n    }\n}</code></pre>\n\n<p><strong>52、List、Set、Map是否继承自Collection接口？</strong><br />\n答：List、Set 是，Map 不是。Map是键值对映射容器，与List和Set有明显的区别，而Set存储的零散的元素且不允许有重复元素（数学中的集合也是如此），List是线性结构的容器，适用于按数值索引访问元素的情形。</p>\n\n<p><strong>53、阐述ArrayList、Vector、LinkedList的存储性能和特性。</strong><br />\n答：ArrayList 和Vector都是使用数组方式存储数据，此数组元素数大于实际存储的数据以便增加和插入元素，它们都允许直接按序号索引元素，但是插入元素要涉及数组元素移动等内存操作，所以索引数据快而插入数据慢，Vector中的方法由于添加了synchronized修饰，因此Vector是线程安全的容器，但性能上较ArrayList差，因此已经是Java中的遗留容器。LinkedList使用双向链表实现存储（将内存中零散的内存单元通过附加的引用关联起来，形成一个可以按序号索引的线性结构，这种链式存储方式与数组的连续存储方式相比，内存的利用率更高），按序号索引数据需要进行前向或后向遍历，但是插入数据时只需要记录本项的前后项即可，所以插入速度较快。Vector属于遗留容器（Java早期的版本中提供的容器，除此之外，Hashtable、Dictionary、BitSet、Stack、Properties都是遗留容器），已经不推荐使用，但是由于ArrayList和LinkedListed都是非线程安全的，如果遇到多个线程操作同一个容器的场景，则可以通过工具类Collections中的synchronizedList方法将其转换成线程安全的容器后再使用（这是对装潢模式的应用，将已有对象传入另一个类的构造器中创建新的对象来增强实现）。</p>\n\n<blockquote>\n<p><strong>补充：</strong>遗留容器中的Properties类和Stack类在设计上有严重的问题，Properties是一个键和值都是字符串的特殊的键值对映射，在设计上应该是关联一个Hashtable并将其两个泛型参数设置为String类型，但是Java API中的Properties直接继承了Hashtable，这很明显是对继承的滥用。这里复用代码的方式应该是Has-A关系而不是Is-A关系，另一方面容器都属于工具类，继承工具类本身就是一个错误的做法，使用工具类最好的方式是Has-A关系（关联）或Use-A关系（依赖）。同理，Stack类继承Vector也是不正确的。</p>\n</blockquote>\n\n<p><strong>54、Collection和Collections的区别？</strong><br />\n答：Collection是一个接口，它是Set、List等容器的父接口；Collections是个一个工具类，提供了一系列的静态方法来辅助容器操作，这些方法包括对容器的搜索、排序、线程安全化等等。</p>\n\n<p><strong>55、List、Map、Set三个接口存取元素时，各有什么特点？</strong><br />\n答：List以特定索引来存取元素，可以有重复元素。Set不能存放重复元素（用对象的equals()方法来区分元素是否重复）。Map保存键值对（key-value pair）映射，映射关系可以是一对一或多对一。Set和Map容器都有基于哈希存储和排序树的两种实现版本，基于哈希存储的版本理论存取时间复杂度为O(1)，而基于排序树版本的实现在插入或删除元素时会按照元素或元素的键（key）构成排序树从而达到排序和去重的效果。</p>\n\n<p><strong>56、TreeMap和TreeSet在排序时如何比较元素？Collections工具类中的sort()方法如何比较元素？</strong><br />\n答：TreeSet要求存放的对象所属的类必须实现Comparable接口，该接口提供了比较元素的compareTo()方法，当插入元素时会回调该方法比较元素的大小。TreeMap要求存放的键值对映射的键必须实现Comparable接口从而根据键对元素进行排序。Collections工具类的sort方法有两种重载的形式，第一种要求传入的待排序容器中存放的对象比较实现Comparable接口以实现元素的比较；第二种不强制性的要求容器中的元素必须可比较，但是要求传入第二个参数，参数是Comparator接口的子类型（需要重写compare方法实现元素的比较），相当于一个临时定义的排序规则，其实就是通过接口注入比较元素大小的算法，也是对回调模式的应用（Java中对函数式编程的支持）。<br />\n例子1：</p>\n\n<pre>\n<code>public class Student implements Comparable&lt;Student&gt; {\n    private String name;        // 姓名\n    private int age;            // 年龄\n \n    public Student(String name, int age) {\n        this.name = name;\n        this.age = age;\n    }\n \n    @Override\n    public String toString() {\n        return \"Student [name=\" + name + \", age=\" + age + \"]\";\n    }\n \n    @Override\n    public int compareTo(Student o) {\n        return this.age - o.age; // 比较年龄(年龄的升序)\n    }\n \n}</code></pre>\n\n<pre>\n<code>import java.util.Set;\nimport java.util.TreeSet;\n \nclass Test01 {\n \n    public static void main(String[] args) {\n        Set&lt;Student&gt; set = new TreeSet&lt;&gt;();     // Java 7的钻石语法(构造器后面的尖括号中不需要写类型)\n        set.add(new Student(\"Hao LUO\", 33));\n        set.add(new Student(\"XJ WANG\", 32));\n        set.add(new Student(\"Bruce LEE\", 60));\n        set.add(new Student(\"Bob YANG\", 22));\n \n        for(Student stu : set) {\n            System.out.println(stu);\n        }\n//      输出结果: \n//      Student [name=Bob YANG, age=22]\n//      Student [name=XJ WANG, age=32]\n//      Student [name=Hao LUO, age=33]\n//      Student [name=Bruce LEE, age=60]\n    }\n}</code></pre>\n\n<pre>\n<code>public class Student {\n    private String name;    // 姓名\n    private int age;        // 年龄\n    public Student(String name, int age) {\n        this.name = name;\n        this.age = age;\n    }\n    //获取学生姓名\n    public String getName() {\n        return name;\n    }\n    //获取学生年龄\n    public int getAge() {\n        return age;\n    }\n \n    @Override\n    public String toString() {\n        return \"Student [name=\" + name + \", age=\" + age + \"]\";\n    }\n}</code></pre>\n\n<pre>\n<code>import java.util.ArrayList;\nimport java.util.Collections;\nimport java.util.Comparator;\nimport java.util.List;\n \nclass Test02 {\n \n    public static void main(String[] args) {\n        List&lt;Student&gt; list = new ArrayList&lt;&gt;();     // Java 7的钻石语法(构造器后面的尖括号中不需要写类型)\n        list.add(new Student(\"Hao LUO\", 33));\n        list.add(new Student(\"XJ WANG\", 32));\n        list.add(new Student(\"Bruce LEE\", 60));\n        list.add(new Student(\"Bob YANG\", 22));\n \n        // 通过sort方法的第二个参数传入一个Comparator接口对象\n        // 相当于是传入一个比较对象大小的算法到sort方法中\n        // 由于Java中没有函数指针、仿函数、委托这样的概念\n        // 因此要将一个算法传入一个方法中唯一的选择就是通过接口回调\n        Collections.sort(list, new Comparator&lt;Student&gt; () {\n \n            @Override\n            public int compare(Student o1, Student o2) {\n                return o1.getName().compareTo(o2.getName());    // 比较学生姓名\n            }\n        });\n \n        for(Student stu : list) {\n            System.out.println(stu);\n        }\n//      输出结果: \n//      Student [name=Bob YANG, age=22]\n//      Student [name=Bruce LEE, age=60]\n//      Student [name=Hao LUO, age=33]\n//      Student [name=XJ WANG, age=32]\n    }\n}</code></pre>\n\n<p><strong>57、Thread类的sleep()方法和对象的wait()方法都可以让线程暂停执行，它们有什么区别?</strong><br />\n答：sleep()方法（休眠）是线程类（Thread）的静态方法，调用此方法会让当前线程暂停执行指定的时间，将执行机会（CPU）让给其他线程，但是对象的锁依然保持，因此休眠时间结束后会自动恢复（线程回到就绪状态，请参考第66题中的线程状态转换图）。wait()是Object类的方法，调用对象的wait()方法导致当前线程放弃对象的锁（线程暂停执行），进入对象的等待池（wait pool），只有调用对象的notify()方法（或notifyAll()方法）时才能唤醒等待池中的线程进入等锁池（lock pool），如果线程重新获得对象的锁就可以进入就绪状态。</p>\n\n<blockquote>\n<p><strong>补充：</strong>可能不少人对什么是进程，什么是线程还比较模糊，对于为什么需要多线程编程也不是特别理解。简单的说：进程是具有一定独立功能的程序关于某个数据集合上的一次运行活动，是操作系统进行资源分配和调度的一个独立单位；线程是进程的一个实体，是CPU调度和分派的基本单位，是比进程更小的能独立运行的基本单位。线程的划分尺度小于进程，这使得多线程程序的并发性高；进程在执行时通常拥有独立的内存单元，而线程之间可以共享内存。使用多线程的编程通常能够带来更好的性能和用户体验，但是多线程的程序对于其他程序是不友好的，因为它可能占用了更多的CPU资源。当然，也不是线程越多，程序的性能就越好，因为线程之间的调度和切换也会浪费CPU时间。时下很时髦的Node.js就采用了单线程异步I/O的工作模式。</p>\n</blockquote>\n\n<p><strong>58、线程的sleep()方法和yield()方法有什么区别？</strong><br />\n答：<br />\n① sleep()方法给其他线程运行机会时不考虑线程的优先级，因此会给低优先级的线程以运行的机会；yield()方法只会给相同优先级或更高优先级的线程以运行的机会；<br />\n② 线程执行sleep()方法后转入阻塞（blocked）状态，而执行yield()方法后转入就绪（ready）状态；<br />\n③ sleep()方法声明抛出InterruptedException，而yield()方法没有声明任何异常；<br />\n④ sleep()方法比yield()方法（跟操作系统CPU调度相关）具有更好的可移植性。</p>\n\n<p><strong>59、当一个线程进入一个对象的synchronized方法A之后，其它线程是否可进入此对象的synchronized方法B？</strong><br />\n答：不能。其它线程只能访问该对象的非同步方法，同步方法则不能进入。因为非静态方法上的synchronized修饰符要求执行方法时要获得对象的锁，如果已经进入A方法说明对象锁已经被取走，那么试图进入B方法的线程就只能在等锁池（<strong>注意不是等待池哦</strong>）中等待对象的锁。</p>\n\n<p><strong>60、请说出与线程同步以及线程调度相关的方法。</strong><br />\n答：<br />\n- wait()：使一个线程处于等待（阻塞）状态，并且释放所持有的对象的锁；<br />\n- sleep()：使一个正在运行的线程处于睡眠状态，是一个静态方法，调用此方法要处理InterruptedException异常；<br />\n- notify()：唤醒一个处于等待状态的线程，当然在调用此方法的时候，并不能确切的唤醒某一个等待状态的线程，而是由JVM确定唤醒哪个线程，而且与优先级无关；<br />\n- notityAll()：唤醒所有处于等待状态的线程，该方法并不是将对象的锁给所有线程，而是让它们竞争，只有获得锁的线程才能进入就绪状态；</p>\n\n<blockquote>\n<p>补充：Java 5通过Lock接口提供了显式的锁机制（explicit lock），增强了灵活性以及对线程的协调。Lock接口中定义了加锁（lock()）和解锁（unlock()）的方法，同时还提供了newCondition()方法来产生用于线程之间通信的Condition对象；此外，Java 5还提供了信号量机制（semaphore），信号量可以用来限制对某个共享资源进行访问的线程的数量。在对资源进行访问之前，线程必须得到信号量的许可（调用Semaphore对象的acquire()方法）；在完成对资源的访问后，线程必须向信号量归还许可（调用Semaphore对象的release()方法）。</p>\n</blockquote>\n\n<p>下面的例子演示了100个线程同时向一个银行账户中存入1元钱，在没有使用同步机制和使用同步机制情况下的执行情况。</p>\n\n<ul>\n	<li>银行账户类：</li>\n</ul>\n\n<pre>\n<code>//银行账户\npublic class Account {\n    private double balance;     // 账户余额\n \n     // 存款\n     // @param money 存入金额\n    public void deposit(double money) {\n        double newBalance = balance + money;\n        try {\n            Thread.sleep(10);   // 模拟此业务需要一段处理时间\n        }\n        catch(InterruptedException ex) {\n            ex.printStackTrace();\n        }\n        balance = newBalance;\n    }\n\n    //获得账户余额\n    public double getBalance() {\n        return balance;\n    }\n}</code></pre>\n\n<ul>\n	<li>存钱线程类：</li>\n</ul>\n\n<pre>\n<code>//存钱线程\npublic class AddMoneyThread implements Runnable {\n    private Account account;    // 存入账户\n    private double money;       // 存入金额\n \n    public AddMoneyThread(Account account, double money) {\n        this.account = account;\n        this.money = money;\n    }\n \n    @Override\n    public void run() {\n        account.deposit(money);\n    } \n}</code></pre>\n\n<ul>\n	<li>测试类：</li>\n</ul>\n\n<pre>\n<code>import java.util.concurrent.ExecutorService;\nimport java.util.concurrent.Executors;\n \npublic class Test01 {\n \n    public static void main(String[] args) {\n        Account account = new Account();\n        ExecutorService service = Executors.newFixedThreadPool(100);\n        for(int i = 1; i &lt;= 100; i++) {\n            service.execute(new AddMoneyThread(account, 1));\n        }\n        service.shutdown();\n        while(!service.isTerminated()) {}\n        System.out.println(\"账户余额: \" + account.getBalance());\n    }\n}</code></pre>\n\n<p>在没有同步的情况下，执行结果通常是显示账户余额在10元以下，出现这种状况的原因是，当一个线程A试图存入1元的时候，另外一个线程B也能够进入存款的方法中，线程B读取到的账户余额仍然是线程A存入1元钱之前的账户余额，因此也是在原来的余额0上面做了加1元的操作，同理线程C也会做类似的事情，所以最后100个线程执行结束时，本来期望账户余额为100元，但实际得到的通常在10元以下（很可能是1元哦）。解决这个问题的办法就是同步，当一个线程对银行账户存钱时，需要将此账户锁定，待其操作完成后才允许其他的线程进行操作，代码有如下几种调整方案：</p>\n\n<ul>\n	<li>在银行账户的存款（deposit）方法上同步（synchronized）关键字</li>\n</ul>\n\n<pre>\n<code>//银行账户\npublic class Account {\n    private double balance;     // 账户余额\n \n    /**\n     * 存款\n     * @param money 存入金额\n     */\n    public synchronized void deposit(double money) {\n        double newBalance = balance + money;\n        try {\n            Thread.sleep(10);   // 模拟此业务需要一段处理时间\n        }\n        catch(InterruptedException ex) {\n            ex.printStackTrace();\n        }\n        balance = newBalance;\n    }\n \n    /**\n     * 获得账户余额\n     */\n    public double getBalance() {\n        return balance;\n    }\n}</code></pre>\n\n<ul>\n	<li>在线程调用存款方法时对银行账户进行同步</li>\n</ul>\n\n<pre>\n<code>/**\n * 存钱线程\n * @author 骆昊\n *\n */\npublic class AddMoneyThread implements Runnable {\n    private Account account;    // 存入账户\n    private double money;       // 存入金额\n \n    public AddMoneyThread(Account account, double money) {\n        this.account = account;\n        this.money = money;\n    }\n \n    @Override\n    public void run() {\n        synchronized (account) {\n            account.deposit(money); \n        }\n    }\n}</code></pre>\n\n<ul>\n	<li>通过Java 5显示的锁机制，为每个银行账户创建一个锁对象，在存款操作进行加锁和解锁的操作</li>\n</ul>\n\n<pre>\n<code>import java.util.concurrent.locks.Lock;\nimport java.util.concurrent.locks.ReentrantLock;\n \n/**\n * 银行账户\n */\npublic class Account {\n    private Lock accountLock = new ReentrantLock();\n    private double balance; // 账户余额\n \n    /**\n     * 存款\n     * @param money\n     * 存入金额\n     */\n    public void deposit(double money) {\n        accountLock.lock();\n        try {\n            double newBalance = balance + money;\n            try {\n                Thread.sleep(10); // 模拟此业务需要一段处理时间\n            }\n            catch (InterruptedException ex) {\n                ex.printStackTrace();\n            }\n            balance = newBalance;\n        }\n        finally {\n            accountLock.unlock();\n        }\n    }\n \n    /**\n     * 获得账户余额\n     */\n    public double getBalance() {\n        return balance;\n    }\n}</code></pre>\n\n<p>按照上述三种方式对代码进行修改后，重写执行测试代码Test01，将看到最终的账户余额为100元。当然也可以使用Semaphore或CountdownLatch来实现同步。</p>\n\n<p><strong>61、编写多线程程序有几种实现方式？</strong><br />\n答：Java 5以前实现多线程有两种实现方法：一种是继承Thread类；另一种是实现Runnable接口。两种方式都要通过重写run()方法来定义线程的行为，推荐使用后者，因为Java中的继承是单继承，一个类有一个父类，如果继承了Thread类就无法再继承其他类了，显然使用Runnable接口更为灵活。</p>\n\n<blockquote>\n<p>补充：Java 5以后创建线程还有第三种方式：实现Callable接口，该接口中的call方法可以在线程执行结束时产生一个返回值，代码如下所示：</p>\n</blockquote>\n\n<pre>\n<code>import java.util.ArrayList;\nimport java.util.List;\nimport java.util.concurrent.Callable;\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.Executors;\nimport java.util.concurrent.Future;\n \nclass MyTask implements Callable&lt;Integer&gt; {\n    private int upperBounds;\n \n    public MyTask(int upperBounds) {\n        this.upperBounds = upperBounds;\n    }\n \n    @Override\n    public Integer call() throws Exception {\n        int sum = 0; \n        for(int i = 1; i &lt;= upperBounds; i++) {\n            sum += i;\n        }\n        return sum;\n    }\n}\n \nclass Test {\n \n    public static void main(String[] args) throws Exception {\n        List&lt;Future&lt;Integer&gt;&gt; list = new ArrayList&lt;&gt;();\n        ExecutorService service = Executors.newFixedThreadPool(10);\n        for(int i = 0; i &lt; 10; i++) {\n            list.add(service.submit(new MyTask((int) (Math.random() * 100))));\n        }\n \n        int sum = 0;\n        for(Future&lt;Integer&gt; future : list) {\n            // while(!future.isDone()) ;\n            sum += future.get();\n        }\n \n        System.out.println(sum);\n    }\n}</code></pre>\n\n<p><strong>62、synchronized关键字的用法？</strong><br />\n答：synchronized关键字可以将对象或者方法标记为同步，以实现对对象和方法的互斥访问，可以用synchronized(对象) { &hellip; }定义同步代码块，或者在声明方法时将synchronized作为方法的修饰符。</p>\n\n<p><strong>63、举例说明同步和异步。</strong><br />\n答：如果系统中存在临界资源（资源数量少于竞争资源的线程数量的资源），例如正在写的数据以后可能被另一个线程读到，或者正在读的数据可能已经被另一个线程写过了，那么这些数据就必须进行同步存取（数据库操作中的排他锁就是最好的例子）。当应用程序在对象上调用了一个需要花费很长时间来执行的方法，并且不希望让程序等待方法的返回时，就应该使用异步编程，在很多情况下采用异步途径往往更有效率。事实上，所谓的同步就是指阻塞式操作，而异步就是非阻塞式操作。</p>\n\n<p><strong>64、启动一个线程是调用run()还是start()方法？</strong><br />\n答：启动一个线程是调用start()方法，使线程所代表的虚拟处理机处于可运行状态，这意味着它可以由JVM 调度并执行，这并不意味着线程就会立即运行。run()方法是线程启动后要进行回调的方法。</p>\n\n<p><strong>65、什么是线程池（thread pool）？</strong><br />\n答：在面向对象编程中，创建和销毁对象是很费时间的，因为创建一个对象要获取内存资源或者其它更多资源。在Java中更是如此，虚拟机将试图跟踪每一个对象，以便能够在对象销毁后进行垃圾回收。所以提高服务程序效率的一个手段就是尽可能减少创建和销毁对象的次数，特别是一些很耗资源的对象创建和销毁，这就是&rdquo;池化资源&rdquo;技术产生的原因。线程池顾名思义就是事先创建若干个可执行的线程放入一个池（容器）中，需要的时候从池中获取线程不用自行创建，使用完毕不需要销毁线程而是放回池中，从而减少创建和销毁线程对象的开销。<br />\nJava 5+中的Executor接口定义一个执行线程的工具。它的子类型即线程池接口是ExecutorService。要配置一个线程池是比较复杂的，尤其是对于线程池的原理不是很清楚的情况下，因此在工具类Executors面提供了一些静态工厂方法，生成一些常用的线程池，如下所示：<br />\n- newSingleThreadExecutor：创建一个单线程的线程池。这个线程池只有一个线程在工作，也就是相当于单线程串行执行所有任务。如果这个唯一的线程因为异常结束，那么会有一个新的线程来替代它。此线程池保证所有任务的执行顺序按照任务的提交顺序执行。<br />\n- newFixedThreadPool：创建固定大小的线程池。每次提交一个任务就创建一个线程，直到线程达到线程池的最大大小。线程池的大小一旦达到最大值就会保持不变，如果某个线程因为执行异常而结束，那么线程池会补充一个新线程。<br />\n- newCachedThreadPool：创建一个可缓存的线程池。如果线程池的大小超过了处理任务所需要的线程，那么就会回收部分空闲（60秒不执行任务）的线程，当任务数增加时，此线程池又可以智能的添加新线程来处理任务。此线程池不会对线程池大小做限制，线程池大小完全依赖于操作系统（或者说JVM）能够创建的最大线程大小。<br />\n- newScheduledThreadPool：创建一个大小无限的线程池。此线程池支持定时以及周期性执行任务的需求。<br />\n- newSingleThreadExecutor：创建一个单线程的线程池。此线程池支持定时以及周期性执行任务的需求。</p>\n\n<p>第60题的例子中演示了通过Executors工具类创建线程池并使用线程池执行线程的代码。如果希望在服务器上使用线程池，强烈建议使用newFixedThreadPool方法来创建线程池，这样能获得更好的性能。</p>\n\n<p><strong>66、线程的基本状态以及状态之间的关系？</strong></p>\n\n<p><img alt=\"\" src=\"http://localhost:8080/myLog/images/interview/20150408002007838.jpg\" style=\"height:283px; width:500px\" /></p>\n\n<blockquote>\n<p><strong>说明：</strong>其中Running表示运行状态，Runnable表示就绪状态（万事俱备，只欠CPU），Blocked表示阻塞状态，阻塞状态又有多种情况，可能是因为调用wait()方法进入等待池，也可能是执行同步方法或同步代码块进入等锁池，或者是调用了sleep()方法或join()方法等待休眠或其他线程结束，或是因为发生了I/O中断。</p>\n</blockquote>\n\n<p><strong>67、简述synchronized 和java.util.concurrent.locks.Lock的异同？</strong><br />\n答：Lock是Java 5以后引入的新的API，和关键字synchronized相比主要相同点：Lock 能完成synchronized所实现的所有功能；主要不同点：Lock有比synchronized更精确的线程语义和更好的性能，而且不强制性的要求一定要获得锁。synchronized会自动释放锁，而Lock一定要求程序员手工释放，并且最好在finally 块中释放（这是释放外部资源的最好的地方）。</p>\n\n<p><strong>68、Java中如何实现序列化，有什么意义？</strong><br />\n答：序列化就是一种用来处理对象流的机制，所谓对象流也就是将对象的内容进行流化。可以对流化后的对象进行读写操作，也可将流化后的对象传输于网络之间。序列化是为了解决对象流读写操作时可能引发的问题（如果不进行序列化可能会存在数据乱序的问题）。<br />\n要实现序列化，需要让一个类实现Serializable接口，该接口是一个标识性接口，标注该类对象是可被序列化的，然后使用一个输出流来构造一个对象输出流并通过writeObject(Object)方法就可以将实现对象写出（即保存其状态）；如果需要反序列化则可以用一个输入流建立对象输入流，然后通过readObject方法从流中读取对象。序列化除了能够实现对象的持久化之外，还能够用于对象的深度克隆（可以参考第29题）。</p>\n\n<p><strong>69、Java中有几种类型的流？</strong><br />\n答：字节流和字符流。字节流继承于InputStream、OutputStream，字符流继承于Reader、Writer。在java.io 包中还有许多其他的流，主要是为了提高性能和使用方便。关于Java的I/O需要注意的有两点：一是两种对称性（输入和输出的对称性，字节和字符的对称性）；二是两种设计模式（适配器模式和装潢模式）。另外Java中的流不同于C#的是它只有一个维度一个方向。</p>\n\n<blockquote>\n<p><strong>面试题</strong>&nbsp;- 编程实现文件拷贝。（这个题目在笔试的时候经常出现，下面的代码给出了两种实现方案）</p>\n</blockquote>\n\n<pre>\n<code>import java.io.FileInputStream;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.OutputStream;\nimport java.nio.ByteBuffer;\nimport java.nio.channels.FileChannel;\n \npublic final class MyUtil {\n \n    private MyUtil() {\n        throw new AssertionError();\n    }\n \n    public static void fileCopy(String source, String target) throws IOException {\n        try (InputStream in = new FileInputStream(source)) {\n            try (OutputStream out = new FileOutputStream(target)) {\n                byte[] buffer = new byte[4096];\n                int bytesToRead;\n                while((bytesToRead = in.read(buffer)) != -1) {\n                    out.write(buffer, 0, bytesToRead);\n                }\n            }\n        }\n    }\n \n    public static void fileCopyNIO(String source, String target) throws IOException {\n        try (FileInputStream in = new FileInputStream(source)) {\n            try (FileOutputStream out = new FileOutputStream(target)) {\n                FileChannel inChannel = in.getChannel();\n                FileChannel outChannel = out.getChannel();\n                ByteBuffer buffer = ByteBuffer.allocate(4096);\n                while(inChannel.read(buffer) != -1) {\n                    buffer.flip();\n                    outChannel.write(buffer);\n                    buffer.clear();\n                }\n            }\n        }\n    }\n}</code></pre>\n\n<blockquote>\n<p><strong>注意：</strong>上面用到Java 7的TWR，使用TWR后可以不用在finally中释放外部资源 ，从而让代码更加优雅。</p>\n</blockquote>\n\n<p><strong>70、写一个方法，输入一个文件名和一个字符串，统计这个字符串在这个文件中出现的次数。</strong><br />\n答：代码如下：</p>\n\n<pre>\n<code>import java.io.BufferedReader;\nimport java.io.FileReader;\n \npublic final class MyUtil {\n \n    // 工具类中的方法都是静态方式访问的因此将构造器私有不允许创建对象(绝对好习惯)\n    private MyUtil() {\n        throw new AssertionError();\n    }\n \n    /**\n     * 统计给定文件中给定字符串的出现次数\n     * @param filename  文件名\n     * @param word 字符串\n     * @return 字符串在文件中出现的次数\n     */\n    public static int countWordInFile(String filename, String word) {\n        int counter = 0;\n        try (FileReader fr = new FileReader(filename)) {\n            try (BufferedReader br = new BufferedReader(fr)) {\n                String line = null;\n                while ((line = br.readLine()) != null) {\n                    int index = -1;\n                    while (line.length() &gt;= word.length() &amp;&amp; (index = line.indexOf(word)) &gt;= 0) {\n                        counter++;\n                        line = line.substring(index + word.length());\n                    }\n                }\n            }\n        } catch (Exception ex) {\n            ex.printStackTrace();\n        }\n        return counter;\n    }\n}</code></pre>\n\n<p><strong>71、如何用Java代码列出一个目录下所有的文件？</strong><br />\n答：<br />\n如果只要求列出当前文件夹下的文件，代码如下所示：</p>\n\n<pre>\n<code>import java.io.File;\n \nclass Test12 {\n \n    public static void main(String[] args) {\n        File f = new File(\"/Users/Hao/Downloads\");\n        for(File temp : f.listFiles()) {\n            if(temp.isFile()) {\n                System.out.println(temp.getName());\n            }\n        }\n    }\n}</code></pre>\n\n<p>如果需要对文件夹继续展开，代码如下所示：</p>\n\n<pre>\n<code>import java.io.File;\n \nclass Test12 {\n \n    public static void main(String[] args) {\n        showDirectory(new File(\"/Users/Hao/Downloads\"));\n    }\n \n    public static void showDirectory(File f) {\n        _walkDirectory(f, 0);\n    }\n \n    private static void _walkDirectory(File f, int level) {\n        if(f.isDirectory()) {\n            for(File temp : f.listFiles()) {\n                _walkDirectory(temp, level + 1);\n            }\n        }\n        else {\n            for(int i = 0; i &lt; level - 1; i++) {\n                System.out.print(\"\\t\");\n            }\n            System.out.println(f.getName());\n        }\n    }\n}</code></pre>\n\n<p>在Java 7中可以使用NIO.2的API来做同样的事情，代码如下所示：</p>\n\n<pre>\n<code>class ShowFileTest {\n \n    public static void main(String[] args) throws IOException {\n        Path initPath = Paths.get(\"/Users/Hao/Downloads\");\n        Files.walkFileTree(initPath, new SimpleFileVisitor&lt;Path&gt;() {\n \n            @Override\n            public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) \n                    throws IOException {\n                System.out.println(file.getFileName().toString());\n                return FileVisitResult.CONTINUE;\n            }\n \n        });\n    }\n}</code></pre>\n\n<p><strong>72、用Java的套接字编程实现一个多线程的回显（echo）服务器。</strong></p>\n\n<pre>\n<code>import java.io.BufferedReader;\nimport java.io.IOException;\nimport java.io.InputStreamReader;\nimport java.io.PrintWriter;\nimport java.net.ServerSocket;\nimport java.net.Socket;\n \npublic class EchoServer {\n \n    private static final int ECHO_SERVER_PORT = 6789;\n \n    public static void main(String[] args) {        \n        try(ServerSocket server = new ServerSocket(ECHO_SERVER_PORT)) {\n            System.out.println(\"服务器已经启动...\");\n            while(true) {\n                Socket client = server.accept();\n                new Thread(new ClientHandler(client)).start();\n            }\n        } catch (IOException e) {\n            e.printStackTrace();\n        }\n    }\n \n    private static class ClientHandler implements Runnable {\n        private Socket client;\n \n        public ClientHandler(Socket client) {\n            this.client = client;\n        }\n \n        @Override\n        public void run() {\n            try(BufferedReader br = new BufferedReader(new InputStreamReader(client.getInputStream()));\n                    PrintWriter pw = new PrintWriter(client.getOutputStream())) {\n                String msg = br.readLine();\n                System.out.println(\"收到\" + client.getInetAddress() + \"发送的: \" + msg);\n                pw.println(msg);\n                pw.flush();\n            } catch(Exception ex) {\n                ex.printStackTrace();\n            } finally {\n                try {\n                    client.close();\n                } catch (IOException e) {\n                    e.printStackTrace();\n                }\n            }\n        }\n    }\n \n}</code></pre>\n\n<blockquote>\n<p><strong>注意：</strong>上面的代码使用了Java 7的TWR语法，由于很多外部资源类都间接的实现了AutoCloseable接口（单方法回调接口），因此可以利用TWR语法在try结束的时候通过回调的方式自动调用外部资源类的close()方法，避免书写冗长的finally代码块。此外，上面的代码用一个静态内部类实现线程的功能，使用多线程可以避免一个用户I/O操作所产生的中断影响其他用户对服务器的访问，简单的说就是一个用户的输入操作不会造成其他用户的阻塞。当然，上面的代码使用线程池可以获得更好的性能，因为频繁的创建和销毁线程所造成的开销也是不可忽视的。</p>\n</blockquote>\n\n<p>下面是一段回显客户端测试代码：</p>\n\n<pre>\n<code>import java.io.BufferedReader;\nimport java.io.InputStreamReader;\nimport java.io.PrintWriter;\nimport java.net.Socket;\nimport java.util.Scanner;\n \npublic class EchoClient {\n \n    public static void main(String[] args) throws Exception {\n        Socket client = new Socket(\"localhost\", 6789);\n        Scanner sc = new Scanner(System.in);\n        System.out.print(\"请输入内容: \");\n        String msg = sc.nextLine();\n        sc.close();\n        PrintWriter pw = new PrintWriter(client.getOutputStream());\n        pw.println(msg);\n        pw.flush();\n        BufferedReader br = new BufferedReader(new InputStreamReader(client.getInputStream()));\n        System.out.println(br.readLine());\n        client.close();\n    }\n}</code></pre>\n\n<p>如果希望用NIO的多路复用套接字实现服务器，代码如下所示。NIO的操作虽然带来了更好的性能，但是有些操作是比较底层的，对于初学者来说还是有些难于理解。</p>\n\n<pre>\n<code>import java.io.IOException;\nimport java.net.InetSocketAddress;\nimport java.nio.ByteBuffer;\nimport java.nio.CharBuffer;\nimport java.nio.channels.SelectionKey;\nimport java.nio.channels.Selector;\nimport java.nio.channels.ServerSocketChannel;\nimport java.nio.channels.SocketChannel;\nimport java.util.Iterator;\n \npublic class EchoServerNIO {\n \n    private static final int ECHO_SERVER_PORT = 6789;\n    private static final int ECHO_SERVER_TIMEOUT = 5000;\n    private static final int BUFFER_SIZE = 1024;\n \n    private static ServerSocketChannel serverChannel = null;\n    private static Selector selector = null;    // 多路复用选择器\n    private static ByteBuffer buffer = null;    // 缓冲区\n \n    public static void main(String[] args) {\n        init();\n        listen();\n    }\n \n    private static void init() {\n        try {\n            serverChannel = ServerSocketChannel.open();\n            buffer = ByteBuffer.allocate(BUFFER_SIZE);\n            serverChannel.socket().bind(new InetSocketAddress(ECHO_SERVER_PORT));\n            serverChannel.configureBlocking(false);\n            selector = Selector.open();\n            serverChannel.register(selector, SelectionKey.OP_ACCEPT);\n        } catch (Exception e) {\n            throw new RuntimeException(e);\n        }\n    }\n \n    private static void listen() {\n        while (true) {\n            try {\n                if (selector.select(ECHO_SERVER_TIMEOUT) != 0) {\n                    Iterator&lt;SelectionKey&gt; it = selector.selectedKeys().iterator();\n                    while (it.hasNext()) {\n                        SelectionKey key = it.next();\n                        it.remove();\n                        handleKey(key);\n                    }\n                }\n            } catch (Exception e) {\n                e.printStackTrace();\n            }\n        }\n    }\n \n    private static void handleKey(SelectionKey key) throws IOException {\n        SocketChannel channel = null;\n \n        try {\n            if (key.isAcceptable()) {\n                ServerSocketChannel serverChannel = (ServerSocketChannel) key.channel();\n                channel = serverChannel.accept();\n                channel.configureBlocking(false);\n                channel.register(selector, SelectionKey.OP_READ);\n            } else if (key.isReadable()) {\n                channel = (SocketChannel) key.channel();\n                buffer.clear();\n                if (channel.read(buffer) &gt; 0) {\n                    buffer.flip();\n                    CharBuffer charBuffer = CharsetHelper.decode(buffer);\n                    String msg = charBuffer.toString();\n                    System.out.println(\"收到\" + channel.getRemoteAddress() + \"的消息：\" + msg);\n                    channel.write(CharsetHelper.encode(CharBuffer.wrap(msg)));\n                } else {\n                    channel.close();\n                }\n            }\n        } catch (Exception e) {\n            e.printStackTrace();\n            if (channel != null) {\n                channel.close();\n            }\n        }\n    }\n}</code></pre>\n\n<pre>\n<code>import java.nio.ByteBuffer;\nimport java.nio.CharBuffer;\nimport java.nio.charset.CharacterCodingException;\nimport java.nio.charset.Charset;\nimport java.nio.charset.CharsetDecoder;\nimport java.nio.charset.CharsetEncoder;\n \npublic final class CharsetHelper {\n    private static final String UTF_8 = \"UTF-8\";\n    private static CharsetEncoder encoder = Charset.forName(UTF_8).newEncoder();\n    private static CharsetDecoder decoder = Charset.forName(UTF_8).newDecoder();\n \n    private CharsetHelper() {\n    }\n \n    public static ByteBuffer encode(CharBuffer in) throws CharacterCodingException{\n        return encoder.encode(in);\n    }\n \n    public static CharBuffer decode(ByteBuffer in) throws CharacterCodingException{\n        return decoder.decode(in);\n    }\n}</code></pre>\n\n<p><strong>73、XML文档定义有几种形式？它们之间有何本质区别？解析XML文档有哪几种方式？</strong><br />\n答：XML文档定义分为DTD和Schema两种形式，二者都是对XML语法的约束，其本质区别在于Schema本身也是一个XML文件，可以被XML解析器解析，而且可以为XML承载的数据定义类型，约束能力较之DTD更强大。对XML的解析主要有DOM（文档对象模型，Document&nbsp;Object&nbsp;Model）、SAX（Simple&nbsp;API for&nbsp;XML）和StAX（Java 6中引入的新的解析XML的方式，Streaming&nbsp;API for&nbsp;XML），其中DOM处理大型文件时其性能下降的非常厉害，这个问题是由DOM树结构占用的内存较多造成的，而且DOM解析方式必须在解析文件之前把整个文档装入内存，适合对XML的随机访问（典型的用空间换取时间的策略）；SAX是事件驱动型的XML解析方式，它顺序读取XML文件，不需要一次全部装载整个文件。当遇到像文件开头，文档结束，或者标签开头与标签结束时，它会触发一个事件，用户通过事件回调代码来处理XML文件，适合对XML的顺序访问；顾名思义，StAX把重点放在流上，实际上StAX与其他解析方式的本质区别就在于应用程序能够把XML作为一个事件流来处理。将XML作为一组事件来处理的想法并不新颖（SAX就是这样做的），但不同之处在于StAX允许应用程序代码把这些事件逐个拉出来，而不用提供在解析器方便时从解析器中接收事件的处理程序。</p>\n\n<p><strong>74、你在项目中哪些地方用到了XML？</strong><br />\n答：XML的主要作用有两个方面：数据交换和信息配置。在做数据交换时，XML将数据用标签组装成起来，然后压缩打包加密后通过网络传送给接收者，接收解密与解压缩后再从XML文件中还原相关信息进行处理，XML曾经是异构系统间交换数据的事实标准，但此项功能几乎已经被JSON（JavaScript&nbsp;Object&nbsp;Notation）取而代之。当然，目前很多软件仍然使用XML来存储配置信息，我们在很多项目中通常也会将作为配置信息的硬代码写在XML文件中，Java的很多框架也是这么做的，而且这些框架都选择了dom4j作为处理XML的工具，因为Sun公司的官方API实在不怎么好用。</p>\n\n<blockquote>\n<p><strong>补充：</strong>现在有很多时髦的软件（如Sublime）已经开始将配置文件书写成JSON格式，我们已经强烈的感受到XML的另一项功能也将逐渐被业界抛弃。</p>\n</blockquote>\n\n<p><strong>75、阐述JDBC操作数据库的步骤。</strong><br />\n答：下面的代码以连接本机的Oracle数据库为例，演示JDBC操作数据库的步骤。</p>\n\n<pre>\n<code>加载驱动。\nClass.forName(\"oracle.jdbc.driver.OracleDriver\");\n\n创建连接。\nConnection con = DriverManager.getConnection(\"jdbc:oracle:thin:@localhost:1521:orcl\", \"scott\", \"tiger\");\n\n创建语句。\nPreparedStatement ps = con.prepareStatement(\"select * from emp where sal between ? and ?\");\nps.setInt(1, 1000);\nps.setInt(2, 3000);\n\n执行语句。\nResultSet rs = ps.executeQuery();\n\n处理结果。\nwhile(rs.next()) {\n    System.out.println(rs.getInt(\"empno\") + \" - \" + rs.getString(\"ename\"));\n}\n\n关闭资源。\nfinally {\n    if(con != null) {\n        try {\n            con.close();\n        } catch (SQLException e) {\n            e.printStackTrace();\n        }\n    }\n}</code></pre>\n\n<blockquote>\n<p><strong>提示：</strong>关闭外部资源的顺序应该和打开的顺序相反，也就是说先关闭ResultSet、再关闭Statement、在关闭Connection。上面的代码只关闭了Connection（连接），虽然通常情况下在关闭连接时，连接上创建的语句和打开的游标也会关闭，但不能保证总是如此，因此应该按照刚才说的顺序分别关闭。此外，第一步加载驱动在JDBC 4.0中是可以省略的（自动从类路径中加载驱动），但是我们建议保留。</p>\n</blockquote>\n\n<p><strong>76、Statement和PreparedStatement有什么区别？哪个性能更好？</strong><br />\n答：与Statement相比，①PreparedStatement接口代表预编译的语句，它主要的优势在于可以减少SQL的编译错误并增加SQL的安全性（减少SQL注射攻击的可能性）；②PreparedStatement中的SQL语句是可以带参数的，避免了用字符串连接拼接SQL语句的麻烦和不安全；③当批量处理SQL或频繁执行相同的查询时，PreparedStatement有明显的性能上的优势，由于数据库可以将编译优化后的SQL语句缓存起来，下次执行相同结构的语句时就会很快（不用再次编译和生成执行计划）。</p>\n\n<blockquote>\n<p><strong>补充：</strong>为了提供对存储过程的调用，JDBC API中还提供了CallableStatement接口。存储过程（Stored Procedure）是数据库中一组为了完成特定功能的SQL语句的集合，经编译后存储在数据库中，用户通过指定存储过程的名字并给出参数（如果该存储过程带有参数）来执行它。虽然调用存储过程会在网络开销、安全性、性能上获得很多好处，但是存在如果底层数据库发生迁移时就会有很多麻烦，因为每种数据库的存储过程在书写上存在不少的差别。</p>\n</blockquote>\n\n<p><strong>77、使用JDBC操作数据库时，如何提升读取数据的性能？如何提升更新数据的性能？</strong><br />\n答：要提升读取数据的性能，可以指定通过结果集（ResultSet）对象的setFetchSize()方法指定每次抓取的记录数（典型的空间换时间策略）；要提升更新数据的性能可以使用PreparedStatement语句构建批处理，将若干SQL语句置于一个批处理中执行。</p>\n\n<p><strong>78、在进行数据库编程时，连接池有什么作用？</strong><br />\n答：由于创建连接和释放连接都有很大的开销（尤其是数据库服务器不在本地时，每次建立连接都需要进行TCP的三次握手，释放连接需要进行TCP四次握手，造成的开销是不可忽视的），为了提升系统访问数据库的性能，可以事先创建若干连接置于连接池中，需要时直接从连接池获取，使用结束时归还连接池而不必关闭连接，从而避免频繁创建和释放连接所造成的开销，这是典型的用空间换取时间的策略（浪费了空间存储连接，但节省了创建和释放连接的时间）。池化技术在Java开发中是很常见的，在使用线程时创建线程池的道理与此相同。基于Java的开源数据库连接池主要有：C3P0、Proxool、DBCP、BoneCP、Druid等。</p>\n\n<blockquote>\n<p><strong>补充：</strong>在计算机系统中时间和空间是不可调和的矛盾，理解这一点对设计满足性能要求的算法是至关重要的。大型网站性能优化的一个关键就是使用缓存，而缓存跟上面讲的连接池道理非常类似，也是使用空间换时间的策略。可以将热点数据置于缓存中，当用户查询这些数据时可以直接从缓存中得到，这无论如何也快过去数据库中查询。当然，缓存的置换策略等也会对系统性能产生重要影响，对于这个问题的讨论已经超出了这里要阐述的范围。</p>\n</blockquote>\n\n<p><strong>79、什么是DAO模式？</strong><br />\n答：DAO（Data Access Object）顾名思义是一个为数据库或其他持久化机制提供了抽象接口的对象，在不暴露底层持久化方案实现细节的前提下提供了各种数据访问操作。在实际的开发中，应该将所有对数据源的访问操作进行抽象化后封装在一个公共API中。用程序设计语言来说，就是建立一个接口，接口中定义了此应用程序中将会用到的所有事务方法。在这个应用程序中，当需要和数据源进行交互的时候则使用这个接口，并且编写一个单独的类来实现这个接口，在逻辑上该类对应一个特定的数据存储。DAO模式实际上包含了两个模式，一是Data Accessor（数据访问器），二是Data Object（数据对象），前者要解决如何访问数据的问题，而后者要解决的是如何用对象封装数据。</p>\n\n<p><strong>80、事务的ACID是指什么？</strong><br />\n答：<br />\n- 原子性(Atomic)：事务中各项操作，要么全做要么全不做，任何一项操作的失败都会导致整个事务的失败；<br />\n- 一致性(Consistent)：事务结束后系统状态是一致的；<br />\n- 隔离性(Isolated)：并发执行的事务彼此无法看到对方的中间状态；<br />\n- 持久性(Durable)：事务完成后所做的改动都会被持久化，即使发生灾难性的失败。通过日志和同步备份可以在故障发生后重建数据。</p>\n\n<blockquote>\n<p><strong>补充：</strong>关于事务，在面试中被问到的概率是很高的，可以问的问题也是很多的。首先需要知道的是，只有存在并发数据访问时才需要事务。当多个事务访问同一数据时，可能会存在5类问题，包括3类数据读取问题（脏读、不可重复读和幻读）和2类数据更新问题（第1类丢失更新和第2类丢失更新）。</p>\n</blockquote>\n\n<p>脏读（Dirty Read）：A事务读取B事务尚未提交的数据并在此基础上操作，而B事务执行回滚，那么A读取到的数据就是脏数据。</p>\n\n<table style=\"width:500px\">\n	<thead>\n		<tr>\n			<th style=\"text-align:left\">时间</th>\n			<th style=\"text-align:left\">转账事务A</th>\n			<th style=\"text-align:left\">取款事务B</th>\n		</tr>\n	</thead>\n	<tbody>\n		<tr>\n			<td style=\"text-align:left\">T1</td>\n			<td style=\"text-align:left\">&nbsp;</td>\n			<td style=\"text-align:left\">开始事务</td>\n		</tr>\n		<tr>\n			<td style=\"text-align:left\">T2</td>\n			<td style=\"text-align:left\">开始事务</td>\n			<td style=\"text-align:left\">&nbsp;</td>\n		</tr>\n		<tr>\n			<td style=\"text-align:left\">T3</td>\n			<td style=\"text-align:left\">&nbsp;</td>\n			<td style=\"text-align:left\">查询账户余额为1000元</td>\n		</tr>\n		<tr>\n			<td style=\"text-align:left\">T4</td>\n			<td style=\"text-align:left\">&nbsp;</td>\n			<td style=\"text-align:left\">取出500元余额修改为500元</td>\n		</tr>\n		<tr>\n			<td style=\"text-align:left\">T5</td>\n			<td style=\"text-align:left\">查询账户余额为500元（脏读）</td>\n			<td style=\"text-align:left\">&nbsp;</td>\n		</tr>\n		<tr>\n			<td style=\"text-align:left\">T6</td>\n			<td style=\"text-align:left\">&nbsp;</td>\n			<td style=\"text-align:left\">撤销事务余额恢复为1000元</td>\n		</tr>\n		<tr>\n			<td style=\"text-align:left\">T7</td>\n			<td style=\"text-align:left\">汇入100元把余额修改为600元</td>\n			<td style=\"text-align:left\">&nbsp;</td>\n		</tr>\n		<tr>\n			<td style=\"text-align:left\">T8</td>\n			<td style=\"text-align:left\">提交事务</td>\n			<td style=\"text-align:left\">&nbsp;</td>\n		</tr>\n	</tbody>\n</table>\n\n<hr />\n<p>不可重复读（Unrepeatable Read）：事务A重新读取前面读取过的数据，发现该数据已经被另一个已提交的事务B修改过了。</p>\n\n<table style=\"width:500px\">\n	<thead>\n		<tr>\n			<th style=\"text-align:left\">时间</th>\n			<th style=\"text-align:left\">转账事务A</th>\n			<th style=\"text-align:left\">取款事务B</th>\n		</tr>\n	</thead>\n	<tbody>\n		<tr>\n			<td style=\"text-align:left\">T1</td>\n			<td style=\"text-align:left\">&nbsp;</td>\n			<td style=\"text-align:left\">开始事务</td>\n		</tr>\n		<tr>\n			<td style=\"text-align:left\">T2</td>\n			<td style=\"text-align:left\">开始事务</td>\n			<td style=\"text-align:left\">&nbsp;</td>\n		</tr>\n		<tr>\n			<td style=\"text-align:left\">T3</td>\n			<td style=\"text-align:left\">&nbsp;</td>\n			<td style=\"text-align:left\">查询账户余额为1000元</td>\n		</tr>\n		<tr>\n			<td style=\"text-align:left\">T4</td>\n			<td style=\"text-align:left\">查询账户余额为1000元</td>\n			<td style=\"text-align:left\">&nbsp;</td>\n		</tr>\n		<tr>\n			<td style=\"text-align:left\">T5</td>\n			<td style=\"text-align:left\">&nbsp;</td>\n			<td style=\"text-align:left\">取出100元修改余额为900元</td>\n		</tr>\n		<tr>\n			<td style=\"text-align:left\">T6</td>\n			<td style=\"text-align:left\">&nbsp;</td>\n			<td style=\"text-align:left\">提交事务</td>\n		</tr>\n		<tr>\n			<td style=\"text-align:left\">T7</td>\n			<td style=\"text-align:left\">查询账户余额为900元（不可重复读）</td>\n			<td style=\"text-align:left\">&nbsp;</td>\n		</tr>\n	</tbody>\n</table>\n\n<hr />\n<p>幻读（Phantom Read）：事务A重新执行一个查询，返回一系列符合查询条件的行，发现其中插入了被事务B提交的行。</p>\n\n<table style=\"width:500px\">\n	<thead>\n		<tr>\n			<th style=\"text-align:left\">时间</th>\n			<th style=\"text-align:left\">统计金额事务A</th>\n			<th style=\"text-align:left\">转账事务B</th>\n		</tr>\n	</thead>\n	<tbody>\n		<tr>\n			<td style=\"text-align:left\">T1</td>\n			<td style=\"text-align:left\">&nbsp;</td>\n			<td style=\"text-align:left\">开始事务</td>\n		</tr>\n		<tr>\n			<td style=\"text-align:left\">T2</td>\n			<td style=\"text-align:left\">开始事务</td>\n			<td style=\"text-align:left\">&nbsp;</td>\n		</tr>\n		<tr>\n			<td style=\"text-align:left\">T3</td>\n			<td style=\"text-align:left\">统计总存款为10000元</td>\n			<td style=\"text-align:left\">&nbsp;</td>\n		</tr>\n		<tr>\n			<td style=\"text-align:left\">T4</td>\n			<td style=\"text-align:left\">&nbsp;</td>\n			<td style=\"text-align:left\">新增一个存款账户存入100元</td>\n		</tr>\n		<tr>\n			<td style=\"text-align:left\">T5</td>\n			<td style=\"text-align:left\">&nbsp;</td>\n			<td style=\"text-align:left\">提交事务</td>\n		</tr>\n		<tr>\n			<td style=\"text-align:left\">T6</td>\n			<td style=\"text-align:left\">再次统计总存款为10100元（幻读）</td>\n			<td style=\"text-align:left\">&nbsp;</td>\n		</tr>\n	</tbody>\n</table>\n\n<hr />\n<p>第1类丢失更新：事务A撤销时，把已经提交的事务B的更新数据覆盖了。</p>\n\n<table style=\"width:500px\">\n	<thead>\n		<tr>\n			<th style=\"text-align:left\">时间</th>\n			<th style=\"text-align:left\">取款事务A</th>\n			<th style=\"text-align:left\">转账事务B</th>\n		</tr>\n	</thead>\n	<tbody>\n		<tr>\n			<td style=\"text-align:left\">T1</td>\n			<td style=\"text-align:left\">开始事务</td>\n			<td style=\"text-align:left\">&nbsp;</td>\n		</tr>\n		<tr>\n			<td style=\"text-align:left\">T2</td>\n			<td style=\"text-align:left\">&nbsp;</td>\n			<td style=\"text-align:left\">开始事务</td>\n		</tr>\n		<tr>\n			<td style=\"text-align:left\">T3</td>\n			<td style=\"text-align:left\">查询账户余额为1000元</td>\n			<td style=\"text-align:left\">&nbsp;</td>\n		</tr>\n		<tr>\n			<td style=\"text-align:left\">T4</td>\n			<td style=\"text-align:left\">&nbsp;</td>\n			<td style=\"text-align:left\">查询账户余额为1000元</td>\n		</tr>\n		<tr>\n			<td style=\"text-align:left\">T5</td>\n			<td style=\"text-align:left\">&nbsp;</td>\n			<td style=\"text-align:left\">汇入100元修改余额为1100元</td>\n		</tr>\n		<tr>\n			<td style=\"text-align:left\">T6</td>\n			<td style=\"text-align:left\">&nbsp;</td>\n			<td style=\"text-align:left\">提交事务</td>\n		</tr>\n		<tr>\n			<td style=\"text-align:left\">T7</td>\n			<td style=\"text-align:left\">取出100元将余额修改为900元</td>\n			<td style=\"text-align:left\">&nbsp;</td>\n		</tr>\n		<tr>\n			<td style=\"text-align:left\">T8</td>\n			<td style=\"text-align:left\">撤销事务</td>\n			<td style=\"text-align:left\">&nbsp;</td>\n		</tr>\n		<tr>\n			<td style=\"text-align:left\">T9</td>\n			<td style=\"text-align:left\">余额恢复为1000元（丢失更新）</td>\n			<td style=\"text-align:left\">&nbsp;</td>\n		</tr>\n	</tbody>\n</table>\n\n<hr />\n<p>第2类丢失更新：事务A覆盖事务B已经提交的数据，造成事务B所做的操作丢失。</p>\n\n<table style=\"width:500px\">\n	<thead>\n		<tr>\n			<th style=\"text-align:left\">时间</th>\n			<th style=\"text-align:left\">转账事务A</th>\n			<th style=\"text-align:left\">取款事务B</th>\n		</tr>\n	</thead>\n	<tbody>\n		<tr>\n			<td style=\"text-align:left\">T1</td>\n			<td style=\"text-align:left\">&nbsp;</td>\n			<td style=\"text-align:left\">开始事务</td>\n		</tr>\n		<tr>\n			<td style=\"text-align:left\">T2</td>\n			<td style=\"text-align:left\">开始事务</td>\n			<td style=\"text-align:left\">&nbsp;</td>\n		</tr>\n		<tr>\n			<td style=\"text-align:left\">T3</td>\n			<td style=\"text-align:left\">&nbsp;</td>\n			<td style=\"text-align:left\">查询账户余额为1000元</td>\n		</tr>\n		<tr>\n			<td style=\"text-align:left\">T4</td>\n			<td style=\"text-align:left\">查询账户余额为1000元</td>\n			<td style=\"text-align:left\">&nbsp;</td>\n		</tr>\n		<tr>\n			<td style=\"text-align:left\">T5</td>\n			<td style=\"text-align:left\">&nbsp;</td>\n			<td style=\"text-align:left\">取出100元将余额修改为900元</td>\n		</tr>\n		<tr>\n			<td style=\"text-align:left\">T6</td>\n			<td style=\"text-align:left\">&nbsp;</td>\n			<td style=\"text-align:left\">提交事务</td>\n		</tr>\n		<tr>\n			<td style=\"text-align:left\">T7</td>\n			<td style=\"text-align:left\">汇入100元将余额修改为1100元</td>\n			<td style=\"text-align:left\">&nbsp;</td>\n		</tr>\n		<tr>\n			<td style=\"text-align:left\">T8</td>\n			<td style=\"text-align:left\">提交事务</td>\n			<td style=\"text-align:left\">&nbsp;</td>\n		</tr>\n		<tr>\n			<td style=\"text-align:left\">T9</td>\n			<td style=\"text-align:left\">查询账户余额为1100元（丢失更新）</td>\n			<td style=\"text-align:left\">&nbsp;</td>\n		</tr>\n	</tbody>\n</table>\n\n<hr />\n<p>数据并发访问所产生的问题，在有些场景下可能是允许的，但是有些场景下可能就是致命的，数据库通常会通过锁机制来解决数据并发访问问题，按锁定对象不同可以分为表级锁和行级锁；按并发事务锁定关系可以分为共享锁和独占锁，具体的内容大家可以自行查阅资料进行了解。<br />\n直接使用锁是非常麻烦的，为此数据库为用户提供了自动锁机制，只要用户指定会话的事务隔离级别，数据库就会通过分析SQL语句然后为事务访问的资源加上合适的锁，此外，数据库还会维护这些锁通过各种手段提高系统的性能，这些对用户来说都是透明的（就是说你不用理解，事实上我确实也不知道）。ANSI/ISO SQL 92标准定义了4个等级的事务隔离级别，如下表所示：</p>\n\n<table style=\"width:600px\">\n	<thead>\n		<tr>\n			<th style=\"text-align:left\">隔离级别</th>\n			<th style=\"text-align:left\">脏读</th>\n			<th style=\"text-align:left\">不可重复读</th>\n			<th style=\"text-align:left\">幻读</th>\n			<th style=\"text-align:left\">第一类丢失更新</th>\n			<th style=\"text-align:left\">第二类丢失更新</th>\n		</tr>\n	</thead>\n	<tbody>\n		<tr>\n			<td style=\"text-align:left\">READ UNCOMMITED</td>\n			<td style=\"text-align:left\">允许</td>\n			<td style=\"text-align:left\">允许</td>\n			<td style=\"text-align:left\">允许</td>\n			<td style=\"text-align:left\">不允许</td>\n			<td style=\"text-align:left\">允许</td>\n		</tr>\n		<tr>\n			<td style=\"text-align:left\">READ COMMITTED</td>\n			<td style=\"text-align:left\">不允许</td>\n			<td style=\"text-align:left\">允许</td>\n			<td style=\"text-align:left\">允许</td>\n			<td style=\"text-align:left\">不允许</td>\n			<td style=\"text-align:left\">允许</td>\n		</tr>\n		<tr>\n			<td style=\"text-align:left\">REPEATABLE READ</td>\n			<td style=\"text-align:left\">不允许</td>\n			<td style=\"text-align:left\">不允许</td>\n			<td style=\"text-align:left\">允许</td>\n			<td style=\"text-align:left\">不允许</td>\n			<td style=\"text-align:left\">不允许</td>\n		</tr>\n		<tr>\n			<td style=\"text-align:left\">SERIALIZABLE</td>\n			<td style=\"text-align:left\">不允许</td>\n			<td style=\"text-align:left\">不允许</td>\n			<td style=\"text-align:left\">不允许</td>\n			<td style=\"text-align:left\">不允许</td>\n			<td style=\"text-align:left\">不允许</td>\n		</tr>\n	</tbody>\n</table>\n\n<p>需要说明的是，事务隔离级别和数据访问的并发性是对立的，事务隔离级别越高并发性就越差。所以要根据具体的应用来确定合适的事务隔离级别，这个地方没有万能的原则。</p>\n\n<p><strong>81、JDBC中如何进行事务处理？</strong><br />\n答：Connection提供了事务处理的方法，通过调用setAutoCommit(false)可以设置手动提交事务；当事务完成后用commit()显式提交事务；如果在事务处理过程中发生异常则通过rollback()进行事务回滚。除此之外，从JDBC 3.0中还引入了Savepoint（保存点）的概念，允许通过代码设置保存点并让事务回滚到指定的保存点。</p>\n\n<p><img alt=\"\" src=\"http://localhost:8080/myLog/images/interview/20150408174308284\" style=\"height:181px; width:471px\" /></p>\n\n<p><strong>82、JDBC能否处理Blob和Clob？</strong><br />\n答： Blob是指二进制大对象（Binary Large Object），而Clob是指大字符对象（Character Large Objec），因此其中Blob是为存储大的二进制数据而设计的，而Clob是为存储大的文本数据而设计的。JDBC的PreparedStatement和ResultSet都提供了相应的方法来支持Blob和Clob操作。下面的代码展示了如何使用JDBC操作LOB：<br />\n下面以MySQL数据库为例，创建一个张有三个字段的用户表，包括编号（id）、姓名（name）和照片（photo），建表语句如下：</p>\n\n<pre>\n<code>create table tb_user (\n    id int primary key auto_increment,\n    name varchar(20) unique not null,\n    photo longblob\n);</code></pre>\n\n<p>下面的Java代码向数据库中插入一条记录：</p>\n\n<pre>\n<code>import java.io.FileInputStream;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.sql.Connection;\nimport java.sql.DriverManager;\nimport java.sql.PreparedStatement;\nimport java.sql.SQLException;\n \nclass JdbcLobTest {\n \n    public static void main(String[] args) {\n        Connection con = null;\n        try {\n            // 1. 加载驱动（Java6以上版本可以省略）\n            Class.forName(\"com.mysql.jdbc.Driver\");\n            // 2. 建立连接\n            con = DriverManager.getConnection(\"jdbc:mysql://localhost:3306/test\", \"root\", \"123456\");\n            // 3. 创建语句对象\n            PreparedStatement ps = con.prepareStatement(\"insert into tb_user values (default, ?, ?)\");\n            ps.setString(1, \"骆昊\");              // 将SQL语句中第一个占位符换成字符串\n            try (InputStream in = new FileInputStream(\"test.jpg\")) {    // Java 7的TWR\n                ps.setBinaryStream(2, in);      // 将SQL语句中第二个占位符换成二进制流\n                // 4. 发出SQL语句获得受影响行数\n                System.out.println(ps.executeUpdate() == 1 ? \"插入成功\" : \"插入失败\");\n            } catch(IOException e) {\n                System.out.println(\"读取照片失败!\");\n            }\n        } catch (ClassNotFoundException | SQLException e) {     // Java 7的多异常捕获\n            e.printStackTrace();\n        } finally { // 释放外部资源的代码都应当放在finally中保证其能够得到执行\n            try {\n                if(con != null &amp;&amp; !con.isClosed()) {\n                    con.close();    // 5. 释放数据库连接 \n                    con = null;     // 指示垃圾回收器可以回收该对象\n                }\n            } catch (SQLException e) {\n                e.printStackTrace();\n            }\n        }\n    }\n}</code></pre>\n\n<p><strong>83、简述正则表达式及其用途。</strong><br />\n答：在编写处理字符串的程序时，经常会有查找符合某些复杂规则的字符串的需要。正则表达式就是用于描述这些规则的工具。换句话说，正则表达式就是记录文本规则的代码。</p>\n\n<blockquote>\n<p><strong>说明：</strong>计算机诞生初期处理的信息几乎都是数值，但是时过境迁，今天我们使用计算机处理的信息更多的时候不是数值而是字符串，正则表达式就是在进行字符串匹配和处理的时候最为强大的工具，绝大多数语言都提供了对正则表达式的支持。</p>\n</blockquote>\n\n<p><strong>84、Java中是如何支持正则表达式操作的？</strong><br />\n答：Java中的String类提供了支持正则表达式操作的方法，包括：matches()、replaceAll()、replaceFirst()、split()。此外，Java中可以用Pattern类表示正则表达式对象，它提供了丰富的API进行各种正则表达式操作，请参考下面面试题的代码。</p>\n\n<blockquote>\n<p><strong>面试题：</strong>&nbsp;- 如果要从字符串中截取第一个英文左括号之前的字符串，例如：北京市(朝阳区)(西城区)(海淀区)，截取结果为：北京市，那么正则表达式怎么写？</p>\n</blockquote>\n\n<pre>\n<code>import java.util.regex.Matcher;\nimport java.util.regex.Pattern;\n \nclass RegExpTest {\n \n    public static void main(String[] args) {\n        String str = \"北京市(朝阳区)(西城区)(海淀区)\";\n        Pattern p = Pattern.compile(\".*?(?=\\\\()\");\n        Matcher m = p.matcher(str);\n        if(m.find()) {\n            System.out.println(m.group());\n        }\n    }\n}</code></pre>\n\n<blockquote>\n<p><strong>说明：</strong>上面的正则表达式中使用了懒惰匹配和前瞻，如果不清楚这些内容，推荐读一下网上很有名的<a href=\"http://www.jb51.net/tools/zhengze.html\" rel=\"nofollow\" target=\"_blank\">《正则表达式30分钟入门教程》</a>。</p>\n</blockquote>\n\n<p><strong>85、获得一个类的类对象有哪些方式？</strong><br />\n答：<br />\n- 方法1：类型.class，例如：String.class<br />\n- 方法2：对象.getClass()，例如：&rdquo;hello&rdquo;.getClass()<br />\n- 方法3：Class.forName()，例如：Class.forName(&ldquo;java.lang.String&rdquo;)</p>\n\n<p><strong>86、如何通过反射创建对象？</strong><br />\n答：<br />\n- 方法1：通过类对象调用newInstance()方法，例如：String.class.newInstance()<br />\n- 方法2：通过类对象的getConstructor()或getDeclaredConstructor()方法获得构造器（Constructor）对象并调用其newInstance()方法创建对象，例如：String.class.getConstructor(String.class).newInstance(&ldquo;Hello&rdquo;);</p>\n\n<p><strong>87、如何通过反射获取和设置对象私有字段的值？</strong><br />\n答：可以通过类对象的getDeclaredField()方法字段（Field）对象，然后再通过字段对象的setAccessible(true)将其设置为可以访问，接下来就可以通过get/set方法来获取/设置字段的值了。下面的代码实现了一个反射的工具类，其中的两个静态方法分别用于获取和设置私有字段的值，字段可以是基本类型也可以是对象类型且支持多级对象操作，例如ReflectionUtil.get(dog, &ldquo;owner.car.engine.id&rdquo;);可以获得dog对象的主人的汽车的引擎的ID号。</p>\n\n<pre>\n<code>import java.lang.reflect.Constructor;\nimport java.lang.reflect.Field;\nimport java.lang.reflect.Modifier;\nimport java.util.ArrayList;\nimport java.util.List;\n \n/**\n * 反射工具类\n * @author 骆昊\n *\n */\npublic class ReflectionUtil {\n \n    private ReflectionUtil() {\n        throw new AssertionError();\n    }\n \n    /**\n     * 通过反射取对象指定字段(属性)的值\n     * @param target 目标对象\n     * @param fieldName 字段的名字\n     * @throws 如果取不到对象指定字段的值则抛出异常\n     * @return 字段的值\n     */\n    public static Object getValue(Object target, String fieldName) {\n        Class&lt;?&gt; clazz = target.getClass();\n        String[] fs = fieldName.split(\"\\\\.\");\n \n        try {\n            for(int i = 0; i &lt; fs.length - 1; i++) {\n                Field f = clazz.getDeclaredField(fs[i]);\n                f.setAccessible(true);\n                target = f.get(target);\n                clazz = target.getClass();\n            }\n \n            Field f = clazz.getDeclaredField(fs[fs.length - 1]);\n            f.setAccessible(true);\n            return f.get(target);\n        }\n        catch (Exception e) {\n            throw new RuntimeException(e);\n        }\n    }\n \n    /**\n     * 通过反射给对象的指定字段赋值\n     * @param target 目标对象\n     * @param fieldName 字段的名称\n     * @param value 值\n     */\n    public static void setValue(Object target, String fieldName, Object value) {\n        Class&lt;?&gt; clazz = target.getClass();\n        String[] fs = fieldName.split(\"\\\\.\");\n        try {\n            for(int i = 0; i &lt; fs.length - 1; i++) {\n                Field f = clazz.getDeclaredField(fs[i]);\n                f.setAccessible(true);\n                Object val = f.get(target);\n                if(val == null) {\n                    Constructor&lt;?&gt; c = f.getType().getDeclaredConstructor();\n                    c.setAccessible(true);\n                    val = c.newInstance();\n                    f.set(target, val);\n                }\n                target = val;\n                clazz = target.getClass();\n            }\n \n            Field f = clazz.getDeclaredField(fs[fs.length - 1]);\n            f.setAccessible(true);\n            f.set(target, value);\n        }\n        catch (Exception e) {\n            throw new RuntimeException(e);\n        }\n    }\n}</code></pre>\n\n<p><strong>88、如何通过反射调用对象的方法？</strong><br />\n答：请看下面的代码：</p>\n\n<pre>\n<code>import java.lang.reflect.Method;\n \nclass MethodInvokeTest {\n \n    public static void main(String[] args) throws Exception {\n        String str = \"hello\";\n        Method m = str.getClass().getMethod(\"toUpperCase\");\n        System.out.println(m.invoke(str));  // HELLO\n    }\n}</code></pre>\n\n<p><strong>89、简述一下面向对象的&rdquo;六原则一法则&rdquo;。</strong><br />\n答：<br />\n- 单一职责原则：一个类只做它该做的事情。（单一职责原则想表达的就是&rdquo;高内聚&rdquo;，它里面的每个功能模块也应该是可以轻易的拿到其他系统中使用的，这样才能实现软件复用的目标。）<br />\n- 开闭原则：软件实体应当对扩展开放，对修改关闭。（在理想的状态下，当我们需要为一个软件系统增加新功能时，只需要从原来的系统派生出一些新类就可以，不需要修改原来的任何一行代码。要做到开闭有两个要点：①抽象是关键，一个系统中如果没有抽象类或接口系统就没有扩展点；②封装可变性，将系统中的各种可变因素封装到一个继承结构中，如果多个可变因素混杂在一起，系统将变得复杂而换乱。）<br />\n- 依赖倒转原则：面向接口编程。（该原则说得直白和具体一些就是声明方法的参数类型、方法的返回类型、变量的引用类型时，尽可能使用抽象类型而不用具体类型，因为抽象类型可以被它的任何一个子类型所替代，请参考下面的里氏替换原则。）<br />\n- 里氏替换原则：任何时候都可以用子类型替换掉父类型。（关于里氏替换原则的描述，简单的说就是能用父类型的地方就一定能使用子类型。里氏替换原则可以检查继承关系是否合理，如果一个继承关系违背了里氏替换原则，那么这个继承关系一定是错误的，需要对代码进行重构。需要注意的是：子类一定是增加父类的能力而不是减少父类的能力。）<br />\n- 接口隔离原则：接口要小而专，绝不能大而全。（臃肿的接口是对接口的污染，既然接口表示能力，那么一个接口只应该描述一种能力，接口也应该是高度内聚的。例如，琴棋书画就应该分别设计为四个接口，而不应设计成一个接口中的四个方法，因为如果设计成一个接口中的四个方法，那么这个接口很难用，毕竟琴棋书画四样都精通的人还是少数，而如果设计成四个接口，会几项就实现几个接口，这样的话每个接口被复用的可能性是很高的。Java中的接口代表能力、代表约定、代表角色，能否正确的使用接口一定是编程水平高低的重要标识。）<br />\n- 合成聚合复用原则：优先使用聚合或合成关系复用代码。（通过继承来复用代码是面向对象程序设计中被滥用得最多的东西，类与类之间简单的说有三种关系，Is-A关系、Has-A关系、Use-A关系，分别代表继承、关联和依赖。其中，关联关系根据其关联的强度又可以进一步划分为关联、聚合和合成，但说白了都是Has-A关系，合成聚合复用原则想表达的是优先考虑Has-A关系而不是Is-A关系复用代码，需要说明的是，即使在Java的API中也有不少滥用继承的例子，例如Properties类继承了Hashtable类，这些继承明显就是错误的，更好的做法是在Properties类中放置一个Hashtable类型的成员并且将其键和值都设置为字符串来存储数据。记住：任何时候都不要继承工具类，工具是可以拥有并可以使用的，而不是拿来继承的。）<br />\n- 迪米特法则：迪米特法则又叫最少知识原则，一个对象应当对其他对象有尽可能少的了解。（迪米特法则简单的说就是如何做到&rdquo;低耦合&rdquo;。）</p>\n\n<p><strong>90、简述一下你了解的设计模式。</strong><br />\n答：所谓设计模式，就是一套被反复使用的代码设计经验的总结（情境中一个问题经过证实的一个解决方案）。使用设计模式是为了可重用代码、让代码更容易被他人理解、保证代码可靠性。设计模式使人们可以更加简单方便的复用成功的设计和体系结构。将已证实的技术表述成设计模式也会使新系统开发者更加容易理解其设计思路。<br />\n在GoF的《Design Patterns: Elements of Reusable Object-Oriented Software》中给出了三类（创建型[对类的实例化过程的抽象化]、结构型[描述如何将类或对象结合在一起形成更大的结构]、行为型[对在不同的对象之间划分责任和算法的抽象化]）共23种设计模式，包括：Abstract Factory（抽象工厂模式），Builder（建造者模式），Factory Method（工厂方法模式），Prototype（原始模型模式），Singleton（单例模式）；Facade（门面模式），Adapter（适配器模式），Bridge（桥梁模式），Composite（合成模式），Decorator（装饰模式），Flyweight（享元模式），Proxy（代理模式）；Command（命令模式），Interpreter（解释器模式），Visitor（访问者模式），Iterator（迭代子模式），Mediator（调停者模式），Memento（备忘录模式），Observer（观察者模式），State（状态模式），Strategy（策略模式），Template Method（模板方法模式）， Chain Of Responsibility（责任链模式）。<br />\n面试被问到关于设计模式的知识时，可以拣最常用的作答，例如：<br />\n- 工厂模式：工厂类可以根据条件生成不同的子类实例，这些子类有一个公共的抽象父类并且实现了相同的方法，但是这些方法针对不同的数据进行了不同的操作（多态方法）。当得到子类的实例后，开发人员可以调用基类中的方法而不必考虑到底返回的是哪一个子类的实例。<br />\n- 代理模式：给一个对象提供一个代理对象，并由代理对象控制原对象的引用。实际开发中，按照使用目的的不同，代理可以分为：远程代理、虚拟代理、保护代理、Cache代理、防火墙代理、同步化代理、智能引用代理。<br />\n- 适配器模式：把一个类的接口变换成客户端所期待的另一种接口，从而使原本因接口不匹配而无法在一起使用的类能够一起工作。<br />\n- 模板方法模式：提供一个抽象类，将部分逻辑以具体方法或构造器的形式实现，然后声明一些抽象方法来迫使子类实现剩余的逻辑。不同的子类可以以不同的方式实现这些抽象方法（多态实现），从而实现不同的业务逻辑。<br />\n除此之外，还可以讲讲上面提到的门面模式、桥梁模式、单例模式、装潢模式（Collections工具类和I/O系统中都使用装潢模式）等，反正基本原则就是拣自己最熟悉的、用得最多的作答，以免言多必失。</p>\n\n<p><strong>91、用Java写一个单例类。</strong></p>\n\n<p>- 饿汉式单例</p>\n\n<pre>\n<code>public class Singleton {\n    private Singleton(){}\n    private static Singleton instance = new Singleton();\n    public static Singleton getInstance(){\n        return instance;\n    }\n}</code></pre>\n\n<p>-懒汉式单例</p>\n\n<pre>\n<code>public class Singleton {\n    private static Singleton instance = null;\n    private Singleton() {}\n    public static synchronized Singleton getInstance(){\n        if (instance == null) instance ＝ new Singleton();\n        return instance;\n    }\n}</code></pre>\n\n<ul>\n</ul>\n\n<blockquote>\n<p><strong>注意：</strong>实现一个单例有两点注意事项，①将构造器私有，不允许外界通过构造器创建对象；②通过公开的静态方法向外界返回类的唯一实例。这里有一个问题可以思考：Spring的IoC容器可以为普通的类创建单例，它是怎么做到的呢？</p>\n</blockquote>\n\n<p><strong>92、什么是UML？</strong><br />\n答：UML是统一建模语言（Unified Modeling Language）的缩写，它发表于1997年，综合了当时已经存在的面向对象的建模语言、方法和过程，是一个支持模型化和软件系统开发的图形化语言，为软件开发的所有阶段提供模型化和可视化支持。使用UML可以帮助沟通与交流，辅助应用设计和文档的生成，还能够阐释系统的结构和行为。</p>\n\n<p><strong>93、UML中有哪些常用的图？</strong><br />\n答：UML定义了多种图形化的符号来描述软件系统部分或全部的静态结构和动态结构，包括：用例图（use case diagram）、类图（class diagram）、时序图（sequence diagram）、协作图（collaboration diagram）、状态图（statechart diagram）、活动图（activity diagram）、构件图（component diagram）、部署图（deployment diagram）等。在这些图形化符号中，有三种图最为重要，分别是：用例图（用来捕获需求，描述系统的功能，通过该图可以迅速的了解系统的功能模块及其关系）、类图（描述类以及类与类之间的关系，通过该图可以快速了解系统）、时序图（描述执行特定任务时对象之间的交互关系以及执行顺序，通过该图可以了解对象能接收的消息也就是说对象能够向外界提供的服务）。<br />\n用例图：</p>\n\n<p><img alt=\"\" src=\"http://localhost:8080/myLog/images/interview/20150408151744237\" style=\"height:284px; width:500px\" /></p>\n\n<p>类图：<br />\n<img alt=\"\" src=\"http://localhost:8080/myLog/images/interview/20150408151843748\" style=\"height:273px; width:500px\" /><br />\n时序图：</p>\n\n<p><img alt=\"\" src=\"http://localhost:8080/myLog/images/interview/20150408153230144\" style=\"height:256px; width:500px\" /></p>\n\n<p><strong>94、用Java写一个冒泡排序。</strong><br />\n答：冒泡排序几乎是个程序员都写得出来，但是面试的时候如何写一个逼格高的冒泡排序却不是每个人都能做到，下面提供一个参考代码：</p>\n\n<pre>\n<code>public int[] main(int[] a) {\n	for (int i = 0; i &lt; a.length-1; i++) {\n		for (int j = 0; j &lt; a.length-i-1; j++) {\n			if(a[j]&gt;a[j+1]){\n				int temp = a[j];\n				a[j] = a[j+1];\n				a[j+1] = temp;\n			}\n		}\n	}\n	return a;\n}</code></pre>\n\n<p><strong>95、用Java写一个折半查找。</strong><br />\n答：折半查找，也称二分查找、二分搜索，是一种在有序数组中查找某一特定元素的搜索算法。搜素过程从数组的中间元素开始，如果中间元素正好是要查找的元素，则搜素过程结束；如果某一特定元素大于或者小于中间元素，则在数组大于或小于中间元素的那一半中查找，而且跟开始一样从中间元素开始比较。如果在某一步骤数组已经为空，则表示找不到指定的元素。这种搜索算法每一次比较都使搜索范围缩小一半，其时间复杂度是O(logN)。</p>\n\n<pre>\n<code>public static int binarySearch(int[] a, int intValue) {\n	int start = 0;\n	int end = a.length;\n	if(a[start]==intValue){\n		return start;\n	}\n	if(a[end]==intValue){\n		return end;\n	}\n	return binarySearch(a, intValue, start, end);\n}\nprivate static int binarySearch(int[] a, int intValue, int start, int end) {\n	int mid = (start+end)&gt;&gt;&gt;1;\n	if(a[mid] == intValue){\n		return mid;\n	}else if(a[mid]&gt;intValue){\n		end = mid;\n	}else{\n		start = mid;\n	}\n	return binarySearch(a, intValue, start, end);\n}\n</code></pre>\n\n<p>&nbsp;</p>', 1, '5f199b8885e24fc8b28672b872edb606', 4, '2018-09-14 10:19:04', '2018-09-17 07:58:15');
COMMIT;

-- ----------------------------
-- Table structure for logtype
-- ----------------------------
DROP TABLE IF EXISTS `logtype`;
CREATE TABLE `logtype` (
  `id` varchar(32) NOT NULL,
  `typeVal` varchar(100) NOT NULL,
  `state` int(1) NOT NULL DEFAULT '0' COMMENT '2删除',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

-- ----------------------------
-- Records of logtype
-- ----------------------------
BEGIN;
INSERT INTO `logtype` VALUES ('1dff2a5fdaeb4886938bb7c70b57acce', 'Java', 0, '2018-08-29 16:14:20', '2018-08-29 21:46:03');
INSERT INTO `logtype` VALUES ('3ba9a3aed3ce48f682974ebcf3552ce2', 'Other', 0, '2018-08-29 16:14:20', '2018-08-29 21:46:07');
INSERT INTO `logtype` VALUES ('4eca926aa543420baea2f03f506d042e', 'Database', 0, '2018-08-29 16:14:20', '2018-08-29 21:46:30');
INSERT INTO `logtype` VALUES ('716b0d3ea6e04d03a9deb097be1e2cf1', '科普', 0, '2018-08-29 16:14:20', '2018-08-29 16:14:20');
INSERT INTO `logtype` VALUES ('7338e53acd514defa1a17e47016f3f4a', 'Elasticsearch', 0, '2018-08-29 16:14:20', '2018-08-29 21:46:37');
INSERT INTO `logtype` VALUES ('811e56b193664cf4b2802f8849a2f7d1', '架构', 0, '2018-08-29 16:14:20', '2018-08-29 16:14:20');
INSERT INTO `logtype` VALUES ('a8b33a9f7934469cadee4e88a796c287', 'Math', 0, '2018-08-29 16:14:20', '2018-08-29 21:45:59');
INSERT INTO `logtype` VALUES ('c7343568da3a4c15a29f90d9c3eabb28', 'Design', 0, '2018-09-09 12:21:31', '2018-09-09 14:45:04');
INSERT INTO `logtype` VALUES ('d7caeba238f0466d87db109b2b9724da', 'MachineLearning', 0, '2018-08-29 16:14:20', '2018-08-29 21:46:41');
INSERT INTO `logtype` VALUES ('de93ceeed0464710af6dbb22e535d0a2', 'Python', 0, '2018-08-29 16:14:20', '2018-08-29 16:14:20');
INSERT INTO `logtype` VALUES ('e5e23ac742ca41a0a7e6f575fc710f4e', 'Linux', 0, '2018-08-29 16:14:20', '2018-08-29 16:14:20');
INSERT INTO `logtype` VALUES ('f29612168904487aadb4043df110361c', 'Software', 0, '2018-08-29 16:14:20', '2018-08-29 21:46:53');
COMMIT;

-- ----------------------------
-- Table structure for loguser
-- ----------------------------
DROP TABLE IF EXISTS `loguser`;
CREATE TABLE `loguser` (
  `id` varchar(32) NOT NULL,
  `userName` varchar(100) NOT NULL,
  `userPassword` varchar(100) NOT NULL,
  `userSalt` varchar(32) NOT NULL,
  `userLv` int(2) NOT NULL,
  `state` int(1) NOT NULL DEFAULT '0',
  `createTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `updateTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

-- ----------------------------
-- Records of loguser
-- ----------------------------
BEGIN;
INSERT INTO `loguser` VALUES ('2827b947011e4b17bce1519f33b38dc5', 'cindy', '1boE5K7S9Tw0pjSVkfhbehVg==', '1bc837e8d2cc4e1ba749ddf1c66b9d2a', 1, 0, '2018-02-21 00:00:00', '2018-08-29 16:13:26');
INSERT INTO `loguser` VALUES ('5f199b8885e24fc8b28672b872edb606', 'c', '1931XOlp+9UGembf5Af2wn0A==', '190143d0c2c64fbd9fbb2403a4a136f7', 1, 0, '2018-02-24 13:43:27', '2018-08-29 16:13:26');
INSERT INTO `loguser` VALUES ('6649efbcc20a48f1bf6f83a950624fc8', 'z', '2f98ZYcvixPoh1+bDIW1p9Ew==', '2fc678e207fa44c5ba6f75b51fab272b', 1, 1, '2018-02-24 00:00:00', '2018-09-01 10:58:22');
INSERT INTO `loguser` VALUES ('adf44a046dee4d8694c1b71a0efbc4cc', 'l', 'c03GSTu2V2t/RNuamGeTJOpg==', 'c0771679d95d4012b022b36c224fcc2f', 1, 0, '2018-02-24 00:00:00', '2018-08-29 16:13:26');
INSERT INTO `loguser` VALUES ('d6e6becd0bdf4d3e90309a6af4565529', 'xintianyu', '24g2F9JZsVWRKHYhp/KoOZLQ==', '24b11047224f409da52827b9941fccd9', 1, 0, '2018-02-24 13:43:30', '2018-08-29 16:13:26');
INSERT INTO `loguser` VALUES ('f174d496cabc4915ade15093cb2a3261', 'admin', '93HCBRP7E0R43wdXiUkuwWQQ==', '93f8127f133d4ec59a7b3f5f78e7718b', 0, 0, '2018-02-24 13:43:34', '2018-08-29 16:13:26');
COMMIT;

-- ----------------------------
-- Table structure for operate_db
-- ----------------------------
DROP TABLE IF EXISTS `operate_db`;
CREATE TABLE `operate_db` (
  `OPERA_ID` varchar(32) NOT NULL,
  `OPERA_USER` varchar(32) NOT NULL,
  `OPERA_CONTENT` varchar(1000) NOT NULL,
  `OPERA_TIME` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  `STATE` varchar(10) NOT NULL,
  PRIMARY KEY (`OPERA_ID`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

SET FOREIGN_KEY_CHECKS = 1;
