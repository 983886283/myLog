<html>
<!DOCTYPE html>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>中文自然语言处理</title>
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
	<link rel="stylesheet" href="../css/bootstrap.min.css">
	<link rel="stylesheet" href="../css/font-awesome.min.css">
    <link rel="stylesheet" href="../plugins/ckeditor/plugins/codesnippet/lib/highlight/styles/googlecode.css">
    <link rel="stylesheet" type="text/css" href="../css/front/screen.css">
    <script src="../plugins/ckeditor/plugins/codesnippet/lib/highlight/highlight.pack.js"></script>
	<style type="text/css">
		.list{
			margin-top:5em;
			position: fixed;
		}
		
		.list h4{
			padding-bottom:0.5em;
			border-bottom:2px solid lightgray;
		}
		
		.list .readTitle li{
			margin-top:0.4em;
			padding-left:1em;
			border:2px solid white;
		}
		.list .readTitle li:hover{
			border-left:2px solid black;
			color:blue;
		}
		.liHover{
			border-left:2px solid black !important;
		}
		.aHover{
			color:blue;
		}
		.list .readTitle li a:hover{
			color:blue;
			text-decoration: none;
		}
	</style>
</head>
<body>
    <section class="content-wrap container-fluid">
        <div class="container">
            <div class="row">
				<div class="col-md-3 visible-lg-block visible-md-block">
					<div class="list">
						<h4>阅读目录</h4>
						<div>
							<ul class="readTitle list-unstyled"></ul>
						</div>
					</div>
				</div>
                 <main class="col-md-19 main-content">
					<article class="post row">
					<header class="post-head">
				        <h1 class="post-title">中文自然语言处理</h1>
				        <section class="post-meta">
					        <span><span class="glyphicon glyphicon-user"></span>:<span>c</span></span>|
				            <span><span class="glyphicon glyphicon-bookmark"></span>:<span>MachineLearning</span></span>|
				            <span><span class="glyphicon glyphicon-time"></span>:<span>2018-09-05</span></span>
				        </section>
				    </header>
				    <section class="post-content"><p><a id="第01课：中文自然语言处理的完整机器处理流程" name="第01课：中文自然语言处理的完整机器处理流程"></a>第01课：中文自然语言处理的完整机器处理流程</p>

<p>2016年全球瞩目的围棋大战中，人类以失败告终，更是激起了各种&ldquo;机器超越、控制人类&rdquo;的讨论，然而机器真的懂人类吗？机器能感受到人类的情绪吗？机器能理解人类的语言吗？如果能，那它又是如何做到呢？带着这样好奇心，本文将带领大家熟悉和回顾一个完整的自然语言处理过程，后续所有章节所有示例开发都将遵从这个处理过程。</p>

<p>首先我们通过一张图（来源：网络）来了解 NLP 所包含的技术知识点，这张图从分析对象和分析内容两个不同的维度来进行表达，个人觉得内容只能作为参考，对于整个 AI 背景下的自然语言处理来说还不够完整。</p>

<p><img alt="enter image description here" src="http://images.gitbook.cn/b5701e90-5f4b-11e8-8a60-1bdde4cc4659" style="height:1620px; width:400px" /></p>

<p>有机器学习相关经验的人都知道，中文自然语言处理的过程和机器学习过程大体一致，但又存在很多细节上的不同点，下面我们就来看看中文自然语言处理的基本过程有哪些呢？</p>

<h3>获取语料</h3>

<p>语料，即语言材料。语料是语言学研究的内容。语料是构成语料库的基本单元。所以，人们简单地用文本作为替代，并把文本中的上下文关系作为现实世界中语言的上下文关系的替代品。我们把一个文本集合称为语料库（Corpus），当有几个这样的文本集合的时候，我们称之为语料库集合(Corpora)。（定义来源：百度百科）按语料来源，我们将语料分为以下两种：</p>

<p><strong>1.已有语料</strong></p>

<p>很多业务部门、公司等组织随着业务发展都会积累有大量的纸质或者电子文本资料。那么，对于这些资料，在允许的条件下我们稍加整合，把纸质的文本全部电子化就可以作为我们的语料库。</p>

<p><strong>2.网上下载、抓取语料</strong></p>

<p>如果现在个人手里没有数据怎么办呢？这个时候，我们可以选择获取国内外标准开放数据集，比如国内的<strong>中文汉语有搜狗语料</strong>、<strong>人民日报语料</strong>。国外的因为大都是英文或者外文，这里暂时用不到。也可以选择通过爬虫自己去抓取一些数据，然后来进行后续内容。</p>

<h3>语料预处理</h3>

<p>这里重点介绍一下语料的预处理，在一个完整的中文自然语言处理工程应用中，语料预处理大概会占到整个50%-70%的工作量，所以开发人员大部分时间就在进行语料预处理。下面通过数据洗清、分词、词性标注、去停用词四个大的方面来完成语料的预处理工作。</p>

<p><strong>1.语料清洗</strong></p>

<p>数据清洗，顾名思义就是在语料中找到我们感兴趣的东西，把不感兴趣的、视为噪音的内容清洗删除，包括对于原始文本提取标题、摘要、正文等信息，对于爬取的网页内容，去除广告、标签、HTML、JS 等代码和注释等。常见的数据清洗方式有：人工去重、对齐、删除和标注等，或者规则提取内容、正则表达式匹配、根据词性和命名实体提取、编写脚本或者代码批处理等。</p>

<p><strong>2.分词</strong></p>

<p>中文语料数据为一批短文本或者长文本，比如：句子，文章摘要，段落或者整篇文章组成的一个集合。一般句子、段落之间的字、词语是连续的，有一定含义。而进行文本挖掘分析时，我们希望文本处理的最小单位粒度是词或者词语，所以这个时候就需要分词来将文本全部进行分词。</p>

<p>常见的分词算法有：基于字符串匹配的分词方法、基于理解的分词方法、基于统计的分词方法和基于规则的分词方法，每种方法下面对应许多具体的算法。</p>

<p>当前中文分词算法的主要难点有歧义识别和新词识别，比如：&ldquo;羽毛球拍卖完了&rdquo;，这个可以切分成&ldquo;羽毛 球拍 卖 完 了&rdquo;，也可切分成&ldquo;羽毛球 拍卖 完 了&rdquo;，如果不依赖上下文其他的句子，恐怕很难知道如何去理解。</p>

<p><strong>3.词性标注</strong></p>

<p>词性标注，就是给每个词或者词语打词类标签，如形容词、动词、名词等。这样做可以让文本在后面的处理中融入更多有用的语言信息。词性标注是一个经典的序列标注问题，不过对于有些中文自然语言处理来说，词性标注不是非必需的。比如，常见的文本分类就不用关心词性问题，但是类似情感分析、知识推理却是需要的，下图是常见的中文词性整理。</p>

<p><img alt="enter image description here" src="http://images.gitbook.cn/0fdc1ad0-5fc1-11e8-a59f-c7ac04233ce1" /></p>

<p>常见的词性标注方法可以分为基于规则和基于统计的方法。其中基于统计的方法，如基于最大熵的词性标注、基于统计最大概率输出词性和基于 HMM 的词性标注。</p>

<p><strong>4.去停用词</strong></p>

<p>停用词一般指对文本特征没有任何贡献作用的字词，比如标点符号、语气、人称等一些词。所以在一般性的文本处理中，分词之后，接下来一步就是去停用词。但是对于中文来说，去停用词操作不是一成不变的，停用词词典是根据具体场景来决定的，比如在情感分析中，语气词、感叹号是应该保留的，因为他们对表示语气程度、感情色彩有一定的贡献和意义。</p>

<h3>特征工程</h3>

<p>做完语料预处理之后，接下来需要考虑如何把分词之后的字和词语表示成计算机能够计算的类型。显然，如果要计算我们至少需要把中文分词的字符串转换成数字，确切的说应该是数学中的向量。有两种常用的表示模型分别是词袋模型和词向量。</p>

<p>词袋模型（Bag of Word, BOW)，即不考虑词语原本在句子中的顺序，直接将每一个词语或者符号统一放置在一个集合（如 list），然后按照计数的方式对出现的次数进行统计。统计词频这只是最基本的方式，TF-IDF 是词袋模型的一个经典用法。</p>

<p>词向量是将字、词语转换成向量矩阵的计算模型。目前为止最常用的词表示方法是 One-hot，这种方法把每个词表示为一个很长的向量。这个向量的维度是词表大小，其中绝大多数元素为 0，只有一个维度的值为 1，这个维度就代表了当前的词。还有 Google 团队的 Word2Vec，其主要包含两个模型：跳字模型（Skip-Gram）和连续词袋模型（Continuous Bag of Words，简称 CBOW），以及两种高效训练的方法：负采样（Negative Sampling）和层序 Softmax（Hierarchical Softmax）。值得一提的是，Word2Vec 词向量可以较好地表达不同词之间的相似和类比关系。除此之外，还有一些词向量的表示方式，如 Doc2Vec、WordRank 和 FastText 等。</p>

<h3>特征选择</h3>

<p>同数据挖掘一样，在文本挖掘相关问题中，特征工程也是必不可少的。在一个实际问题中，构造好的特征向量，是要选择合适的、表达能力强的特征。文本特征一般都是词语，具有语义信息，使用特征选择能够找出一个特征子集，其仍然可以保留语义信息；但通过特征提取找到的特征子空间，将会丢失部分语义信息。所以特征选择是一个很有挑战的过程，更多的依赖于经验和专业知识，并且有很多现成的算法来进行特征的选择。目前，常见的特征选择方法主要有 DF、 MI、 IG、 CHI、WLLR、WFO 六种。</p>

<h3>模型训练</h3>

<p>在特征向量选择好之后，接下来要做的事情当然就是训练模型，对于不同的应用需求，我们使用不同的模型，传统的有监督和无监督等机器学习模型， 如 KNN、SVM、Naive Bayes、决策树、GBDT、K-means 等模型；深度学习模型比如 CNN、RNN、LSTM、 Seq2Seq、FastText、TextCNN 等。这些模型在后续的分类、聚类、神经序列、情感分析等示例中都会用到，这里不再赘述。下面是在模型训练时需要注意的几个点。</p>

<p><strong>1.注意过拟合、欠拟合问题，不断提高模型的泛化能力。</strong></p>

<p><strong>过拟合</strong>：模型学习能力太强，以至于把噪声数据的特征也学习到了，导致模型泛化能力下降，在训练集上表现很好，但是在测试集上表现很差。</p>

<p>常见的解决方法有：</p>

<ul>
	<li>增大数据的训练量；</li>
	<li>增加正则化项，如 L1 正则和 L2 正则；</li>
	<li>特征选取不合理，人工筛选特征和使用特征选择算法；</li>
	<li>采用 Dropout 方法等。</li>
</ul>

<p><strong>欠拟合</strong>：就是模型不能够很好地拟合数据，表现在模型过于简单。</p>

<p>常见的解决方法有：</p>

<ul>
	<li>添加其他特征项；</li>
	<li>增加模型复杂度，比如神经网络加更多的层、线性模型通过添加多项式使模型泛化能力更强；</li>
	<li>减少正则化参数，正则化的目的是用来防止过拟合的，但是现在模型出现了欠拟合，则需要减少正则化参数。</li>
</ul>

<p><strong>2.对于神经网络，注意梯度消失和梯度爆炸问题。</strong></p>

<h3>评价指标</h3>

<p>训练好的模型，上线之前要对模型进行必要的评估，目的让模型对语料具备较好的泛化能力。具体有以下这些指标可以参考。</p>

<p><strong>1.错误率、精度、准确率、精确度、召回率、F1 衡量。</strong></p>

<p><strong>错误率</strong>：是分类错误的样本数占样本总数的比例。对样例集 D，分类错误率计算公式如下：</p>

<p><img alt="enter image description here" src="http://images.gitbook.cn/03e7a210-5fc8-11e8-b864-0bd1f4b74dfb" /></p>

<p><strong>精度</strong>：是分类正确的样本数占样本总数的比例。这里的分类正确的样本数指的不仅是正例分类正确的个数还有反例分类正确的个数。对样例集 D，精度计算公式如下：</p>

<p><img alt="enter image description here" src="http://images.gitbook.cn/270be8f0-5fc8-11e8-b864-0bd1f4b74dfb" /></p>

<p>对于二分类问题，可将样例根据其真实类别与学习器预测类别的组合划分为真正例（True Positive）、假正例（False Positive）、真反例（True Negative)、假反例（False Negative）四种情形，令 TP、FP、TN、FN 分别表示其对应的样例数，则显然有 TP+FP++TN+FN=样例总数。分类结果的&ldquo;混淆矩阵&rdquo;（Confusion Matrix）如下：</p>

<p><img alt="enter image description here" src="http://images.gitbook.cn/96e35960-5fc8-11e8-a59f-c7ac04233ce1" /></p>

<p><strong>准确率</strong>，缩写表示用 P。准确率是针对我们预测结果而言的，它表示的是预测为正的样例中有多少是真正的正样例。定义公式如下：</p>

<p><img alt="enter image description here" src="http://images.gitbook.cn/2448b4d0-5fc9-11e8-8a60-1bdde4cc4659" /></p>

<p><strong>精确度</strong>，缩写表示用 A。精确度则是分类正确的样本数占样本总数的比例。Accuracy 反应了分类器对整个样本的判定能力（即能将正的判定为正的，负的判定为负的）。定义公式如下：</p>

<p><img alt="enter image description here" src="http://images.gitbook.cn/2bc44b70-5fc9-11e8-a59f-c7ac04233ce1" /></p>

<p><strong>召回率</strong>，缩写表示用 R。召回率是针对我们原来的样本而言的，它表示的是样本中的正例有多少被预测正确。定义公式如下：</p>

<p><img alt="enter image description here" src="http://images.gitbook.cn/320a2130-5fc9-11e8-b864-0bd1f4b74dfb" /></p>

<p><strong>F1 衡量</strong>，表达出对查准率/查全率的不同偏好。定义公式如下：</p>

<p><img alt="enter image description here" src="http://images.gitbook.cn/b74ad4c0-5fc9-11e8-a59f-c7ac04233ce1" /></p>

<p><strong>2.ROC 曲线、AUC 曲线。</strong></p>

<p>ROC 全称是&ldquo;受试者工作特征&rdquo;（Receiver Operating Characteristic）曲线。我们根据模型的预测结果，把阈值从0变到最大，即刚开始是把每个样本作为正例进行预测，随着阈值的增大，学习器预测正样例数越来越少，直到最后没有一个样本是正样例。在这一过程中，每次计算出两个重要量的值，分别以它们为横、纵坐标作图，就得到了 ROC 曲线。</p>

<p>ROC 曲线的纵轴是&ldquo;真正例率&rdquo;（True Positive Rate, 简称 TPR)，横轴是&ldquo;假正例率&rdquo;（False Positive Rate,简称FPR），两者分别定义为：</p>

<p><img alt="enter image description here" src="http://images.gitbook.cn/20696b90-63aa-11e8-b82b-ffbb9d1e8856" /></p>

<p><img alt="enter image description here" src="http://images.gitbook.cn/f6e06a60-63ab-11e8-b82b-ffbb9d1e8856" /></p>

<p><strong>ROC 曲线的意义有以下几点：</strong></p>

<ol>
	<li>ROC 曲线能很容易的查出任意阈值对模型的泛化性能影响；</li>
	<li>有助于选择最佳的阈值；</li>
	<li>可以对不同的模型比较性能，在同一坐标中，靠近左上角的 ROC 曲所代表的学习器准确性最高。</li>
</ol>

<p>如果两条 ROC 曲线没有相交，我们可以根据哪条曲线最靠近左上角哪条曲线代表的学习器性能就最好。但是实际任务中，情况很复杂，若两个模型的 ROC 曲线发生交叉，则难以一般性的断言两者孰优孰劣。此时如果一定要进行比较，则比较合理的判断依据是比较 ROC 曲线下的面积，即AUC（Area Under ROC Curve）。</p>

<p>AUC 就是 ROC 曲线下的面积，衡量学习器优劣的一种性能指标。AUC 是衡量二分类模型优劣的一种评价指标，表示预测的正例排在负例前面的概率。</p>

<p>前面我们所讲的都是针对二分类问题，那么如果实际需要在多分类问题中用 ROC 曲线的话，一般性的转化为多个&ldquo;一对多&rdquo;的问题。即把其中一个当作正例，其余当作负例来看待，画出多个 ROC 曲线。</p>

<h3>模型上线应用</h3>

<p>模型线上应用，目前主流的应用方式就是提供服务或者将模型持久化。</p>

<p>第一就是线下训练模型，然后将模型做线上部署，发布成接口服务以供业务系统使用。</p>

<p>第二种就是在线训练，在线训练完成之后把模型 pickle 持久化，然后在线服务接口模板通过读取 pickle 而改变接口服务。</p>

<h3>模型重构（非必须）</h3>

<p>随着时间和变化，可能需要对模型做一定的重构，包括根据业务不同侧重点对上面提到的一至七步骤也进行调整，重新训练模型进行上线。</p>

<h3>参考文献</h3>

<ol>
	<li>周志华《机器学习》</li>
	<li>李航《统计学习方法》</li>
	<li>伊恩&middot;古德费洛《深度学习》</li>
</ol>

<p><a id="第02课：简单好用的中文分词利器 jieba 和 HanLP" name="第02课：简单好用的中文分词利器 jieba 和 HanLP"></a>第02课：简单好用的中文分词利器 jieba 和 HanLP</p>

<h3>前言</h3>

<p>从本文开始，我们就要真正进入实战部分。首先，我们按照中文自然语言处理流程的第一步获取语料，然后重点进行中文分词的学习。中文分词有很多种，常见的比如有中科院计算所 NLPIR、哈工大 LTP、清华大学 THULAC 、斯坦福分词器、Hanlp 分词器、jieba 分词、IKAnalyzer 等。这里针对 jieba 和 HanLP 分别介绍不同场景下的中文分词应用。</p>

<h3>jieba 分词</h3>

<h4>jieba 安装</h4>

<p>（1）Python 2.x 下 jieba 的三种安装方式，如下：</p>

<ul>
	<li>
	<p><strong>全自动安装</strong>：执行命令&nbsp;<code>easy_install jieba</code>&nbsp;或者&nbsp;<code>pip install jieba</code>&nbsp;/<code>pip3 install jieba</code>，可实现全自动安装。</p>
	</li>
	<li>
	<p><strong>半自动安装</strong>：先<a href="https://pypi.python.org/pypi/jieba/">下载 jieba</a>，解压后运行&nbsp;<code>python setup.py install</code>。</p>
	</li>
	<li>
	<p><strong>手动安装</strong>：将 jieba 目录放置于当前目录或者 site-packages 目录。</p>
	</li>
</ul>

<p>安装完通过&nbsp;<code>import jieba</code>&nbsp;验证安装成功与否。</p>

<p>（2）Python 3.x 下的安装方式。</p>

<p>Github 上 jieba 的 Python3.x 版本的路径是：https://github.com/fxsjy/jieba/tree/jieba3k。</p>

<p>通过&nbsp;<code>git clone https://github.com/fxsjy/jieba.git</code>&nbsp;命令下载到本地，然后解压，再通过命令行进入解压目录，执行&nbsp;<code>python setup.py install</code>&nbsp;命令，即可安装成功。</p>

<h4>jieba 的分词算法</h4>

<p>主要有以下三种：</p>

<ol>
	<li>基于统计词典，构造前缀词典，基于前缀词典对句子进行切分，得到所有切分可能，根据切分位置，构造一个有向无环图（DAG）；</li>
	<li>基于DAG图，采用动态规划计算最大概率路径（最有可能的分词结果），根据最大概率路径分词；</li>
	<li>对于新词(词库中没有的词），采用有汉字成词能力的 HMM 模型进行切分。</li>
</ol>

<h4>jieba 分词</h4>

<p>下面我们进行 jieba 分词练习，第一步首先引入 jieba 和语料:</p>

<pre>
<code>import jieba
content = "现如今，机器学习和深度学习带动人工智能飞速的发展，并在图片处理、语音识别领域取得巨大成功。"
</code></pre>

<p>（1）<strong>精确分词</strong></p>

<p>精确分词：精确模式试图将句子最精确地切开，精确分词也是默认分词。</p>

<pre>
<code>segs_1 = jieba.cut(content, cut_all=False)
print("/".join(segs_1))
</code></pre>

<p>其结果为：</p>

<blockquote>
<p>现如今/，/机器/学习/和/深度/学习/带动/人工智能/飞速/的/发展/，/并/在/图片/处理/、/语音/识别/领域/取得/巨大成功/。</p>
</blockquote>

<p>（2）<strong>全模式</strong></p>

<p>全模式分词：把句子中所有的可能是词语的都扫描出来，速度非常快，但不能解决歧义。</p>

<pre>
<code>segs_3 = jieba.cut(content, cut_all=True)
print("/".join(segs_3))
</code></pre>

<p>结果为：</p>

<blockquote>
<p>现如今/如今///机器/学习/和/深度/学习/带动/动人/人工/人工智能/智能/飞速/的/发展///并/在/图片/处理///语音/识别/领域/取得/巨大/巨大成功/大成/成功//</p>
</blockquote>

<p>（3）<strong>搜索引擎模式</strong></p>

<p>搜索引擎模式：在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。</p>

<pre>
<code>segs_4 = jieba.cut_for_search(content)
print("/".join(segs_4))
</code></pre>

<p>结果为：</p>

<blockquote>
<p>如今/现如今/，/机器/学习/和/深度/学习/带动/人工/智能/人工智能/飞速/的/发展/，/并/在/图片/处理/、/语音/识别/领域/取得/巨大/大成/成功/巨大成功/。</p>
</blockquote>

<p>（4）<strong>用 lcut 生成 list</strong></p>

<p>jieba.cut 以及&nbsp;<code>jieba.cut_for_search</code>&nbsp;返回的结构都是一个可迭代的 Generator，可以使用 for 循环来获得分词后得到的每一个词语（Unicode）。jieba.lcut 对 cut 的结果做了封装，l 代表 list，即返回的结果是一个 list 集合。同样的，用&nbsp;<code>jieba.lcut_for_search</code>&nbsp;也直接返回 list 集合。</p>

<pre>
<code>segs_5 = jieba.lcut(content)
print(segs_5)
</code></pre>

<p>结果为：</p>

<blockquote>
<p>[&#39;现如今&#39;, &#39;，&#39;, &#39;机器&#39;, &#39;学习&#39;, &#39;和&#39;, &#39;深度&#39;, &#39;学习&#39;, &#39;带动&#39;, &#39;人工智能&#39;, &#39;飞速&#39;, &#39;的&#39;, &#39;发展&#39;, &#39;，&#39;, &#39;并&#39;, &#39;在&#39;, &#39;图片&#39;, &#39;处理&#39;, &#39;、&#39;, &#39;语音&#39;, &#39;识别&#39;, &#39;领域&#39;, &#39;取得&#39;, &#39;巨大成功&#39;, &#39;。&#39;]</p>
</blockquote>

<p>（5）<strong>获取词性</strong></p>

<p>jieba 可以很方便地获取中文词性，通过 jieba.posseg 模块实现词性标注。</p>

<pre>
<code>import jieba.posseg as psg
print([(x.word,x.flag) for x in psg.lcut(content)])
</code></pre>

<p>结果为：</p>

<blockquote>
<p>[(&#39;现如今&#39;, &#39;t&#39;), (&#39;，&#39;, &#39;x&#39;), (&#39;机器&#39;, &#39;n&#39;), (&#39;学习&#39;, &#39;v&#39;), (&#39;和&#39;, &#39;c&#39;), (&#39;深度&#39;, &#39;ns&#39;), (&#39;学习&#39;, &#39;v&#39;), (&#39;带动&#39;, &#39;v&#39;), (&#39;人工智能&#39;, &#39;n&#39;), (&#39;飞速&#39;, &#39;n&#39;), (&#39;的&#39;, &#39;uj&#39;), (&#39;发展&#39;, &#39;vn&#39;), (&#39;，&#39;, &#39;x&#39;), (&#39;并&#39;, &#39;c&#39;), (&#39;在&#39;, &#39;p&#39;), (&#39;图片&#39;, &#39;n&#39;), (&#39;处理&#39;, &#39;v&#39;), (&#39;、&#39;, &#39;x&#39;), (&#39;语音&#39;, &#39;n&#39;), (&#39;识别&#39;, &#39;v&#39;), (&#39;领域&#39;, &#39;n&#39;), (&#39;取得&#39;, &#39;v&#39;), (&#39;巨大成功&#39;, &#39;nr&#39;), (&#39;。&#39;, &#39;x&#39;)]</p>
</blockquote>

<p>（6）<strong>并行分词</strong></p>

<p>并行分词原理为文本按行分隔后，分配到多个 Python 进程并行分词，最后归并结果。</p>

<p>用法：</p>

<pre>
<code>jieba.enable_parallel(4) # 开启并行分词模式，参数为并行进程数 。
jieba.disable_parallel() # 关闭并行分词模式 。
</code></pre>

<p>注意： 并行分词仅支持默认分词器 jieba.dt 和 jieba.posseg.dt。目前暂不支持 Windows。</p>

<p>（7）<strong>获取分词结果中词列表的 top n</strong></p>

<pre>
<code>from collections import Counter
top5= Counter(segs_5).most_common(5)
print(top5)
</code></pre>

<p>结果为：</p>

<blockquote>
<p>[(&#39;，&#39;, 2), (&#39;学习&#39;, 2), (&#39;现如今&#39;, 1), (&#39;机器&#39;, 1), (&#39;和&#39;, 1)]</p>
</blockquote>

<p>（8）<strong>自定义添加词和字典</strong></p>

<p>默认情况下，使用默认分词，是识别不出这句话中的&ldquo;铁甲网&rdquo;这个新词，这里使用用户字典提高分词准确性。</p>

<pre>
<code>txt = "铁甲网是中国最大的工程机械交易平台。"
print(jieba.lcut(txt))
</code></pre>

<p>结果为：</p>

<blockquote>
<p>[&#39;铁甲&#39;, &#39;网是&#39;, &#39;中国&#39;, &#39;最大&#39;, &#39;的&#39;, &#39;工程机械&#39;, &#39;交易平台&#39;, &#39;。&#39;]</p>
</blockquote>

<p>如果添加一个词到字典，看结果就不一样了。</p>

<pre>
<code>jieba.add_word("铁甲网")
print(jieba.lcut(txt))
</code></pre>

<p>结果为：</p>

<blockquote>
<p>[&#39;铁甲网&#39;, &#39;是&#39;, &#39;中国&#39;, &#39;最大&#39;, &#39;的&#39;, &#39;工程机械&#39;, &#39;交易平台&#39;, &#39;。&#39;]</p>
</blockquote>

<p>但是，如果要添加很多个词，一个个添加效率就不够高了，这时候可以定义一个文件，然后通过&nbsp;<code>load_userdict()</code>函数，加载自定义词典，如下：</p>

<pre>
<code>jieba.load_userdict('user_dict.txt')
print(jieba.lcut(txt))
</code></pre>

<p>结果为：</p>

<blockquote>
<p>[&#39;铁甲网&#39;, &#39;是&#39;, &#39;中国&#39;, &#39;最大&#39;, &#39;的&#39;, &#39;工程机械&#39;, &#39;交易平台&#39;, &#39;。&#39;]</p>
</blockquote>

<p><strong>注意事项：</strong></p>

<p>jieba.cut 方法接受三个输入参数: 需要分词的字符串；<code>cut_all</code>&nbsp;参数用来控制是否采用全模式；HMM 参数用来控制是否使用 HMM 模型。</p>

<p><code>jieba.cut_for_search</code>&nbsp;方法接受两个参数：需要分词的字符串；是否使用 HMM 模型。该方法适合用于搜索引擎构建倒排索引的分词，粒度比较细。</p>

<h3>HanLP 分词</h3>

<h4>pyhanlp 安装</h4>

<p>其为 HanLP 的 Python 接口，支持自动下载与升级 HanLP，兼容 Python2、Python3。</p>

<p>安装命令为&nbsp;<code>pip install pyhanlp</code>，使用命令 hanlp 来验证安装。</p>

<p>pyhanlp 目前使用 jpype1 这个 Python 包来调用 HanLP，如果遇到：</p>

<blockquote>
<p>building &#39;_jpype&#39; extensionerror: Microsoft Visual C++ 14.0 is required. Get it with &quot;Microsoft VisualC++ Build Tools&quot;: http://landinghub.visualstudio.com/visual-cpp-build-tools</p>
</blockquote>

<p><strong>则推荐利用轻量级的 Miniconda 来下载编译好的 jpype1。</strong></p>

<pre>
<code>conda install -c conda-forge jpype1
pip install pyhanlp
</code></pre>

<p><strong>未安装 Java 时会报错</strong>：</p>

<blockquote>
<p>jpype.<em>jvmfinder.JVMNotFoundException: No JVM shared library file (jvm.dll) found. Try setting up the JAVA</em>HOME environment variable properly.</p>
</blockquote>

<p>HanLP 主项目采用 Java 开发，所以需要 Java 运行环境，请安装 JDK。</p>

<h4>命令行交互式分词模式</h4>

<p>在命令行界面，使用命令 hanlp segment 进入交互分词模式，输入一个句子并回车，HanLP 会输出分词结果：</p>

<p><img alt="enter image description here" src="http://images.gitbook.cn/009c2f60-616f-11e8-b864-0bd1f4b74dfb" /></p>

<p>可见，pyhanlp 分词结果是带有词性的。</p>

<h4>服务器模式</h4>

<p>通过 hanlp serve 来启动内置的 HTTP 服务器，默认本地访问地址为：http://localhost:8765 。</p>

<p><img alt="enter image description here" src="http://images.gitbook.cn/d29d52f0-616f-11e8-b864-0bd1f4b74dfb" /></p>

<p><img alt="enter image description here" src="http://images.gitbook.cn/e79a06b0-6171-11e8-b864-0bd1f4b74dfb" /></p>

<p>也可以访问官网演示页面：<a href="http://hanlp.hankcs.com/">http://hanlp.hankcs.com/</a>。</p>

<h4>通过工具类 HanLP 调用常用接口</h4>

<p>通过工具类 HanLP 调用常用接口，这种方式应该是我们在项目中最常用的方式。</p>

<p>（1）分词</p>

<pre>
<code>from pyhanlp import *
content = "现如今，机器学习和深度学习带动人工智能飞速的发展，并在图片处理、语音识别领域取得巨大成功。"
print(HanLP.segment(content))
</code></pre>

<p>结果为：</p>

<blockquote>
<p>[现如今/t, ，/w, 机器学习/gi, 和/cc, 深度/n, 学习/v, 带动/v, 人工智能/n, 飞速/d, 的/ude1, 发展/vn, ，/w, 并/cc, 在/p, 图片/n, 处理/vn, 、/w, 语音/n, 识别/vn, 领域/n, 取得/v, 巨大/a, 成功/a, 。/w]</p>
</blockquote>

<p>（2）自定义词典分词</p>

<p>在没有使用自定义字典时的分词。</p>

<pre>
<code>txt = "铁甲网是中国最大的工程机械交易平台。"
print(HanLP.segment(txt))
</code></pre>

<p>结果为：</p>

<blockquote>
<p>[铁甲/n, 网/n, 是/vshi, 中国/ns, 最大/gm, 的/ude1, 工程/n, 机械/n, 交易/vn, 平台/n, 。/w]</p>
</blockquote>

<p>添加自定义新词：</p>

<pre>
<code>CustomDictionary.add("铁甲网")
CustomDictionary.insert("工程机械", "nz 1024")
CustomDictionary.add("交易平台", "nz 1024 n 1")
print(HanLP.segment(txt))
</code></pre>

<p>结果为：</p>

<blockquote>
<p>[铁甲网/nz, 是/vshi, 中国/ns, 最大/gm, 的/ude1, 工程机械/nz, 交易平台/nz, 。/w]</p>
</blockquote>

<p>当然了，jieba 和 pyhanlp 能做的事还有很多，关键词提取、自动摘要、依存句法分析、情感分析等，后面章节我们将会讲到，这里不再赘述。</p>

<p>参考文献：</p>

<ol>
	<li>https://github.com/fxsjy/jieba</li>
	<li>https://github.com/hankcs/pyhanlp</li>
</ol>

<p><a id="第03课：动手实战中文文本中的关键字提取" name="第03课：动手实战中文文本中的关键字提取"></a>第03课：动手实战中文文本中的关键字提取</p>

<h3>前言</h3>

<p>关键词提取就是从文本里面把跟这篇文章意义最相关的一些词语抽取出来。这个可以追溯到文献检索初期，关键词是为了文献标引工作，从报告、论文中选取出来用以表示全文主题内容信息的单词或术语，在现在的报告和论文中，我们依然可以看到关键词这一项。因此，关键词在文献检索、自动文摘、文本聚类/分类等方面有着重要的应用，它不仅是进行这些工作不可或缺的基础和前提，也是互联网上信息建库的一项重要工作。</p>

<p>关键词抽取从方法来说主要有两种：</p>

<ul>
	<li>
	<p>第一种是关键词分配：就是给定一个已有的关键词库，对于新来的文档从该词库里面匹配几个词语作为这篇文档的关键词。</p>
	</li>
	<li>
	<p>第二种是关键词提取：针对新文档，通过算法分析，提取文档中一些词语作为该文档的关键词。</p>
	</li>
</ul>

<p>目前大多数应用领域的关键词抽取算法都是基于后者实现的，从逻辑上说，后者比前者在实际应用中更准确。</p>

<p>下面介绍一些关于关键词抽取的常用和经典的算法实现。</p>

<h3>基于 TF-IDF 算法进行关键词提取</h3>

<p>在信息检索理论中，TF-IDF 是 Term Frequency - Inverse Document Frequency 的简写。TF-IDF 是一种数值统计，用于反映一个词对于语料中某篇文档的重要性。在信息检索和文本挖掘领域，它经常用于因子加权。TF-IDF 的主要思想就是：如果某个词在一篇文档中出现的频率高，也即 TF 高；并且在语料库中其他文档中很少出现，即 DF 低，也即 IDF 高，则认为这个词具有很好的类别区分能力。</p>

<p>TF 为词频（Term Frequency），表示词 t 在文档 d 中出现的频率，计算公式：</p>

<p><img alt="enter image description here" src="http://images.gitbook.cn/332cea00-61b0-11e8-8a60-1bdde4cc4659" /></p>

<p>其中，ni,jni,j&nbsp;是该词&nbsp;titi&nbsp;在文件&nbsp;djdj&nbsp;中的出现次数，而分母则是在文件&nbsp;djdj&nbsp;中所有字词的出现次数之和。</p>

<p>IDF 为逆文档频率（Inverse Document Frequency），表示语料库中包含词 t 的文档的数目的倒数，计算公式：</p>

<p><img alt="enter image description here" src="http://images.gitbook.cn/8412c0c0-61b0-11e8-b864-0bd1f4b74dfb" /></p>

<p>其中，<code>|D|</code>&nbsp;表示语料库中的文件总数，|{j:ti&isin;dj}||{j:ti&isin;dj}|&nbsp;包含词&nbsp;titi&nbsp;的文件数目，如果该词语不在语料库中，就会导致被除数为零，因此一般情况下使用&nbsp;1+|{j:ti&isin;dj}|1+|{j:ti&isin;dj}|。</p>

<p>TF-IDF 在实际中主要是将二者相乘，也即 TF * IDF， 计算公式：</p>

<p><img alt="enter image description here" src="http://images.gitbook.cn/b78eeb70-61b2-11e8-a59f-c7ac04233ce1" /></p>

<p>因此，TF-IDF 倾向于过滤掉常见的词语，保留重要的词语。例如，某一特定文件内的高频率词语，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的 TF-IDF。</p>

<p>好在 jieba 已经实现了基于 TF-IDF 算法的关键词抽取，通过命令&nbsp;<code>import jieba.analyse</code>&nbsp;引入，函数参数解释如下：</p>

<pre>
<code>jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=())
</code></pre>

<ul>
	<li>
	<p>sentence：待提取的文本语料；</p>
	</li>
	<li>
	<p>topK：返回 TF/IDF 权重最大的关键词个数，默认值为 20；</p>
	</li>
	<li>
	<p>withWeight：是否需要返回关键词权重值，默认值为 False；</p>
	</li>
	<li>
	<p>allowPOS：仅包括指定词性的词，默认值为空，即不筛选。</p>
	</li>
</ul>

<p>接下来看例子，我采用的语料来自于百度百科对人工智能的定义，获取 Top20 关键字，用空格隔开打印：</p>

<pre>
<code>import jieba.analyse
sentence  = "人工智能（Artificial Intelligence），英文缩写为AI。它是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。人工智能是计算机科学的一个分支，它企图了解智能的实质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器，该领域的研究包括机器人、语言识别、图像识别、自然语言处理和专家系统等。人工智能从诞生以来，理论和技术日益成熟，应用领域也不断扩大，可以设想，未来人工智能带来的科技产品，将会是人类智慧的“容器”。人工智能可以对人的意识、思维的信息过程的模拟。人工智能不是人的智能，但能像人那样思考、也可能超过人的智能。人工智能是一门极富挑战性的科学，从事这项工作的人必须懂得计算机知识，心理学和哲学。人工智能是包括十分广泛的科学，它由不同的领域组成，如机器学习，计算机视觉等等，总的说来，人工智能研究的一个主要目标是使机器能够胜任一些通常需要人类智能才能完成的复杂工作。但不同的时代、不同的人对这种“复杂工作”的理解是不同的。2017年12月，人工智能入选“2017年度中国媒体十大流行语”。"
keywords = "  ".join(jieba.analyse.extract_tags(sentence , topK=20, withWeight=False, allowPOS=()))
print(keywords)
</code></pre>

<p>执行结果：</p>

<blockquote>
<p>人工智能 智能 2017 机器 不同 人类 科学 模拟 一门 技术 计算机 研究 工作 Artificial Intelligence AI 图像识别 12 复杂 流行语</p>
</blockquote>

<p>下面只获取 Top10 的关键字，并修改一下词性，只选择名词和动词，看看结果有何不同？</p>

<pre>
<code>keywords =(jieba.analyse.extract_tags(sentence , topK=10, withWeight=True, allowPOS=(['n','v'])))
print(keywords)
</code></pre>

<p>执行结果：</p>

<blockquote>
<p>[(&#39;人工智能&#39;, 0.9750542675762887), (&#39;智能&#39;, 0.5167124540885567), (&#39;机器&#39;, 0.20540911929525774), (&#39;人类&#39;, 0.17414426566082475), (&#39;科学&#39;, 0.17250169374402063), (&#39;模拟&#39;, 0.15723537382948452), (&#39;技术&#39;, 0.14596259315164947), (&#39;计算机&#39;, 0.14030483362639176), (&#39;图像识别&#39;, 0.12324502580309278), (&#39;流行语&#39;, 0.11242211730309279)]</p>
</blockquote>

<h3>基于 TextRank 算法进行关键词提取</h3>

<p>TextRank 是由 PageRank 改进而来，核心思想将文本中的词看作图中的节点，通过边相互连接，不同的节点会有不同的权重，权重高的节点可以作为关键词。这里给出 TextRank 的公式：</p>

<p><img alt="enter image description here" src="http://images.gitbook.cn/a041fb90-61af-11e8-a59f-c7ac04233ce1" /></p>

<p>节点 i 的权重取决于节点 i 的邻居节点中 i-j 这条边的权重 / j 的所有出度的边的权重 * 节点 j 的权重，将这些邻居节点计算的权重相加，再乘上一定的阻尼系数，就是节点 i 的权重，阻尼系数 d 一般取 0.85。</p>

<p>TextRank 用于关键词提取的算法如下：</p>

<p>（1）把给定的文本 T 按照完整句子进行分割，即:</p>

<p><img alt="enter image description here" src="http://images.gitbook.cn/82165be0-61ad-11e8-a59f-c7ac04233ce1" /></p>

<p>（2）对于每个句子，进行分词和词性标注处理，并过滤掉停用词，只保留指定词性的单词，如名词、动词、形容词，其中&nbsp;ti,jti,j&nbsp;是保留后的候选关键词。</p>

<p><img alt="enter image description here" src="http://images.gitbook.cn/736f86a0-61af-11e8-8a60-1bdde4cc4659" /></p>

<p>（3）构建候选关键词图 G = (V,E)，其中 V 为节点集，由（2）生成的候选关键词组成，然后采用共现关系（Co-Occurrence）构造任两点之间的边，两个节点之间存在边仅当它们对应的词汇在长度为 K 的窗口中共现，K 表示窗口大小，即最多共现 K 个单词。</p>

<p>（4）根据 TextRank 的公式，迭代传播各节点的权重，直至收敛。</p>

<p>（5）对节点权重进行倒序排序，从而得到最重要的 T 个单词，作为候选关键词。</p>

<p>（6）由（5）得到最重要的 T 个单词，在原始文本中进行标记，若形成相邻词组，则组合成多词关键词。</p>

<p>同样 jieba 已经实现了基于 TextRank 算法的关键词抽取，通过命令&nbsp;<code>import jieba.analyse</code>&nbsp;引用，函数参数解释如下：</p>

<pre>
<code>jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v'))
</code></pre>

<p>直接使用，接口参数同 TF-IDF 相同，注意默认过滤词性。</p>

<p>接下来，我们继续看例子，语料继续使用上例中的句子。</p>

<pre>
<code>result = "  ".join(jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v')))
print(result)
</code></pre>

<p>执行结果：</p>

<blockquote>
<p>智能 人工智能 机器 人类 研究 技术 模拟 包括 科学 工作 领域 理论 计算机 年度 需要 语言 相似 方式 做出 心理学</p>
</blockquote>

<p>如果修改一下词性，只需要名词和动词，看看结果有何不同？</p>

<pre>
<code>result = "  ".join(jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=('n','v')))
print(result)
</code></pre>

<p>执行结果：</p>

<blockquote>
<p>智能 人工智能 机器 人类 技术 模拟 包括 科学 理论 计算机 领域 年度 需要 心理学 信息 语言 识别 带来 过程 延伸</p>
</blockquote>

<h3>基于 LDA 主题模型进行关键词提取</h3>

<p>其实，使用 LDA 获取文本关键词在我的第一次 Chat<a href="http://gitbook.cn/gitchat/activity/5ae2c9475d06502947fb1d98">《NLP 中文短文本分类项目实践（上）》</a>已经讲过了，为了保持内容的完整性，在这里我继续写一下。</p>

<p>语料是一个关于汽车的短文本，下面通过 Gensim 库完成基于 LDA 的关键字提取。整个过程的步骤为：文件加载 -&gt; jieba 分词 -&gt; 去停用词 -&gt; 构建词袋模型 -&gt; LDA 模型训练 -&gt; 结果可视化。</p>

<pre>
<code>#引入库文件
import jieba.analyse as analyse
import jieba
import pandas as pd
from gensim import corpora, models, similarities
import gensim
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
#设置文件路径
dir = "D://ProgramData//PythonWorkSpace//study//"
file_desc = "".join([dir,'car.csv'])
stop_words = "".join([dir,'stopwords.txt'])
#定义停用词
stopwords=pd.read_csv(stop_words,index_col=False,quoting=3,sep="\t",names=['stopword'], encoding='utf-8')
stopwords=stopwords['stopword'].values
#加载语料
df = pd.read_csv(file_desc, encoding='gbk')
#删除nan行
df.dropna(inplace=True)
lines=df.content.values.tolist()
#开始分词
sentences=[]
for line in lines:
    try:
        segs=jieba.lcut(line)
        segs = [v for v in segs if not str(v).isdigit()]#去数字
        segs = list(filter(lambda x:x.strip(), segs))   #去左右空格
        segs = list(filter(lambda x:x not in stopwords, segs)) #去掉停用词
        sentences.append(segs)
    except Exception:
        print(line)
        continue
#构建词袋模型
dictionary = corpora.Dictionary(sentences)
corpus = [dictionary.doc2bow(sentence) for sentence in sentences]
#lda模型，num_topics是主题的个数，这里定义了5个
lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=10)
#我们查一下第1号分类，其中最常出现的5个词是：
print(lda.print_topic(1, topn=5))
#我们打印所有5个主题，每个主题显示8个词
for topic in lda.print_topics(num_topics=10, num_words=8):
    print(topic[1])</code></pre>

<p>执行结果如下图所示：</p>

<p><img alt="enter image description here" src="http://images.gitbook.cn/a55a6950-61b9-11e8-b864-0bd1f4b74dfb" /></p>

<pre>
<code>#显示中文matplotlib
plt.rcParams['font.sans-serif'] = [u'SimHei']
plt.rcParams['axes.unicode_minus'] = False
# 在可视化部分，我们首先画出了九个主题的7个词的概率分布图
num_show_term = 8 # 每个主题下显示几个词
num_topics  = 10  
for i, k in enumerate(range(num_topics)):
    ax = plt.subplot(2, 5, i+1)
    item_dis_all = lda.get_topic_terms(topicid=k)
    item_dis = np.array(item_dis_all[:num_show_term])
    ax.plot(range(num_show_term), item_dis[:, 1], 'b*')
    item_word_id = item_dis[:, 0].astype(np.int)
    word = [dictionary.id2token[i] for i in item_word_id]
    ax.set_ylabel(u"概率")
    for j in range(num_show_term):
        ax.text(j, item_dis[j, 1], word[j], bbox=dict(facecolor='green',alpha=0.1))
plt.suptitle(u'9个主题及其7个主要词的概率', fontsize=18)
plt.show()</code></pre>

<p>执行结果如下图所示：</p>

<p><img alt="enter image description here" src="http://images.gitbook.cn/ba1ae950-61b9-11e8-b977-f33e31f528f0" /></p>

<h3>基于 pyhanlp 进行关键词提取</h3>

<p>除了 jieba，也可以选择使用 HanLP 来完成关键字提取，内部采用 TextRankKeyword 实现，语料继续使用上例中的句子。</p>

<pre>
<code>from pyhanlp import *
result = HanLP.extractKeyword(sentence, 20)
print(result)
</code></pre>

<p>执行结果：</p>

<blockquote>
<p>[人工智能, 智能, 领域, 人类, 研究, 不同, 工作, 包括, 模拟, 新的, 机器, 计算机, 门, 科学, 应用, 系统, 理论, 技术, 入选, 复杂]</p>
</blockquote>

<h3>总结</h3>

<p>本节内容的重点就是掌握关键字提取的基本方法，常规的关键词提取方法如上所述，当然还有其他算法及其改进，有深入研究需求的，可以下载关键字提取方面的论文阅读。</p>

<p>参考文献：</p>

<ol>
	<li>Mihalcea R, Tarau P. TextRank: Bringing order into texts[C]//Proceedings of EMNLP. 2004, 4(4): 275.</li>
	<li>Witten I H, Paynter G W, Frank E, et al. KEA: Practical automatic keyphrase extraction[C]//Proceedings of the fourth ACM conference on Digital libraries. ACM, 1999: 254-255.</li>
	<li>Chien L F. PAT-tree-based keyword extraction for Chinese information retrieval[C]//ACM SIGIR Forum. ACM, 1997, 31(SI): 50-58.</li>
</ol>

<p><a id="第04课：了解数据必备的文本可视化技巧" name="第04课：了解数据必备的文本可视化技巧"></a>第04课：了解数据必备的文本可视化技巧</p>

<h3>为什么要文本数据可视化</h3>

<p>文字是传递信息最常用的载体，随着海量文本的涌现，信息超载和数据过剩等问题日益凸显，当大段大段的文字摆在面前，已经很少有人耐心、认真把它读完，人们急需一种更高效的信息接收方式，从视觉的角度出发，文本可视化正是解药良方。所谓一图胜千言，其实就是文本可视化的一种表现。</p>

<p><img alt="enter image description here" src="http://images.gitbook.cn/8388b2b0-6280-11e8-b7c9-aba3c5c7330f" /></p>

<p>因此，文本可视化技术将文本中复杂的或者难以通过文字表达的内容和规律以视觉符号的形式表达出来，使人们能够利用与生俱来的视觉感知的并行化处理能力，快速获取文本中所蕴含的关键信息。</p>

<h3>文本可视化的流程</h3>

<p>文本可视化依赖于自然语言处理，因此词袋模型、命名实体识别、关键词抽取、主题分析、情感分析等是较常用的文本分析技术。文本分析的过程主要包括特征提取，通过分词、抽取、归一化等操作提取出文本词汇级的内容，利用特征构建向量空间模型并进行降维，以便将其呈现在低维空间，或者利用主题模型处理特征，最终以灵活有效的形式表示这些处理过的数据，以便进行可视化呈现。下图（来源：网络）是一个文本可视化的基本流程图：</p>

<p><img alt="enter image description here" src="http://images.gitbook.cn/5d192a50-6231-11e8-b82c-e1d608026a45" /></p>

<p>文本可视化类型，除了包含常规的图表类，如柱状图、饼图、折线图等的表现形式，在文本领域用的比较多的可视化类型有：</p>

<p>（1）基于文本内容的可视化。</p>

<p>基于文本内容的可视化研究包括基于词频的可视化和基于词汇分布的可视化，常用的有词云、分布图和 Document Cards 等。</p>

<p>（2）基于文本关系的可视化。</p>

<p>基于文本关系的可视化研究文本内外关系，帮助人们理解文本内容和发现规律。常用的可视化形式有树状图、节点连接的网络图、力导向图、叠式图和 Word Tree 等。</p>

<p>（3）基于多层面信息的可视化</p>

<p>基于多层面信息的可视化主要研究如何结合信息的多个方面帮助用户从更深层次理解文本数据，发现其内在规律。其中，包含时间信息和地理坐标的文本可视化近年来受到越来越多的关注。常用的有地理热力图、ThemeRiver、SparkClouds、TextFlow 和基于矩阵视图的情感分析可视化等。</p>

<h3>动手实战文本可视化</h3>

<h4>词云</h4>

<p>在 Chat<a href="http://gitbook.cn/gitchat/activity/5ae2c9475d06502947fb1d98">《NLP 中文短文本分类项目实践（上）》</a>中已经讲过如何绘制 Wordcloud，这里只给出关键代码。具体过程是分词、去停用词和统计词频，然后绘制 Wordcloud 词云，这里提供下面两种方式。</p>

<pre>
<code>#**第一种是默认的样式**
wordcloud=WordCloud(font_path=simhei,background_color="white",max_font_size=80)
word_frequence = {x[0]:x[1] for x in words_stat.head(1000).values}
wordcloud=wordcloud.fit_words(word_frequence)
#**第二种是自定义图片**
text = " ".join(words_stat['segment'].head(100).astype(str))
abel_mask = imread(r"china.jpg")  #这里设置了一张中国地图
wordcloud2 = WordCloud(background_color='white',  # 设置背景颜色 
   mask = abel_mask,  # 设置背景图片
   max_words = 3000,  # 设置最大现实的字数
   font_path = simhei,  # 设置字体格式
   width=2048,
   height=1024,
   scale=4.0,
   max_font_size= 300,  # 字体最大值
                     random_state=42).generate(text)
# 根据图片生成词云颜色
image_colors = ImageColorGenerator(abel_mask)
wordcloud2.recolor(color_func=image_colors)
# 以下代码显示图片
plt.imshow(wordcloud2)
plt.axis("off")
plt.show()
wordcloud2.to_file(r'wordcloud_2.jpg') #保存结果</code></pre>

<p>得到的词云如下图所示：</p>

<p><img alt="enter image description here" src="http://images.gitbook.cn/25a32c00-624b-11e8-b82c-e1d608026a45" /></p>

<h4>关系图</h4>

<p>关系图法，是指用连线图来表示事物相互关系的一种方法。最常见的关系图是数据库里的 E-R 图，表示实体、关系、属性三者之间的关系。在文本可视化里面，关系图也经常被用来表示有相互关系、原因与结果和目的与手段等复杂关系，下面我们来看看如何用 Python 实现关系图制作。</p>

<p><strong>基本步骤</strong>：</p>

<ul>
	<li>安装 Matplotlib、NetworkX；</li>
	<li>解决 Matplotlib 无法写中文问题。</li>
</ul>

<p>我们需要知道 NetworkX 绘制关系图的数据组织结构，节点和边都是 list 格式，边的 list 里面是成对的节点。下面我们看一个真实的例子，学生课程和上课地点的关系图。</p>

<pre>
<code>classes= df['class'].values.tolist()
classrooms=df['classroom'].values.tolist()
nodes = list(set(classes + classrooms))
weights = [(df.loc[index,'class'],df.loc[index,'classroom'])for index in df.index]   
weights =  list(set(weights))
# 设置matplotlib正常显示中文
plt.rcParams['font.sans-serif']=['SimHei']   # 用黑体显示中文
plt.rcParams['axes.unicode_minus']=False 
colors = ['red', 'green', 'blue', 'yellow']
#有向图
DG = nx.DiGraph()
#一次性添加多节点，输入的格式为列表
DG.add_nodes_from(nodes)
#添加边，数据格式为列表
DG.add_edges_from(weights)
#作图，设置节点名显示,节点大小，节点颜色
nx.draw(DG,with_labels=True, node_size=1000, node_color = colors)
plt.show() </code></pre>

<p>得到的关系图如下图所示：</p>

<p><img alt="enter image description here" src="http://images.gitbook.cn/0af7aa00-6257-11e8-b7c9-aba3c5c7330f" /></p>

<h4>地理热力图</h4>

<p>地理热力图，是以特殊高亮的形式显示用户的地理位置，借助热力图，可以直观地观察到用户的总体情况和偏好。</p>

<p><strong>基本步骤</strong>：</p>

<ul>
	<li>安装 Folium；</li>
	<li>将地理名词通过百度转换成经纬度。</li>
</ul>

<p>在通过分词得到城市名称后，将地理名词通过百度转换成经纬度。首先注册密钥，使用百度 Web 服务 API 下的 Geocoding API 接口来获取你所需要地址的经纬度坐标，并转化为 JSON 结构的数据（个人接口，百度每天限制调用6000次），接下来定义经纬度获取函数：</p>

<pre>
<code>#经纬度转换
def getlnglat(address):
    url = 'http://api.map.baidu.com/geocoder/v2/'
    output = 'json'
    ak = 'sqGDDvCDEZPSz24bt4b0BpKLnMk1dv6d'
    add = quote(address) #由于本文城市变量为中文，为防止乱码，先用quote进行编码
    uri = url + '?' + 'address=' + add  + '&amp;output=' + output + '&amp;ak=' + ak
    req = urlopen(uri)
    res = req.read().decode() #将其他编码的字符串解码成unicode
    temp = json.loads(res)  #对json数据进行解析
    return temp</code></pre>

<p>转换后数据格式：</p>

<blockquote>
<p>北京,116.39564503787867,39.92998577808024,840</p>

<p>成都,104.06792346330406,30.679942845419564,291</p>

<p>重庆,106.53063501341296,29.54460610888615,261</p>

<p>昆明,102.71460113878045,25.049153100453157,238</p>

<p>潍坊,119.14263382297052,36.71611487305138,214</p>

<p>济南,117.02496706629023,36.68278472716141,212</p>
</blockquote>

<p>然后，使用 Folium 库进行热力图绘制地图：</p>

<pre>
<code>lat = np.array(cities["lat"][0:num])   # 获取维度之维度值
lon = np.array(cities["lng"][0:num])   # 获取经度值
pop = np.array(cities["count"][0:num],dtype=float)    # 获取人口数，转化为numpy浮点型
data1 = [[lat[i],lon[i],pop[i]] for i in range(num)]    #将数据制作成[lats,lons,weights]的形式
map_osm = folium.Map(location=[35,110],zoom_start=5)    #绘制Map，开始缩放程度是5倍
HeatMap(data1).add_to(map_osm)  # 将热力图添加到前面建立的map里
file_path = dir + "heatmap.html"
map_osm.save(file_path)</code></pre>

<p>得到的地图热力图如下图所示：</p>

<p><img alt="enter image description here" src="http://images.gitbook.cn/3e258970-6260-11e8-95e8-074055ff8c6f" /></p>

<p>上面列举了三种典型的文本可视化方式，当然还有更多漂亮的方式。个人在开发过程中，觉得采用前端技术实现的可视化效果最好，但是这次课程仅限于 Pyhton，所以就不讲前端知识了。</p>

<p>最后，我推荐三个前端可视化学习网站，第一个是百度的&nbsp;<a href="http://echarts.baidu.com/echarts2/doc/example.html">Echarts</a>，基于 Canvas，适合刚入门的新手，遵循了数据可视化的一些经典范式，只要把数据组织好，就可以轻松得到很漂亮的图表；第二个推荐&nbsp;<a href="https://github.com/d3/d3/wiki/Gallery">D3.js</a>，基于 SVG 方便自己定制，D3 V4 支持 Canvas+SVG，D3.js 比 Echarts 稍微难点，适合有一定开发经验的人；第三个&nbsp;<a href="https://threejs.org/">three.js</a>，是一个基于 WebGL 的 3D 图形的框架，可以让用户通过 JavaScript 搭建 WebGL 项目。</p>

<p><a id="第05课：面向非结构化数据转换的词袋和词向量模型" name="第05课：面向非结构化数据转换的词袋和词向量模型"></a>第05课：面向非结构化数据转换的词袋和词向量模型</p>

<p>通过前面几个小节的学习，我们现在已经学会了如何获取文本预料，然后分词，在分词之后的结果上，我们可以提取文本的关键词查看文本核心思想，进而可以通过可视化技术把文档从视觉的角度表达出来。</p>

<p>下面，我们来看看，文本数据如何转换成计算机能够计算的数据。这里介绍两种常用的模型：词袋和词向量模型。</p>

<h3>词袋模型（Bag of Words Model）</h3>

<h4>词袋模型的概念</h4>

<p>先来看张图，从视觉上感受一下词袋模型的样子。</p>

<p><img alt="enter image description here" src="http://images.gitbook.cn/bcd2fa60-62eb-11e8-8a60-1bdde4cc4659" /></p>

<p>词袋模型看起来好像一个口袋把所有词都装进去，但却不完全如此。在自然语言处理和信息检索中作为一种简单假设，词袋模型把文本（段落或者文档）被看作是无序的词汇集合，忽略语法甚至是单词的顺序，把每一个单词都进行统计，同时计算每个单词出现的次数，常常被用在文本分类中，如贝叶斯算法、LDA 和 LSA 等。</p>

<h4>动手实战词袋模型</h4>

<p>（1）词袋模型</p>

<p>本例中，我们自己动手写代码看看词袋模型是如何操作的。</p>

<p>首先，引入 jieba 分词器、语料和停用词（标点符号集合，自己可以手动添加或者用一个文本字典代替）。</p>

<pre>
<code>import jieba
#定义停用词、标点符号
punctuation = ["，","。", "：", "；", "？"]
#定义语料
content = ["机器学习带动人工智能飞速的发展。",
           "深度学习带动人工智能飞速的发展。",
           "机器学习和深度学习带动人工智能飞速的发展。"
          ]</code></pre>

<p>接下来，我们先对语料进行分词操作，这里用到 lcut() 方法：</p>

<pre>
<code>#分词
segs_1 = [jieba.lcut(con) for con in content]
print(segs_1)
</code></pre>

<p>得到分词后的结果如下：</p>

<blockquote>
<p>[[&#39;机器&#39;, &#39;学习&#39;, &#39;带动&#39;, &#39;人工智能&#39;, &#39;飞速&#39;, &#39;的&#39;, &#39;发展&#39;, &#39;。&#39;], [&#39;深度&#39;, &#39;学习&#39;, &#39;带动&#39;, &#39;人工智能&#39;, &#39;飞速&#39;, &#39;的&#39;, &#39;发展&#39;, &#39;。&#39;], [&#39;机器&#39;, &#39;学习&#39;, &#39;和&#39;, &#39;深度&#39;, &#39;学习&#39;, &#39;带动&#39;, &#39;人工智能&#39;, &#39;飞速&#39;, &#39;的&#39;, &#39;发展&#39;, &#39;。&#39;]]</p>
</blockquote>

<p>因为中文语料带有停用词和标点符号，所以需要去停用词和标点符号，这里语料很小，我们直接去标点符号：</p>

<pre>
<code>tokenized = []
for sentence in segs_1:
    words = []
    for word in sentence:
        if word not in punctuation:          
            words.append(word)
    tokenized.append(words)
print(tokenized)</code></pre>

<p>去标点符号后，我们得到结果如下：</p>

<blockquote>
<p>[[&#39;机器&#39;, &#39;学习&#39;, &#39;带动&#39;, &#39;人工智能&#39;, &#39;飞速&#39;, &#39;的&#39;, &#39;发展&#39;], [&#39;深度&#39;, &#39;学习&#39;, &#39;带动&#39;, &#39;人工智能&#39;, &#39;飞速&#39;, &#39;的&#39;, &#39;发展&#39;], [&#39;机器&#39;, &#39;学习&#39;, &#39;和&#39;, &#39;深度&#39;, &#39;学习&#39;, &#39;带动&#39;, &#39;人工智能&#39;, &#39;飞速&#39;, &#39;的&#39;, &#39;发展&#39;]]</p>
</blockquote>

<p>下面操作就是把所有的分词结果放到一个袋子（List）里面，也就是取并集，再去重，获取对应的特征词。</p>

<pre>
<code>#求并集
bag_of_words = [ x for item in segs_1 for x in item if x not in punctuation]
#去重
bag_of_words = list(set(bag_of_words))
print(bag_of_words)</code></pre>

<p>得到的特征词结果如下：</p>

<blockquote>
<p>[&#39;飞速&#39;, &#39;的&#39;, &#39;深度&#39;, &#39;人工智能&#39;, &#39;发展&#39;, &#39;和&#39;, &#39;机器&#39;, &#39;学习&#39;, &#39;带动&#39;]</p>
</blockquote>

<p>我们以上面特征词的顺序，完成词袋化：</p>

<pre>
<code>bag_of_word2vec = []
for sentence in tokenized:
    tokens = [1 if token in sentence else 0 for token in bag_of_words ]
    bag_of_word2vec.append(tokens)</code></pre>

<p>最后得到词袋向量：</p>

<blockquote>
<p>[[1, 1, 0, 1, 1, 0, 1, 1, 1], [1, 1, 1, 1, 1, 0, 0, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1]]</p>
</blockquote>

<p>上面的例子在编码时，对于 for 循环多次直接用到列表推导式。在 Python 中，列表推导式的效率比 for 快很多，尤其在数据量大的时候效果更明显，建议多使用列表推导式。</p>

<p>（2）Gensim 构建词袋模型</p>

<p>下面我们介绍 Gensim 库的使用，继续沿用上面的例子：</p>

<pre>
<code>from gensim import corpora
import gensim
#tokenized是去标点之后的
dictionary = corpora.Dictionary(tokenized)
#保存词典
dictionary.save('deerwester.dict') 
print(dictionary)</code></pre>

<p>这时我们得到的结果不全，但通过提示信息可知道共9个独立的词：</p>

<blockquote>
<p>Dictionary(9 unique tokens: [&#39;人工智能&#39;, &#39;发展&#39;, &#39;学习&#39;, &#39;带动&#39;, &#39;机器&#39;]...)</p>
</blockquote>

<p>那我们如何查看所有词呢？通过下面方法，可以查看到所有词和对应的下标：</p>

<pre>
<code>#查看词典和下标 id 的映射
print(dictionary.token2id)
</code></pre>

<p>最后结果如下：</p>

<blockquote>
<p>{&#39;人工智能&#39;: 0, &#39;发展&#39;: 1, &#39;学习&#39;: 2, &#39;带动&#39;: 3, &#39;机器&#39;: 4, &#39;的&#39;: 5, &#39;飞速&#39;: 6, &#39;深度&#39;: 7, &#39;和&#39;: 8}</p>
</blockquote>

<p>根据得到的结果，我们同样可以得到词袋模型的特征向量。这里顺带提一下函数 doc2bow()，作用只是计算每个不同单词的出现次数，将单词转换为其整数单词 id 并将结果作为稀疏向量返回。</p>

<pre>
<code>corpus = [dictionary.doc2bow(sentence) for sentence in segs_1]
print(corpus )
</code></pre>

<p>得到的稀疏向量结果如下：</p>

<blockquote>
<p>[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)], [(0, 1), (1, 1), (2, 1), (3, 1), (5, 1), (6, 1), (7, 1)], [(0, 1), (1, 1), (2, 2), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)]]</p>
</blockquote>

<h3>词向量 （Word Embedding）</h3>

<p>深度学习带给自然语言处理最令人兴奋的突破是词向量（Word Embedding）技术。词向量技术是将词语转化成为稠密向量。在自然语言处理应用中，词向量作为机器学习、深度学习模型的特征进行输入。因此，最终模型的效果很大程度上取决于词向量的效果。</p>

<h3>词向量的概念</h3>

<p>在 Word2Vec 出现之前，自然语言处理经常把字词进行独热编码，也就是 One-Hot Encoder。</p>

<blockquote>
<p>大数据 [0,0,0,0,0,0,0,1,0,&hellip;&hellip;，0,0,0,0,0,0,0]</p>

<p>云计算[0,0,0,0,1,0,0,0,0,&hellip;&hellip;，0,0,0,0,0,0,0]</p>

<p>机器学习[0,0,0,1,0,0,0,0,0,&hellip;&hellip;，0,0,0,0,0,0,0]</p>

<p>人工智能[0,0,0,0,0,0,0,0,0,&hellip;&hellip;，1,0,0,0,0,0,0]</p>
</blockquote>

<p>比如上面的例子中，大数据 、云计算、机器学习和人工智能各对应一个向量，向量中只有一个值为1，其余都为0。所以使用 One-Hot Encoder有以下问题：</p>

<ul>
	<li>第一，词语编码是随机的，向量之间相互独立，看不出词语之间可能存在的关联关系。</li>
	<li>第二，向量维度的大小取决于语料库中词语的多少，如果语料包含的所有词语对应的向量合为一个矩阵的话，那这个矩阵过于稀疏，并且会造成维度灾难。</li>
</ul>

<p>而解决这个问题的手段，就是使用向量表示（Vector Representations）。比如 Word2Vec 可以将 One-Hot Encoder 转化为低维度的连续值，也就是稠密向量，并且其中意思相近的词也将被映射到向量空间中相近的位置。经过降维，在二维空间中，相似的单词在空间中的距离也很接近。</p>

<p>这里简单给词向量一个定义，词向量就是要用某个固定维度的向量去表示单词。也就是说要把单词变成固定维度的向量，作为机器学习（Machine Learning）或深度学习模型的特征向量输入。</p>

<h4>动手实战词向量</h4>

<p>（1）Word2Vec</p>

<p>Word2Vec 是 Google 团队2013年推出的，自提出后被广泛应用在自然语言处理任务中，并且受到它的启发，后续出现了更多形式的词向量模型。Word2Vec 主要包含两种模型：Skip-Gram 和 CBOW，值得一提的是，Word2Vec 词向量可以较好地表达不同词之间的相似和类比关系。</p>

<p>下面我们通过代码实战来体验一下 Word2Vec。通过&nbsp;<code>pip install gensim</code>&nbsp;安装好库后，即可导入使用。</p>

<p><img alt="enter image description here" src="http://images.gitbook.cn/9e0a13c0-6b01-11e8-8431-a75f9cd1b0ae" /></p>

<p>先导入 Gensim 中的 Word2Vec 和 jieba 分词器，再引入从百度百科抓取的黄河和长江的语料：</p>

<pre>
<code>from gensim.models import Word2Vec  
import jieba
#定义停用词、标点符号
punctuation = [",","。", ":", ";", ".", "'", '"', "’", "?", "/", "-", "+", "&amp;", "(", ")"]
sentences = [
"长江是中国第一大河，干流全长6397公里（以沱沱河为源），一般称6300公里。流域总面积一百八十余万平方公里，年平均入海水量约九千六百余亿立方米。以干流长度和入海水量论，长江均居世界第三位。",
"黄河，中国古代也称河，发源于中华人民共和国青海省巴颜喀拉山脉，流经青海、四川、甘肃、宁夏、内蒙古、陕西、山西、河南、山东9个省区，最后于山东省东营垦利县注入渤海。干流河道全长5464千米，仅次于长江，为中国第二长河。黄河还是世界第五长河。",
"黄河,是中华民族的母亲河。作为中华文明的发祥地,维系炎黄子孙的血脉.是中华民族民族精神与民族情感的象征。",
"黄河被称为中华文明的母亲河。公元前2000多年华夏族在黄河领域的中原地区形成、繁衍。",
"在兰州的“黄河第一桥”内蒙古托克托县河口镇以上的黄河河段为黄河上游。",
"黄河上游根据河道特性的不同，又可分为河源段、峡谷段和冲积平原三部分。 ",
"黄河,是中华民族的母亲河。"
]</code></pre>

<p>上面定义好语料，接下来进行分词，去标点符号操作 ：</p>

<pre>
<code>sentences = [jieba.lcut(sen) for sen in sentences]
tokenized = []
for sentence in sentences:
    words = []
    for word in sentence:
        if word not in punctuation:          
            words.append(word)
    tokenized.append(words)</code></pre>

<p>这样我们获取的语料在分词之后，去掉了标点符号，如果做得更严谨，大家可以去停用词，然后进行模型训练：</p>

<pre>
<code>model = Word2Vec(tokenized, sg=1, size=100,  window=5,  min_count=2,  negative=1, sample=0.001, hs=1, workers=4)
</code></pre>

<p>参数解释如下：</p>

<ul>
	<li>sg=1 是&nbsp;<code>skip-gram</code>&nbsp;算法，对低频词敏感；默认 sg=0 为 CBOW 算法。</li>
	<li>size 是输出词向量的维数，值太小会导致词映射因为冲突而影响结果，值太大则会耗内存并使算法计算变慢，一般值取为100到200之间。</li>
	<li>window 是句子中当前词与目标词之间的最大距离，3表示在目标词前看3-b 个词，后面看 b 个词（b 在0-3之间随机）。</li>
	<li><code>min_count</code>&nbsp;是对词进行过滤，频率小于&nbsp;<code>min-count</code>&nbsp;的单词则会被忽视，默认值为5。</li>
	<li>negative 和 sample 可根据训练结果进行微调，sample 表示更高频率的词被随机下采样到所设置的阈值，默认值为 1e-3。</li>
	<li>hs=1 表示层级 softmax 将会被使用，默认 hs=0 且 negative 不为0，则负采样将会被选择使用。</li>
	<li>详细参数说明可查看 Word2Vec 源代码。</li>
</ul>

<p>训练后的模型可以保存与加载，如下代码所示：</p>

<pre>
<code>model.save('model')  #保存模型
model = Word2Vec.load('model')   #加载模型</code></pre>

<p>模型训练好之后，接下来就可以使用模型，可以用来计算句子或者词的相似性、最大匹配程度等。</p>

<p>例如，我们判断一下黄河和黄河自己的相似度：</p>

<pre>
<code>print(model.similarity('黄河', '黄河'))
</code></pre>

<p>结果输出为：</p>

<blockquote>
<p>1.0000000000000002</p>
</blockquote>

<p>例如，当输入黄河和长江来计算相似度的时候，结果就比较小，因为我们的语料实在太小了。</p>

<pre>
<code>print(model.similarity('黄河', '长江'))
</code></pre>

<p>结果输出为：</p>

<blockquote>
<p>-0.036808977457324699</p>
</blockquote>

<p>下面我们预测最接近的词，预测与黄河和母亲河最接近，而与长江不接近的词：</p>

<pre>
<code>print(model.most_similar(positive=['黄河', '母亲河'], negative=['长江']))
</code></pre>

<p>得到结果如下，可以根据相似度大小找到与黄河和母亲河最接近的词（实际处理建议增大数据量和去停用词）。</p>

<blockquote>
<p>[(&#39;是&#39;, 0.14632007479667664), (&#39;以&#39;, 0.14630728960037231), (&#39;长河&#39;, 0.13878652453422546), (&#39;河道&#39;, 0.13716217875480652), (&#39;在&#39;, 0.11577725410461426), (&#39;全长&#39;, 0.10969121754169464), (&#39;内蒙古&#39;, 0.07590540498495102), (&#39;入海&#39;, 0.06970417499542236), (&#39;民族&#39;, 0.06064444035291672), (&#39;中华文明&#39;, 0.057667165994644165)]</p>
</blockquote>

<p>上面通过小数据量的语料实战，加强了对 Word2Vec 的理解，总之 Word2Vec 是一种将词变成词向量的工具。通俗点说，只有这样文本预料才转化为计算机能够计算的矩阵向量。</p>

<p>（2）Doc2Vec</p>

<p>Doc2Vec 是 Mikolov 在 Word2Vec 基础上提出的另一个用于计算长文本向量的工具。在 Gensim 库中，Doc2Vec 与 Word2Vec 都极为相似。但两者在对输入数据的预处理上稍有不同，Doc2vec 接收一个由 LabeledSentence 对象组成的迭代器作为其构造函数的输入参数。其中，LabeledSentence 是 Gensim 内建的一个类，它接收两个 List 作为其初始化的参数：word list 和 label list。</p>

<p>Doc2Vec 也包括两种实现方式：DBOW（Distributed Bag of Words）和 DM （Distributed Memory）。DBOW 和 DM 的实现，二者在 gensim 库中的实现用的是同一个方法，该方法中参数 dm = 0 或者 dm=1 决定调用 DBOW 还是 DM。Doc2Vec 将文档语料通过一个固定长度的向量表达。</p>

<p>下面是 Gensim 中 Doc2Vec 模型的实战，我们把上述语料每一句话当做一个文本，添加上对应的标签。接下来，定义数据预处理类，作用是给每个文章添加对应的标签：</p>

<pre>
<code>#定义数据预处理类，作用是给每个文章添加对应的标签
from gensim.models.doc2vec import Doc2Vec,LabeledSentence
doc_labels = ["长江","黄河","黄河","黄河","黄河","黄河","黄河"]
class LabeledLineSentence(object):
    def __init__(self, doc_list, labels_list):
       self.labels_list = labels_list
       self.doc_list = doc_list
    def __iter__(self):
        for idx, doc in enumerate(self.doc_list):
            yield LabeledSentence(words=doc,tags=[self.labels_list[idx]])

    model = Doc2Vec(documents,dm=1, size=100, window=8, min_count=5, workers=4)
    model.save('model')
    model = Doc2Vec.load('model')  </code></pre>

<p>上面定义好了数据预处理函数，我们将 Word2Vec 中分词去标点后的数据，进行转换：</p>

<pre>
<code>iter_data = LabeledLineSentence(tokenized, doc_labels)
</code></pre>

<p>得到一个数据集，我开始定义模型参数，这里 dm=1，采用了 Gensim 中的 DM 实现。</p>

<pre>
<code>model = Doc2Vec(dm=1, size=100, window=8, min_count=5, workers=4)
model.build_vocab(iter_data)</code></pre>

<p>接下来训练模型， 设置迭代次数1000次，<code>start_alpha</code>&nbsp;为开始学习率，<code>end_alpha</code>&nbsp;与&nbsp;<code>start_alpha</code>&nbsp;线性递减。</p>

<pre>
<code>model.train(iter_data,total_examples=model.corpus_count,epochs=1000,start_alpha=0.01,end_alpha =0.001)
</code></pre>

<p>最后我们对模型进行一些预测：</p>

<pre>
<code>#根据标签找最相似的，这里只有黄河和长江，所以结果为长江，并计算出了相似度
print(model.docvecs.most_similar('黄河'))
</code></pre>

<p>得到的结果：</p>

<blockquote>
<p>[(&#39;长江&#39;, 0.25543850660324097)]</p>
</blockquote>

<p>然后对黄河和长江标签做相似性计算：</p>

<pre>
<code>print(model.docvecs.similarity('黄河','长江'))
</code></pre>

<p>得到的结果：</p>

<blockquote>
<p>0.25543848271351405</p>
</blockquote>

<p>上面只是在小数据量进行的小练习，而最终影响模型准确率的因素有：文档的数量越多，文档的相似性越好，也就是基于大数据量的模型训练。在工业界，Word2Vec 和 Doc2Vec 常见的应用有：做相似词计算；相关词挖掘，在推荐系统中用在品牌、用户、商品挖掘中；上下文预测句子；机器翻译；作为特征输入其他模型等。</p>

<p>总结，本文只是简单的介绍了词袋和词向量模型的典型应用，对于两者的理论和其他词向量模型，比如 TextRank 、FastText 和 GloVe 等，阅读文末给出参考文献将了解更多。</p>

<p><strong>参考文献：</strong></p>

<ol>
	<li><a href="https://radimrehurek.com/gensim/tut1.html">https://radimrehurek.com/gensim/tut1.html</a></li>
	<li><a href="https://radimrehurek.com/gensim/models/word2vec.html">https://radimrehurek.com/gensim/models/word2vec.html</a></li>
	<li><a href="https://radimrehurek.com/gensim/summarization/summariser.html">https://radimrehurek.com/gensim/summarization/summariser.html</a></li>
	<li><a href="https://radimrehurek.com/gensim/models/fasttext.html">https://radimrehurek.com/gensim/models/fasttext.html</a></li>
	<li><a href="https://nlp.stanford.edu/projects/glove/">https://nlp.stanford.edu/projects/glove/</a></li>
</ol>

<p><a id="第06课：动手实战基于 ML 的中文短文本分类" name="第06课：动手实战基于 ML 的中文短文本分类"></a>第06课：动手实战基于 ML 的中文短文本分类</p>

<p>文本分类，属于有监督学习中的一部分，在很多场景下都有应用，下面通过小数据的实例，一步步完成中文短文本的分类实现，整个过程尽量做到少理论重实战。</p>

<p><img alt="enter image description here" src="http://images.gitbook.cn/f3726f10-7d0d-11e8-be78-bb5c0f92d7f1" /></p>

<p><strong>开发环境，我们选择</strong>：</p>

<ol>
	<li>Windows 系统</li>
	<li>Python 3.6</li>
	<li>Jupyter Notebook</li>
</ol>

<p>本文使用的数据是我曾经做过的一份司法数据，需求是对每一条输入数据，判断事情的主体是谁，比如报警人被老公打，报警人被老婆打，报警人被儿子打，报警人被女儿打等来进行文本有监督的分类操作。</p>

<p><strong>整个过程分为以下几个步骤</strong>：</p>

<ul>
	<li>语料加载</li>
	<li>分词</li>
	<li>去停用词</li>
	<li>抽取词向量特征</li>
	<li>分别进行算法建模和模型训练</li>
	<li>评估、计算 AUC 值</li>
	<li>模型对比</li>
</ul>

<p><strong>基本流程如下图所示</strong>：</p>

<p><img alt="enter image description here" src="http://images.gitbook.cn/7cb83ee0-7d08-11e8-ab3b-8f5f773d8874" /></p>

<p>下面开始项目实战。</p>

<p><strong>1.</strong>&nbsp;首先进行语料加载，在这之前，引入所需要的 Python 依赖包，并将全部语料和停用词字典读入内存中。</p>

<p>第一步，引入依赖库，有随机数库、jieba 分词、pandas 库等：</p>

<pre>
<code>import random
import jieba
import pandas as pd
</code></pre>

<p>第二步，加载停用词字典，停用词词典为 stopwords.txt 文件，可以根据场景自己在该文本里面添加要去除的词（比如冠词、人称、数字等特定词）：</p>

<pre>
<code>#加载停用词
stopwords=pd.read_csv('stopwords.txt',index_col=False,quoting=3,sep="\t",names=['stopword'], encoding='utf-8')
stopwords=stopwords['stopword'].values</code></pre>

<p>第三步，加载语料，语料是4个已经分好类的 csv 文件，直接用 pandas 加载即可，加载之后可以首先删除 nan 行，并提取要分词的 content 列转换为 list 列表：</p>

<pre>
<code>#加载语料
laogong_df = pd.read_csv('beilaogongda.csv', encoding='utf-8', sep=',')
laopo_df = pd.read_csv('beilaogongda.csv', encoding='utf-8', sep=',')
erzi_df = pd.read_csv('beierzida.csv', encoding='utf-8', sep=',')
nver_df = pd.read_csv('beinverda.csv', encoding='utf-8', sep=',')
#删除语料的nan行
laogong_df.dropna(inplace=True)
laopo_df.dropna(inplace=True)
erzi_df.dropna(inplace=True)
nver_df.dropna(inplace=True)
#转换
laogong = laogong_df.segment.values.tolist()
laopo = laopo_df.segment.values.tolist()
erzi = erzi_df.segment.values.tolist()
nver = nver_df.segment.values.tolist()</code></pre>

<p><strong>2.</strong>&nbsp;分词和去停用词。</p>

<p>第一步，定义分词、去停用词和批量打标签的函数，函数包含3个参数：<code>content_lines</code>&nbsp;参数为语料列表；sentences 参数为预先定义的 list，用来存储分词并打标签后的结果；category 参数为标签 ：</p>

<pre>
<code>#定义分词和打标签函数preprocess_text
#参数content_lines即为上面转换的list
#参数sentences是定义的空list，用来储存打标签之后的数据
#参数category 是类型标签
def preprocess_text(content_lines, sentences, category):
    for line in content_lines:
        try:
            segs=jieba.lcut(line)
            segs = [v for v in segs if not str(v).isdigit()]#去数字
            segs = list(filter(lambda x:x.strip(), segs))   #去左右空格
            segs = list(filter(lambda x:len(x)&gt;1, segs)) #长度为1的字符
            segs = list(filter(lambda x:x not in stopwords, segs)) #去掉停用词
            sentences.append((" ".join(segs), category))# 打标签
        except Exception:
            print(line)
            continue </code></pre>

<p>第二步，调用函数、生成训练数据，根据我提供的司法语料数据，分为报警人被老公打，报警人被老婆打，报警人被儿子打，报警人被女儿打，标签分别为0、1、2、3，具体如下：</p>

<pre>
<code>sentences = []
preprocess_text(laogong, sentences,0)
preprocess_text(laopo, sentences, 1)
preprocess_text(erzi, sentences, 2)
preprocess_text(nver, sentences, 3)</code></pre>

<p>第三步，将得到的数据集打散，生成更可靠的训练集分布，避免同类数据分布不均匀：</p>

<pre>
<code>random.shuffle(sentences)
</code></pre>

<p>第四步，我们在控制台输出前10条数据，观察一下：</p>

<pre>
<code>for sentence in sentences[:10]:
    print(sentence[0], sentence[1])  #下标0是词列表，1是标签</code></pre>

<p>得到的结果如图所示：</p>

<p><img alt="enter image description here" src="http://images.gitbook.cn/ce9d3950-7d0c-11e8-be78-bb5c0f92d7f1" /></p>

<p><strong>3.</strong>&nbsp;抽取词向量特征。</p>

<p>第一步，抽取特征，我们定义文本抽取词袋模型特征：</p>

<pre>
<code>from sklearn.feature_extraction.text import CountVectorizer
vec = CountVectorizer(
    analyzer='word', # tokenise by character ngrams
    max_features=4000,  # keep the most common 1000 ngrams
)</code></pre>

<p>第二步，把语料数据切分，用&nbsp;<code>sk-learn</code>&nbsp;对数据切分，分成训练集和测试集：</p>

<pre>
<code>from sklearn.model_selection import train_test_split
x, y = zip(*sentences)
x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=1256)</code></pre>

<p>第三步，把训练数据转换为词袋模型：</p>

<pre>
<code>vec.fit(x_train)
</code></pre>

<p><strong>4.</strong>&nbsp;分别进行算法建模和模型训练。</p>

<p>定义朴素贝叶斯模型，然后对训练集进行模型训练，直接使用 sklearn 中的 MultinomialNB：</p>

<pre>
<code>from sklearn.naive_bayes import MultinomialNB
classifier = MultinomialNB()
classifier.fit(vec.transform(x_train), y_train)</code></pre>

<p><strong>5.</strong>&nbsp;评估、计算 AUC 值。</p>

<p>第一步，上面步骤1-4完成了从语料到模型的训练，训练之后，我们要用测试集来计算 AUC 值：</p>

<pre>
<code>print(classifier.score(vec.transform(x_test), y_test))
</code></pre>

<p>得到的结果评分为：0.647331786543。</p>

<p>第二步，进行测试集的预测：</p>

<pre>
<code>pre = classifier.predict(vec.transform(x_test))
</code></pre>

<p><strong>6.</strong>&nbsp;模型对比。</p>

<p>整个模型从语料到训练评估步骤1-5就完成了，接下来我们来看看，改变特征向量模型和训练模型对结果有什么变化。</p>

<p>（1）改变特征向量模型</p>

<p>下面可以把特征做得更强一点，尝试加入抽取&nbsp;<code>2-gram</code>&nbsp;和&nbsp;<code>3-gram</code>&nbsp;的统计特征，把词库的量放大一点。</p>

<pre>
<code>from sklearn.feature_extraction.text import CountVectorizer
vec = CountVectorizer(
    analyzer='word', # tokenise by character ngrams
    ngram_range=(1,4),  # use ngrams of size 1 and 2
    max_features=20000,  # keep the most common 1000 ngrams
)
vec.fit(x_train)
#用朴素贝叶斯算法进行模型训练
classifier = MultinomialNB()
classifier.fit(vec.transform(x_train), y_train)
#对结果进行评分
print(classifier.score(vec.transform(x_test), y_test))
</code></pre>

<p>得到的结果评分为：0.649651972158，确实有一点提高，但是不太明显。</p>

<p>（2）改变训练模型</p>

<p>使用 SVM 训练：</p>

<pre>
<code>from sklearn.svm import SVC
svm = SVC(kernel='linear')
svm.fit(vec.transform(x_train), y_train)
print(svm.score(vec.transform(x_test), y_test))</code></pre>

<p>使用决策树、随机森林、XGBoost、神经网络等等：</p>

<pre>
<code>import xgboost as xgb  
from sklearn.model_selection import StratifiedKFold  
import numpy as np
# xgb矩阵赋值  
xgb_train = xgb.DMatrix(vec.transform(x_train), label=y_train)  
xgb_test = xgb.DMatrix(vec.transform(x_test)) </code></pre>

<p>在 XGBoost 中，下面主要是调参指标，可以根据参数进行调参：</p>

<pre>
<code>params = {  
        'booster': 'gbtree',     #使用gbtree
        'objective': 'multi:softmax',  # 多分类的问题、  
        # 'objective': 'multi:softprob',   # 多分类概率  
        #'objective': 'binary:logistic',  #二分类
        'eval_metric': 'merror',   #logloss
        'num_class': 4,  # 类别数，与 multisoftmax 并用  
        'gamma': 0.1,  # 用于控制是否后剪枝的参数,越大越保守，一般0.1、0.2这样子。  
        'max_depth': 8,  # 构建树的深度，越大越容易过拟合  
        'alpha': 0,   # L1正则化系数  
        'lambda': 10,  # 控制模型复杂度的权重值的L2正则化项参数，参数越大，模型越不容易过拟合。  
        'subsample': 0.7,  # 随机采样训练样本  
        'colsample_bytree': 0.5,  # 生成树时进行的列采样  
        'min_child_weight': 3,  
        # 这个参数默认是 1，是每个叶子里面 h 的和至少是多少，对正负样本不均衡时的 0-1 分类而言  
        # 假设 h 在 0.01 附近，min_child_weight 为 1 叶子节点中最少需要包含 100 个样本。  
        'silent': 0,  # 设置成1则没有运行信息输出，最好是设置为0.  
        'eta': 0.03,  # 如同学习率  
        'seed': 1000,  
        'nthread': -1,  # cpu 线程数  
        'missing': 1 
    }</code></pre>

<p><strong>总结</strong></p>

<p>上面通过真实司法数据，一步步实现中文短文本分类的方法，整个示例代码可以当做模板来用，从优化和提高模型准确率来说，主要有两方面可以尝试：</p>

<ol>
	<li>特征向量的构建，除了词袋模型，可以考虑使用 word2vec 和 doc2vec 等；</li>
	<li>模型上可以选择有监督的分类算法、集成学习以及神经网络等。</li>
</ol>

<p>最后如果想了解更多，推荐我的两篇 Chat 文章：</p>

<ul>
	<li><a href="http://gitbook.cn/gitchat/activity/5ae2c9475d06502947fb1d98">NLP 中文短文本分类项目实践（上）</a></li>
	<li><a href="http://gitbook.cn/gitchat/activity/5afcf897109cd76e3c1dcd99">NLP 中文短文本分类项目实践（下）</a></li>
</ul>

<p><a id="第07课：动手实战基于 ML 的中文短文本聚类" name="第07课：动手实战基于 ML 的中文短文本聚类"></a>第07课：动手实战基于 ML 的中文短文本聚类</p>

<p>关于文本聚类，我曾在 Chat<a href="http://gitbook.cn/gitchat/activity/5b15556785040e095b60d67a">《NLP 中文文本聚类之无监督学习》</a>中介绍过，文本聚类是将一个个文档由原有的自然语言文字信息转化成数学信息，以高维空间点的形式展现出来，通过计算哪些点距离比较近，从而将那些点聚成一个簇，簇的中心叫做簇心。一个好的聚类要保证簇内点的距离尽量的近，但簇与簇之间的点要尽量的远。</p>

<p>如下图，以 K、M、N 三个点分别为聚类的簇心，将结果聚为三类，使得簇内点的距离尽量的近，但簇与簇之间的点尽量的远。</p>

<p><img alt="enter image description here" src="http://images.gitbook.cn/ee62afe0-7d2f-11e8-8748-9f97e9dc7c3b" /></p>

<p><strong>开发环境，我们选择：</strong>：</p>

<ol>
	<li>Windows 系统</li>
	<li>Python 3.6</li>
	<li>Jupyter Notebook</li>
</ol>

<p>本文继续沿用上篇文本分类中的语料来进行文本无监督聚类操作。</p>

<p><strong>整个过程分为以下几个步骤</strong>：</p>

<ul>
	<li>语料加载</li>
	<li>分词</li>
	<li>去停用词</li>
	<li>抽取词向量特征</li>
	<li>实战 TF-IDF 的中文文本 K-means 聚类</li>
	<li>实战 word2Vec 的中文文本 K-means 聚类</li>
</ul>

<p>下面开始项目实战。</p>

<p><strong>1.</strong>&nbsp;首先进行语料加载，在这之前，引入所需要的 Python 依赖包，并将全部语料和停用词字典读入内存中。</p>

<p>第一步，引入依赖库，有随机数库、jieba 分词、pandas 库等：</p>

<pre>
<code>import random
import jieba
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import gensim
from gensim.models import Word2Vec
from sklearn.preprocessing import scale
import multiprocessing</code></pre>

<p>第二步，加载停用词字典，停用词词典为 stopwords.txt 文件，可以根据场景自己在该文本里面添加要去除的词（比如冠词、人称、数字等特定词）：</p>

<pre>
<code>#加载停用词
stopwords=pd.read_csv('stopwords.txt',index_col=False,quoting=3,sep="\t",names=['stopword'], encoding='utf-8')
stopwords=stopwords['stopword'].values</code></pre>

<p>第三步，加载语料，语料是4个已经分好类的 csv 文件，直接用 pandas 加载即可，加载之后可以首先删除 nan 行，并提取要分词的 content 列转换为 list 列表：</p>

<pre>
<code>#加载语料
laogong_df = pd.read_csv('beilaogongda.csv', encoding='utf-8', sep=',')
laopo_df = pd.read_csv('beilaogongda.csv', encoding='utf-8', sep=',')
erzi_df = pd.read_csv('beierzida.csv', encoding='utf-8', sep=',')
nver_df = pd.read_csv('beinverda.csv', encoding='utf-8', sep=',')
#删除语料的nan行
laogong_df.dropna(inplace=True)
laopo_df.dropna(inplace=True)
erzi_df.dropna(inplace=True)
nver_df.dropna(inplace=True)
#转换
laogong = laogong_df.segment.values.tolist()
laopo = laopo_df.segment.values.tolist()
erzi = erzi_df.segment.values.tolist()
nver = nver_df.segment.values.tolist()</code></pre>

<p><strong>2.</strong>&nbsp;分词和去停用词。</p>

<p>第一步，定义分词、去停用词的函数，函数包含两个参数：<code>content_lines</code>&nbsp;参数为语料列表；sentences 参数为预先定义的 list，用来存储分词后的结果：</p>

<pre>
<code>#定义分词函数preprocess_text
#参数content_lines即为上面转换的list
#参数sentences是定义的空list，用来储存分词后的数据

def preprocess_text(content_lines, sentences):
    for line in content_lines:
        try:
            segs=jieba.lcut(line)
            segs = [v for v in segs if not str(v).isdigit()]#去数字
            segs = list(filter(lambda x:x.strip(), segs))   #去左右空格
            segs = list(filter(lambda x:len(x)&gt;1, segs)) #长度为1的字符
            segs = list(filter(lambda x:x not in stopwords, segs)) #去掉停用词
            sentences.append(" ".join(segs))
        except Exception:
            print(line)
            continue </code></pre>

<p>第二步，调用函数、生成训练数据，根据我提供的司法语料数据，分为报警人被老公打，报警人被老婆打，报警人被儿子打，报警人被女儿打，具体如下：</p>

<pre>
<code>sentences = []
preprocess_text(laogong, sentences)
preprocess_text(laopo, sentences)
preprocess_text(erzi, sentences)
preprocess_text(nver, sentences)</code></pre>

<p>第三步，将得到的数据集打散，生成更可靠的训练集分布，避免同类数据分布不均匀：</p>

<pre>
<code>random.shuffle(sentences)
</code></pre>

<p>第四步，我们控制台输出前10条数据，观察一下（因为上面进行了随机打散，你看到的前10条可能不一样）：</p>

<pre>
<code>for sentence in sentences[:10]:
    print(sentenc)</code></pre>

<p>得到的结果聚类和分类是不同的，这里没有标签：</p>

<p><img alt="enter image description here" src="http://images.gitbook.cn/bddb2e50-7d94-11e8-be78-bb5c0f92d7f1" /></p>

<p><strong>3.</strong>&nbsp;抽取词向量特征。</p>

<p>抽取特征，将文本中的词语转换为词频矩阵，统计每个词语的&nbsp;<code>tf-idf</code>&nbsp;权值，获得词在对应文本中的&nbsp;<code>tf-idf</code>&nbsp;权重：</p>

<pre>
<code>#将文本中的词语转换为词频矩阵 矩阵元素a[i][j] 表示j词在i类文本下的词频
vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5)
#统计每个词语的tf-idf权值
transformer = TfidfTransformer()
# 第一个fit_transform是计算tf-idf 第二个fit_transform是将文本转为词频矩阵
tfidf = transformer.fit_transform(vectorizer.fit_transform(sentences))
# 获取词袋模型中的所有词语
word = vectorizer.get_feature_names()
# 将tf-idf矩阵抽取出来，元素w[i][j]表示j词在i类文本中的tf-idf权重
weight = tfidf.toarray()
#查看特征大小
print ('Features length: ' + str(len(word)))</code></pre>

<p><strong>4.</strong>&nbsp;实战&nbsp;<code>TF-IDF</code>&nbsp;的中文文本&nbsp;<code>K-means</code>&nbsp;聚类</p>

<p>第一步，使用&nbsp;<code>k-means++</code>&nbsp;来初始化模型，当然也可以选择随机初始化，即<code>init=&quot;random&quot;</code>，然后通过 PCA 降维把上面的权重 weight 降到10维，进行聚类模型训练：</p>

<pre>
<code>numClass=4 #聚类分几簇
clf = KMeans(n_clusters=numClass, max_iter=10000, init="k-means++", tol=1e-6)  #这里也可以选择随机初始化init="random"
pca = PCA(n_components=10)  # 降维
TnewData = pca.fit_transform(weight)  # 载入N维
s = clf.fit(TnewData)</code></pre>

<p>第二步，定义聚类结果可视化函数<code>plot_cluster(result,newData,numClass)</code>，该函数包含3个参数，其中 result 表示聚类拟合的结果集；newData 表示权重 weight 降维的结果，这里需要降维到2维，即平面可视化；numClass 表示聚类分为几簇，绘制代码第一部分绘制结果 newData，第二部分绘制聚类的中心点：</p>

<pre>
<code>def plot_cluster(result,newData,numClass):
    plt.figure(2)
    Lab = [[] for i in range(numClass)]
    index = 0
    for labi in result:
        Lab[labi].append(index)
        index += 1
    color = ['oy', 'ob', 'og', 'cs', 'ms', 'bs', 'ks', 'ys', 'yv', 'mv', 'bv', 'kv', 'gv', 'y^', 'm^', 'b^', 'k^',
             'g^'] * 3 
    for i in range(numClass):
        x1 = []
        y1 = []
        for ind1 in newData[Lab[i]]:
            # print ind1
            try:
                y1.append(ind1[1])
                x1.append(ind1[0])
            except:
                pass
        plt.plot(x1, y1, color[i])

    #绘制初始中心点
    x1 = []
    y1 = []
    for ind1 in clf.cluster_centers_:
        try:
            y1.append(ind1[1])
            x1.append(ind1[0])
        except:
            pass
    plt.plot(x1, y1, "rv") #绘制中心
    plt.show()</code></pre>

<p>第三步，对数据降维到2维，然后获得结果，最后绘制聚类结果图：</p>

<pre>
<code>pca = PCA(n_components=2)  # 输出两维
newData = pca.fit_transform(weight)  # 载入N维
result = list(clf.predict(TnewData))
plot_cluster(result,newData,numClass)</code></pre>

<p>第四步，得到的聚类结果图，4个中心点和4个簇，我们看到结果还比较好，簇的边界很清楚：</p>

<p><img alt="enter image description here" src="http://images.gitbook.cn/6d68b500-7da3-11e8-8748-9f97e9dc7c3b" /></p>

<p>第五步，上面演示的可视化过程，降维使用了 PCA，我们还可以试试 TSNE，两者同为降维工具，主要区别在于，所在的包不同（也即机制和原理不同）：</p>

<pre>
<code>from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
</code></pre>

<p>因为原理不同，导致 TSNE 保留下的属性信息，更具代表性，也即最能体现样本间的差异，但是 TSNE 运行极慢，PCA 则相对较快，下面看看 TSNE 运行的可视化结果：</p>

<pre>
<code>from sklearn.manifold import TSNE
ts =TSNE(2)
newData = ts.fit_transform(weight)
result = list(clf.predict(TnewData))
plot_cluster(result,newData,numClass)</code></pre>

<p>得到的可视化结果，为一个中心点，不同簇落在围绕中心点的不同半径之内，我们看到在这里结果并不是很好：</p>

<p><img alt="enter image description here" src="http://images.gitbook.cn/4445a4c0-7da9-11e8-8a07-2345656531ad" /></p>

<p>第六步，为了更好的表达和获取更具有代表性的信息，在展示（可视化）高维数据时，更为一般的处理，常常先用 PCA 进行降维，再使用 TSNE：</p>

<pre>
<code>from sklearn.manifold import TSNE
newData = PCA(n_components=4).fit_transform(weight)  # 载入N维
newData =TSNE(2).fit_transform(newData)
result = list(clf.predict(TnewData))
plot_cluster(result,newData,numClass)</code></pre>

<p>得到的可视化结果，不同簇落在围绕中心点的不同半径之内：</p>

<p><img alt="enter image description here" src="http://images.gitbook.cn/3809f890-7daa-11e8-8a07-2345656531ad" /></p>

<p><strong>总结</strong></p>

<p>上面通过真实小案例，对司法数据一步步实现中文短文本聚类，从优化和提高模型准确率来说，主要有两方面可以尝试：</p>

<ol>
	<li>特征向量的构建，除了词袋模型，可以考虑使用 word2vec 和 doc2vec 等；</li>
	<li>模型上可以采用基于密度的 DBSCAN、层次聚类等算法。</li>
</ol>

<p><a id="第08课：从自然语言处理角度看 HMM 和 CRF" name="第08课：从自然语言处理角度看 HMM 和 CRF"></a>第08课：从自然语言处理角度看 HMM 和 CRF</p>

<p>近几年在自然语言处理领域中，HMM（隐马尔可夫模型）和 CRF（条件随机场）算法常常被用于分词、句法分析、命名实体识别、词性标注等。由于两者之间有很大的共同点，所以在很多应用上往往是重叠的，但在命名实体、句法分析等领域 CRF 似乎更胜一筹。通常来说如果做自然语言处理，这两个模型应该都要了解，下面我们来看看本文的内容。</p>

<h3>从贝叶斯定义理解生成式模型和判别式模型</h3>

<p>理解 HMM（隐马尔可夫模型）和 CRF（条件随机场）模型之前，我们先来看两个概念：生成式模型和判别式模型。</p>

<p>在机器学习中，生成式模型和判别式模型都用于有监督学习，有监督学习的任务就是从数据中学习一个模型（也叫分类器），应用这一模型，对给定的输入 X 预测相应的输出 Y。这个模型的一般形式为：决策函数 Y=f(X) 或者条件概率分布 P(Y|X)。</p>

<p>首先，简单从贝叶斯定理说起，若记 P(A)、P(B) 分别表示事件 A 和事件 B 发生的概率，则 P(A|B) 表示事件 B 发生的情况下事件 A 发生的概率；P(AB)表示事件 A 和事件 B 同时发生的概率。</p>

<p>根据贝叶斯公式可以得出：</p>

<p><img alt="enter image description here" src="http://images.gitbook.cn/ba258630-7e8d-11e8-abd3-eb6d72babbec" /></p>

<p><strong>生成式模型：</strong>估计的是联合概率分布，P(Y, X)=P(Y|X)*P(X)，由联合概率密度分布 P(X,Y)，然后求出条件概率分布 P(Y|X) 作为预测的模型，即生成模型公式为：P(Y|X)= P(X,Y)/ P(X)。基本思想是首先建立样本的联合概率密度模型 P(X,Y)，然后再得到后验概率 P(Y|X)，再利用它进行分类，其主要关心的是给定输入 X 产生输出 Y 的生成关系。</p>

<p><strong>判别式模型：</strong>估计的是条件概率分布， P(Y|X)，是给定观测变量 X 和目标变量 Y 的条件模型。由数据直接学习决策函数 Y=f(X) 或者条件概率分布 P(Y|X) 作为预测的模型，其主要关心的是对于给定的输入 X，应该预测什么样的输出 Y。</p>

<p>所以，HMM 使用隐含变量生成可观测状态，其生成概率有标注集统计得到，是一个生成模型。其他常见的生成式模型有：Gaussian、 Naive Bayes、Mixtures of multinomials 等。</p>

<p>而 CRF 就像一个反向的隐马尔可夫模型（HMM），通过可观测状态判别隐含变量，其概率亦通过标注集统计得来，是一个判别模型。其他常见的判别式模型有：K 近邻法、感知机、决策树、逻辑斯谛回归模型、最大熵模型、支持向量机、提升方法等。</p>

<p>HMM（隐马尔可夫模型）和 CRF（条件随机场）的理论部分，推荐看周志华老师的西瓜书《机器学习》。</p>

<h3>动手实战：基于 HMM 训练自己的 Python 中文分词器</h3>

<h4>模型介绍</h4>

<p>HMM 模型是由一个&ldquo;五元组&rdquo;组成的集合：</p>

<ul>
	<li>
	<p>StatusSet：状态值集合，状态值集合为 (B, M, E, S)，其中 B 为词的首个字，M 为词中间的字，E 为词语中最后一个字，S 为单个字，B、M、E、S 每个状态代表的是该字在词语中的位置。</p>

	<p>举个例子，对&ldquo;中国的人工智能发展进入高潮阶段&rdquo;，分词可以标注为：&ldquo;中B国E的S人B工E智B能E发B展E进B入E高B潮E阶B段E&rdquo;，最后的分词结果为：[&#39;中国&#39;, &#39;的&#39;, &#39;人工&#39;, &#39;智能&#39;, &#39;发展&#39;, &#39;进入&#39;, &#39;高潮&#39;, &#39;阶段&#39;]。</p>
	</li>
	<li>
	<p>ObservedSet：观察值集合，观察值集合就是所有语料的汉字，甚至包括标点符号所组成的集合。</p>
	</li>
	<li>
	<p>TransProbMatrix：转移概率矩阵，状态转移概率矩阵的含义就是从状态 X 转移到状态 Y 的概率，是一个4&times;4的矩阵，即 {B,E,M,S}&times;{B,E,M,S}。</p>
	</li>
	<li>
	<p>EmitProbMatrix：发射概率矩阵，发射概率矩阵的每个元素都是一个条件概率，代表 P(Observed[i]|Status[j]) 概率。</p>
	</li>
	<li>
	<p>InitStatus：初始状态分布，初始状态概率分布表示句子的第一个字属于 {B,E,M,S} 这四种状态的概率。</p>
	</li>
</ul>

<p>将 HMM 应用在分词上，要解决的问题是：参数（ObservedSet、TransProbMatrix、EmitRobMatrix、InitStatus）已知的情况下，求解状态值序列。</p>

<p>解决这个问题的最有名的方法是 Viterbi 算法。</p>

<h4>语料准备</h4>

<p>本次训练使用的预料&nbsp;<code>syj_trainCorpus_utf8.txt</code>&nbsp;是我爬取的短文本处理生成的。整个语料大小 264M，包含1116903条数据，UTF-8 编码，词与词之间用空格隔开，用来训练分词模型。</p>

<p>语料已上传到 CSDN 资源上，下载地址请点击：<a href="https://download.csdn.net/download/qq_36330643/10514771">中文自然语言处理中文分词训练语料</a>&nbsp;。</p>

<p>语料格式，用空格隔开的：</p>

<blockquote>
<p>如果 继续 听任 资产阶级 自由化 的 思潮 泛滥 ，</p>

<p>党 就 失去 了 凝聚力 和 战斗力 ，</p>

<p>怎么 能 成为 全国 人民 的 领导 核心 ？</p>

<p>中国 又 会 成为 一盘散沙 ，</p>

<p>那 还有 什么 希望 ？</p>
</blockquote>

<h4>编码实现</h4>

<p>（1）预定义</p>

<p>首先引出库，这两个库的作用是用来模型保存的：</p>

<pre>
<code>import pickle
import json
</code></pre>

<p>接下来定义 HMM 中的状态，初始化概率，以及中文停顿词：</p>

<pre>
<code>STATES = {'B', 'M', 'E', 'S'}
EPS = 0.0001
#定义停顿标点
seg_stop_words = {" ","，","。","“","”",'“', "？", "！", "：", "《", "》", "、", "；", "·", "‘ ", "’", "──", ",", ".", "?", "!", "`", "~", "@", "#", "$", "%", "^", "&amp;", "*", "(", ")", "-", "_", "+", "=", "[", "]", "{", "}", '"', "'", "&lt;", "&gt;", "\\", "|" "\r", "\n","\t"}
</code></pre>

<p>（2）面向对象封装成类</p>

<p>首先，将 HMM 模型封装为独立的类&nbsp;<code>HMM_Model</code>，下面先给出类的结构定义：</p>

<pre>
<code>class HMM_Model:
    def __init__(self):
        pass
    #初始化    
    def setup(self):
        pass
     #模型保存   
    def save(self, filename, code):
        pass
    #模型加载
    def load(self, filename, code):
        pass
    #模型训练
    def do_train(self, observes, states):
        pass
    #HMM计算
    def get_prob(self):
        pass
    #模型预测
    def do_predict(self, sequence):
        pass</code></pre>

<p>第一个方法&nbsp;<code>__init__()</code>&nbsp;是一种特殊的方法，被称为类的构造函数或初始化方法，当创建了这个类的实例时就会调用该方法，其中定义了数据结构和初始变量，实现如下：</p>

<pre>
<code>def __init__(self):
        self.trans_mat = {}  
        self.emit_mat = {} 
        self.init_vec = {}  
        self.state_count = {} 
        self.states = {}
        self.inited = False</code></pre>

<p>其中的数据结构定义：</p>

<ul>
	<li>
	<p><code>trans_mat</code>：状态转移矩阵，<code>trans_mat[state1][state2]</code>&nbsp;表示训练集中由 state1 转移到 state2 的次数。</p>
	</li>
	<li>
	<p><code>emit_mat</code>：观测矩阵，<code>emit_mat[state][char]</code>&nbsp;表示训练集中单字 char 被标注为 state 的次数。</p>
	</li>
	<li>
	<p><code>init_vec</code>：初始状态分布向量，<code>init_vec[state]</code>&nbsp;表示状态 state 在训练集中出现的次数。</p>
	</li>
	<li>
	<p><code>state_count</code>：状态统计向量，<code>state_count[state]</code>表示状态 state 出现的次数。</p>
	</li>
	<li>
	<p><code>word_set</code>：词集合，包含所有单词。</p>
	</li>
</ul>

<p>第二个方法 setup()，初始化第一个方法中的数据结构，具体实现如下：</p>

<pre>
<code>#初始化数据结构    
def setup(self):
    for state in self.states:
        # build trans_mat
        self.trans_mat[state] = {}
        for target in self.states:
            self.trans_mat[state][target] = 0.0
        self.emit_mat[state] = {}
        self.init_vec[state] = 0
        self.state_count[state] = 0
    self.inited = True</code></pre>

<p>第三个方法 save()，用来保存训练好的模型，filename 指定模型名称，默认模型名称为 hmm.json，这里提供两种格式的保存类型，JSON 或者 pickle 格式，通过参数 code 来决定，code 的值为&nbsp;<code>code=&#39;json&#39;</code>&nbsp;或者&nbsp;<code>code = &#39;pickle&#39;</code>，默认为&nbsp;<code>code=&#39;json&#39;</code>，具体实现如下：</p>

<pre>
<code>#模型保存   
def save(self, filename="hmm.json", code='json'):
    fw = open(filename, 'w', encoding='utf-8')
    data = {
        "trans_mat": self.trans_mat,
        "emit_mat": self.emit_mat,
        "init_vec": self.init_vec,
        "state_count": self.state_count
    }
    if code == "json":
        txt = json.dumps(data)
        txt = txt.encode('utf-8').decode('unicode-escape')
        fw.write(txt)
    elif code == "pickle":
        pickle.dump(data, fw)
    fw.close()</code></pre>

<p>第四个方法 load()，与第三个 save() 方法对应，用来加载模型，filename 指定模型名称，默认模型名称为 hmm.json，这里提供两种格式的保存类型，JSON 或者 pickle 格式，通过参数 code 来决定，code 的值为&nbsp;<code>code=&#39;json&#39;</code>&nbsp;或者<code>code = &#39;pickle&#39;</code>，默认为&nbsp;<code>code=&#39;json&#39;</code>，具体实现如下：</p>

<pre>
<code>#模型加载
def load(self, filename="hmm.json", code="json"):
    fr = open(filename, 'r', encoding='utf-8')
    if code == "json":
        txt = fr.read()
        model = json.loads(txt)
    elif code == "pickle":
        model = pickle.load(fr)
    self.trans_mat = model["trans_mat"]
    self.emit_mat = model["emit_mat"]
    self.init_vec = model["init_vec"]
    self.state_count = model["state_count"]
    self.inited = True
    fr.close()</code></pre>

<p>第五个方法&nbsp;<code>do_train()</code>，用来训练模型，因为使用的标注数据集， 因此可以使用更简单的监督学习算法，训练函数输入观测序列和状态序列进行训练， 依次更新各矩阵数据。类中维护的模型参数均为频数而非频率， 这样的设计使得模型可以进行在线训练，使得模型随时都可以接受新的训练数据继续训练，不会丢失前次训练的结果。具体实现如下：</p>

<pre>
<code>#模型训练
def do_train(self, observes, states):
    if not self.inited:
        self.setup()

    for i in range(len(states)):
        if i == 0:
            self.init_vec[states[0]] += 1
            self.state_count[states[0]] += 1
        else:
            self.trans_mat[states[i - 1]][states[i]] += 1
            self.state_count[states[i]] += 1
            if observes[i] not in self.emit_mat[states[i]]:
                self.emit_mat[states[i]][observes[i]] = 1
            else:
                self.emit_mat[states[i]][observes[i]] += 1</code></pre>

<p>第六个方法&nbsp;<code>get_prob()</code>，在进行预测前，需将数据结构的频数转换为频率，具体实现如下：</p>

<pre>
<code>#频数转频率
def get_prob(self):
    init_vec = {}
    trans_mat = {}
    emit_mat = {}
    default = max(self.state_count.values())  

    for key in self.init_vec:
        if self.state_count[key] != 0:
            init_vec[key] = float(self.init_vec[key]) / self.state_count[key]
        else:
            init_vec[key] = float(self.init_vec[key]) / default

    for key1 in self.trans_mat:
        trans_mat[key1] = {}
        for key2 in self.trans_mat[key1]:
            if self.state_count[key1] != 0:
                trans_mat[key1][key2] = float(self.trans_mat[key1][key2]) / self.state_count[key1]
            else:
                trans_mat[key1][key2] = float(self.trans_mat[key1][key2]) / default

    for key1 in self.emit_mat:
        emit_mat[key1] = {}
        for key2 in self.emit_mat[key1]:
            if self.state_count[key1] != 0:
                emit_mat[key1][key2] = float(self.emit_mat[key1][key2]) / self.state_count[key1]
            else:
                emit_mat[key1][key2] = float(self.emit_mat[key1][key2]) / default
    return init_vec, trans_mat, emit_mat</code></pre>

<p>第七个方法&nbsp;<code>do_predict()</code>，预测采用 Viterbi 算法求得最优路径， 具体实现如下：</p>

<pre>
<code>#模型预测
def do_predict(self, sequence):
    tab = [{}]
    path = {}
    init_vec, trans_mat, emit_mat = self.get_prob()

    # 初始化
    for state in self.states:
        tab[0][state] = init_vec[state] * emit_mat[state].get(sequence[0], EPS)
        path[state] = [state]

    # 创建动态搜索表
    for t in range(1, len(sequence)):
        tab.append({})
        new_path = {}
        for state1 in self.states:
            items = []
            for state2 in self.states:
                if tab[t - 1][state2] == 0:
                    continue
                prob = tab[t - 1][state2] * trans_mat[state2].get(state1, EPS) * emit_mat[state1].get(sequence[t], EPS)
                items.append((prob, state2))
            best = max(items)  
            tab[t][state1] = best[0]
            new_path[state1] = path[best[1]] + [state1]
        path = new_path

    # 搜索最有路径
    prob, state = max([(tab[len(sequence) - 1][state], state) for state in self.states])
    return path[state]</code></pre>

<p>上面实现了类&nbsp;<code>HMM_Model</code>&nbsp;的7个方法，接下来我们来实现分词器，这里先定义两个函数，这两个函数是独立的，不在类中。</p>

<p>（1）定义一个工具函数</p>

<p>对输入的训练语料中的每个词进行标注，因为训练数据是空格隔开的，可以进行转态标注，该方法用在训练数据的标注，具体实现如下：</p>

<pre>
<code>    def get_tags(src):
        tags = []
        if len(src) == 1:
            tags = ['S']
        elif len(src) == 2:
            tags = ['B', 'E']
        else:
            m_num = len(src) - 2
            tags.append('B')
            tags.extend(['M'] * m_num)
            tags.append('E')
        return tags
</code></pre>

<p>（2）定义一个工具函数</p>

<p>根据预测得到的标注序列将输入的句子分割为词语列表，也就是预测得到的状态序列，解析成一个 list 列表进行返回，具体实现如下：</p>

<pre>
<code>    def cut_sent(src, tags):
        word_list = []
        start = -1
        started = False

        if len(tags) != len(src):
            return None

        if tags[-1] not in {'S', 'E'}:
            if tags[-2] in {'S', 'E'}:
                tags[-1] = 'S'  
            else:
                tags[-1] = 'E'  

        for i in range(len(tags)):
            if tags[i] == 'S':
                if started:
                    started = False
                    word_list.append(src[start:i])  
                word_list.append(src[i])
            elif tags[i] == 'B':
                if started:
                    word_list.append(src[start:i])  
                start = i
                started = True
            elif tags[i] == 'E':
                started = False
                word = src[start:i+1]
                word_list.append(word)
            elif tags[i] == 'M':
                continue
        return word_list
</code></pre>

<p>最后，我们来定义分词器类 HMMSoyoger，继承&nbsp;<code>HMM_Model</code>&nbsp;类并实现中文分词器训练、分词功能，先给出 HMMSoyoger 类的结构定义：</p>

<pre>
<code>    class HMMSoyoger(HMM_Model):
        def __init__(self, *args, **kwargs):
            pass
        #加载训练数据
        def read_txt(self, filename):
            pass
        #模型训练函数
        def train(self):
            pass
        #模型分词预测
        def lcut(self, sentence):
            pass
</code></pre>

<p>第一个方法 init()，构造函数，定义了初始化变量，具体实现如下：</p>

<pre>
<code>    def __init__(self, *args, **kwargs):
            super(HMMSoyoger, self).__init__(*args, **kwargs)
            self.states = STATES
            self.data = None
</code></pre>

<p>第二个方法&nbsp;<code>read_txt()</code>，加载训练语料，读入文件为 txt，并且 UTF-8 编码，防止中文出现乱码，具体实现如下：</p>

<pre>
<code>    #加载语料
    def read_txt(self, filename):
            self.data = open(filename, 'r', encoding="utf-8")
</code></pre>

<p>第三个方法 train()，根据单词生成观测序列和状态序列，并通过父类的<code>do_train()</code>&nbsp;方法进行训练，具体实现如下：</p>

<pre>
<code>    def train(self):
            if not self.inited:
                self.setup()

            for line in self.data:
                line = line.strip()
                if not line:
                    continue

               #观测序列
                observes = []
                for i in range(len(line)):
                    if line[i] == " ":
                        continue
                    observes.append(line[i])

                #状态序列
                words = line.split(" ")  

                states = []
                for word in words:
                    if word in seg_stop_words:
                        continue
                    states.extend(get_tags(word))
                #开始训练
                if(len(observes) &gt;= len(states)):
                    self.do_train(observes, states)
                else:
                    pass
</code></pre>

<p>第四个方法 lcut()，模型训练好之后，通过该方法进行分词测试，具体实现如下：</p>

<pre>
<code>    def lcut(self, sentence):
            try:
                tags = self.do_predict(sentence)
                return cut_sent(sentence, tags)
            except:
                return sentence
</code></pre>

<p>通过上面两个类和两个方法，就完成了基于 HMM 的中文分词器编码，下面我们来进行模型训练和测试。</p>

<h4>训练模型</h4>

<p>首先实例化 HMMSoyoger 类，然后通过&nbsp;<code>read_txt()</code>&nbsp;方法加载语料，再通过 train() 进行在线训练，如果训练语料比较大，可能需要等待一点时间，具体实现如下：</p>

<pre>
<code>    soyoger = HMMSoyoger()
    soyoger.read_txt("syj_trainCorpus_utf8.txt")
    soyoger.train()
</code></pre>

<h4>模型测试</h4>

<p>模型训练完成之后，我们就可以进行测试：</p>

<pre>
<code>    soyoger.lcut("中国的人工智能发展进入高潮阶段。")
</code></pre>

<p>得到结果为：</p>

<blockquote>
<p>[&#39;中国&#39;, &#39;的&#39;, &#39;人工&#39;, &#39;智能&#39;, &#39;发展&#39;, &#39;进入&#39;, &#39;高潮&#39;, &#39;阶段&#39;, &#39;。&#39;]</p>
</blockquote>

<pre>
<code>soyoger.lcut("中文自然语言处理是人工智能技术的一个重要分支。")
</code></pre>

<p>得到结果为：</p>

<blockquote>
<p>[&#39;中文&#39;, &#39;自然&#39;, &#39;语言&#39;, &#39;处理&#39;, &#39;是人&#39;, &#39;工智&#39;, &#39;能技&#39;, &#39;术的&#39;, &#39;一个&#39;, &#39;重要&#39;, &#39;分支&#39;, &#39;。&#39;]</p>
</blockquote>

<p>可见，最后的结果还是不错的，如果想取得更好的结果，可自行制备更大更丰富的训练数据集。</p>

<h3>基于 CRF 的开源中文分词工具 Genius 实践</h3>

<p>Genius 是一个基于 CRF 的开源中文分词工具，采用了 Wapiti 做训练与序列标注，支持 Python 2.x、Python 3.x。</p>

<h4>安装</h4>

<p>（1）下载源码</p>

<p>在&nbsp;<a href="https://github.com/duanhongyi/genius">Github</a>&nbsp;上下载源码地址，解压源码，然后通过&nbsp;<code>python setup.py install</code>安装。</p>

<p>（2）Pypi 安装</p>

<p>通过执行命令：<code>easy_install genius</code>&nbsp;或者&nbsp;<code>pip install genius</code>&nbsp;安装。</p>

<h4>分词</h4>

<p>首先引入 Genius，然后对 text 文本进行分词。</p>

<pre>
<code>import genius
text = u"""中文自然语言处理是人工智能技术的一个重要分支。"""
seg_list = genius.seg_text(
    text,
    use_combine=True,
    use_pinyin_segment=True,
    use_tagging=True,
    use_break=True
)
print(' '.join([word.text for word in seg_list])
</code></pre>

<p>其中，<code>genius.seg_text</code>&nbsp;函数接受5个参数，其中 text 是必填参数：</p>

<ul>
	<li>text 第一个参数为需要分词的字。</li>
	<li><code>use_break</code>&nbsp;代表对分词结构进行打断处理，默认值 True。</li>
	<li><code>use_combine</code>&nbsp;代表是否使用字典进行词合并，默认值 False。</li>
	<li><code>use_tagging</code>&nbsp;代表是否进行词性标注，默认值 True。</li>
	<li><code>use_pinyin_segment</code>&nbsp;代表是否对拼音进行分词处理，默认值 True。</li>
</ul>

<h3>总结</h3>

<p>本文首先通过贝叶斯定理，理解了判别式模型和生成式模型的区别，接着通过动手实战&mdash;&mdash;基于 HMM 训练出自己的 Python 中文分词器，并进行了模型验证，最后给出一个基于 CRF 的开源中文分词工具。</p>

<h3>参考文献</h3>

<ol>
	<li><a href="https://github.com/duanhongyi/genius">Genius</a></li>
	<li>周志华《机器学习》</li>
</ol>

<p><a id="第09课：一网打尽神经序列模型之 RNN 及其变种 LSTM、GRU" name="第09课：一网打尽神经序列模型之 RNN 及其变种 LSTM、GRU"></a>第09课：一网打尽神经序列模型之 RNN 及其变种 LSTM、GRU</p>

<p>首先，我们来思考下，当人工神经网络从浅层发展到深层；从全连接到卷积神经网络。在此过程中，人类在图片分类、语音识别等方面都取得了非常好的结果，那么我们为什么还需要循环神经网络呢？</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/99202320-832c-11e8-a90b-215c3565b75a" /></p>

<p>因为，上面提到的这些网络结构的层与层之间是全连接或部分连接的，但在每层之间的节点是无连接的，这样的网络结构并不能很好的处理序列数据。</p>

<p>序列数据的处理，我们从语言模型 N-gram 模型说起，然后着重谈谈 RNN，并通过 RNN 的变种 LSTM 和 GRU 来实战文本分类。</p>

<h3>语言模型 N-gram 模型</h3>

<p>通过前面的课程，我们了解到一般自然语言处理的传统方法是将句子处理为一个词袋模型（Bag-of-Words，BoW），而不考虑每个词的顺序，比如用朴素贝叶斯算法进行垃圾邮件识别或者文本分类。在中文里有时候这种方式没有问题，因为有些句子即使把词的顺序打乱，还是可以看懂这句话在说什么，比如：</p>

<blockquote>
<p>T：研究表明，汉字的顺序并不一定能影响阅读，比如当你看完这句话后。</p>

<p>F：研表究明，汉字的序顺并不定一能影阅响读，比如当你看完这句话后。</p>
</blockquote>

<p>但有时候不行，词的顺序打乱，句子意思就变得让人不可思议了，例如：</p>

<blockquote>
<p>T：我喜欢吃烧烤。</p>

<p>F：烧烤喜欢吃我。</p>
</blockquote>

<p>那么，有没有模型是考虑句子中词与词之间的顺序的呢？有，语言模型中的 N-gram 就是一种。</p>

<p>N-gram 模型是一种语言模型（Language Model，LM），是一个基于概率的判别模型，它的输入是一句话（词的顺序序列），输出是这句话的概率，即这些词的联合概率（Joint Probability）。<br />
使用 N-gram 语言模型思想，一般是需要知道当前词以及前面的词，因为一个句子中每个词的出现并不是独立的。比如，如果第一个词是&ldquo;空气&rdquo;，接下来的词是&ldquo;很&rdquo;，那么下一个词很大概率会是&ldquo;新鲜&rdquo;。类似于我们人的联想，N-gram 模型知道的信息越多，得到的结果也越准确。</p>

<p>在前面课程中讲解的文本分类中，我们曾用到基于 sklearn 的词袋模型，尝试加入抽取&nbsp;<code>2-gram</code>&nbsp;和&nbsp;<code>3-gram</code>&nbsp;的统计特征，把词库的量放大，获得更强的特征。</p>

<p>通过 ngram_range 参数来控制，代码如下：</p>

<pre>
<code>     from sklearn.feature_extraction.text import CountVectorizer
        vec = CountVectorizer(
            analyzer='word', # tokenise by character ngrams
            ngram_range=(1,4),  # use ngrams of size 1 and 2
            max_features=20000,  # keep the most common 1000 ngrams
        )
</code></pre>

<p>因此，N-gram 模型，在自然语言处理中主要应用在如词性标注、垃圾短信分类、分词器、机器翻译和语音识别、语音识别等领域。</p>

<p>然而 N-gram 模型并不是完美的，它存在如下优缺点：</p>

<ul>
	<li>
	<p>优点：包含了前 N-1 个词所能提供的全部信息，这些词对于当前词的出现概率具有很强的约束力；</p>
	</li>
	<li>
	<p>缺点：需要很大规模的训练文本来确定模型的参数，当 N 很大时，模型的参数空间过大。所以常见的 N 值一般为1，2，3等。还有因数据稀疏而导致的数据平滑问题，解决方法主要是拉普拉斯平滑和内插与回溯。</p>
	</li>
</ul>

<p>所以，根据 N-gram 的优缺点，它的进化版 NNLM（Neural Network based Language Model）诞生了。</p>

<p>NNLM 由 Bengio 在2003年提出，它是一个很简单的模型，由四层组成，输入层、嵌入层、隐层和输出层，模型结构如下图（来自百度图片）：</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/ab99c060-832c-11e8-9f90-95294e517933" /></p>

<p>NNLM 接收的输入是长度为 N 的词序列，输出是下一个词的类别。首先，输入是词序列的 index 序列，例如词&ldquo;我&rdquo;在字典（大小为|V|）中的 index 是10，词&ldquo;是&rdquo;的 index 是23， &ldquo;小明&rdquo;的 index 是65，则句子&ldquo;我是小明&rdquo;的 index 序列就是 10、 23、65。嵌入层（Embedding）是一个大小为&nbsp;<code>|V|&times;K</code>&nbsp;的矩阵，从中取出第10、23、65行向量拼成 3&times;K 的矩阵就是 Embedding 层的输出了。隐层接受拼接后的 Embedding 层输出作为输入，以 tanh 为激活函数，最后送入带 softmax 的输出层，输出概率。</p>

<p>NNLM 最大的缺点就是参数多，训练慢，要求输入定长 N 这一点很不灵活，同时不能利用完整的历史信息。</p>

<p>因此，针对 NNLM 存在的问题，Mikolov 在2010年提出了 RNNLM，有兴趣可以阅读相关<a href="http://www.fit.vutbr.cz/~imikolov/rnnlm/thesis.pdf" target="_blank">论文</a>，其结构实际上是用 RNN 代替 NNLM 里的隐层，这样做的好处，包括减少模型参数、提高训练速度、接受任意长度输入、利用完整的历史信息。同时，RNN 的引入意味着可以使用 RNN 的其他变体，像 LSTM、BLSTM、GRU 等等，从而在序列建模上进行更多更丰富的优化。</p>

<p>以上，从词袋模型说起，引出语言模型 N-gram 以及其优化模型 NNLM 和 RNNLM，后续内容从 RNN 说起，来看看其变种 LSTM 和 GRU 模型如何处理类似序列数据。</p>

<h3>RNN 以及变种 LSTM 和 GRU 原理</h3>

<h4>RNN 为序列数据而生</h4>

<p>RNN 称为循环神经网路，因为这种网络有&ldquo;记忆性&rdquo;，主要应用在自然语言处理（NLP）和语音领域。RNN 具体的表现形式为网络会对前面的信息进行记忆并应用于当前输出的计算中，即隐藏层之间的节点不再无连接而是有连接的，并且隐藏层的输入不仅包括输入层的输出还包括上一时刻隐藏层的输出。</p>

<p>理论上，RNN 能够对任何长度的序列数据进行处理，但由于该网络结构存在&ldquo;梯度消失&rdquo;问题，所以在实际应用中，解决梯度消失的方法有：梯度裁剪（Clipping Gradient）和 LSTM（<code>Long Short-Term Memory</code>）。</p>

<p>下图是一个简单的 RNN 经典结构：</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/6772fa00-8354-11e8-a90b-215c3565b75a" /></p>

<p>RNN 包含输入单元（Input Units），输入集标记为&nbsp;\{x_0,x_1,...,x_t,x_t...\}{x0​,x1​,...,xt​,xt​...}；输出单元（Output Units）的输出集则被标记为&nbsp;\{y_0,y_1,...,y_t,...\}{y0​,y1​,...,yt​,...}；RNN 还包含隐藏单元（Hidden Units），我们将其输出集标记为&nbsp;\{h_0,h_1,...,h_t,...\}{h0​,h1​,...,ht​,...}，这些隐藏单元完成了最为主要的工作。</p>

<h4>LSTM 结构</h4>

<p>LSTM 在1997年由&ldquo;Hochreiter &amp; Schmidhuber&rdquo;提出，目前已经成为 RNN 中的标准形式，用来解决上面提到的 RNN 模型存在&ldquo;长期依赖&rdquo;的问题。</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/b986e5f0-8353-11e8-81ea-e357bbe10665" /></p>

<p>LSTM 通过三个&ldquo;门&rdquo;结构来控制不同时刻的状态和输出。所谓的&ldquo;门&rdquo;结构就是使用了 Sigmoid 激活函数的全连接神经网络和一个按位做乘法的操作，Sigmoid 激活函数会输出一个0~1之间的数值，这个数值代表当前有多少信息能通过&ldquo;门&rdquo;，0表示任何信息都无法通过，1表示全部信息都可以通过。其中，&ldquo;遗忘门&rdquo;和&ldquo;输入门&rdquo;是 LSTM 单元结构的核心。下面我们来详细分析下三种&ldquo;门&rdquo;结构。</p>

<ul>
	<li>
	<p>遗忘门，用来让 LSTM&ldquo;忘记&rdquo;之前没有用的信息。它会根据当前时刻节点的输入&nbsp;X_tXt​、上一时刻节点的状态&nbsp;Ct&minus;1Ct&minus;1&nbsp;和上一时刻节点的输出&nbsp;h_{t-1}ht&minus;1​&nbsp;来决定哪些信息将被遗忘。</p>
	</li>
	<li>
	<p>输入门，LSTM 来决定当前输入数据中哪些信息将被留下来。在 LSTM 使用遗忘门&ldquo;忘记&rdquo;部分信息后需要从当前的输入留下最新的记忆。输入门会根据当前时刻节点的输入&nbsp;X_tXt​、上一时刻节点的状态&nbsp;C_{t-1}Ct&minus;1​&nbsp;和上一时刻节点的输出&nbsp;h_{t-1}ht&minus;1​&nbsp;来决定哪些信息将进入当前时刻节点的状态&nbsp;C_tCt​，模型需要记忆这个最新的信息。</p>
	</li>
	<li>
	<p>输出门，LSTM 在得到最新节点状态&nbsp;C_tCt​&nbsp;后，结合上一时刻节点的输出&nbsp;h_{t-1}ht&minus;1​&nbsp;和当前时刻节点的输入&nbsp;X_tXt​&nbsp;来决定当前时刻节点的输出。</p>
	</li>
</ul>

<h4>GRU 结构</h4>

<p>GRU（Gated Recurrent Unit）是2014年提出来的新的 RNN 架构，它是简化版的 LSTM。下面是 LSTM 和 GRU 的结构比较图（来自于网络）：</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/9a01d620-8364-11e8-9e1d-f13ca808ab04" /></p>

<p>在超参数均调优的前提下，据说效果和 LSTM 差不多，但是参数少了1/3，不容易过拟合。如果发现 LSTM 训练出来的模型过拟合比较严重，可以试试 GRU。</p>

<h3>实战基于 Keras 的 LSTM 和 GRU 文本分类</h3>

<p>上面讲了那么多，但是 RNN 的知识还有很多，比如双向 RNN 等，这些需要自己去学习，下面，我们来实战一下基于 LSTM 和 GRU 的文本分类。</p>

<p>本次开发使用 Keras 来快速构建和训练模型，使用的数据集还是第06课使用的司法数据。</p>

<p>整个过程包括：</p>

<ol>
	<li>语料加载</li>
	<li>分词和去停用词</li>
	<li>数据预处理</li>
	<li>使用 LSTM 分类</li>
	<li>使用 GRU 分类</li>
</ol>

<p>第一步，引入数据处理库，停用词和语料加载：</p>

<pre>
<code>    #引入包
    import random
    import jieba
    import pandas as pd
    
    #加载停用词
    stopwords=pd.read_csv('stopwords.txt',index_col=False,quoting=3,sep="\t",names=['stopword'], encoding='utf-8')
    stopwords=stopwords['stopword'].values
    
    #加载语料
    laogong_df = pd.read_csv('beilaogongda.csv', encoding='utf-8', sep=',')
    laopo_df = pd.read_csv('beilaogongda.csv', encoding='utf-8', sep=',')
    erzi_df = pd.read_csv('beierzida.csv', encoding='utf-8', sep=',')
    nver_df = pd.read_csv('beinverda.csv', encoding='utf-8', sep=',')
    #删除语料的nan行
    laogong_df.dropna(inplace=True)
    laopo_df.dropna(inplace=True)
    erzi_df.dropna(inplace=True)
    nver_df.dropna(inplace=True)
    #转换
    laogong = laogong_df.segment.values.tolist()
    laopo = laopo_df.segment.values.tolist()
    erzi = erzi_df.segment.values.tolist()
    nver = nver_df.segment.values.tolist()
</code></pre>

<p>第二步，分词和去停用词：</p>

<pre>
<code>    #定义分词和打标签函数preprocess_text
    #参数content_lines即为上面转换的list
    #参数sentences是定义的空list，用来储存打标签之后的数据
    #参数category 是类型标签
    def preprocess_text(content_lines, sentences, category):
        for line in content_lines:
            try:
                segs=jieba.lcut(line)
                segs = [v for v in segs if not str(v).isdigit()]#去数字
                segs = list(filter(lambda x:x.strip(), segs)) #去左右空格
                segs = list(filter(lambda x:len(x)&gt;1, segs))#长度为1的字符
                segs = list(filter(lambda x:x not in stopwords, segs)) #去掉停用词
                sentences.append((" ".join(segs), category))# 打标签
            except Exception:
                print(line)
                continue 
    
    #调用函数、生成训练数据
    sentences = []
    preprocess_text(laogong, sentences,0)
    preprocess_text(laopo, sentences, 1)
    preprocess_text(erzi, sentences, 2)
    preprocess_text(nver, sentences, 3)
</code></pre>

<p>第三步，先打散数据，使数据分布均匀，然后获取特征和标签列表：</p>

<pre>
<code>    #打散数据，生成更可靠的训练集
    random.shuffle(sentences)
    
    #控制台输出前10条数据，观察一下
    for sentence in sentences[:10]:
        print(sentence[0], sentence[1])
    #所有特征和对应标签
    all_texts = [ sentence[0] for sentence in sentences]
    all_labels = [ sentence[1] for sentence in sentences]
</code></pre>

<p>第四步，使用 LSTM 对数据进行分类：</p>

<pre>
<code>    #引入需要的模块
    from keras.preprocessing.text import Tokenizer
    from keras.preprocessing.sequence import pad_sequences
    from keras.utils import to_categorical
    from keras.layers import Dense, Input, Flatten, Dropout
    from keras.layers import LSTM, Embedding,GRU
    from keras.models import Sequential
    
    #预定义变量
    MAX_SEQUENCE_LENGTH = 100    #最大序列长度
    EMBEDDING_DIM = 200    #embdding 维度
    VALIDATION_SPLIT = 0.16    #验证集比例
    TEST_SPLIT = 0.2    #测试集比例
    #keras的sequence模块文本序列填充
    tokenizer = Tokenizer()
    tokenizer.fit_on_texts(all_texts)
    sequences = tokenizer.texts_to_sequences(all_texts)
    word_index = tokenizer.word_index
    print('Found %s unique tokens.' % len(word_index))
    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)
    labels = to_categorical(np.asarray(all_labels))
    print('Shape of data tensor:', data.shape)
    print('Shape of label tensor:', labels.shape)
    
    #数据切分
    p1 = int(len(data)*(1-VALIDATION_SPLIT-TEST_SPLIT))
    p2 = int(len(data)*(1-TEST_SPLIT))
    x_train = data[:p1]
    y_train = labels[:p1]
    x_val = data[p1:p2]
    y_val = labels[p1:p2]
    x_test = data[p2:]
    y_test = labels[p2:]
    
    #LSTM训练模型
    model = Sequential()
    model.add(Embedding(len(word_index) + 1, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))
    model.add(LSTM(200, dropout=0.2, recurrent_dropout=0.2))
    model.add(Dropout(0.2))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(labels.shape[1], activation='softmax'))
    model.summary()
    #模型编译
    model.compile(loss='categorical_crossentropy',
                  optimizer='rmsprop',
                  metrics=['acc'])
    print(model.metrics_names)
    model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=10, batch_size=128)
    model.save('lstm.h5')
    #模型评估
    print(model.evaluate(x_test, y_test))
</code></pre>

<p>训练过程结果为：</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/1646b580-8382-11e8-a90b-215c3565b75a" /></p>

<p>第五步，使用 GRU 进行文本分类，上面就是完整的使用 LSTM 进行 文本分类，如果使用 GRU 只需要改变模型训练部分：</p>

<pre>
<code>    model = Sequential()
    model.add(Embedding(len(word_index) + 1, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))
    model.add(GRU(200, dropout=0.2, recurrent_dropout=0.2))
    model.add(Dropout(0.2))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(labels.shape[1], activation='softmax'))
    model.summary()
    
    model.compile(loss='categorical_crossentropy',
                  optimizer='rmsprop',
                  metrics=['acc'])
    print(model.metrics_names)
    model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=10, batch_size=128)
    model.save('lstm.h5')
    
    print(model.evaluate(x_test, y_test))
</code></pre>

<p>训练过程结果：</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/80bb3620-8382-11e8-878d-d106b6fbe32a" /></p>

<h3>总结</h3>

<p>本文从词袋模型谈起，旨在引出语言模型 N-gram 以及其优化模型 NNLM 和 RNNLM，并通过 RNN 以及其变种 LSTM 和 GRU 模型，理解其如何处理类似序列数据的原理，并实战基于 LSTM 和 GRU 的中文文本分类。</p>

<p><strong>参考文献：</strong></p>

<ol>
	<li>
	<p><a href="https://blog.csdn.net/u012328159/article/details/72847297" target="_blank">Hinton 神经网络公开课编程题2&mdash;&mdash;神经概率语言模型（NNLM）</a></p>
	</li>
	<li>
	<p><a href="http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/" target="_blank">Recurrent Neural Networks Tutorial, Part 3 &ndash; Backpropagation Through Time and Vanishing Gradients</a></p>
	</li>
</ol>

<p><a id="第10课：动手实战基于 CNN 的电影推荐系统" name="第10课：动手实战基于 CNN 的电影推荐系统"></a>第10课：动手实战基于 CNN 的电影推荐系统</p>

<p>本文从深度学习卷积神经网络入手，基于 Github 的开源项目来完成 MovieLens 数据集的电影推荐系统。</p>

<h3>什么是推荐系统呢？</h3>

<p>什么是推荐系统呢？首先我们来看看几个常见的推荐场景。</p>

<p>如果你经常通过豆瓣电影评分来找电影，你会发现下图所示的推荐：</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/ae7b9690-84cc-11e8-b31a-735b9fbd81d4" /></p>

<p>如果你喜欢购物，根据你的选择和购物行为，平台会给你推荐相似商品：</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/452fa580-84ce-11e8-bd72-e72df50fbc8a" /></p>

<p>在互联网的很多场景下都可以看到推荐的影子。因为推荐可以帮助用户和商家满足不同的需求：</p>

<ul>
	<li>
	<p>对用户而言：找到感兴趣的东西，帮助发现新鲜、有趣的事物。</p>
	</li>
	<li>
	<p>对商家而言：提供个性化服务，提高信任度和粘性，增加营收。</p>
	</li>
</ul>

<p>常见的推荐系统主要包含两个方面的内容，基于用户的推荐系统（UserCF）和基于物品的推荐系统（ItemCF）。两者的区别在于，UserCF 给用户推荐那些和他有共同兴趣爱好的用户喜欢的商品，而 ItemCF 给用户推荐那些和他之前喜欢的商品类似的商品。这两种方式都会遭遇冷启动问题。</p>

<p>下面是 UserCF 和 ItemCF 的对比：</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/f5728ff0-84d0-11e8-9658-97176c275e7c" /></p>

<h3>CNN 是如何应用在文本处理上的？</h3>

<p>提到卷积神经网络（CNN），相信大部分人首先想到的是图像分类，比如 MNIST 手写体识别，CAFRI10 图像分类。CNN 已经在图像识别方面取得了较大的成果，随着近几年的不断发展，在文本处理领域，基于文本挖掘的文本卷积神经网络被证明是有效的。</p>

<p>首先，来看看 CNN 是如何应用到 NLP 中的，下面是一个简单的过程图：</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/5ccd6290-84d3-11e8-815d-196036bcf3c7" /></p>

<p>和图像像素处理不一样，自然语言通常是一段文字，那么在特征矩阵中，矩阵的每一个行向量（比如 word2vec 或者 doc2vec）代表一个 Token，包括词或者字符。如果一段文字包含有 n 个词，每个词有 m 维的词向量，那么我们可以构造出一个&nbsp;<code>n*m</code>&nbsp;的词向量矩阵，在 NLP 处理过程中，让过滤器宽度和矩阵宽度保持一致整行滑动。</p>

<h3>动手实战基于 CNN 的电影推荐系统</h3>

<p>将 CNN 的技术应用到自然语言处理中并与电影推荐相结合，来训练一个基于文本的卷积神经网络，实现电影个性化推荐系统。</p>

<p>首先感谢作者 chengstone 的分享，源码请访问下面网址：</p>

<ul>
	<li><a href="https://github.com/chengstone/movie_recommender">Github</a></li>
</ul>

<p>在验证了 CNN 应用在自然语言处理上是有效的之后，从推荐系统的个性化推荐入手，在文本上，把 CNN 成果应用到电影的个性化推荐上。并在特征工程中，对训练集和测试集做了相应的特征处理，其中有部分字段是类型性变量，特征工程上可以采用&nbsp;<code>one-hot</code>&nbsp;编码，但是对于 UserID、MovieID 这样非常稀疏的变量，如果使用&nbsp;<code>one-hot</code>，那么数据的维度会急剧膨胀，对于这份数据集来说是不合适的。</p>

<p>具体算法设计如下：</p>

<p><strong>1.</strong>&nbsp;定义用户嵌入矩阵。</p>

<p>用户的特征矩阵主要是通过用户信息嵌入网络来生成的，在预处理数据的时候，我们将 UserID、MovieID、性别、年龄、职业特征全部转成了数字类型，然后把这个数字当作嵌入矩阵的索引，在网络的第一层就使用嵌入层，这样数据输入的维度保持在（N，32）和（N，16）。然后进行全连接层，转成（N，128）的大小，再进行全连接层，转成（N，200）的大小，这样最后输出的用户特征维度相对比较高，也保证了能把每个用户所带有的特征充分携带并通过特征表达。</p>

<p>具体流程如下：</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/3983eae0-88cd-11e8-a341-85988e434d41" /></p>

<p><strong>2.</strong>&nbsp;生成用户特征。</p>

<p>生成用户特征是在用户嵌入矩阵网络输出结果的基础上，通过2层全连接层实现的。第一个全连接层把特征矩阵转成（N，128）的大小，再进行第二次全连接层，转成（N，200）的大小，这样最后输出的用户特征维度相对比较高，也保证了能把每个用户所带有的特征充分携带并通过特征表达。</p>

<p>具体流程如下：</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/747e2f20-88cd-11e8-9944-4f7451c47515" /></p>

<p><strong>3.</strong>&nbsp;定义电影 ID 嵌入矩阵。</p>

<p>通过电影 ID 和电影类型分别生成电影 ID 和电影类型特征，电影类型的多个嵌入向量做加和输出。电影 ID 的实现过程和上面一样，但是对于电影类型的处理相较于上面，稍微复杂一点。因为电影类型有重叠性，一个电影可以属于多个类别，当把电影类型从嵌入矩阵索引出来之后是一个（N，32）形状的矩阵，因为有多个类别，这里采用的处理方式是矩阵求和，把类别加上去，变成（1，32）形状，这样使得电影的类别信息不会丢失。</p>

<p>具体流程如下：</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/9483d0e0-88cd-11e8-8555-ffb97409ea91" /></p>

<p><strong>4.</strong>&nbsp;文本卷积神经网络设计。</p>

<p>文本卷积神经网络和单纯的 CNN 网络结构有点不同，因为自然语言通常是一段文字与图片像素组成的矩阵是不一样的。在电影文本特征矩阵中，矩阵的每一个行构成的行向量代表一个 Token，包括词或者字符。如果一段文字有 n 个词，每个词有 m 维的词向量，那么我们可以构造出一个&nbsp;<code>n*m</code>&nbsp;的矩阵。而且 NLP 处理过程中，会有多个不同大小的过滤器串行执行，且过滤器宽度和矩阵宽度保持一致，是整行滑动。在执行完卷积操作之后采用了 ReLU 激活函数，然后采用最大池化操作，最后通过全连接并 Dropout 操作和 Softmax 输出。这里电影名称的处理比较特殊，并没有采用循环神经网络，而采用的是文本在 CNN 网络上的应用。</p>

<p>对于电影数据集，我们对电影名称做 CNN 处理，其大致流程，从嵌入矩阵中得到电影名对应的各个单词的嵌入向量，由于电影名称比较特殊一点，名称长度有一定限制，这里过滤器大小使用时，就选择2、3、4、5长度。然后对文本嵌入层使用滑动2、3、4、5个单词尺寸的卷积核做卷积和最大池化，然后 Dropout 操作，全连接层输出。</p>

<p>具体流程如下：</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/49e32370-8e3a-11e8-8ee0-a17ea463076e" /></p>

<p>具体过程描述：</p>

<p>（1）首先输入一个&nbsp;<code>32*32</code>&nbsp;的矩阵；</p>

<p>（2）第一次卷积核大小为&nbsp;<code>2*2</code>，得到&nbsp;<code>31*31</code>&nbsp;的矩阵，然后通过&nbsp;<code>[1,14,1,1]</code>的&nbsp;<code>max-pooling</code>&nbsp;操作，得到的矩阵为&nbsp;<code>18*31</code>；</p>

<p>（3）第二次卷积核大小为&nbsp;<code>3*3</code>，得到&nbsp;<code>16*29的矩阵，然后通过[1,13,1,1]</code>&nbsp;的<code>max-pooling</code>&nbsp;操作，得到的矩阵为&nbsp;<code>4*29</code>；</p>

<p>（4）第三次卷积核大小&nbsp;<code>4*4</code>，得到&nbsp;<code>1*26</code>&nbsp;的矩阵，然后通过&nbsp;<code>[1,12,1,1]</code>&nbsp;的<code>max-pooling</code>&nbsp;操作，得到的矩阵为&nbsp;<code>1*26</code>；</p>

<p>（5）第四次卷积核大小&nbsp;<code>5*5</code>，得到&nbsp;<code>1*22</code>&nbsp;的矩阵，然后通过&nbsp;<code>[1,11,1,1]</code>&nbsp;的<code>max-pooling</code>&nbsp;操作，得到的矩阵为&nbsp;<code>1*22</code>；</p>

<p>（6）最后通过 Dropout 和全连接层，<code>len(window_sizes) * filter_num =32</code>，得到&nbsp;<code>1*32</code>的矩阵。</p>

<p><strong>5.</strong>&nbsp;电影各层做一个全连接层。</p>

<p>将上面几步生成的特征向量，通过2个全连接层连接在一起，第一个全连接层是电影 ID 特征和电影类型特征先全连接，之后再和 CNN 生成的电影名称特征全连接，生成最后的特征集。</p>

<p>具体流程如下：</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/336d6d10-88ce-11e8-bee1-4fab1cb0896f" /></p>

<p><strong>6.</strong>&nbsp;完整的基于 CNN 的电影推荐流程。</p>

<p>把以上实现的模块组合成整个算法，将网络模型作为回归问题进行训练，得到训练好的用户特征矩阵和电影特征矩阵进行推荐。</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/abffaae0-88ce-11e8-950d-3f19ed380d25" /></p>

<h3>基于 CNN 的电影推荐系统代码调参过程</h3>

<p>在训练过程中，我们需要对算法预先设置一些超参数，这里给出的最终的设置结果：</p>

<pre>
<code>    # 设置迭代次数
    num_epochs = 5
    # 设置BatchSize大小
    batch_size = 256
    #设置dropout保留比例
    dropout_keep = 0.5
    # 设置学习率
    learning_rate = 0.0001
    # 设置每轮显示的batches大小
    show_every_n_batches = 20
</code></pre>

<p>首先对数据集进行划分，按照 4:1 的比例划分为训练集和测试集，下面给出的是算法模型最终训练集合测试集使用的划分结果：</p>

<pre>
<code>    #将数据集分成训练集和测试集，随机种子不固定
    train_X,test_X, train_y, test_y = train_test_split(features,  
                                                 targets_values,  
                                                 test_size = 0.3,  
                                                 random_state = 0) 
</code></pre>

<p>接下来是具体模型训练过程。训练过程，要不断调参，根据经验调参粒度可以选择从粗到细分阶段进行。</p>

<p>调参过程对比：</p>

<p>（1）第一步，先固定，<code>learning_rate=0.01</code>&nbsp;和&nbsp;<code>num_epochs=10</code>，测试<code>batch_size=128</code>&nbsp;对迭代时间和 Loss 的影响；</p>

<p>（2）第二步，先固定，<code>learning_rate=0.01</code>&nbsp;和&nbsp;<code>num_epochs=10</code>，测试<code>batch_size=256</code>&nbsp;对迭代时间和 Loss 的影响；</p>

<p>（3）第三步，先固定，<code>learning_rate=0.01</code>&nbsp;和&nbsp;<code>num_epochs=10</code>，测试<code>batch_size=512</code>&nbsp;对迭代时间和 Loss 的影响；</p>

<p>（4）第四步，先固定，<code>learning_rate=0.01</code>&nbsp;和&nbsp;<code>num_epochs=5</code>，测试<code>batch_size=128</code>&nbsp;对迭代时间和 Loss 的影响；</p>

<p>（5）第五步，先固定，<code>learning_rate=0.01</code>&nbsp;和&nbsp;<code>num_epochs=5</code>，测试<code>batch_size=256</code>&nbsp;对迭代时间和 Loss 的影响；</p>

<p>（6）第六步，先固定，<code>learning_rate=0.01</code>&nbsp;和&nbsp;<code>num_epochs=5</code>，测试<code>batch_size=512</code>&nbsp;对迭代时间和 Loss 的影响；</p>

<p>（7）第七步，先固定，<code>batch_size=256</code>&nbsp;和&nbsp;<code>num_epochs=5</code>，测试<code>learning_rate=0.001</code>&nbsp;对 Loss 的影响；</p>

<p>（8）第八步，先固定，<code>batch_size=256</code>&nbsp;和&nbsp;<code>num_epochs=5</code>，测试<code>learning_rate=0.0005</code>&nbsp;对 Loss 的影响；</p>

<p>（9）第九步，先固定，<code>batch_size=256</code>&nbsp;和&nbsp;<code>num_epochs=5</code>，测试<code>learning_rate=0.0001</code>&nbsp;对 Loss 的影响；</p>

<p>（10）第十步，先固定，<code>batch_size=256</code>&nbsp;和&nbsp;<code>num_epochs=5</code>，测试<code>learning_rate=0.00005</code>&nbsp;对 Loss 的影响。</p>

<p>得到的调参结果对比表如下：</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/64b2fe20-88cf-11e8-81e9-19f90a93f5ff" /></p>

<p>通过上面（1）-（6）步调参比较，在&nbsp;<code>learning_rate</code>、<code>batch_size</code>&nbsp;相同的情况下，<code>num_epochs</code>&nbsp;对于训练时间影响较大；而在<code>learning_rate</code>、<code>num_epochs</code>&nbsp;相同情况下，<code>batch_size</code>&nbsp;对 Loss 的影响较大，<code>batch_size</code>&nbsp;选择512，Loss 有抖动情况，权衡之下，最终确定后续调参固定采用&nbsp;<code>batch_size=256</code>、<code>num_epochs=5</code>&nbsp;的超参数值，后续（7）-（10）步，随着&nbsp;<code>learning_rate</code>&nbsp;逐渐减小，发现 Loss 是先逐渐减小，而在<code>learning_rate=0.00005</code>&nbsp;时反而增大，最终选择出学习率为<code>learning_rate=0.0001</code>&nbsp;的超参数值。</p>

<h3>基于 CNN 的电影推荐系统电影推荐</h3>

<p>在上面，完成模型训练验证之后，实际来进行推荐电影，这里使用生产的用户特征矩阵和电影特征矩阵做电影推荐，主要有三种方式的推荐。</p>

<p><strong>1.</strong>&nbsp;推荐同类型的电影。</p>

<p>思路是：计算当前看的电影特征向量与整个电影特征矩阵的余弦相似度，取相似度最大的&nbsp;<code>top_k</code>&nbsp;个，这里加了些随机选择在里面，保证每次的推荐稍稍有些不同。</p>

<pre>
<code>    def recommend_same_type_movie(movie_id_val, top_k = 20):

        loaded_graph = tf.Graph()  #
        with tf.Session(graph=loaded_graph) as sess:  #
            # Load saved model
            loader = tf.train.import_meta_graph(load_dir + '.meta')
            loader.restore(sess, load_dir)

            norm_movie_matrics = tf.sqrt(tf.reduce_sum(tf.square(movie_matrics), 1, keep_dims=True))
            normalized_movie_matrics = movie_matrics / norm_movie_matrics

            #推荐同类型的电影
            probs_embeddings = (movie_matrics[movieid2idx[movie_id_val]]).reshape([1, 200])
            probs_similarity = tf.matmul(probs_embeddings, tf.transpose(normalized_movie_matrics))
            sim = (probs_similarity.eval())
            print("您看的电影是：{}".format(movies_orig[movieid2idx[movie_id_val]]))
            print("以下是给您的推荐：")
            p = np.squeeze(sim)
            p[np.argsort(p)[:-top_k]] = 0
            p = p / np.sum(p)
            results = set()
            while len(results) != 5:
                c = np.random.choice(3883, 1, p=p)[0]
                results.add(c)
            for val in (results):
                print(val)
                print(movies_orig[val])
            return result
</code></pre>

<p><strong>2.</strong>&nbsp;推荐您喜欢的电影。</p>

<p>思路是：使用用户特征向量与电影特征矩阵计算所有电影的评分，取评分最高的<code>top_k</code>&nbsp;个，同样加了些随机选择部分。</p>

<pre>
<code>    def recommend_your_favorite_movie(user_id_val, top_k = 10):

        loaded_graph = tf.Graph()  #
        with tf.Session(graph=loaded_graph) as sess:  #
            # Load saved model
            loader = tf.train.import_meta_graph(load_dir + '.meta')
            loader.restore(sess, load_dir)

            #推荐您喜欢的电影
            probs_embeddings = (users_matrics[user_id_val-1]).reshape([1, 200])
            probs_similarity = tf.matmul(probs_embeddings, tf.transpose(movie_matrics))
            sim = (probs_similarity.eval())

            print("以下是给您的推荐：")
            p = np.squeeze(sim)
            p[np.argsort(p)[:-top_k]] = 0
            p = p / np.sum(p)
            results = set()
            while len(results) != 5:
                c = np.random.choice(3883, 1, p=p)[0]
                results.add(c)
            for val in (results):
                print(val)
                print(movies_orig[val])

            return results
</code></pre>

<p><strong>3.</strong>&nbsp;看过这个电影的人还看了（喜欢）哪些电影。</p>

<p>（1）首先选出喜欢某个电影的&nbsp;<code>top_k</code>&nbsp;个人，得到这几个人的用户特征向量；</p>

<p>（2）然后计算这几个人对所有电影的评分 ；</p>

<p>（3）选择每个人评分最高的电影作为推荐；</p>

<p>（4）同样加入了随机选择。</p>

<pre>
<code>    def recommend_other_favorite_movie(movie_id_val, top_k = 20):
        loaded_graph = tf.Graph()  #
        with tf.Session(graph=loaded_graph) as sess:  #
            # Load saved model
            loader = tf.train.import_meta_graph(load_dir + '.meta')
            loader.restore(sess, load_dir)
            probs_movie_embeddings = (movie_matrics[movieid2idx[movie_id_val]]).reshape([1, 200])
            probs_user_favorite_similarity = tf.matmul(probs_movie_embeddings, tf.transpose(users_matrics))
            favorite_user_id = np.argsort(probs_user_favorite_similarity.eval())[0][-top_k:]

            print("您看的电影是：{}".format(movies_orig[movieid2idx[movie_id_val]]))

            print("喜欢看这个电影的人是：{}".format(users_orig[favorite_user_id-1]))
            probs_users_embeddings = (users_matrics[favorite_user_id-1]).reshape([-1, 200])
            probs_similarity = tf.matmul(probs_users_embeddings, tf.transpose(movie_matrics))
            sim = (probs_similarity.eval())
            p = np.argmax(sim, 1)
            print("喜欢看这个电影的人还喜欢看：")
            results = set()
            while len(results) != 5:
                c = p[random.randrange(top_k)]
                results.add(c)
            for val in (results):
                print(val)
                print(movies_orig[val])
            return results
</code></pre>

<h3>基于 CNN 的电影推荐系统不足</h3>

<p>这里讨论一下基于上述方法所带来的不足：</p>

<ol>
	<li>
	<p>由于一个新的用户在刚开始的时候并没有任何行为记录，所以系统会出现冷启动的问题；</p>
	</li>
	<li>
	<p>由于神经网络是一个黑盒子过程，我们并不清楚在反向传播的过程中的具体细节，也不知道每一个卷积层抽取的特征细节，所以此算法缺乏一定的可解释性；</p>
	</li>
	<li>
	<p>一般来说，在工业界，用户的数据量是海量的，而卷积神经网络又要耗费大量的计算资源，所以进行集群计算是非常重要的。但是由于本课程所做实验环境有限，还是在单机上运行，所以后期可以考虑在服务器集群上全量跑数据，这样获得的结果也更准确。</p>
	</li>
</ol>

<h3>总结</h3>

<p>上面通过&nbsp;<a href="https://github.com/chengstone/movie_recommender">Github</a>&nbsp;上一个开源的项目，梳理了 CNN 在文本推荐上的应用，并通过模型训练调参，给出一般的模型调参思路，最后建议大家自己把源码下载下来跑跑模型，效果更好。</p>

<h3>参考文献及推荐阅读</h3>

<ol>
	<li>
	<p><a href="https://blog.csdn.net/dream_catcher_10/article/details/50733172">推荐系统</a></p>
	</li>
	<li>
	<p>Deep Convolutional Neural Networks for Sentiment Analysis of ShortTexts,CND Santos ,M Gattit ,2014.</p>
	</li>
	<li>
	<p>推荐系统实践，p50-60，p120-130，项亮。</p>
	</li>
</ol>

<p><a id="第11课：动手实战基于 LSTM 轻松生成各种古诗" name="第11课：动手实战基于 LSTM 轻松生成各种古诗"></a>第11课：动手实战基于 LSTM 轻松生成各种古诗</p>

<p>目前循环神经网络（RNN）已经广泛用于自然语言处理中，可以处理大量的序列数据，可以说是最强大的神经网络模型之一。人们已经给 RNN 找到了越来越多的事情做，比如画画和写诗，微软的小冰都已经出版了一本诗集了。</p>

<p>而其实训练一个能写诗的神经网络并不难，下面我们就介绍如何简单快捷地建立一个会写诗的网络模型。</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/0fdb0170-8db8-11e8-80d1-2d51ff7e1c55" /></p>

<p>本次开发环境如下：</p>

<ul>
	<li>Python 3.6</li>
	<li>Keras 环境</li>
	<li>Jupyter Notebook</li>
</ul>

<p>整个过程分为以下步骤完成：</p>

<ol>
	<li>语料准备</li>
	<li>语料预处理</li>
	<li>模型参数配置</li>
	<li>构建模型</li>
	<li>训练模型</li>
	<li>模型作诗</li>
	<li>绘制模型网络结构图</li>
</ol>

<p>下面一步步来构建和训练一个会写诗的模型。</p>

<p><strong>第一</strong>，语料准备。一共四万多首古诗，每行一首诗，标题在预处理的时候已经去掉了。</p>

<p><strong>第二</strong>，文件预处理。首先，机器并不懂每个中文汉字代表的是什么，所以要将文字转换为机器能理解的形式，这里我们采用 One-Hot 的形式，这样诗句中的每个字都能用向量来表示，下面定义函数&nbsp;<code>preprocess_file()</code>&nbsp;来处理。</p>

<pre>
<code>    puncs = [']', '[', '（', '）', '{', '}', '：', '《', '》']
    def preprocess_file(Config):
        # 语料文本内容
        files_content = ''
        with open(Config.poetry_file, 'r', encoding='utf-8') as f:
            for line in f:
                # 每行的末尾加上"]"符号代表一首诗结束
                for char in puncs:
                    line = line.replace(char, "")
                files_content += line.strip() + "]"

        words = sorted(list(files_content))
        words.remove(']')
        counted_words = {}
        for word in words:
            if word in counted_words:
                counted_words[word] += 1
            else:
                counted_words[word] = 1

        # 去掉低频的字
        erase = []
        for key in counted_words:
            if counted_words[key] &lt;= 2:
                erase.append(key)
        for key in erase:
            del counted_words[key]
        del counted_words[']']
        wordPairs = sorted(counted_words.items(), key=lambda x: -x[1])

        words, _ = zip(*wordPairs)
        # word到id的映射
        word2num = dict((c, i + 1) for i, c in enumerate(words))
        num2word = dict((i, c) for i, c in enumerate(words))
        word2numF = lambda x: word2num.get(x, 0)
        return word2numF, num2word, words, files_content
</code></pre>

<p>在每行末尾加上&nbsp;<code>]</code>&nbsp;符号是为了标识这首诗已经结束了。我们给模型学习的方法是，给定前六个字，生成第七个字，所以在后面生成训练数据的时候，会以6的跨度，1的步长截取文字，生成语料。如果出现了&nbsp;<code>]</code>&nbsp;符号，说明&nbsp;<code>]</code>&nbsp;符号之前的语句和之后的语句是两首诗里面的内容，两首诗之间是没有关联关系的，所以我们后面会舍弃掉包含&nbsp;<code>]</code>&nbsp;符号的训练数据。</p>

<p><strong>第三</strong>，模型参数配置。预先定义模型参数和加载语料以及模型保存名称，通过类 Config 实现。</p>

<pre>
<code>class Config(object):
    poetry_file = 'poetry.txt'
    weight_file = 'poetry_model.h5'
    # 根据前六个字预测第七个字
    max_len = 6
    batch_size = 512
    learning_rate = 0.001
</code></pre>

<p><strong>第四</strong>，构建模型，通过 PoetryModel 类实现，类的代码结构如下：</p>

<pre>
<code>    class PoetryModel(object):
        def __init__(self, config):
            pass

        def build_model(self):
            pass

        def sample(self, preds, temperature=1.0):
            pass

        def generate_sample_result(self, epoch, logs):
            pass

        def predict(self, text):
            pass

        def data_generator(self):
            pass
        def train(self):
            pass
</code></pre>

<p>类中定义的方法具体实现功能如下：</p>

<p>（1）init 函数定义，通过加载 Config 配置信息，进行语料预处理和模型加载，如果模型文件存在则直接加载模型，否则开始训练。</p>

<pre>
<code>    def __init__(self, config):
            self.model = None
            self.do_train = True
            self.loaded_model = False
            self.config = config

            # 文件预处理
            self.word2numF, self.num2word, self.words, self.files_content = preprocess_file(self.config)
            if os.path.exists(self.config.weight_file):
                self.model = load_model(self.config.weight_file)
                self.model.summary()
            else:
                self.train()
            self.do_train = False
            self.loaded_model = True
</code></pre>

<p>（2）<code>build_model</code>&nbsp;函数主要用 Keras 来构建网络模型，这里使用 LSTM 的 GRU 来实现，当然直接使用 LSTM 也没问题。</p>

<pre>
<code>    def build_model(self):
            '''建立模型'''
            input_tensor = Input(shape=(self.config.max_len,))
            embedd = Embedding(len(self.num2word)+1, 300, input_length=self.config.max_len)(input_tensor)
            lstm = Bidirectional(GRU(128, return_sequences=True))(embedd)
            dropout = Dropout(0.6)(lstm)
            lstm = Bidirectional(GRU(128, return_sequences=True))(embedd)
            dropout = Dropout(0.6)(lstm)
            flatten = Flatten()(lstm)
            dense = Dense(len(self.words), activation='softmax')(flatten)
            self.model = Model(inputs=input_tensor, outputs=dense)
            optimizer = Adam(lr=self.config.learning_rate)
            self.model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
</code></pre>

<p>（3）sample 函数，在训练过程的每个 epoch 迭代中采样。</p>

<pre>
<code>    def sample(self, preds, temperature=1.0):
        '''
        当temperature=1.0时，模型输出正常
        当temperature=0.5时，模型输出比较open
        当temperature=1.5时，模型输出比较保守
        在训练的过程中可以看到temperature不同，结果也不同
        '''
        preds = np.asarray(preds).astype('float64')
        preds = np.log(preds) / temperature
        exp_preds = np.exp(preds)
        preds = exp_preds / np.sum(exp_preds)
        probas = np.random.multinomial(1, preds, 1)
        return np.argmax(probas)
</code></pre>

<p>（4）训练过程中，每个 epoch 打印出当前的学习情况。</p>

<pre>
<code>    def generate_sample_result(self, epoch, logs):  
            print("\n==================Epoch {}=====================".format(epoch))
            for diversity in [0.5, 1.0, 1.5]:
                print("------------Diversity {}--------------".format(diversity))
                start_index = random.randint(0, len(self.files_content) - self.config.max_len - 1)
                generated = ''
                sentence = self.files_content[start_index: start_index + self.config.max_len]
                generated += sentence
                for i in range(20):
                    x_pred = np.zeros((1, self.config.max_len))
                    for t, char in enumerate(sentence[-6:]):
                        x_pred[0, t] = self.word2numF(char)

                    preds = self.model.predict(x_pred, verbose=0)[0]
                    next_index = self.sample(preds, diversity)
                    next_char = self.num2word[next_index]
                    generated += next_char
                    sentence = sentence + next_char
                print(sentence)
</code></pre>

<p>（5）predict 函数，用于根据给定的提示，来进行预测。</p>

<p>根据给出的文字，生成诗句，如果给的 text 不到四个字，则随机补全。</p>

<pre>
<code>    def predict(self, text):
            if not self.loaded_model:
                return
            with open(self.config.poetry_file, 'r', encoding='utf-8') as f:
                file_list = f.readlines()
            random_line = random.choice(file_list)
            # 如果给的text不到四个字，则随机补全
            if not text or len(text) != 4:
                for _ in range(4 - len(text)):
                    random_str_index = random.randrange(0, len(self.words))
                    text += self.num2word.get(random_str_index) if self.num2word.get(random_str_index) not in [',', '。',
                                                                                                               '，'] else self.num2word.get(
                        random_str_index + 1)
            seed = random_line[-(self.config.max_len):-1]
            res = ''
            seed = 'c' + seed
            for c in text:
                seed = seed[1:] + c
                for j in range(5):
                    x_pred = np.zeros((1, self.config.max_len))
                    for t, char in enumerate(seed):
                        x_pred[0, t] = self.word2numF(char)
                    preds = self.model.predict(x_pred, verbose=0)[0]
                    next_index = self.sample(preds, 1.0)
                    next_char = self.num2word[next_index]
                    seed = seed[1:] + next_char
                res += seed
            return res
</code></pre>

<p>（6）&nbsp;<code>data_generator</code>&nbsp;函数，用于生成数据，提供给模型训练时使用。</p>

<pre>
<code>     def data_generator(self):
            i = 0
            while 1:
                x = self.files_content[i: i + self.config.max_len]
                y = self.files_content[i + self.config.max_len]
                puncs = [']', '[', '（', '）', '{', '}', '：', '《', '》', ':']
                if len([i for i in puncs if i in x]) != 0:
                    i += 1
                    continue
                if len([i for i in puncs if i in y]) != 0:
                    i += 1
                    continue
                y_vec = np.zeros(
                    shape=(1, len(self.words)),
                    dtype=np.bool
                )
                y_vec[0, self.word2numF(y)] = 1.0
                x_vec = np.zeros(
                    shape=(1, self.config.max_len),
                    dtype=np.int32
                )
                for t, char in enumerate(x):
                    x_vec[0, t] = self.word2numF(char)
                yield x_vec, y_vec
                i += 1
</code></pre>

<p>（7）train 函数，用来进行模型训练，其中迭代次数&nbsp;<code>number_of_epoch</code>&nbsp;，是根据训练语料长度除以&nbsp;<code>batch_size</code>&nbsp;计算的，如果在调试中，想用更小一点的<code>number_of_epoch</code>&nbsp;，可以自定义大小，把 train 函数的第一行代码注释即可。</p>

<pre>
<code>    def train(self):
            #number_of_epoch = len(self.files_content) // self.config.batch_size
            number_of_epoch = 10
            if not self.model:
                self.build_model()
            self.model.summary()
            self.model.fit_generator(
                generator=self.data_generator(),
                verbose=True,
                steps_per_epoch=self.config.batch_size,
                epochs=number_of_epoch,
                callbacks=[
                    keras.callbacks.ModelCheckpoint(self.config.weight_file, save_weights_only=False),
                    LambdaCallback(on_epoch_end=self.generate_sample_result)
                ]
            )
</code></pre>

<p><strong>第五</strong>，整个模型构建好以后，接下来进行模型训练。</p>

<pre>
<code>    model = PoetryModel(Config)
</code></pre>

<p>训练过程中的第1-2轮迭代：</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/2a5e35c0-8dbe-11e8-80d1-2d51ff7e1c55" /></p>

<p>训练过程中的第9-10轮迭代：</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/30ce9df0-8dbe-11e8-aa21-25f031a4e022" /></p>

<p>虽然训练过程写出的诗句不怎么能看得懂，但是可以看到模型从一开始标点符号都不会用 ，到最后写出了有一点点模样的诗句，能看到模型变得越来越聪明了。</p>

<p><strong>第六</strong>，模型作诗，模型迭代10次之后的测试，首先输入几个字，模型根据输入的提示，做出诗句。</p>

<pre>
<code>    text = input("text:")
    sentence = model.predict(text)
    print(sentence)
</code></pre>

<p>比如输入：小雨，模型做出的诗句为：</p>

<blockquote>
<p>输入：text：小雨</p>

<p>结果：小妃侯里守。雨封即客寥。俘剪舟过槽。傲老槟冬绛。</p>
</blockquote>

<p><strong>第七</strong>，绘制网络结构图。</p>

<p>模型结构绘图，采用 Keras自带的功能实现：</p>

<pre>
<code>    plot_model(model.model, to_file='model.png')
</code></pre>

<p>得到的模型结构图如下：</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/d431b450-8dbe-11e8-80d1-2d51ff7e1c55" /></p>

<p>本节使用 LSTM 的变形 GRU 训练出一个能作诗的模型，当然大家可以替换训练语料为歌词或者小说，让机器人自动创作不同风格的歌曲或者小说。</p>

<p><strong>参考文献以及推荐阅读：</strong></p>

<ol>
	<li><a href="https://blog.csdn.net/qiansg123/article/details/80131355">基于 Keras 和 LSTM 的文本生成</a></li>
</ol>

<p><a id="第12课：完全基于情感词典的文本情感分析" name="第12课：完全基于情感词典的文本情感分析"></a>第12课：完全基于情感词典的文本情感分析</p>

<p>目前情感分析在中文自然语言处理中比较火热，很多场景下，我们都需要用到情感分析。比如，做金融产品量化交易，需要根据爬取的舆论数据来分析政策和舆论对股市或者基金期货的态度；电商交易，根据买家的评论数据，来分析商品的预售率等等。</p>

<p>下面我们通过以下几点来介绍中文自然语言处理情感分析：</p>

<ol>
	<li>中文情感分析方法简介；</li>
	<li>SnowNLP 快速进行评论数据情感分析；</li>
	<li>基于标注好的情感词典来计算情感值；</li>
	<li>pytreebank 绘制情感树；</li>
	<li>股吧数据情感分类。</li>
</ol>

<h3>中文情感分析方法简介</h3>

<p>情感倾向可认为是主体对某一客体主观存在的内心喜恶，内在评价的一种倾向。它由两个方面来衡量：一个情感倾向方向，一个是情感倾向度。</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/0b26dd00-8a3b-11e8-affa-b587dc6ff574" /></p>

<p>目前，情感倾向分析的方法主要分为两类：一种是基于情感词典的方法；一种是基于机器学习的方法，如基于大规模语料库的机器学习。前者需要用到标注好的情感词典；后者则需要大量的人工标注的语料作为训练集，通过提取文本特征，构建分类器来实现情感的分类。</p>

<p>文本情感分析的分析粒度可以是词语、句子、段落或篇章。</p>

<p>段落篇章级情感分析主要是针对某个主题或事件进行情感倾向判断，一般需要构建对应事件的情感词典，如电影评论的分析，需要构建电影行业自己的情感词典，这样效果会比通用情感词典更好；也可以通过人工标注大量电影评论来构建分类器。句子级的情感分析大多通过计算句子里包含的所有情感词的值来得到。</p>

<p>篇章级的情感分析，也可以通过聚合篇章中所有的句子的情感倾向来计算得出。因此，针对句子级的情感倾向分析，既能解决短文本的情感分析，同时也是篇章级文本情感分析的基础。</p>

<p>中文情感分析的一些难点，比如句子是由词语根据一定的语言规则构成的，应该把句子中词语的依存关系纳入到句子情感的计算过程中去，不同的依存关系，进行情感倾向计算是不一样的。文档的情感，根据句子对文档的重要程度赋予不同权重，调整其对文档情感的贡献程度等。</p>

<h3>SnowNLP 快速进行评论数据情感分析</h3>

<p>如果有人问，有没有比较快速简单的方法能判断一句话的情感倾向，那么 SnowNLP 库就是答案。</p>

<p>SnowNLP 主要可以进行中文分词、词性标注、情感分析、文本分类、转换拼音、繁体转简体、提取文本关键词、提取摘要、分割句子、文本相似等。</p>

<p>需要注意的是，用 SnowNLP 进行情感分析，官网指出进行电商评论的准确率较高，其实是因为它的语料库主要是电商评论数据，但是可以自己构建相关领域语料库，替换单一的电商评论语料，准确率也挺不错的。</p>

<p><strong>1.</strong>&nbsp;SnowNLP 安装。</p>

<p>（1） 使用 pip 安装：</p>

<pre>
<code>pip install snownlp==0.11.1
</code></pre>

<p>（2）使用 Github 源码安装。</p>

<p>首先，下载 SnowNLP 的&nbsp;<a href="https://github.com/isnowfy/snownlp">Github</a>&nbsp;源码并解压，在解压目录，通过下面命令安装：</p>

<pre>
<code>python  setup.py install 
</code></pre>

<p>以上方式，二选一安装完成之后，就可以引入 SnowNLP 库使用了。</p>

<pre>
<code>from snownlp import SnowNLP
</code></pre>

<p><strong>2.</strong>&nbsp;评论语料获取情感值。</p>

<p>首先，SnowNLP 对情感的测试值为0到1，值越大，说明情感倾向越积极。下面我们通过 SnowNLP 测试在京东上找的好评、中评、差评的结果。</p>

<p>首先，引入 SnowNLP 库：</p>

<pre>
<code>from snownlp import SnowNLP
</code></pre>

<p>（1） 测试一条京东的好评数据：</p>

<pre>
<code>SnowNLP(u'本本已收到，体验还是很好，功能方面我不了解，只看外观还是很不错很薄，很轻，也有质感。').sentiments
</code></pre>

<p>得到的情感值很高，说明买家对商品比较认可，情感值为：</p>

<blockquote>
<p>0.999950702449061</p>
</blockquote>

<p>（2）测试一条京东的中评数据：</p>

<pre>
<code>SnowNLP(u'屏幕分辨率一般，送了个极丑的鼠标。').sentiments
</code></pre>

<p>得到的情感值一般，说明买家对商品看法一般，甚至不喜欢，情感值为：</p>

<blockquote>
<p>0.03251402883400323</p>
</blockquote>

<p>（3）测试一条京东的差评数据：</p>

<pre>
<code>SnowNLP(u'很差的一次购物体验，细节做得极差了，还有发热有点严重啊，散热不行，用起来就是烫得厉害，很垃圾！！！').sentiments
</code></pre>

<p>得到的情感值一般，说明买家对商品不认可，存在退货嫌疑，情感值为：</p>

<blockquote>
<p>0.0036849517156107847</p>
</blockquote>

<p>以上就完成了简单快速的情感值计算，对评论数据是不是很好用呀！！！</p>

<p>使用 SnowNLP 来计算情感值，官方推荐的是电商评论数据计算准确度比较高，难道非评论数据就不能使用 SnowNLP 来计算情感值了吗？当然不是，虽然 SnowNLP 默认提供的模型是用评论数据训练的，但是它还支持我们根据现有数据训练自己的模型。</p>

<p>首先我们来看看自定义训练模型的<strong>源码 Sentiment 类</strong>，代码定义如下：</p>

<pre>
<code>class Sentiment(object):

    def __init__(self):
        self.classifier = Bayes()

    def save(self, fname, iszip=True):
        self.classifier.save(fname, iszip)

    def load(self, fname=data_path, iszip=True):
        self.classifier.load(fname, iszip)

    def handle(self, doc):
        words = seg.seg(doc)
        words = normal.filter_stop(words)
        return words

    def train(self, neg_docs, pos_docs):
        data = []
        for sent in neg_docs:
            data.append([self.handle(sent), 'neg'])
        for sent in pos_docs:
            data.append([self.handle(sent), 'pos'])
        self.classifier.train(data)

    def classify(self, sent):
        ret, prob = self.classifier.classify(self.handle(sent))
        if ret == 'pos':
            return prob
        return 1-prob
</code></pre>

<p>通过源代码，我们可以看到，可以使用 train方法训练数据，并使用 save 方法和 load 方法保存与加载模型。下面训练自己的模型，训练集 pos.txt 和 neg.txt 分别表示积极和消极情感语句，两个 TXT 文本中每行表示一句语料。</p>

<p>下面代码进行自定义模型训练和保存：</p>

<pre>
<code>from snownlp import sentiment
sentiment.train('neg.txt', 'pos.txt')
sentiment.save('sentiment.marshal')
</code></pre>

<h3>基于标注好的情感词典来计算情感值</h3>

<p>这里我们使用一个行业标准的情感词典&mdash;&mdash;玻森情感词典，来自定义计算一句话、或者一段文字的情感值。</p>

<p>整个过程如下：</p>

<ol>
	<li>加载玻森情感词典；</li>
	<li>jieba 分词；</li>
	<li>获取句子得分。</li>
</ol>

<p>首先引入包：</p>

<pre>
<code>import pandas as pd
import jieba
</code></pre>

<p>接下来加载情感词典：</p>

<pre>
<code>df = pd.read_table("bosonnlp//BosonNLP_sentiment_score.txt",sep= " ",names=['key','score'])
</code></pre>

<p>查看一下情感词典前5行：</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/d07af1c0-8a38-11e8-974b-497483da0812" /></p>

<p>将词 key 和对应得分 score 转成2个 list 列表，目的是找到词 key 的时候，能对应获取到 score 值：</p>

<pre>
<code>key = df['key'].values.tolist()
score = df['score'].values.tolist()
</code></pre>

<p>定义分词和统计得分函数：</p>

<pre>
<code>def getscore(line):
    segs = jieba.lcut(line)  #分词
    score_list  = [score[key.index(x)] for x in segs if(x in key)]
    return  sum(score_list)  #计算得分
</code></pre>

<p>最后来进行结果测试：</p>

<pre>
<code>line = "今天天气很好，我很开心"
print(round(getscore(line),2))

line = "今天下雨，心情也受到影响。"
print(round(getscore(line),2))
</code></pre>

<p>获得的情感得分保留2位小数：</p>

<blockquote>
<p>5.26</p>

<p>-0.96</p>
</blockquote>

<h3>pytreebank 绘制情感树</h3>

<p><strong>1.</strong>&nbsp;安装 pytreebank。</p>

<p>在 Github 上下载&nbsp;<a href="https://github.com/JonathanRaiman/pytreebank">pytreebank 源码</a>，解压之后，进入解压目录命令行，执行命令：</p>

<pre>
<code>python setup.py install
</code></pre>

<p>最后通过引入命令，判断是否安装成功：</p>

<pre>
<code>import pytreebank
</code></pre>

<p>提示，如果在 Windows 下安装之后，报错误：</p>

<pre>
<code>UnicodeDecodeError: 'gbk' codec can't decode byte 0x92 in position 24783: illegal multibyte sequence 
</code></pre>

<p>这是由于编码问题引起的，可以在安装目录下报错的文件中报错的代码地方加个<code>encoding=&#39;utf-8&#39;</code>&nbsp;编码：</p>

<pre>
<code>import_tag( "script", contents=format_replacements(open(scriptname,encoding='utf-8').read(), replacements), type="text/javascript" )
</code></pre>

<p><strong>2.</strong>&nbsp;绘制情感树。</p>

<p>首先引入 pytreebank 包：</p>

<pre>
<code>    import pytreebank
</code></pre>

<p>然后，加载用来可视化的 JavaScript 和 CSS 脚本：</p>

<pre>
<code>pytreebank.LabeledTree.inject_visualization_javascript()
</code></pre>

<p>绘制情感树，把句子首先进行组合再绘制图形：</p>

<pre>
<code>line = '(4 (0 你) (3 (2 是) (3 (3 (3 谁) (2 的)) (2 谁))))'
pytreebank.create_tree_from_string(line).display()
</code></pre>

<p>得到的情感树如下：</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/d23c18d0-8a2f-11e8-b6c7-73392c05a2ed" /></p>

<h3>股吧数据情感分类</h3>

<p>历经89天的煎熬之后，7月15日中兴终于盼来了解禁，在此首先恭喜中兴，解禁了，希望再踏征程。</p>

<p>但在7月15日之前，随着中美贸易战不断升级，中兴股价又上演了一场&ldquo;跌跌不休&rdquo;的惨状，我以中美贸易战背景下中兴通讯在股吧解禁前一段时间的评论数据，来进行情感数据人工打标签和分类。其中，把消极 、中性 、积极分别用0、1、2来表示。</p>

<p>整个文本分类流程主要包括以下6个步骤：</p>

<ul>
	<li>中文语料；</li>
	<li>分词；</li>
	<li>复杂规则；</li>
	<li>特征向量；</li>
	<li>算法建模；</li>
	<li>情感分析。</li>
</ul>

<p><img alt="enter image description here" src="https://images.gitbook.cn/d4586f00-81b1-11e8-b718-fd519f27386c" /></p>

<p>本次分类算法采用 CNN，首先引入需要的包：</p>

<pre>
<code>import pandas as pd
import numpy as np
import jieba
import random
import keras
from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation
from keras.layers import Embedding
from keras.layers import Conv1D, GlobalMaxPooling1D
from keras.datasets import imdb
from keras.models import model_from_json
from keras.utils import np_utils
import matplotlib.pyplot as plt
</code></pre>

<p>继续引入停用词和语料文件：</p>

<pre>
<code>dir = "D://ProgramData//PythonWorkSpace//chat//chat8//"
stopwords=pd.read_csv(dir +"stopwords.txt",index_col=False,quoting=3,sep="\t",names=['stopword'], encoding='utf-8')
stopwords=stopwords['stopword'].values
df_data1 = pd.read_csv(dir+"data1.csv",encoding='utf-8')
df_data1.head()
</code></pre>

<p>下图展示数据的前5行：</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/7b3e3740-8a28-11e8-b1ae-51065196ffc8" /></p>

<p>接着进行数据预处理，把消极、中性、积极分别为0、1、2的预料分别拿出来：</p>

<pre>
<code>#把内容有缺失值的删除
df_data1.dropna(inplace=True)
#抽取文本数据和标签
data_1 = df_data1.loc[:,['content','label']]
#把消极  中性  积极分别为0、1、2的预料分别拿出来
data_label_0 = data_1.loc[data_1['label'] ==0,:]
data_label_1 = data_1.loc[data_1['label'] ==1,:]
data_label_2 = data_1.loc[data_1['label'] ==2,:]
</code></pre>

<p>接下来，定义中文分词函数：</p>

<pre>
<code>#定义分词函数
def preprocess_text(content_lines, sentences, category):
    for line in content_lines:
        try:
            segs=jieba.lcut(line)
            segs = filter(lambda x:len(x)&gt;1, segs)
            segs = [v for v in segs if not str(v).isdigit()]#去数字
            segs = list(filter(lambda x:x.strip(), segs)) #去左右空格
            segs = filter(lambda x:x not in stopwords, segs)
            temp = " ".join(segs)
            if(len(temp)&gt;1):
                sentences.append((temp, category))
        except Exception:
            print(line)
            continue 
</code></pre>

<p>生成训练的分词数据，并进行打散，使其分布均匀：</p>

<pre>
<code>#获取数据
data_label_0_content = data_label_0['content'].values.tolist()
data_label_1_content = data_label_1['content'].values.tolist()
data_label_2_content = data_label_2['content'].values.tolist()
#生成训练数据
sentences = []
preprocess_text(data_label_0_content, sentences, 0)
preprocess_text(data_label_1_content, sentences, 1)
preprocess_text(data_label_2_content, sentences,2)
#我们打乱一下顺序，生成更可靠的训练集
random.shuffle(sentences)
</code></pre>

<p>对数据集进行切分，按照训练集合测试集7:3的比例：</p>

<pre>
<code>#所以把原数据集分成训练集的测试集，咱们用sklearn自带的分割函数。
from sklearn.model_selection import train_test_split
x, y = zip(*sentences)
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3,random_state=1234)
</code></pre>

<p>然后，对特征构造词向量：</p>

<pre>
<code>#抽取特征，我们对文本抽取词袋模型特征
from sklearn.feature_extraction.text import CountVectorizer
vec = CountVectorizer(
    analyzer='word', #tokenise by character ngrams
    max_features=4000,  #keep the most common 1000 ngrams
)
vec.fit(x_train)
</code></pre>

<p>定义模型参数：</p>

<pre>
<code># 设置参数
max_features = 5001
maxlen = 100
batch_size = 32
embedding_dims = 50
filters = 250
kernel_size = 3
hidden_dims = 250
epochs = 10
nclasses = 3
</code></pre>

<p>输入特征转成 Array 和标签处理，打印训练集和测试集的 shape：</p>

<pre>
<code>x_train = vec.transform(x_train)
x_test = vec.transform(x_test)
x_train = x_train.toarray()
x_test = x_test.toarray()
y_train = np_utils.to_categorical(y_train,nclasses)
y_test = np_utils.to_categorical(y_test,nclasses)
x_train = sequence.pad_sequences(x_train, maxlen=maxlen)
x_test = sequence.pad_sequences(x_test, maxlen=maxlen)
print('x_train shape:', x_train.shape)
print('x_test shape:', x_test.shape)
</code></pre>

<p>定义一个绘制 Loss 曲线的类：</p>

<pre>
<code>class LossHistory(keras.callbacks.Callback):
    def on_train_begin(self, logs={}):
        self.losses = {'batch':[], 'epoch':[]}
        self.accuracy = {'batch':[], 'epoch':[]}
        self.val_loss = {'batch':[], 'epoch':[]}
        self.val_acc = {'batch':[], 'epoch':[]}

    def on_batch_end(self, batch, logs={}):
        self.losses['batch'].append(logs.get('loss'))
        self.accuracy['batch'].append(logs.get('acc'))
        self.val_loss['batch'].append(logs.get('val_loss'))
        self.val_acc['batch'].append(logs.get('val_acc'))

    def on_epoch_end(self, batch, logs={}):
        self.losses['epoch'].append(logs.get('loss'))
        self.accuracy['epoch'].append(logs.get('acc'))
        self.val_loss['epoch'].append(logs.get('val_loss'))
        self.val_acc['epoch'].append(logs.get('val_acc'))

    def loss_plot(self, loss_type):
        iters = range(len(self.losses[loss_type]))
        plt.figure()
        # acc
        plt.plot(iters, self.accuracy[loss_type], 'r', label='train acc')
        # loss
        plt.plot(iters, self.losses[loss_type], 'g', label='train loss')
        if loss_type == 'epoch':
            # val_acc
            plt.plot(iters, self.val_acc[loss_type], 'b', label='val acc')
            # val_loss
            plt.plot(iters, self.val_loss[loss_type], 'k', label='val loss')
        plt.grid(True)
        plt.xlabel(loss_type)
        plt.ylabel('acc-loss')
        plt.legend(loc="upper right")
        plt.show()
</code></pre>

<p>然后，初始化上面类的对象，并作为模型的回调函数输入，训练模型：</p>

<pre>
<code>history = LossHistory()
print('Build model...')
model = Sequential()

model.add(Embedding(max_features,
                        embedding_dims,
                        input_length=maxlen))
model.add(Dropout(0.5))
model.add(Conv1D(filters,
                     kernel_size,
                     padding='valid',
                     activation='relu',
                     strides=1))
model.add(GlobalMaxPooling1D())
model.add(Dense(hidden_dims))
model.add(Dropout(0.5))
model.add(Activation('relu'))
model.add(Dense(nclasses))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy',
                  optimizer='adam',
                  metrics=['accuracy'])
model.fit(x_train, y_train,
              batch_size=batch_size,
              epochs=epochs,
              validation_data=(x_test, y_test),callbacks=[history])
</code></pre>

<p>得到的模型迭代次数为10轮的训练过程：</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/f329cce0-8c2d-11e8-b9de-5bb0fbe09c97" /></p>

<p>最后绘制 Loss 图像：</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/0886db00-8c2e-11e8-8e9b-93ccda1ef1a2" /></p>

<p>关于本次分类，这里重点讨论的一个知识点就是数据分布不均匀的情况，我们都知道，本次贸易战中兴公司受影响很大，导致整个股票价格处于下跌趋势，所以整个舆论上，大多数评论都是消极的态度，导致数据分布极不均匀。</p>

<p>那数据分布不均匀一般怎么处理呢？从以下几个方面考虑：</p>

<ul>
	<li>
	<p>数据采样，包括上采样、下采样和综合采样；</p>
	</li>
	<li>
	<p>改变分类算法，在传统分类算法的基础上对不同类别采取不同的加权方式，使得模型更看重少数类；</p>
	</li>
	<li>
	<p>采用合理的性能评价指标；</p>
	</li>
	<li>
	<p>代价敏感。</p>
	</li>
</ul>

<p>总结，本文通过第三方、基于词典等方式计算中文文本情感值，以及通过情感树来进行可视化，然而这些内容只是情感分析的入门知识，情感分析还涉及句法依存等，最后通过一个 CNN 分类模型，提供一种有监督的情感分类思路。</p>

<p><strong>参考文献及推荐阅读：</strong></p>

<ol>
	<li>
	<p><a href="https://gitbook.cn/gitchat/activity/5b3341ed28f60a20b62890f8">基于情感词典的中文自然语言处理情感分析（上）</a></p>
	</li>
	<li>
	<p><a href="https://gitbook.cn/gitchat/activity/5b3f2a34041b5c0e72c93383">基于情感词典的中文自然语言处理情感分析（下）</a></p>
	</li>
</ol>

<p><a id="第13课：动手制作自己的简易聊天机器人" name="第13课：动手制作自己的简易聊天机器人"></a>第13课：动手制作自己的简易聊天机器人</p>

<h3>自动问答简介</h3>

<p>自动聊天机器人，也称为自动问答系统，由于所使用的场景不同，叫法也不一样。自动问答（Question Answering，QA）是指利用计算机自动回答用户所提出的问题以满足用户知识需求的任务。不同于现有搜索引擎，问答系统是信息服务的一种高级形式，系统返回用户的不再是基于关键词匹配排序的文档列表，而是精准的自然语言答案。近年来，随着人工智能的飞速发展，自动问答已经成为倍受关注且发展前景广泛的研究方向。</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/e893ccf0-8e48-11e8-8ee0-a17ea463076e" /></p>

<p>自动问答主要研究的内容和关键科学问题如下：</p>

<ol>
	<li>
	<p><strong>问句理解</strong>：给定用户问题，自动问答首先需要理解用户所提问题。用户问句的语义理解包含词法分析、句法分析、语义分析等多项关键技术，需要从文本的多个维度理解其中包含的语义内容。</p>
	</li>
	<li>
	<p><strong>文本信息抽取</strong>：自动问答系统需要在已有语料库、知识库或问答库中匹配相关的信息，并抽取出相应的答案。</p>
	</li>
	<li>
	<p><strong>知识推理</strong>：自动问答中，由于语料库、知识库和问答库本身的覆盖度有限，并不是所有问题都能直接找到答案。这就需要在已有的知识体系中，通过知识推理的手段获取这些隐含的答案。</p>
	</li>
</ol>

<p>纵观自动问答研究的发展态势和技术现状，以下研究方向或问题将可能成为未来整个领域和行业重点关注的方向：基于深度学习的端到端自动问答，多领域、多语言的自动问答，面向问答的深度推理，篇章阅读理解、对话等。</p>

<h3>基于 Chatterbot 制作中文聊天机器人</h3>

<p>ChatterBot 是一个构建在 Python 上，基于一系列规则和机器学习算法完成的聊天机器人，具有结构清晰，可扩展性好，简单实用的特点。</p>

<p>Chatterbot 安装有两种方式：</p>

<ul>
	<li>使用&nbsp;<code>pip install chatterbot</code>&nbsp;安装；</li>
	<li>直接在<a href="https://github.com/gunthercox/ChatterBot">&nbsp;Github Chatterbot</a>&nbsp;下载这个项目，通过&nbsp;<code>python setup.py install</code>&nbsp;安装，其中 examples 文件夹中包含几个例子，可以根据例子加深自己的理解。</li>
</ul>

<p>安装过程如果出现错误，主要是需要安装这些依赖库：</p>

<pre>
<code>chatterbot-corpus&gt;=1.1,&lt;1.2
mathparse&gt;=0.1,&lt;0.2
nltk&gt;=3.2,&lt;4.0
pymongo&gt;=3.3,&lt;4.0
python-dateutil&gt;=2.6,&lt;2.7
python-twitter&gt;=3.0,&lt;4.0
sqlalchemy&gt;=1.2,&lt;1.3
pint&gt;=0.8.1
</code></pre>

<p><strong>1.</strong>&nbsp;手动设置一点语料，体验基于规则的聊天机器人回答。</p>

<pre>
<code>from chatterbot import ChatBot
from chatterbot.trainers import ListTrainer
Chinese_bot = ChatBot("Training demo") #创建一个新的实例
Chinese_bot.set_trainer(ListTrainer)
Chinese_bot.train([
    '亲，在吗？',
    '亲，在呢',
    '这件衣服的号码大小标准吗？',
    '亲，标准呢，请放心下单吧。',
    '有红色的吗？',
    '有呢，目前有白红蓝3种色调。',
])
</code></pre>

<p>下面进行测试：</p>

<pre>
<code># 测试一下
question = '亲，在吗'
print(question)
response = Chinese_bot.get_response(question)
print(response)
print("\n")
question = '有红色的吗？'
print(question)
response = Chinese_bot.get_response(question)
print(response)
</code></pre>

<p>从得到的结果可以看出，这应该完全是基于规则的判断：</p>

<blockquote>
<p>亲，在吗</p>

<p>亲，在呢</p>

<p>有红色的吗？</p>

<p>有呢，目前有白红蓝3种色调。</p>
</blockquote>

<p><strong>2.</strong>&nbsp;训练自己的语料。</p>

<p>本次使用的语料来自 QQ 群的聊天记录，导出的 QQ 聊天记录稍微处理一下即可使用，整个过程如下。</p>

<p>（1）首先载入语料，第二行代码主要是想把每句话后面的换行&nbsp;<code>\n</code>&nbsp;去掉。</p>

<pre>
<code>lines = open("QQ.txt","r",encoding='gbk').readlines()
sec = [ line.strip() for line in lines]
</code></pre>

<p>（2）接下来就可以训练模型了，由于整个语料比较大，训练过程也比较耗时。</p>

<pre>
<code>from chatterbot import ChatBot
from chatterbot.trainers import ListTrainer
Chinese_bot = ChatBot("Training")
Chinese_bot.set_trainer(ListTrainer)
Chinese_bot.train(sec)
</code></pre>

<p>这里需要注意，如果训练过程很慢，可以在第一步中加入如下代码，即只取前1000条进行训练：</p>

<pre>
<code>sec = sec[0:1000]
</code></pre>

<p>（3）最后，对训练好的模型进行测试，可见训练数据是 QQ 群技术对话，也看得出程序员们都很努力，整体想的都是学习。</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/cc2657e0-8fce-11e8-8472-d52f55e7330e" /></p>

<p>以上只是简单的 Chatterbot 演示，如果想看更好的应用，推荐看官方文档。</p>

<h3>基于 Seq2Seq 制作中文聊天机器人</h3>

<p>前面，我们在第09课<a href="https://gitbook.cn/gitchat/column/5b10b073aafe4e5a7516708b/topic/5b1110c0f96a5f71f138462e">《一网打尽神经序列模型之 RNN 及其变种 LSTM、GRU》</a>中讲了序列数据处理模型，从&nbsp;<code>N-gram</code>&nbsp;语言模型到 RNN 及其变种。这里我们讲另外一个基于深度学习的 Seq2Seq 模型。</p>

<p>从 RNN 结构说起，根据输出和输入序列不同数量 RNN ，可以有多种不同的结构，不同结构自然就有不同的引用场合。</p>

<ul>
	<li>One To One 结构，仅仅只是简单的给一个输入得到一个输出，此处并未体现序列的特征，例如图像分类场景。</li>
	<li>One To Many 结构，给一个输入得到一系列输出，这种结构可用于生产图片描述的场景。</li>
	<li>Many To One 结构，给一系列输入得到一个输出，这种结构可用于文本情感分析，对一些列的文本输入进行分类，看是消极还是积极情感。</li>
	<li>Many To Many 结构，给一系列输入得到一系列输出，这种结构可用于翻译或聊天对话场景，将输入的文本转换成另外一系列文本。</li>
	<li>同步 Many To Many 结构，它是经典的 RNN 结构，前一输入的状态会带到下一个状态中，而且每个输入都会对应一个输出，我们最熟悉的应用场景是字符预测，同样也可以用于视频分类，对视频的帧打标签。</li>
</ul>

<p>在 Many To Many 的两种模型中，第四和第五种是有差异的，经典 RNN 结构的输入和输出序列必须要等长，它的应用场景也比较有限。而第四种，输入和输出序列可以不等长，这种模型便是 Seq2Seq 模型，即 Sequence to Sequence。它实现了从一个序列到另外一个序列的转换，比如 Google 曾用 Seq2Seq 模型加 Attention 模型实现了翻译功能，类似的还可以实现聊天机器人对话模型。经典的 RNN 模型固定了输入序列和输出序列的大小，而 Seq2Seq 模型则突破了该限制。</p>

<p>Seq2Seq 属于&nbsp;<code>Encoder-Decoder</code>&nbsp;结构，这里看看常见的&nbsp;<code>Encoder-Decoder</code>&nbsp;结构。基本思想就是利用两个 RNN，一个 RNN 作为 Encoder，另一个 RNN 作为 Decoder。Encoder 负责将输入序列压缩成指定长度的向量，这个向量就可以看成是这个序列的语义，这个过程称为编码，如下图，获取语义向量最简单的方式就是直接将最后一个输入的隐状态作为语义向量。也可以对最后一个隐含状态做一个变换得到语义向量，还可以将输入序列的所有隐含状态做一个变换得到语义变量。</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/33ed6e10-8e56-11e8-aa21-25f031a4e022" /></p>

<p>具体理论知识这里不再赘述，下面重点看看，如何通过 Keras 实现一个<code>LSTM_Seq2Seq</code>&nbsp;自动问答机器人。</p>

<p><strong>1.</strong>&nbsp;语料准备。</p>

<p>语料我们使用 Tab 键&nbsp;<code>\t</code>&nbsp;把问题和答案区分，每一对为一行。其中，语料为爬虫爬取的工程机械网站的问答。</p>

<p><strong>2.</strong>&nbsp;模型构建和训练。</p>

<p>第一步，引入需要的包：</p>

<pre>
<code>from keras.models import Model
from keras.layers import Input, LSTM, Dense
import numpy as np
import pandas as pd
</code></pre>

<p>第二步，定义模型超参数、迭代次数、语料路径：</p>

<pre>
<code>#Batch size 的大小
batch_size = 32  
# 迭代次数epochs
epochs = 100
# 编码空间的维度Latent dimensionality 
latent_dim = 256  
# 要训练的样本数
num_samples = 5000 
#设置语料的路径
data_path = 'D://nlp//ch13//files.txt'
</code></pre>

<p>第三步，把语料向量化：</p>

<pre>
<code>#把数据向量话
input_texts = []
target_texts = []
input_characters = set()
target_characters = set()

with open(data_path, 'r', encoding='utf-8') as f:
    lines = f.read().split('\n')
for line in lines[: min(num_samples, len(lines) - 1)]:
    #print(line)
    input_text, target_text = line.split('\t')
    # We use "tab" as the "start sequence" character
    # for the targets, and "\n" as "end sequence" character.
    target_text = target_text[0:100]
    target_text = '\t' + target_text + '\n'
    input_texts.append(input_text)
    target_texts.append(target_text)

    for char in input_text:
        if char not in input_characters:
            input_characters.add(char)
    for char in target_text:
        if char not in target_characters:
            target_characters.add(char)

input_characters = sorted(list(input_characters))
target_characters = sorted(list(target_characters))
num_encoder_tokens = len(input_characters)
num_decoder_tokens = len(target_characters)
max_encoder_seq_length = max([len(txt) for txt in input_texts])
max_decoder_seq_length = max([len(txt) for txt in target_texts])

print('Number of samples:', len(input_texts))
print('Number of unique input tokens:', num_encoder_tokens)
print('Number of unique output tokens:', num_decoder_tokens)
print('Max sequence length for inputs:', max_encoder_seq_length)
print('Max sequence length for outputs:', max_decoder_seq_length)

input_token_index = dict(
    [(char, i) for i, char in enumerate(input_characters)])
target_token_index = dict(
    [(char, i) for i, char in enumerate(target_characters)])

encoder_input_data = np.zeros(
    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),dtype='float32')
decoder_input_data = np.zeros(
    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),dtype='float32')
decoder_target_data = np.zeros(
    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),dtype='float32')

for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):
    for t, char in enumerate(input_text):
        encoder_input_data[i, t, input_token_index[char]] = 1.
    for t, char in enumerate(target_text):
        # decoder_target_data is ahead of decoder_input_data by one timestep
        decoder_input_data[i, t, target_token_index[char]] = 1.
        if t &gt; 0:
            # decoder_target_data will be ahead by one timestep
            # and will not include the start character.
            decoder_target_data[i, t - 1, target_token_index[char]] = 1.
</code></pre>

<p>第四步，<code>LSTM_Seq2Seq</code>&nbsp;模型定义、训练和保存：</p>

<pre>
<code>encoder_inputs = Input(shape=(None, num_encoder_tokens))
encoder = LSTM(latent_dim, return_state=True)
encoder_outputs, state_h, state_c = encoder(encoder_inputs)
# 输出 `encoder_outputs` 
encoder_states = [state_h, state_c]

# 状态 `encoder_states` 
decoder_inputs = Input(shape=(None, num_decoder_tokens))
decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)
decoder_outputs, _, _ = decoder_lstm(decoder_inputs,
                       initial_state=encoder_states)
decoder_dense = Dense(num_decoder_tokens, activation='softmax')
decoder_outputs = decoder_dense(decoder_outputs)

# 定义模型
model = Model([encoder_inputs, decoder_inputs], decoder_outputs)

# 训练
model.compile(optimizer='rmsprop', loss='categorical_crossentropy')
model.fit([encoder_input_data, decoder_input_data], decoder_target_data,
          batch_size=batch_size,
          epochs=epochs,
          validation_split=0.2)
# 保存模型
model.save('s2s.h5')
</code></pre>

<p>第五步，Seq2Seq 的 Encoder 操作：</p>

<pre>
<code>encoder_model = Model(encoder_inputs, encoder_states)

decoder_state_input_h = Input(shape=(latent_dim,))
decoder_state_input_c = Input(shape=(latent_dim,))
decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]
decoder_outputs, state_h, state_c = decoder_lstm(
    decoder_inputs, initial_state=decoder_states_inputs)
decoder_states = [state_h, state_c]
decoder_outputs = decoder_dense(decoder_outputs)
decoder_model = Model(
    [decoder_inputs] + decoder_states_inputs,
    [decoder_outputs] + decoder_states)
</code></pre>

<p>第六步，把索引和分词转成序列：</p>

<pre>
<code>reverse_input_char_index = dict(
    (i, char) for char, i in input_token_index.items())
reverse_target_char_index = dict(
    (i, char) for char, i in target_token_index.items())
</code></pre>

<p>第七步，定义预测函数，先使用预模型预测，然后编码成汉字结果：</p>

<pre>
<code>def decode_sequence(input_seq):
    # Encode the input as state vectors.
    states_value = encoder_model.predict(input_seq)
    #print(states_value)

    # Generate empty target sequence of length 1.
    target_seq = np.zeros((1, 1, num_decoder_tokens))
    # Populate the first character of target sequence with the start character.
    target_seq[0, 0, target_token_index['\t']] = 1.

    # Sampling loop for a batch of sequences
    # (to simplify, here we assume a batch of size 1).
    stop_condition = False
    decoded_sentence = ''
    while not stop_condition:
        output_tokens, h, c = decoder_model.predict(
            [target_seq] + states_value)

        # Sample a token
        sampled_token_index = np.argmax(output_tokens[0, -1, :])
        sampled_char = reverse_target_char_index[sampled_token_index]
        decoded_sentence += sampled_char
        if (sampled_char == '\n' or
           len(decoded_sentence) &gt; max_decoder_seq_length):
            stop_condition = True

        # Update the target sequence (of length 1).
        target_seq = np.zeros((1, 1, num_decoder_tokens))
        target_seq[0, 0, sampled_token_index] = 1.
        # 更新状态
        states_value = [h, c]
    return decoded_sentence
</code></pre>

<p><strong>3.</strong>&nbsp;模型预测。</p>

<p>首先，定义一个预测函数：</p>

<pre>
<code>def predict_ans(question):
        inseq = np.zeros((len(question), max_encoder_seq_length, num_encoder_tokens),dtype='float16')
        decoded_sentence = decode_sequence(inseq)
        return decoded_sentence
</code></pre>

<p>然后就可以预测了：</p>

<pre>
<code>print('Decoded sentence:', predict_ans("挖机履带掉了怎么装上去"))
</code></pre>

<h3>总结</h3>

<p>本文我们首先基于 Chatterbot 制作了中文聊天机器人，并用 QQ 群对话语料自己尝试训练。然后通过 LSTM 和 Seq2Seq 模型，根据爬取的语料，训练了一个自动问答的模型，通过以上两种方式，我们们对自动问答有了一个简单的入门。</p>

<p><strong>参考文献及推荐阅读：</strong></p>

<ol>
	<li><a href="http://www.cipsc.org.cn/">《中文信息处理发展报告（2016）》</a></li>
	<li><a href="http://chatterbot.readthedocs.io/en/stable/training.html">ChatterBot 文档</a></li>
	<li><a href="https://github.com/gunthercox/ChatterBot">ChatterBot 的 GitHub</a></li>
	<li><a href="https://arxiv.org/abs/1409.3215">Sutskever, Vinyals and Le (2014)</a></li>
	<li><a href="https://blog.csdn.net/starzhou/article/details/78171936">漫谈四种神经网络序列解码模型</a></li>
	<li><a href="https://jingyan.baidu.com/article/d621e8da5b3b482865913f99.html">怎样导出 QQ 群里的所有聊天记录？</a></li>
	<li><a href="https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py">Keras 中的&nbsp;<code>LSTM_Seq2Seq</code>&nbsp;例子</a></li>
</ol>

<p><a id="第14课：动手实战中文命名实体提取" name="第14课：动手实战中文命名实体提取"></a>第14课：动手实战中文命名实体提取</p>

<p>命名实体识别（Named EntitiesRecognition，NER）是自然语言处理的一个基础任务。其目的是识别语料中人名、地名、组织机构名等命名实体，比如，<a href="https://baike.baidu.com/item/2015%E5%B9%B4%E4%B8%AD%E5%9B%BD%E5%91%BD%E5%90%8D%E7%9A%84124%E4%B8%AA%E5%9B%BD%E9%99%85%E6%B5%B7%E5%BA%95%E5%9C%B0%E7%90%86%E5%AE%9E%E4%BD%93%E5%90%8D%E7%A7%B0%E4%BF%A1%E6%81%AF/18705238">2015年中国国家海洋局对124个国际海底地理实体的命名</a>。</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/ed101af0-9617-11e8-9c35-b59aad3fef8b" /></p>

<p>由于命名实体数量不断增加，通常不可能在词典中穷尽列出，且其构成方法具有各自的一些规律性，因而，通常把对这些词的识别从词汇形态处理（如汉语切分）任务中独立处理，称为命名实体识别。</p>

<p>命名实体识别技术是信息抽取、信息检索、机器翻译、问答系统等多种自然语言处理技术必不可少的组成部分。</p>

<h3>常见的命名实体识别方法综述</h3>

<p>命名实体是命名实体识别的研究主体，一般包括三大类（实体类、时间类和数字类）和七小类（人名、地名、机构名、时间、日期、货币和百分比）命名实体。评判一个命名实体是否被正确识别包括两个方面：实体的边界是否正确和实体的类型是否标注正确。</p>

<p>命名实体识别的主要技术方法分为：基于规则和词典的方法、基于统计的方法、二者混合的方法等。</p>

<p><strong>1.基于规则和词典的方法。</strong></p>

<p>基于规则的方法多采用语言学专家手工构造规则模板，选用特征包括统计信息、标点符号、关键字、指示词和方向词、位置词（如尾字）、中心词等方法，以模式和字符串相匹配为主要手段，这类系统大多依赖于知识库和词典的建立。基于规则和词典的方法是命名实体识别中最早使用的方法，一般而言，当提取的规则能比较精确地反映语言现象时，基于规则的方法性能要优于基于统计的方法。但是这些规则往往依赖于具体语言、领域和文本风格，编制过程耗时且难以涵盖所有的语言现象，特别容易产生错误，系统可移植性不好，对于不同的系统需要语言学专家重新书写规则。基于规则的方法的另外一个缺点是代价太大，存在系统建设周期长、移植性差而且需要建立不同领域知识库作为辅助以提高系统识别能力等问题。</p>

<p><strong>2.基于统计的方法。</strong></p>

<p>基于统计机器学习的方法主要包括隐马尔可夫模型（HiddenMarkovMode，HMM）、最大熵（MaxmiumEntropy，ME）、支持向量机（Support VectorMachine，SVM）、条件随机场（ConditionalRandom Fields，CRF）等。</p>

<p>在基于统计的这四种学习方法中，最大熵模型结构紧凑，具有较好的通用性，主要缺点是训练时间长复杂性高，有时甚至导致训练代价难以承受，另外由于需要明确的归一化计算，导致开销比较大。而条件随机场为命名实体识别提供了一个特征灵活、全局最优的标注框架，但同时存在收敛速度慢、训练时间长的问题。一般说来，最大熵和支持向量机在正确率上要比隐马尔可夫模型高一些，但隐马尔可夫模型在训练和识别时的速度要快一些，主要是由于在利用 Viterbi 算法求解命名实体类别序列时的效率较高。隐马尔可夫模型更适用于一些对实时性有要求以及像信息检索这样需要处理大量文本的应用，如短文本命名实体识别。</p>

<p>基于统计的方法对特征选取的要求较高，需要从文本中选择对该项任务有影响的各种特征，并将这些特征加入到特征向量中。依据特定命名实体识别所面临的主要困难和所表现出的特性，考虑选择能有效反映该类实体特性的特征集合。主要做法是通过对训练语料所包含的语言信息进行统计和分析，从训练语料中挖掘出特征。有关特征可以分为具体的单词特征、上下文特征、词典及词性特征、停用词特征、核心词特征以及语义特征等。</p>

<p>基于统计的方法对语料库的依赖也比较大，而可以用来建设和评估命名实体识别系统的大规模通用语料库又比较少。</p>

<p><strong>3.混合方法。</strong></p>

<p>自然语言处理并不完全是一个随机过程，单独使用基于统计的方法使状态搜索空间非常庞大，必须借助规则知识提前进行过滤修剪处理。目前几乎没有单纯使用统计模型而不使用规则知识的命名实体识别系统，在很多情况下是使用混合方法：</p>

<ol>
	<li>统计学习方法之间或内部层叠融合。</li>
	<li>规则、词典和机器学习方法之间的融合，其核心是融合方法技术。在基于统计的学习方法中引入部分规则，将机器学习和人工知识结合起来。</li>
	<li>将各类模型、算法结合起来，将前一级模型的结果作为下一级的训练数据，并用这些训练数据对模型进行训练，得到下一级模型。</li>
</ol>

<h3>命名实体识别的一般流程</h3>

<p>如下图所示，一般的命名实体流程主要分为四个步骤：</p>

<ol>
	<li>对需要进行提取的文本语料进行分词；</li>
	<li>获取需要识别的领域标签，并对分词结果进行标签标注；</li>
	<li>对标签标注的分词进行抽取；</li>
	<li>将抽取的分词组成需要的领域的命名实体。</li>
</ol>

<p><img alt="enter image description here" src="https://images.gitbook.cn/b4114e00-9616-11e8-afe4-2b97d4c05a56" /></p>

<h3>动手实战命名实体识别</h3>

<p>命名实体的类别，我们在<a href="https://gitbook.cn/gitchat/column/5b10b073aafe4e5a7516708b/topic/5b10f818b1d64f71299765a0">第01课《中文自然语言处理的完整机器处理流程》</a>中已经给出了，这里不再赘述，下面通过 jieba 分词包和 pyhanlp 来实战命名实体识别和提取。</p>

<p><strong>1.jieba 进行命名实体识别和提取。</strong></p>

<p>第一步，引入 jieba 包：</p>

<pre>
<code>    import jieba
    import jieba.analyse
    import jieba.posseg as posg
</code></pre>

<p>第二步，使用 jieba 进行词性切分，allowPOS 指定允许的词性，这里选择名词 n 和地名 ns：</p>

<pre>
<code>    sentence=u'''上线三年就成功上市,拼多多上演了互联网企业的上市奇迹,却也放大平台上存在的诸多问题，拼多多在美国上市。'''
    kw=jieba.analyse.extract_tags(sentence,topK=10,withWeight=True,allowPOS=('n','ns'))
    for item in kw:
        print(item[0],item[1])
</code></pre>

<p>在这里，我们可以得到打印出来的结果：</p>

<blockquote>
<p>上市 1.437080435586</p>

<p>上线 0.820694551317</p>

<p>奇迹 0.775434839431</p>

<p>互联网 0.712189275429</p>

<p>平台 0.6244340485550001</p>

<p>企业 0.422177218495</p>

<p>美国 0.415659623166</p>

<p>问题 0.39635135730800003</p>
</blockquote>

<p>可以看得出，上市和上线应该是动词，这里给出的结果不是很准确。接下来，我们使用 textrank 算法来试试：</p>

<pre>
<code>    kw=jieba.analyse.textrank(sentence,topK=20,withWeight=True,allowPOS=('ns','n'))
    for item in kw:
        print(item[0],item[1])
</code></pre>

<p>这次得到的结果如下，可见，两次给出的结果还是不一样的。</p>

<blockquote>
<p>上市 1.0</p>

<p>奇迹 0.572687398431635</p>

<p>企业 0.5710407272273452</p>

<p>互联网 0.5692560484441649</p>

<p>上线 0.23481844682115297</p>

<p>美国 0.23481844682115297</p>
</blockquote>

<p><strong>2.pyhanlp 进行命名实体识别和提取。</strong></p>

<p>第一步，引入pyhanlp包：</p>

<pre>
<code>    from pyhanlp import *
</code></pre>

<p>第二步，进行词性切分：</p>

<pre>
<code>    sentence=u'''上线三年就成功上市,拼多多上演了互联网企业的上市奇迹,却也放大平台上存在的诸多问题，拼多多在美国上市。'''
    analyzer = PerceptronLexicalAnalyzer()
    segs = analyzer.analyze(sentence)
    arr = str(segs).split(" ")
</code></pre>

<p>第三步，定义一个函数，从得到的结果中，根据词性获取指定词性的词：</p>

<pre>
<code>    def get_result(arr):
        re_list = []
        ner = ['n','ns']
        for x in arr:
            temp = x.split("/")
            if(temp[1] in ner):
                re_list.append(temp[0])
        return re_list
</code></pre>

<p>第四步，我们获取结果：</p>

<pre>
<code>    result = get_result(arr)
    print(result)
</code></pre>

<p>得到的结果如下，可见比 jieba 更准确：</p>

<blockquote>
<pre>
<code>['互联网', '企业', '奇迹', '平台', '问题', '美国']
</code></pre>
</blockquote>

<h3>总结</h3>

<p>本文对命名实体识别的方法进行了总结，并给出一般的处理流程，最后通过简单的 jieba 分词和 pyhanlp 分词根据词性获取实体对象，后续大家也可以尝试通过哈工大和斯坦福的包来处理，下篇我们通过条件随机场 CRF 来训练一个命名实体识别模型。</p>

<p><strong>参考文献及推荐阅读</strong></p>

<ol>
	<li>命名实体识别研究[J]，国防科技大学计算机学院-张晓艳、王挺、陈火旺。</li>
	<li><a href="https://www.lookfor404.com/category/note/%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB/">命名实体识别</a></li>
</ol>

<p><a id="第15课：基于 CRF 的中文命名实体识别模型实现" name="第15课：基于 CRF 的中文命名实体识别模型实现"></a>第15课：基于 CRF 的中文命名实体识别模型实现</p>

<p>命名实体识别在越来越多的场景下被应用，如自动问答、知识图谱等。非结构化的文本内容有很多丰富的信息，但找到相关的知识始终是一个具有挑战性的任务，命名实体识别也不例外。</p>

<p>前面我们用隐马尔可夫模型（HMM）自己尝试训练过一个分词器，其实 HMM 也可以用来训练命名实体识别器，但在本文，我们讲另外一个算法&mdash;&mdash;条件随机场（CRF），来训练一个命名实体识别器。</p>

<h3>浅析条件随机场（CRF）</h3>

<p>条件随机场（Conditional Random Fields，简称 CRF）是给定一组输入序列条件下另一组输出序列的条件概率分布模型，在自然语言处理中得到了广泛应用。</p>

<p>首先，我们来看看什么是随机场。&ldquo;随机场&rdquo;的名字取的很玄乎，其实理解起来不难。随机场是由若干个位置组成的整体，当按照某种分布给每一个位置随机赋予一个值之后，其全体就叫做随机场。</p>

<p>还是举词性标注的例子。假如我们有一个十个词形成的句子需要做词性标注。这十个词每个词的词性可以在我们已知的词性集合（名词，动词&hellip;&hellip;)中去选择。当我们为每个词选择完词性后，这就形成了一个随机场。</p>

<p>了解了随机场，我们再来看看马尔科夫随机场。马尔科夫随机场是随机场的特例，它假设随机场中某一个位置的赋值仅仅与和它相邻的位置的赋值有关，和与其不相邻的位置的赋值无关。</p>

<p>继续举十个词的句子词性标注的例子。如果我们假设所有词的词性只和它相邻的词的词性有关时，这个随机场就特化成一个马尔科夫随机场。比如第三个词的词性除了与自己本身的位置有关外，还只与第二个词和第四个词的词性有关。</p>

<p>理解了马尔科夫随机场，再理解 CRF 就容易了。CRF 是马尔科夫随机场的特例，它假设马尔科夫随机场中只有 X 和 Y 两种变量，X 一般是给定的，而 Y 一般是在给定 X 的条件下我们的输出。这样马尔科夫随机场就特化成了条件随机场。</p>

<p>在我们十个词的句子词性标注的例子中，X 是词，Y 是词性。因此，如果我们假设它是一个马尔科夫随机场，那么它也就是一个 CRF。</p>

<p>对于 CRF，我们给出准确的数学语言描述：设 X 与 Y 是随机变量，P(Y|X) 是给定 X 时 Y 的条件概率分布，若随机变量 Y 构成的是一个马尔科夫随机场，则称条件概率分布 P(Y|X) 是条件随机场。</p>

<h3>基于 CRF 的中文命名实体识别模型实现</h3>

<p>在常规的命名实体识别中，通用场景下最常提取的是时间、人物、地点及组织机构名，因此本模型也将提取以上四种实体。</p>

<p><strong>1.开发环境。</strong></p>

<p>本次开发所选用的环境为：</p>

<ul>
	<li><code>Sklearn_crfsuite</code></li>
	<li>Python 3.6</li>
	<li>Jupyter Notebook</li>
</ul>

<p><strong>2.数据预处理。</strong></p>

<p>本模型使用人民日报1998年标注数据，进行预处理。语料库词性标记中，对应的实体词依次为 t、nr、ns、nt。对语料需要做以下处理：</p>

<ul>
	<li>将语料全角字符统一转为半角；</li>
	<li>合并语料库分开标注的姓和名，例如：<code>温/nr 家宝/nr</code>；</li>
	<li>合并语料库中括号中的大粒度词，例如：<code>[国家/n 环保局/n]nt</code>；</li>
	<li>合并语料库分开标注的时间，例如：<code>（/w 一九九七年/t 十二月/t 三十一日/t ）/w</code>。</li>
</ul>

<p>首先引入需要用到的库：</p>

<pre>
<code>    import re
    import sklearn_crfsuite
    from sklearn_crfsuite import metrics
    from sklearn.externals import joblib
</code></pre>

<p>数据预处理，定义 CorpusProcess 类，我们还是先给出类实现框架：</p>

<pre>
<code>class CorpusProcess(object):

    def __init__(self):
        """初始化"""
        pass

    def read_corpus_from_file(self, file_path):
        """读取语料"""
        pass

    def write_corpus_to_file(self, data, file_path):
        """写语料"""
        pass

    def q_to_b(self,q_str):
        """全角转半角"""
        pass

    def b_to_q(self,b_str):
        """半角转全角"""
        pass

    def pre_process(self):
        """语料预处理 """
        pass

    def process_k(self, words):
        """处理大粒度分词,合并语料库中括号中的大粒度分词,类似：[国家/n  环保局/n]nt """
        pass

    def process_nr(self, words):
        """ 处理姓名，合并语料库分开标注的姓和名，类似：温/nr  家宝/nr"""
        pass

    def process_t(self, words):
        """处理时间,合并语料库分开标注的时间词，类似： （/w  一九九七年/t  十二月/t  三十一日/t  ）/w   """
        pass

    def pos_to_tag(self, p):
        """由词性提取标签"""
        pass

    def tag_perform(self, tag, index):
        """标签使用BIO模式"""
        pass

    def pos_perform(self, pos):
        """去除词性携带的标签先验知识"""
        pass

    def initialize(self):
        """初始化 """
        pass

    def init_sequence(self, words_list):
        """初始化字序列、词性序列、标记序列 """
        pass

    def extract_feature(self, word_grams):
        """特征选取"""
        pass

    def segment_by_window(self, words_list=None, window=3):
        """窗口切分"""
        pass

    def generator(self):
        """训练数据"""
        pass
</code></pre>

<p>由于整个代码实现过程较长，我这里给出重点步骤，最后会在&nbsp;<strong>Github 上连同语料代码一同给出</strong>，下面是关键过程实现。</p>

<p>对语料中的句子、词性，实体分类标记进行区分。标签采用&ldquo;BIO&rdquo;体系，即实体的第一个字为&nbsp;<code>B_*</code>，其余字为&nbsp;<code>I_*</code>，非实体字统一标记为 O。大部分情况下，标签体系越复杂，准确度也越高，但这里采用简单的 BIO 体系也能达到相当不错的效果。这里模型采用&nbsp;<code>tri-gram</code>&nbsp;形式，所以在字符列中，要在句子前后加上占位符。</p>

<pre>
<code>def init_sequence(self, words_list):
            """初始化字序列、词性序列、标记序列 """
            words_seq = [[word.split(u'/')[0] for word in words] for words in words_list]
            pos_seq = [[word.split(u'/')[1] for word in words] for words in words_list]
            tag_seq = [[self.pos_to_tag(p) for p in pos] for pos in pos_seq]
            self.pos_seq = [[[pos_seq[index][i] for _ in range(len(words_seq[index][i]))]
                            for i in range(len(pos_seq[index]))] for index in range(len(pos_seq))]
            self.tag_seq = [[[self.tag_perform(tag_seq[index][i], w) for w in range(len(words_seq[index][i]))]
                            for i in range(len(tag_seq[index]))] for index in range(len(tag_seq))]
            self.pos_seq = [[u'un']+[self.pos_perform(p) for pos in pos_seq for p in pos]+[u'un'] for pos_seq in self.pos_seq]
            self.tag_seq = [[t for tag in tag_seq for t in tag] for tag_seq in self.tag_seq]
            self.word_seq = [[u'&lt;BOS&gt;']+[w for word in word_seq for w in word]+[u'&lt;EOS&gt;'] for word_seq in words_seq] 
</code></pre>

<p>处理好语料之后，紧接着进行模型定义和训练，定义&nbsp;<code>CRF_NER</code>&nbsp;类，我们还是采用先给出类实现框架，再具体讲解其实现：</p>

<pre>
<code>    class CRF_NER(object):
        def __init__(self):
            """初始化参数"""
            pass

        def initialize_model(self):
            """初始化"""
            pass

        def train(self):
            """训练"""
            pass

        def predict(self, sentence):
            """预测"""
            pass
        def load_model(self):
            """加载模型 """
            pass
        def save_model(self):
            """保存模型"""
            pass
</code></pre>

<p>在&nbsp;<code>CRF_NER</code>&nbsp;类中，分别完成了语料预处理和模型训练、保存、预测功能，具体实现如下。</p>

<p>第一步，init 函数实现了模型参数定义和 CorpusProcess 的实例化和语料预处理：</p>

<pre>
<code>    def __init__(self):
            """初始化参数"""
            self.algorithm = "lbfgs"
            self.c1 ="0.1"
            self.c2 = "0.1"
            self.max_iterations = 100 #迭代次数
            self.model_path = dir + "model.pkl"
            self.corpus = CorpusProcess()  #Corpus 实例
            self.corpus.pre_process()  #语料预处理
            self.corpus.initialize()  #初始化语料
            self.model = None
</code></pre>

<p>第二步，给出模型定义，了解&nbsp;<code>sklearn_crfsuite.CRF</code>&nbsp;详情可查该<a href="https://sklearn-crfsuite.readthedocs.io/en/latest/api.html#sklearn_crfsuite.CRF">文档</a>。</p>

<pre>
<code>    def initialize_model(self):
            """初始化"""
            algorithm = self.algorithm
            c1 = float(self.c1)
            c2 = float(self.c2)
            max_iterations = int(self.max_iterations)
            self.model = sklearn_crfsuite.CRF(algorithm=algorithm, c1=c1, c2=c2,
                                              max_iterations=max_iterations, all_possible_transitions=True)
</code></pre>

<p>第三步，模型训练和保存，分为训练集和测试集：</p>

<pre>
<code>    def train(self):
            """训练"""
            self.initialize_model()
            x, y = self.corpus.generator()
            x_train, y_train = x[500:], y[500:]
            x_test, y_test = x[:500], y[:500]
            self.model.fit(x_train, y_train)
            labels = list(self.model.classes_)
            labels.remove('O')
            y_predict = self.model.predict(x_test)
            metrics.flat_f1_score(y_test, y_predict, average='weighted', labels=labels)
            sorted_labels = sorted(labels, key=lambda name: (name[1:], name[0]))
            print(metrics.flat_classification_report(y_test, y_predict, labels=sorted_labels, digits=3))
            self.save_model()
</code></pre>

<p>第四至第六步中 predict、<code>load_model</code>、<code>save_model</code>&nbsp;方法的实现，大家可以在文末给出的地址中查看源码，这里就不堆代码了。</p>

<p>最后，我们来看看模型训练和预测的过程和结果：</p>

<pre>
<code>    ner = CRF_NER()
    model = ner.train()
</code></pre>

<p>经过模型训练，得到的准确率和召回率如下：</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/f6165800-97be-11e8-b78f-09922e3c574f" /></p>

<p>进行模型预测，其结果还不错，如下：</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/1aa70020-97bf-11e8-911c-dd974a01956f" /></p>

<p>基于 CRF 的中文命名实体识别模型实现先讲到这儿，项目源码和涉及到的语料，大家可以到：<a href="https://github.com/sujeek/chinese_nlp">Github</a>&nbsp;上查看。</p>

<h3>总结</h3>

<p>本文浅析了条件随机场，并使用&nbsp;<code>sklearn_crfsuite.CRF</code>&nbsp;模型，对人民日报1998年标注数据进行了模型训练和预测，以帮助大家加强对条件随机场的理解。</p>

<p><strong>参考资料及推荐阅读</strong></p>

<ol>
	<li><a href="https://blog.csdn.net/a819825294/article/details/53893231">条件随机场（CRF）</a></li>
	<li><a href="https://www.cnblogs.com/pinard/p/7048333.html">条件随机场CRF（一）从随机场到线性链条件随机场</a></li>
	<li><a href="https://blog.csdn.net/sinat_33741547/article/details/79131223">命名实体：基于 CRF 的中文命名实体识别模型</a></li>
	<li><a href="http://x-algo.cn/index.php/2016/02/15/conditional-random-field-crf-theory-and-implementation/">条件随机场（CRF）理论及应用</a></li>
</ol>

<p><a id="第16课：动手实战中文句法依存分析" name="第16课：动手实战中文句法依存分析"></a>第16课：动手实战中文句法依存分析</p>

<p>句法分析是自然语言处理（NLP）中的关键技术之一，其基本任务是确定句子的句法结构或者句子中词汇之间的依存关系。主要包括两方面的内容：一是确定语言的语法体系，即对语言中合法句子的语法结构给予形式化的定义；另一方面是句法分析技术，即根据给定的语法体系，自动推导出句子的句法结构，分析句子所包含的句法单位和这些句法单位之间的关系。</p>

<p>句法分析被用在很多场景中，比如搜索引擎用户日志分析和关键词识别，比如信息抽取、自动问答、机器翻译等其他自然语言处理相关的任务。</p>

<h3>语法体系</h3>

<p>句法分析需要遵循某一语法体系，根据该体系的语法确定语法树的表示形式，我们看下面这个句子：</p>

<blockquote>
<p>西门子将努力参与中国的三峡工程建设。</p>
</blockquote>

<p>用可视化的工具&nbsp;<a href="http://nlp.stanford.edu:8080/parser/index.jsp">Stanford Parser</a>&nbsp;来看看句法分析的整个过程：</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/e7813240-9ea0-11e8-bf0f-5103efdb7be8" /></p>

<p>短语结构树由终节点、非终结点以及短语标记三部分组成。句子分裂的语法规则为若干终结点构成一个短语，作为非终结点参与下一次规约，直至结束。如下图：</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/115a7610-97eb-11e8-b78f-09922e3c574f" /></p>

<h3>句法分析技术</h3>

<h4>依存句法分析</h4>

<h5><strong>依存句法</strong></h5>

<p>依存句法（Dependency Parsing， DP）通过分析语言单位内成分之间的依存关系揭示其句法结构。</p>

<p>直观来讲，依存句法的目的在于分析识别句子中的&ldquo;主谓宾&rdquo;、&ldquo;定状补&rdquo;这些语法成分，并分析各成分之间的关系。</p>

<p>依存句法的结构没有非终结点，词与词之间直接发生依存关系，构成一个依存对，其中一个是核心词，也叫支配词，另一个叫修饰词，也叫从属词。</p>

<p>依存关系用一个有向弧表示，叫做依存弧。依存弧的方向为由从属词指向支配词，当然反过来也是可以的，按个人习惯统一表示即可。</p>

<p>例如，下面这个句子：</p>

<blockquote>
<p>国务院总理李克强调研上海外高桥时提出，支持上海积极探索新机制。</p>
</blockquote>

<p>依存句法的分析结果见下（利用哈工大 LTP）：</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/c6000940-97eb-11e8-b78f-09922e3c574f" /></p>

<p>从分析结果中我们可以看到，句子的核心谓词为&ldquo;提出&rdquo;，主语是&ldquo;李克强&rdquo;，提出的宾语是&ldquo;支持上海&hellip;&hellip;&rdquo;，&ldquo;调研&hellip;&hellip;时&rdquo;是&ldquo;提出&rdquo;的（时间） 状语，&ldquo;李克强&rdquo;的修饰语是&ldquo;国务院总理&rdquo;，&ldquo;支持&rdquo;的宾语是&ldquo;探索新机制&rdquo;。</p>

<p>有了上面的依存句法分析结果，我们就可以比较容易的看到，&ldquo;提出者&rdquo;是&ldquo;李克强&rdquo;，而不是&ldquo;上海&rdquo;或&ldquo;外高桥&rdquo;，即使它们都是名词，而且距离&ldquo;提出&rdquo;更近。</p>

<h5><strong>依存关系</strong></h5>

<p>依存句法通过分析语言单位内成分之前的依存关系解释其句法结构，主张句子中核心动词是支配其他成分的中心成分。而它本身却不受其他任何成分的支配，所有受支配成分都以某种关系从属于支配者。</p>

<p>在20世纪70年代，Robinson 提出依存句法中关于依存关系的四条公理，在处理中文信息的研究中，中国学者提出了依存关系的第五条公理，分别如下：</p>

<ol>
	<li>一个句子中只有一个成分是独立的；</li>
	<li>句子的其他成分都从属于某一成分；</li>
	<li>任何一个成分都不能依存于两个或两个以上的成分；</li>
	<li>如果成分 A 直接从属成分 B，而成分 C 在句子中位于 A 和 B 之间，那么，成分 C 或者从属于 A，或者从属于 B，或者从属于 A 和 B 之间的某一成分；</li>
	<li>中心成分左右两边的其他成分相互不发生关系。</li>
</ol>

<p>句子成分之间相互支配与被支配、依存与被依存的现象，普遍存在于汉语的词汇（合成语）、短语、单句、段落、篇章等能够独立运用和表达的语言之中，这一特点体现了依存关系的普遍性。依存句法分析可以反映出句子各成分之间的语义修饰关系，它可以获得长距离的搭配信息，并与句子成分的物理位置无关。</p>

<p>依存句法分析标注关系（共14种）及含义如下表所示：</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/c564e540-97ec-11e8-a5a1-17130ea31e3a" /></p>

<h4>语义依存分析</h4>

<p>语义依存分析（Semantic Dependency Parsing， SDP），分析句子各个语言单位之间的语义关联，并将语义关联以依存结构呈现。使用语义依存刻画句子语义，好处在于不需要去抽象词汇本身，而是通过词汇所承受的语义框架来描述该词汇，而论元的数目相对词汇来说数量总是少了很多。</p>

<p>语义依存分析目标是跨越句子表层句法结构的束缚，直接获取深层的语义信息。例如以下三个句子，用不同的表达方式表达了同一个语义信息，即张三实施了一个吃的动作，吃的动作是对苹果实施的。</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/3be55a10-97ed-11e8-b78f-09922e3c574f" /></p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/430a6c40-97ed-11e8-8841-e548fce76345" /></p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/4dcd8cc0-97ed-11e8-911c-dd974a01956f" /></p>

<p>语义依存分析不受句法结构的影响，将具有直接语义关联的语言单元直接连接依存弧并标记上相应的语义关系。这也是语义依存分析与依存句法分析的重要区别。</p>

<p>语义依存关系分为三类，分别是主要语义角色，每一种语义角色对应存在一个嵌套关系和反关系；事件关系，描述两个事件间的关系；语义依附标记，标记说话者语气等依附性信息。</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/8accc8c0-97ed-11e8-8841-e548fce76345" /></p>

<h4>Pyhanlp 实战依存句法</h4>

<p>最后，我们通过 Pyhanlp 库实现依存句法的实战练习。这个过程中，我们选用 Dependency Viewer 工具进行可视化展示。可视化时， txt 文档需要采用 UTF-8 编码。</p>

<p>首先，引入包，然后可直接进行分析：</p>

<pre>
<code>    from pyhanlp import *
    sentence = "徐先生还具体帮助他确定了把画雄鹰、松鼠和麻雀作为主攻目标。"
    print(HanLP.parseDependency(sentence))
</code></pre>

<p>得到的结果：</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/8c1d52b0-97ef-11e8-a5a1-17130ea31e3a" /></p>

<p>然后，我们将结果保存在 txt 文件中：</p>

<pre>
<code>    f = open("D://result.txt",'a+')
    print((HanLP.parseDependency(sentence )),file = f)
</code></pre>

<p>最后，通过 Dependency Viewer 工具进行可视化，如果出现乱码，记得把 txt 文档保存为 UTF-8 式即可，得到的可视化结果如下图所示：</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/ab05d260-97f4-11e8-a5a1-17130ea31e3a" /></p>

<h3>总结</h3>

<p>本文，首先为大家介绍了语法体系，以及如何根据语法体系确定一个句子的语法树，为后面的句法分析奠定基础。</p>

<p>接着，介绍了依存句法，它的目的是通过分析语言单位内成分之间的依存关系揭示其句法结构，随之讲解了依存句法中的五大依存关系。</p>

<p>最后，进一步介绍了区别于依存句法的语义依存，其目的是分析句子各个语言单位之间的语义关联，并将语义关联以依存结构呈现。</p>

<p>文章结尾，通过 Pyhanlp 实战以及可视化，带大家进一步加深对中文依存句法的了解。</p>

<p><strong>参考资料以及推荐阅读：</strong></p>

<ol>
	<li><a href="https://blog.csdn.net/liu_zhlai/article/details/52444422">中文依存句法分析概述及应用</a></li>
	<li><a href="http://ir.hit.edu.cn/demo/ltp">LTP 依存分析模块所使用的依存关系标记含义</a></li>
	<li><a href="http://hanlp.linrunsoft.com/doc/_build/html/dependency_parser.html#id2">依存句法解析</a></li>
	<li><a href="https://blog.csdn.net/sinat_33741547/article/details/79258045">依存分析：中文依存句法分析简介</a></li>
	<li><a href="https://www.cnblogs.com/CheeseZH/p/5768389.html">依存句法分析与语义依存分析的区别</a></li>
	<li><a href="https://github.com/HIT-SCIR/pyltp">pyltp:the python extension for LTP</a></li>
	<li><a href="http://nlp.nju.edu.cn/tanggc/tools/DependencyViewer.html">Dependency Viewer</a></li>
</ol>

<p><a id="第17课：基于 CRF 的中文句法依存分析模型实现" name="第17课：基于 CRF 的中文句法依存分析模型实现"></a>第17课：基于 CRF 的中文句法依存分析模型实现</p>

<p>句法分析是自然语言处理中的关键技术之一，其基本任务是确定句子的句法结构或者句子中词汇之间的依存关系。主要包括两方面的内容，一是确定语言的语法体系，即对语言中合法句子的语法结构给予形式化的定义；另一方面是句法分析技术，即根据给定的语法体系，自动推导出句子的句法结构，分析句子所包含的句法单位和这些句法单位之间的关系。</p>

<p>依存关系本身是一个树结构，每一个词看成一个节点，依存关系就是一条有向边。本文主要通过清华大学的句法标注语料库，来实现基于 CRF 的中文句法依存分析模型。</p>

<h3>清华大学句法标注语料库</h3>

<p>清华大学的句法标注语料，包括训练集（train.conll）和开发集合文件（dev.conll）。训练集大小 5.41M，共185541条数据。测试集大小为 578kb，共19302条数据。</p>

<p>语料本身格式如下图所示：</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/ea8a9910-97fa-11e8-8841-e548fce76345" /></p>

<p>通过上图，我们可以看出，每行语料包括有8个标签，分别是 ID、FROM、lEMMA、CPOSTAG、POSTAG、FEATS、HEAD、DEPREL。详细介绍如下图：</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/fa233670-97fa-11e8-b78f-09922e3c574f" /></p>

<h3>模型的实现</h3>

<p>通过上面对句法依存关键技术的定义，我们明白了，句法依存的基本任务是确定句子的句法结构或者句子中词汇之间的依存关系。同时，我们也对此次模型实现的语料有了基本了解。</p>

<p>有了这些基础内容，我们便可以开始着手开发了。</p>

<p>本模型的实现过程，我们将主要分为训练集和测试集数据预处理、语料特征生成、模型训练及预测三大部分来实现，最终将通过模型预测得到正确的预测结果。</p>

<p>本次实战演练，我们选择以下模型和软件：</p>

<ul>
	<li>Sklearn_crfsuite</li>
	<li>Python3.6</li>
	<li>Jupyter Notebook</li>
</ul>

<h4>训练集和测试集数据预处理</h4>

<p>由于上述给定的语料，在模型中，我们不能直接使用，必须先经过预处理，把上述语料格式重新组织成具有词性、方向和距离的格式。</p>

<p>首先，我们通过一个 Python 脚本&nbsp;<code>get_parser_train_test_input.py</code>，生成所需要的训练集和测试集，执行如下命令即可：</p>

<pre>
<code>    cat train.conll | python get_parser_train_test_input.py &gt; train.data 
    cat dev.conll | python get_parser_train_test_input.py &gt; dev.data 
</code></pre>

<p>上面的脚本通过 cat 命令和管道符把内容传递给脚本进行处理。这里需要注意的是，脚本需要在 Linux 环境下执行，且语料和脚本应放在同一目录下。</p>

<p><code>get_parser_train_test_input.py</code>&nbsp;这一脚本的目的，就是重新组织语料，组织成可以使用 CRF 算法的格式，具有词性、方向和距离的格式。我们认为，如果词 A 依赖词 B，A 就是孩子，B 就是父亲。按照这种假设得到父亲节点的粗词性和详细词性，以及和依赖次之间的距离。</p>

<p>我们打开该脚本，看看它的代码，如下所示，重要的代码给出了注释。</p>

<pre>
<code>    #coding=utf-8
    '''词A依赖词B，A就是孩子，B就是父亲'''
    import sys 

    sentence = ["Root"]
    def do_parse(sentence):
        if len(sentence) == 1:return 
        for line in sentence[1:]:
            line_arr = line.strip().split("\t")
            c_id = int(line_arr[0])
            f_id = int(line_arr[6])
            if f_id == 0:
                print("\t".join(line_arr[2:5])+"\t" + "0_Root")
                continue
            f_post,f_detail_post = sentence[f_id].strip().split("\t")[3:5] #得到父亲节点的粗词性和详细词性
            c_edge_post = f_post #默认是依赖词的粗粒度词性，但是名词除外；名词取细粒度词性
            if f_post == "n":
                c_edge_post = f_detail_post
            #计算是第几个出现这种词行
            diff = f_id - c_id #确定要走几步
            step = 1 if f_id &gt; c_id  else -1 #确定每一步方向
            same_post_num = 0 #中间每一步统计多少个一样的词性
            cmp_idx = 4 if f_post == "n" else 3  #根据是否是名词决定取的是粗or详细词性
            for i in range(0, abs(diff)):
                idx = c_id + (i+1)*step
                if sentence[idx].strip().split("\t")[cmp_idx] == c_edge_post:
                    same_post_num += step

            print("\t".join(line_arr[2:5])+"\t" + "%d_%s"%(same_post_num, c_edge_post))
        print("")

    for line in sys.stdin:
        line = line.strip()
        line_arr = line.split("\t")
        if  line == "" or line_arr[0] == "1":
            do_parse(sentence)
            sentence = ["Root"]
        if line =="":continue 
        sentence.append(line)
</code></pre>

<p>整个脚本按行读入，每行按 Tab 键分割，首先得到父亲节点的词性，然后根据词性是否是名词 n 进行判断，默认是依赖词的粗粒度词性，如果是名词取细粒度词性。</p>

<p>脚本处理完，数据集的格式如下：</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/6ceea0a0-984a-11e8-911c-dd974a01956f" /></p>

<p>根据依存文法，决定两个词之间依存关系的主要有两个因素：方向和距离。正如上图中第四列类别标签所示，该列可以定义为以下形式：</p>

<blockquote>
<p>[+|-]dPOS</p>
</blockquote>

<p>其中，<code>[+|-]</code>&nbsp;表示中心词在句子中相对坐标轴的方向；POS 代表中心词具有的词性类别；d 表示与中心词词性相同的词的数量，即距离。</p>

<h4>语料特征生成</h4>

<p>语料特征提取，主要采用 N-gram 模型来完成。这里我们使用 3-gram 完成提取，将词性与词语两两进行匹配，分别返回特征集合和标签集合，需要注意整个语料采用的是 UTF-8 编码格式。</p>

<p>整个编码过程中，我们首先需要引入需要的库，然后对语料进行读文件操作。语料采用 UTF-8 编码格式，以句子为单位，按 Tab 键作分割处理，从而实现句子 3-gram 模型的特征提取。具体实现如下。</p>

<pre>
<code>    import sklearn_crfsuite
    from sklearn_crfsuite import metrics
    from sklearn.externals import joblib
</code></pre>

<p>首先引入需要用到的库，如上面代码所示。其目的是使用模型<code>sklearn_crfsuite .CRF</code>，metrics 用来进行模型性能测试，joblib 用来保存和加载训练好的模型。</p>

<p>接着，定义包含特征处理方法的类，命名为 CorpusProcess，类结构定义如下：</p>

<pre>
<code>class CorpusProcess(object):

    def __init__(self):
        """初始化"""
        pass

    def read_corpus_from_file(self, file_path):
        """读取语料"""
        pass

    def write_corpus_to_file(self, data, file_path):
        """写语料"""
        pass

    def process_sentence(self,lines):
        """处理句子"""
        pass   

    def initialize(self):
        """语料初始化"""
        pass   

    def generator(self, train=True):
        """特征生成器"""
        pass  

    def extract_feature(self, sentences):
        """提取特征"""
        pass
</code></pre>

<p>下面介绍下 CorpusProcess 类中各个方法的具体实现。</p>

<p>第1步， 实现 init 构造函数，目的初始化预处理好的语料的路径：</p>

<pre>
<code>    def __init__(self):
            """初始化"""
            self.train_process_path =  dir +  "data//train.data"   #预处理之后的训练集
            self.test_process_path =  dir +  "data//dev.data"  #预处理之后的测试集
</code></pre>

<p>这里的路径可以自定义，这里的语料之前已经完成了预处理过程。</p>

<p>第2-3步，<code>read_corpus_from_file</code>&nbsp;方法和&nbsp;<code>write_corpus_to_file</code>&nbsp;方法，分别定义了语料文件的读和写操作：</p>

<pre>
<code>    def read_corpus_from_file(self, file_path):
        """读取语料"""
        f = open(file_path, 'r',encoding='utf-8')
        lines = f.readlines()
        f.close()
        return lines

    def write_corpus_to_file(self, data, file_path):
        """写语料"""
        f = open(file_path, 'w')
        f.write(str(data))
        f.close()
</code></pre>

<p>这一步，主要用 open 函数来实现语料文件的读和写。</p>

<p>第4-5步，<code>process_sentence</code>&nbsp;方法和 initialize 方法，用来处理句子和初始化语料，把语料按句子结构用 list 存储起来，存储到内存中：</p>

<pre>
<code>    def process_sentence(self,lines):
            """处理句子"""
            sentence = []
            for line in lines:
                if not line.strip():
                    yield sentence
                    sentence = []
                else:
                    lines = line.strip().split(u'\t')
                    result = [line for line in lines]
                    sentence.append(result)   

        def initialize(self):
            """语料初始化"""
            train_lines = self.read_corpus_from_file(self.train_process_path)
            test_lines = self.read_corpus_from_file(self.test_process_path)
            self.train_sentences = [sentence for sentence in self.process_sentence(train_lines)]
            self.test_sentences = [sentence for sentence in self.process_sentence(test_lines)] 
</code></pre>

<p>这一步，通过&nbsp;<code>process_sentence</code>&nbsp;把句子收尾的空格去掉，然后通过 initialize 函数调用上面&nbsp;<code>read_corpus_from_file</code>&nbsp;方法读取语料，分别加载训练集和测试集。</p>

<p>第6步，特征生成器，分别用来指定生成训练集或者测试集的特征集：</p>

<pre>
<code>    def generator(self, train=True):
        """特征生成器"""
        if train: 
            sentences = self.train_sentences
        else: 
            sentences = self.test_sentences
        return self.extract_feature(sentences)    
</code></pre>

<p>这一步，对训练集和测试集分别处理，如果参数 train 为 True，则表示处理训练集，如果是 False，则表示处理测试集。</p>

<p>第7步，特征提取，简单的进行&nbsp;<code>3-gram</code>&nbsp;的抽取，将词性与词语两两进行匹配，分别返回特征集合和标签集合：</p>

<pre>
<code>    def extract_feature(self, sentences):
            """提取特征"""
            features, tags = [], []
            for index in range(len(sentences)):
                feature_list, tag_list = [], []
                for i in range(len(sentences[index])):
                    feature = {"w0": sentences[index][i][0],
                               "p0": sentences[index][i][1],
                               "w-1": sentences[index][i-1][0] if i != 0 else "BOS",
                               "w+1": sentences[index][i+1][0] if i != len(sentences[index])-1 else "EOS",
                               "p-1": sentences[index][i-1][1] if i != 0 else "un",
                               "p+1": sentences[index][i+1][1] if i != len(sentences[index])-1 else "un"}
                    feature["w-1:w0"] = feature["w-1"]+feature["w0"]
                    feature["w0:w+1"] = feature["w0"]+feature["w+1"]
                    feature["p-1:p0"] = feature["p-1"]+feature["p0"]
                    feature["p0:p+1"] = feature["p0"]+feature["p+1"]
                    feature["p-1:w0"] = feature["p-1"]+feature["w0"]
                    feature["w0:p+1"] = feature["w0"]+feature["p+1"]
                    feature_list.append(feature)
                    tag_list.append(sentences[index][i][-1])
                features.append(feature_list)
                tags.append(tag_list)
            return features, tags    
</code></pre>

<p>经过第6步，确定处理的是训练集还是测试集之后，通过&nbsp;<code>extract_feature</code>&nbsp;对句子进行特征抽取，使用 3-gram 模型，得到特征集合和标签集合的对应关系。</p>

<h4>模型训练及预测</h4>

<p>在完成特征工程和特征提取之后，接下来，我们要进行模型训练和预测，要预定义模型需要的一些参数，并初始化模型对象，进而完成模型训练和预测，以及模型的保存与加载。</p>

<p>首先，我们定义模型 ModelParser 类，进行初始化参数、模型初始化，以及模型训练、预测、保存和加载，类的结构定义如下：</p>

<pre>
<code>    class ModelParser(object):

        def __init__(self):
            """初始化参数"""
            pass

        def initialize_model(self):
            """模型初始化"""
            pass

        def train(self):
            """训练"""
            pass

        def predict(self, sentences):
            """模型预测"""
            pass

        def load_model(self, name='model'):
            """加载模型 """
            pass

        def save_model(self, name='model'):
            """保存模型"""
            pass
</code></pre>

<p>接下来，我们分析 ModelParser 类中方法的具体实现。</p>

<p>第1步，init 方法实现算法模型参数和语料预处理 CorpusProcess 类的实例化和初始化：</p>

<pre>
<code>    def __init__(self):
            """初始化参数"""
            self.algorithm = "lbfgs"
            self.c1 = 0.1
            self.c2 = 0.1
            self.max_iterations = 100
            self.model_path = "model.pkl"
            self.corpus = CorpusProcess()  #初始化CorpusProcess类
            self.corpus.initialize()  #语料预处理
            self.model = None
</code></pre>

<p>这一步，init 方法初始化参数以及 CRF 模型的参数，算法选用 LBFGS，c1 和 c2 分别为0.1，最大迭代次数100次。然后定义模型保存的文件名称，以及完成对 CorpusProcess 类 的初始化。</p>

<p>第2-3步，<code>initialize_model</code>&nbsp;方法和 train 实现模型定义和训练：</p>

<pre>
<code>     def initialize_model(self):
            """模型初始化"""
            algorithm = self.algorithm
            c1 = float(self.c1)
            c2 = float(self.c2)
            max_iterations = int(self.max_iterations)
            self.model = sklearn_crfsuite.CRF(algorithm=algorithm, c1=c1, c2=c2,
                                              max_iterations=max_iterations, all_possible_transitions=True)

        def train(self):
            """训练"""
            self.initialize_model()
            x_train, y_train = self.corpus.generator()
            self.model.fit(x_train, y_train)
            labels = list(self.model.classes_)
            x_test, y_test = self.corpus.generator(train=False)
            y_predict = self.model.predict(x_test)
            metrics.flat_f1_score(y_test, y_predict, average='weighted', labels=labels)
            sorted_labels = sorted(labels, key=lambda name: (name[1:], name[0]))
            print(metrics.flat_classification_report(y_test, y_predict, labels=sorted_labels, digits=3))
            self.save_model()
</code></pre>

<p>这一步，<code>initialize_model</code>&nbsp;方法实现 了&nbsp;<code>sklearn_crfsuite.CRF</code>&nbsp;模型的初始化。然后在 train 方法中，先通过 fit 方法训练模型，再通过<code>metrics.flat_f1_score</code>&nbsp;对测试集进行 F1 性能测试，最后将模型保存。</p>

<p>第4-6步，分别实现模型预测、保存和加载方法，具体代码大家可以访问<a href="https://github.com/sujeek/chinese_nlp">Github</a>。</p>

<p>最后，实例化类，并进行模型训练：</p>

<pre>
<code>    model = ModelParser()
    model.train()
</code></pre>

<p>对模型进行预测，预测数据输入格式为三维，表示完整的一句话：</p>

<blockquote>
<p>[[[&#39;坚决&#39;, &#39;a&#39;, &#39;ad&#39;, &#39;1_v&#39;],</p>

<pre>
<code> ['惩治', 'v', 'v', '0_Root'],

 ['贪污', 'v', 'v', '1_v'],

 ['贿赂', 'n', 'n', '-1_v'],

 ['等', 'u', 'udeng', '-1_v'],

 ['经济', 'n', 'n', '1_v'],

 ['犯罪', 'v', 'vn', '-2_v']]]
</code></pre>
</blockquote>

<p>模型预测的结果如下图所示：</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/87377f80-985a-11e8-b78f-09922e3c574f" /></p>

<p>预测的结果，和原始语料预处理得到的标签格式保持一致。</p>

<p>语料和代码下载，请访问：<a href="https://github.com/sujeek/chinese_nlp">Github</a>。</p>

<h3>总结</h3>

<p>本文通过清华大学的句法标注语料库，实现了基于 CRF 的中文句法依存分析模型。借此实例，相信大家对句法依存已有了一个完整客观的认识。</p>

<p><strong>参考文献及推荐阅读</strong></p>

<ol>
	<li><a href="https://github.com/scrapinghub/python-crfsuite/blob/master/examples/CoNLL%202002.ipynb">使用 CoNLL 2002 数据的英文句法依存分析</a></li>
	<li><a href="http://x-algo.cn/index.php/2016/03/02/crf-dependency-parsing/">CRF++ 依存句法分析</a></li>
	<li><a href="https://blog.csdn.net/sinat_33741547/article/details/79321401">依存分析：基于序列标注的中文依存句法分析模型实现</a></li>
</ol>

<p><a id="第18课：模型部署上线的几种服务发布方式" name="第18课：模型部署上线的几种服务发布方式"></a>第18课：模型部署上线的几种服务发布方式</p>

<p>在前面所有的模型训练和预测中，我们训练好的模型都是直接通过控制台或者 Jupyter Notebook 来进行预测和交互的，在一个系统或者项目中使用这种方式显然不可能，那在 Web 应用中如何使用我们训练好的模型呢？本文将通过以下四个方面对该问题进行讲解：</p>

<ol>
	<li>微服务架构简介；</li>
	<li>模型的持久化与加载方式；</li>
	<li>Flask 和 Bottle 微服务框架；</li>
	<li>Tensorflow Serving 模型部署和服务。</li>
</ol>

<h3>微服务架构简介</h3>

<p>微服务是指开发一个单个小型的但有业务功能的服务，每个服务都有自己的处理和轻量通讯机制，可以部署在单个或多个服务器上。微服务也指一种松耦合的、有一定的有界上下文的面向服务架构。也就是说，如果每个服务都要同时修改，那么它们就不是微服务，因为它们紧耦合在一起；如果你需要掌握一个服务太多的上下文场景使用条件，那么它就是一个有上下文边界的服务，这个定义来自 DDD 领域驱动设计。</p>

<p>相对于单体架构和 SOA，它的主要特点是组件化、松耦合、自治、去中心化，体现在以下几个方面：</p>

<ol>
	<li>
	<p>一组小的服务：服务粒度要小，而每个服务是针对一个单一职责的业务能力的封装，专注做好一件事情；</p>
	</li>
	<li>
	<p>独立部署运行和扩展：每个服务能够独立被部署并运行在一个进程内。这种运行和部署方式能够赋予系统灵活的代码组织方式和发布节奏，使得快速交付和应对变化成为可能。</p>
	</li>
	<li>
	<p>独立开发和演化：技术选型灵活，不受遗留系统技术约束。合适的业务问题选择合适的技术可以独立演化。服务与服务之间采取与语言无关的 API 进行集成。相对单体架构，微服务架构是更面向业务创新的一种架构模式。</p>
	</li>
	<li>
	<p>独立团队和自治：团队对服务的整个生命周期负责，工作在独立的上下文中，自己决策自己治理，而不需要统一的指挥中心。团队和团队之间通过松散的社区部落进行衔接。</p>
	</li>
</ol>

<p>由此，我们可以看到整个微服务的思想，与我们现在面对信息爆炸、知识爆炸做事情的思路是相通的：通过解耦我们所做的事情，分而治之以减少不必要的损耗，使得整个复杂的系统和组织能够快速地应对变化。</p>

<p>我们为什么采用微服务呢？</p>

<blockquote>
<p>&ldquo;让我们的系统尽可能快地响应变化&rdquo;</p>

<p>&mdash;&mdash;Rebecca Parson</p>
</blockquote>

<p>下面是一个简单的微服务模型架构设计：</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/ce581560-9868-11e8-9965-cbd322341912" /></p>

<h3>模型的持久化与加载方式</h3>

<p>开发过 J2EE 应用的人应该对持久化的概念很清楚。通俗得讲，就是临时数据（比如内存中的数据，是不能永久保存的）持久化为持久数据（比如持久化至数据库中，能够长久保存）。</p>

<p>那我们训练好的模型一般都是存储在内存中，这个时候就需要用到持久化方式，在 Python 中，常用的模型持久化方式有三种，并且都是以文件的方式持久化。</p>

<p><strong>1.JSON（JavaScript Object Notation）格式。</strong></p>

<p>JSON 是一种轻量级的数据交换格式，易于人们阅读和编写。使用 JSON 函数需要导入 JSON 库：</p>

<pre>
<code>import json
</code></pre>

<p>它拥有两个格式处理函数：</p>

<ul>
	<li>json.dumps：将 Python 对象编码成 JSON 字符串；</li>
	<li>json.loads：将已编码的 JSON 字符串解码为 Python 对象。</li>
</ul>

<p>下面看一个例子。</p>

<p>首先我们创建一个 List 对象 data，然后把 data 编码成 JSON 字符串保存在 data.json 文件中，之后再读取 data.json 文件中的字符串解码成 Python 对象，代码如下：</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/9a645770-986b-11e8-9195-95b521cd20b9" /></p>

<p><strong>2. pickle 模块</strong></p>

<p>pickle 提供了一个简单的持久化功能。可以将对象以文件的形式存放在磁盘上。pickle 模块只能在 Python 中使用，Python 中几乎所有的数据类型（列表、字典、集合、类等）都可以用 pickle 来序列化。pickle 序列化后的数据，可读性差，人一般无法识别。</p>

<p>使用的时候需要引入库：</p>

<pre>
<code>import pickle
</code></pre>

<p>它有以下两个方法：</p>

<ul>
	<li>
	<p>pickle.dump(obj, file[, protocol])：序列化对象，并将结果数据流写入到文件对象中。参数 protocol 是序列化模式，默认值为0，表示以文本的形式序列化。protocol 的值还可以是1或2，表示以二进制的形式序列化。</p>
	</li>
	<li>
	<p>pickle.load(file)：反序列化对象。将文件中的数据解析为一个 Python 对象。</p>
	</li>
</ul>

<p>我们继续延用上面的例子。实现的不同点在于，这次文件打开时用了<code>with...as...</code>&nbsp;语法，使用 pickle 保存结果，文件保存为 data.pkl，代码如下。</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/191e6640-986d-11e8-9195-95b521cd20b9" /></p>

<p><strong>3. sklearn 中的 joblib 模块。</strong></p>

<p>使用 joblib，首先需要引入包：</p>

<pre>
<code>from sklearn.externals import joblib
</code></pre>

<p>使用方法如下，基本和 JSON、pickle一样，这里不再详细讲解。第17课中，进行模型保存时使用的就是这种方式，可以看代码，回顾一下。</p>

<pre>
<code>joblib.dump(model, model_path)  #模型保存
joblib.load(model_path)  #模型加载
</code></pre>

<h3>Flask 和 Bottle 微服务框架</h3>

<p>通过上面，我们对微服务和 Python 中三种模型持久化和加载方式有了基本了解。下面我们看看，Python 中如何把模型发布成一个微服务的。</p>

<p>这里给出两个微服务框架&nbsp;<a href="http://www.bottlepy.com/docs/dev/">Bottle</a>&nbsp;和&nbsp;<a href="http://docs.jinkan.org/docs/flask/">Flask</a>。</p>

<p>Bottle 是一个非常小巧但高效的微型 Python Web 框架，它被设计为仅仅只有一个文件的 Python 模块，并且除 Python 标准库外，它不依赖于任何第三方模块。</p>

<p>Bottle 本身主要包含以下四个模块，依靠它们便可快速开发微 Web 服务：</p>

<ul>
	<li>路由（Routing）：将请求映射到函数，可以创建十分优雅的 URL；</li>
	<li>模板（Templates）：可以快速构建 Python 内置模板引擎，同时还支持 Mako、Jinja2、Cheetah 等第三方模板引擎；</li>
	<li>工具集（Utilites）：用于快速读取 form 数据，上传文件，访问 Cookies，Headers 或者其它 HTTP 相关的 metadata；</li>
	<li>服务器（Server）：内置 HTTP 开发服务器，并且支持 paste、fapws3、 bjoern、Google App Engine、Cherrypy 或者其它任何 WSGI HTTP 服务器。</li>
</ul>

<p>Flask 也是一个 Python 编写的 Web 微框架，可以让我们使用 Python 语言快速实现一个网站或 Web 服务。并使用方式和 Bottle 相似，Flask 依赖 Jinja2 模板和 Werkzeug WSGI 服务。Werkzeug 本质是 Socket 服务端，其用于接收 HTTP 请求并对请求进行预处理，然后触发 Flask 框架，开发人员基于 Flask 框架提供的功能对请求进行相应的处理，并返回给用户，如果返回给用户的内容比较复杂时，需要借助 Jinja2 模板来实现对模板的处理，即将模板和数据进行渲染，将渲染后的字符串返回给用户浏览器。</p>

<p>Bottle 和 Flask 在使用上相似，而且 Flask 的文档资料更全，发布的服务更稳定，因此下面重点以 Flask 为例，来说明模型的微服务发布过程。</p>

<p>如果大家想进一步了解这两个框架，可以参考说明文档。</p>

<p><strong>1.安装。</strong></p>

<p>对 Bottle 和 Flask 进行安装，分别执行如下命令即可安装成功：</p>

<pre>
<code>pip install bottle
pip install Flask
</code></pre>

<p>安装好之后，分别进入需要的包就可以写微服务程序了。这两个框架在使用时，用法、语法结构都差不多，网上 Flask 的中文资料相对多一些，所以这里用 Flask 来举例。</p>

<p><strong>2. 第一个最小的 Flask 应用。</strong></p>

<p>第一个最小的 Flask 应用看起来会是这样:</p>

<pre>
<code>from flask import Flask
app = Flask(__name__)

@app.route('/')
def hello_world():
    return 'Hello World!'

if __name__ == '__main__':
    app.run()
</code></pre>

<p>把它保存为 hello.py（或是类似的），然后用 Python 解释器来运行：</p>

<pre>
<code>python hello.py
</code></pre>

<p>或者直接在 Jupyter Notebook 里面执行，都没有问题。服务启动将在控制台打印如下消息：</p>

<blockquote>
<p>Running on http://127.0.0.1:5000/</p>
</blockquote>

<p>意思就是，可以通过 localhost 和 5000 端口，在浏览器访问：</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/616e7240-98b9-11e8-9965-cbd322341912" /></p>

<p>这时我们就得到了服务在浏览器上的返回结果，于是也成功构建了与浏览器交互的服务。</p>

<p>如果要修改服务对应的 IP 地址和端口怎么办？只需要修改这行代码，即可修改 IP 地址和端口：</p>

<pre>
<code>app.run(host='192.168.31.19',port=8088)
</code></pre>

<p><strong>3. Flask 发布一个预测模型。</strong></p>

<p>首先，我们这里使用第17课保存的模型&ldquo;model.pkl&rdquo;。如果不使用浏览器，常规的控制台交互，我们这样就可以实现：</p>

<pre>
<code>    from sklearn.externals import joblib
    model_path = "D://达人课//中文自然语言处理入门实战课程//ch18//model.pkl"
    model = joblib.load(model_path)
    sen =[[['坚决', 'a', 'ad', '1_v'],
               ['惩治', 'v', 'v', '0_Root'],
               ['贪污', 'v', 'v', '1_v'],
               ['贿赂', 'n', 'n', '-1_v'],
               ['等', 'u', 'udeng', '-1_v'],
               ['经济', 'n', 'n', '1_v'],
               ['犯罪', 'v', 'vn', '-2_v']]]
    print(model.predict(sen))
</code></pre>

<p>如果你现在有个需求，要求你的模型和浏览器进行交互，那 Flask 就可以实现。</p>

<p>在第一个最小的 Flask 应用基础上，我们增加模型预测接口，这里注意：<strong>启动之前把 IP 地址修改为自己本机的地址或者服务器工作站所在的 IP 地址。</strong></p>

<p>完整的代码如下，首先在启动之前先把模型预加载到内存中，然后重新定义 predict 函数，接受一个参数 sen：</p>

<pre>
<code>    from sklearn.externals import joblib
    from flask import Flask,request
    app = Flask(__name__)

    @app.route('/')
    def hello_world():
        return 'Hello World!'

    @app.route('/predict/&lt;sen&gt;')
    def predict(sen):
        result = model.predict(sen)
        return str(result)

    if __name__ == '__main__':
        model_path = "D://ch18//model.pkl"
        model = joblib.load(model_path)
        app.run(host='192.168.31.19')
</code></pre>

<p>启动 Flask 服务之后，在浏览器地址中输入：</p>

<blockquote>
<p>http://192.168.31.19:5000/predict/[[[&#39;坚决&#39;, &#39;a&#39;, &#39;ad&#39;, &#39;1<em>v&#39;], [&#39;惩治&#39;, &#39;v&#39;, &#39;v&#39;, &#39;0</em>Root&#39;], [&#39;贪污&#39;, &#39;v&#39;, &#39;v&#39;, &#39;1<em>v&#39;], [&#39;贿赂&#39;, &#39;n&#39;, &#39;n&#39;, &#39;-1</em>v&#39;], [&#39;等&#39;, &#39;u&#39;, &#39;udeng&#39;, &#39;-1<em>v&#39;], [&#39;经济&#39;, &#39;n&#39;, &#39;n&#39;, &#39;1</em>v&#39;], [&#39;犯罪&#39;, &#39;v&#39;, &#39;vn&#39;, &#39;-2_v&#39;]]]</p>
</blockquote>

<p>得到预测结果，这样就完成了微服务的发布，并实现了模型和前端浏览器的交互。</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/4177b380-988b-11e8-bb33-7f37906bb913" /></p>

<h3>Tensorflow Serving 模型部署和服务</h3>

<p>TensorFlow Serving 是一个用于机器学习模型 Serving 的高性能开源库。它可以将训练好的机器学习模型部署到线上，使用 gRPC 作为接口接受外部调用。更加让人眼前一亮的是，它支持模型热更新与自动模型版本管理。这意味着一旦部署 TensorFlow Serving 后，你再也不需要为线上服务操心，只需要关心你的线下模型训练。</p>

<p>同样，TensorFlow Serving 可以将模型部署在移动端，如安卓或者 iOS 系统的 App 应用上。关于 Tensorflow Serving 模型部署和服务，这里不在列举示例，直接参考文末的推荐阅读。</p>

<h3>总结</h3>

<p>本节对微服务架构做了简单介绍，并介绍了三种机器学习模型持久化和加载的方式，接着介绍了 Python 的两个轻量级微服务框架 Bottle 和 Flask。随后，我们通过 Flask 制作了一个简单的微服务预测接口，实现模型的预测和浏览器交互功能，最后简单介绍了 TensorFlow Servin 模型的部署和服务功能。</p>

<p>学完上述内容，读者可轻易实现自己训练的模型和 Web 应用的结合，提供微服务接口，实现模型上线应用。</p>

<p><strong>参考文献以及推荐阅读</strong></p>

<ol>
	<li><a href="http://www.bottlepy.com/docs/dev/">Bottle 文档</a></li>
	<li><a href="http://docs.jinkan.org/docs/flask/">Flask 文档</a></li>
	<li><a href="https://blog.csdn.net/heyc861221/article/details/80129169">面向机器智能的 TensorFlow 实践：产品环境中模型的部署</a></li>
	<li><a href="https://blog.csdn.net/shin627077/article/details/78592729">Tensorflow Serving 服务部署与访问（Python + Java）</a></li>
</ol>

<p><a id="第19课：知识挖掘与知识图谱概述" name="第19课：知识挖掘与知识图谱概述"></a>第19课：知识挖掘与知识图谱概述</p>

<p>搜索技术日新月异，如今它不再是搜索框中输入几个单词那么简单了。不仅输入方式多样化，并且还要在非常短的时间内给出一个精准而又全面的答案。目前，谷歌给出的解决方案就是&mdash;&mdash;知识图谱（Knowledge Graph）。</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/99711d60-9dc7-11e8-a606-7910b86fc801" /></p>

<h3>知识图谱能做什么？</h3>

<p>知识图谱想做的，就是在不同数据（来自现实世界）之间建立联系，从而带给我们更有意义的搜索结果。</p>

<p>比如，在上图中，用 Google 搜索自然语言处理，右侧会显示研究领域和相关概念。点击这些知识点，又可以深入了解；再比如，搜索一个人名时，右侧会给出此人的生平、背景、居住位置、作品等信息。</p>

<p>这就是知识图谱，它不再是单一的信息，而是一个多元的信息网络。</p>

<h3>知识图谱的源头</h3>

<p>知识图谱的雏形好几年前就已出现，一家名为 Metaweb 的小公司，将现实世界中实体（人或事）的各种数据信息存储在系统中，并在数据之间建立起联系，从而发展出有别于传统关键词搜索的技术。</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/83c3ba70-9dc9-11e8-991f-1fa5582600fd" /></p>

<p>谷歌认为这一系统很有发展潜力，于2010年收购了 Metaweb。那时 Metawab 已经存储了1200万个节点（Reference Point，相当于一个词条或者一个页面），谷歌收购后的两年中，大大加速这一进程，现已有超过5.7亿个节点并在它们之间建了180亿个有效连接（这可是一个相当大的数字，维基百科英文版也才有大约400万个节点）。</p>

<h3>知识图谱的通用表示方法</h3>

<p>本质上，知识图谱是一种揭示实体之间关系的语义网络 ，可以对现实世界的事物及其相互关系进行形式化地描述 。现在的知识图谱己被用来泛指各种大规模的知识库 。</p>

<p>三元组是知识图谱的一种通用表示方式，即&nbsp;G=(E，R，S)G=(E，R，S)，其中&nbsp;E=e1，e2，&hellip;，e|E|E=e1，e2，&hellip;，e|E|&nbsp;是知识库中的实体集合，共包含&nbsp;|E||E|&nbsp;种不同实体，R=r1，r2，&hellip;,r|E|R=r1，r2，&hellip;,r|E|&nbsp;是知识库中的关系集合，共包含&nbsp;|R||R|&nbsp;种不同关系，S&sube;E&times;R&times;ES&sube;E&times;R&times;E&nbsp;代表知识库中的三元组集合。</p>

<p>三元组的基本形式主要包括实体 A、关系、实体 B 和概念、属性、属性值等，实体是知识图谱中的最基本元素，不同的实体间存在不同的关系。概念主要指集合、类别、对象类型、事物的种类，例如人物、地理等；属性主要指对象可能具有的属性、特征、特性、特点以及参数，例如国籍、生日等；属性值主要指对象指定属性的值，例如中国、1988&mdash;09&mdash;08等。每个实体（概念的外延）可用一个全局唯一确定的 ID 来标识，每个属性&mdash;属性值对可用来刻画实体的内在特性，而关系可用来连接两个实体，刻画它们之间的关联。</p>

<p>如下图是实体 A 与实体 B 组成的一个简单三元组形式。</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/c31662b0-9dd1-11e8-8538-cb29bbd5eb8b" /></p>

<h3>知识图谱的架构</h3>

<p>知识图谱的架构主要包括自身的逻辑结构以及体系架构，分别说明如下。</p>

<p><strong>1. 知识图谱的逻辑结构。</strong></p>

<p>知识图谱在逻辑上可分为模式层与数据层两个层次，数据层主要是由一系列的事实组成，而知识将以事实为单位进行存储。如果用（实体 A，关系，实体 B）、（实体、属性，属性值）这样的三元组来表达事实，可选择图数据库作为存储介质，例如开源的 Neo4j、Twitter 的 FlockDB、Sones 的 GraphDB 等。模式层构建在数据层之上，主要是通过本体库来规范数据层的一系列事实表达。本体是结构化知识库的概念模板，通过本体库而形成的知识库不仅层次结构较强，并且冗余程度较小。</p>

<p><strong>2. 知识图谱的体系架构。</strong></p>

<p>知识图谱的体系架构是指其构建模式结构，如图下图所示。</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/03944f10-9ddb-11e8-b6f3-454e1d4b65e0" /></p>

<p>知识图谱主要有自顶向下与自底向上两种构建方式。自顶向下指的是先为知识图谱定义好本体与数据模式，再将实体加入到知识库。该构建方式需要利用一些现有的结构化知识库作为其基础知识库，例如 Freebase 项目就是采用这种方式，它的绝大部分数据是从维基百科中得到的。自底向上指的是从一些开放链接数据中提取出实体，选择其中置信度较高的加入到知识库，再构建顶层的本体模式。目前，大多数知识图谱都采用自底向上的方式进行构建，其中最典型就是 Google 的 Knowledge Vault。</p>

<h3>知识图谱的关键技术</h3>

<p>大规模知识库的构建与应用需要多种智能信息处理技术的支持。这就涉及到当下异常火爆的人工智能中的自然语言处理（NLP）技术。</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/2cc327b0-9dec-11e8-b6f3-454e1d4b65e0" /></p>

<p>所谓自然语言，就是我们平时所说的话（包括语音或文字），但这些话计算机如何能&ldquo;理解&rdquo;？过程很复杂，下面是其中的几个关键步骤。</p>

<p><strong>1. 知识抽取。</strong></p>

<p>知识抽取技术，可以从一些公开的半结构化、非结构化的数据中提取出实体、关系、属性等知识要素。</p>

<p>知识抽取主要包含实体抽取、关系抽取、属性抽取等，涉及到的 NLP 技术有命名实体识别、句法依存、实体关系识别等。</p>

<p><strong>2. 知识表示。</strong></p>

<p>知识表示形成的综合向量对知识库的构建、推理、融合以及应用均具有重要的意义。</p>

<p>基于三元组的知识表示形式受到了人们广泛的认可，但是其在计算效率、数据稀疏性等方面却面临着诸多问题。近年来，以深度学习为代表的表示学习技术取得了重要的进展，可以将实体的语义信息表示为稠密低维实值向量，进而在低维空间中高效计算实体、关系及其之间的复杂语义关联。</p>

<p>知识表示学习主要包含的 NLP 技术有语义相似度计算、复杂关系模型，知识代表模型如距离模型、双线性模型、神经张量模型、矩阵分解模型、翻译模型等。</p>

<p><strong>3.知识融合。</strong></p>

<p>由于知识图谱中的知识来源广泛，存在知识质量良莠不齐、来自不同数据源的知识重复、知识间的关联不够明确等问题，所以必须要进行知识的融合。知识融合是高层次的知识组织，使来自不同知识源的知识在同一框架规范下进行异构数据整合、消歧、加工、推理验证、更新等步骤，达到数据、信息、方法、经验以及人的思想的融合，形成高质量的知识库。</p>

<p>在知识融合过程中，实体对齐、知识加工是两个重要的过程。</p>

<p><strong>4.知识推理。</strong></p>

<p>知识推理则是在已有的知识库基础上进一步挖掘隐含的知识，从而丰富、扩展知识库。在推理的过程中，往往需要关联规则的支持。由于实体、实体属性以及关系的多样性，人们很难穷举所有的推理规则，一些较为复杂的推理规则往往是手动总结的。对于推理规则的挖掘，主要还是依赖于实体以及关系间的丰富情况。知识推理的对象可以是实体、实体的属性、实体间的关系、本体库中概念的层次结构等。</p>

<p>知识推理方法主要可分为基于逻辑的推理与基于图的推理两种类别。</p>

<h3>大规模开放知识库</h3>

<p>互联网的发展为知识工程提供了新的机遇。从一定程度上看，是互联网的出现帮助突破了传统知识工程在知识获取方面的瓶颈。从1998年 Tim Berners Lee 提出语义网至今，涌现出大量以互联网资源为基础的新一代知识库。这类知识库的构建方法可以分为三类：互联网众包、专家协作和互联网挖掘，如下图所示：</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/84543f00-9de7-11e8-8538-cb29bbd5eb8b" /></p>

<p>下面介绍几个知名的中文知识图谱资源：</p>

<ul>
	<li>
	<p>OpenKG.CN：中文开放知识图谱联盟旨在通过建设开放的社区来促进中文知识图谱数据的开放与互联，促进中文知识图谱工具的标准化和技术普及。</p>
	</li>
	<li>
	<p>Zhishi.me ：Zhishi.me 是中文常识知识图谱。主要通过从开放的百科数据中抽取结构化数据，已融合了百度百科，互动百科以及维基百科中的中文数据。</p>
	</li>
	<li>
	<p>CN-DBPeidia：CN-DBpedia 是由复旦大学知识工场实验室研发并维护的大规模通用领域结构化百科。</p>
	</li>
	<li>
	<p>cnSchema.org: cnSchema.org 是一个基于社区维护的开放的知识图谱 Schema 标准。cnSchema 的词汇集包括了上千种概念分类、数据类型、属性和关系等常用概念定义，以支持知识图谱数据的通用性、复用性和流动性。</p>
	</li>
</ul>

<h3>知识图谱的典型应用</h3>

<p>知识图谱为互联网上海量、异构、动态的大数据表达、组织、管理以及利用提供了一种更为有效的方式，使得网络的智能化水平更高，更加接近于人类的认知思维。</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/176dd730-9deb-11e8-b6f3-454e1d4b65e0" /></p>

<p>基于大规模开放知识库或知识图谱的应用，目前尚处在持续不断的发展与探索的阶段。下面列出了一些国内外比较出色的应用。</p>

<p><strong>1. 语义检索。</strong></p>

<p>谷歌公司通过建立 Google Knowledge Graph，实现了对知识的体系化组织与展示，试图从用户搜索意图感知、以及查询扩展的角度，直接提供给用户想要的知识。</p>

<p><strong>2. 智能问答。</strong></p>

<p>IBM 公司通过搭建知识图谱，并通过自然语言处理和机器学习等技术，开发出了 Watson 系统。在2011年2月的美国问答节目《Jeopardy!》上，Watson 战胜了这一节目的两位冠军选手，可与1996年同样来自 IBM 的&ldquo;深蓝&rdquo;战胜国际象棋大师卡斯帕罗夫产生的影响相提并论，被认为是人工智能历史上的一个里程碑。</p>

<p><strong>3. 领域专家快速生成。</strong></p>

<p>构建面向特定领域、特定主题的大规模知识库是实现对某一领域深度分析和计算的重要基础，OpenKN 通过实现端到端的开放知识库构建工具集，实现了在给定部分种子（Seed）的情况下，从无到有的生成领域知识库，进而形成领域专家。</p>

<p><strong>4. 行业生态深度分析与预测。</strong></p>

<p>利用开放大数据可以帮助企业发现潜伏在数据中的威胁，将结构化网络日志、文本数据、开源和第三方数据整合进一个单一的环境，屏蔽可疑的信号与噪声，有效保护用户网络，可在信用卡欺诈行为识别、医疗行业疾病预测、电商商品推荐、强化组织数据安全、不一致性验证、异常分析、金融量化交易、法律分析服务等多方面提供有价值的服务。</p>

<h3>知识图谱的前景与挑战</h3>

<p>在关注到知识图谱在自然语言处理、人工智能等领域展现巨大潜力的同时，也不难发现知识图谱中的知识获取、知识表示、知识推理等技术依然面临着一些困难与挑战，在未来的一段时间内，知识图谱将是大数据智能的前沿研究问题，有很多重要的开放性问题亟待学术界和产业界协力解决。我们认为，未来知识图谱研究有以下几个重要挑战：</p>

<ul>
	<li>
	<p>知识类型与表示。知识图谱主要采用（实体1、关系、实体2）三元组的形式来表示知识，这种方法可以较好地表示很多事实性知识。然而，人类知识类型多样，面对很多复杂知识，三元组就束手无策了。例如，人们的购物记录信息、新闻事件等，包含大量实体及其之间的复杂关系，更不用说人类大量的涉及主观感受、主观情感和模糊的知识了。</p>
	</li>
	<li>
	<p>知识获取。如何从互联网大数据萃取知识，是构建知识图谱的重要问题。目前已经提出各种知识获取方案，并已成功抽取大量有用的知识。但在抽取知识的准确率、覆盖率和效率等方面，都仍不如人意，有极大的提升空间。</p>
	</li>
	<li>
	<p>知识融合。来自不同数据的抽取知识可能存在大量噪音和冗余，或者使用了不同的语言。如何将这些知识有机融合起来，建立更大规模的知识图谱，是实现大数据智能的必由之路。</p>
	</li>
	<li>
	<p>知识应用。目前大规模知识图谱的应用场景和方式还比较有限，如何有效实现知识图谱的应用，利用知识图谱实现深度知识推理，提高大规模知识图谱计算效率，需要人们不断锐意发掘用户需求，探索更重要的应用场景，提出新的应用算法。</p>
	</li>
</ul>

<h3>总结</h3>

<p>本文对知识图谱的起源、定义、架构、大规模知识库、应用以及未来挑战等内容，进行了全面阐述。</p>

<p>知识抽取、知识表示、知识融合以及知识推理为构建知识图谱的四大核心技术，本文就当前产业界的需求介绍了它在智能搜索、深度问答、社交网络以及一些垂直行业中的实际应用。此外，还总结了目前知识图谱面临的主要挑战，并对其未来的研究方向进行了展望。</p>

<p>知识图谱的重要性不仅在于它是一个拥有强大语义处理能力与开放互联能力的知识库，并且还是一把开启智能机器大脑的钥匙，能够打开 Web3.0 时代的知识宝库，为相关学科领域开启新的发展方向。</p>

<p><strong>参考资料以及推荐阅读</strong></p>

<ol>
	<li>柳絮飞.《知识图谱：谷歌打造未来搜索》，电脑爱好者，2013年。</li>
	<li>徐增林，盛泳潘，贺丽荣，王雅芳.《知识图谱技术综述》，电子科技大学统计机器智能与学习实验室，2016年7月。</li>
	<li><a href="https://blog.csdn.net/qingqingpiaoguo/article/details/53366240">知识图谱&mdash;&mdash;机器大脑中的知识库</a></li>
	<li><a href="https://www.sohu.com/a/137145473_160850">人工智能2.0时代的开放知识计算</a></li>
</ol>

<p><a id="第20课：Neo4j 从入门到构建一个简单知识图谱" name="第20课：Neo4j 从入门到构建一个简单知识图谱"></a>第20课：Neo4j 从入门到构建一个简单知识图谱</p>

<p>Neo4j 对于大多数人来说，可能是比较陌生的。其实，Neo4j 是一个图形数据库，就像传统的关系数据库中的 Oracel 和 MySQL一样，用来持久化数据。Neo4j 是最近几年发展起来的新技术，属于 NoSQL 数据库中的一种。</p>

<p>本文主要从 Neo4j 为什么被用来做知识图谱，Neo4j 的简单安装，在 Neo4j 浏览器中创建节点和关系，Neo4j 的 Python 接口操作以及用 Neo4j 构建一个简单的农业知识图谱五个方面来讲。</p>

<h3>Neo4j 为什么被用来做知识图谱</h3>

<p>从第19课《知识挖掘与知识图谱概述》中，我们已经明白，知识图谱是一种基于图的数据结构，由节点和边组成。其中节点即实体，由一个全局唯一的 ID 标示，关系（也称属性）用于连接两个节点。通俗地讲，知识图谱就是把所有不同种类的信息连接在一起而得到一个关系网络，提供了从&ldquo;关系&rdquo;的角度去分析问题的能力。</p>

<p>而 Neo4j 作为一种经过特别优化的图形数据库，有以下优势：</p>

<ul>
	<li>
	<p><strong>数据存储</strong>：不像传统数据库整条记录来存储数据，Neo4j 以图的结构存储，可以存储图的节点、属性和边。属性、节点都是分开存储的，属性与节点的关系构成边，这将大大有助于提高数据库的性能。</p>
	</li>
	<li>
	<p><strong>数据读写</strong>：在 Neo4j 中，存储节点时使用了&nbsp;<code>Index-free Adjacency</code>&nbsp;技术，即每个节点都有指向其邻居节点的指针，可以让我们在时间复杂度为 O(1) 的情况下找到邻居节点。另外，按照官方的说法，在 Neo4j 中边是最重要的，是&nbsp;<code>First-class Entities</code>，所以单独存储，更有利于在图遍历时提高速度，也可以很方便地以任何方向进行遍历。</p>
	</li>
	<li>
	<p><strong>资源丰富</strong>：Neo4j 作为较早的一批图形数据库之一，其文档和各种技术博客较多。</p>
	</li>
	<li>
	<p><strong>同类对比</strong>：Flockdb 安装过程中依赖太多，安装复杂；Orientdb，Arangodb 与 Neo4j 做对比，从易用性来说都差不多，但是从稳定性来说，neo4j 是最好的。</p>
	</li>
</ul>

<p>综合上述以及因素，我认为 Neo4j 是做知识图谱比较简单、灵活、易用的图形数据库。</p>

<h3>Neo4j 的简单安装</h3>

<p>Neo4j 是基于 Java 的图形数据库，运行 Neo4j 需要启动 JVM 进程，因此必须安装 Java SE 的 JDK。从 Oracle 官方网站下载&nbsp;<a href="http://www.oracle.com/technetwork/java/javase/downloads/index.html">Java SE JDK</a>，选择版本 JDK8 以上版本即可。</p>

<p>下面简单介绍下 Neo4j 在 Linux 和 Windows 的安装过程。首先去<a href="https://neo4j.com/download/other-releases/#releases">官网</a>下载对应版本。解压之后，Neo4j 应用程序有如下主要的目录结构：</p>

<ul>
	<li>bin 目录：用于存储 Neo4j 的可执行程序；</li>
	<li>conf 目录：用于控制 Neo4j 启动的配置文件；</li>
	<li>data 目录：用于存储核心数据库文件；</li>
	<li>plugins 目录：用于存储 Neo4j 的插件。</li>
</ul>

<h4>Linux 系统下的安装</h4>

<p>通过 tar 解压命令解压到一个目录下：</p>

<pre>
<code>tar -xzvf neo4j-community-3.3.1-unix.tar.gz
</code></pre>

<p>然后进入 Neo4j 解压目录：</p>

<pre>
<code>cd /usr/local/neo4j/neo4j-community-3.1.0
</code></pre>

<p>通过启动命令，可以实现启动、控制台、停止服务：</p>

<pre>
<code>bin/neo4j  start/console/stop（启动/控制台/停止）
</code></pre>

<p>通过&nbsp;<code>cypher-shell</code>&nbsp;命令，可以进入命令行：</p>

<pre>
<code>bin/cypher-shell
</code></pre>

<h4>Windows 系统下的安装</h4>

<p>启动 DOS 命令行窗口，切换到解压目录 bin 下，以管理员身份运行命令，分别为启动服务、停止服务、重启服务和查询服务的状态：</p>

<pre>
<code>bin\neo4j start
bin\neo4j stop
bin\neo4j restart
bin\neo4j status
</code></pre>

<p>把 Neo4j 安装为服务（Windows Services），可通过以下命令：</p>

<pre>
<code>bin\neo4j install-service
bin\neo4j uninstall-service
</code></pre>

<p>Neo4j 的配置文档存储在 conf 目录下，Neo4j 通过配置文件 neo4j.conf 控制服务器的工作。默认情况下，不需要进行任意配置，就可以启动服务器。</p>

<p>下面我们在 Windows 环境下启动 Neo4j：</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/a76e8bd0-a0fd-11e8-8279-afd456e25292" /></p>

<p>Neo4j 服务器具有一个集成的浏览器，在一个运行的服务器实例上访问： http://localhost:7474/，打开浏览器，显示启动页面：</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/d39a6080-a0fd-11e8-aa70-d319955d5dbe" /></p>

<p>默认的 Host 是&nbsp;<code>bolt://localhost:7687</code>，默认的用户是 neo4j，其默认的密码是 neo4j，第一次成功登录到 Neo4j 服务器之后，需要重置密码。访问 Graph Database 需要输入身份验证，Host 是 Bolt 协议标识的主机。登录成功后界面：</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/456d52d0-a0fe-11e8-aa70-d319955d5dbe" /></p>

<p>到此为止，我们就完成了 Neo4j 的基本安装过程，更详细的参数配置，可以参考官方文档。</p>

<h3>在 Neo4j 浏览器中创建节点和关系</h3>

<p>下面，我们简单编写 Cypher 命令，Cypher 命令可以通过&nbsp;<a href="https://www.w3cschool.cn/neo4j/">Neo4j 教程</a>学习，在浏览器中通过 Neo4j 创建两个节点和两个关系。</p>

<p>在&nbsp;<code>$</code>&nbsp;命令行中，编写 Cypher 脚本代码，点击 Play 按钮完成创建，依次执行下面的语句：</p>

<pre>
<code>CREATE (n:Person { name: 'Andres', title: 'Developer' }) return n;
</code></pre>

<p>作用是创建一个 Person，并包含属性名字和职称。</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/eb544300-a100-11e8-8279-afd456e25292" /></p>

<p>下面这条语句也创建了一个 Person 对象，属性中只是名字和职称不一样。</p>

<pre>
<code>CREATE (n:Person { name: 'Vic', title: 'Developer' }) return n;
</code></pre>

<p>紧接着，通过下面两行命令进行两个 Person 的关系匹配：</p>

<pre>
<code>match(n:Person{name:"Vic"}),(m:Person{name:"Andres"}) create (n)-[r:Friend]-&gt;(m) return r;

match(n:Person{name:"Vic"}),(m:Person{name:"Andres"}) create (n)&lt;-[r:Friend]-(m) return r;
</code></pre>

<p>最后，在创建完两个节点和关系之后，查看数据库中的图形：</p>

<pre>
<code>match(n) return n;
</code></pre>

<p>如下图，返回两个 Person 节点，以及其关系网，两个 Person 之间组成 Friend 关系：</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/fbbae950-a101-11e8-82f2-9d33eeaf0679" /></p>

<h3>Neo4j 的 Python 操作</h3>

<p>既然 Neo4j 作为一个图库数据库，那我们在项目中使用的时候，必然不能通过上面那种方式完成任务，一般都要通过代码来完成数据的持久化操作。其中，对于 Java 编程者来说，可通过&nbsp;<a href="https://docs.spring.io/spring-data/neo4j/docs/current/reference/html/">Spring Data Neo4j</a>&nbsp;达到这一目的。</p>

<p>而对于 Python 开发者来说，Py2neo 库也可以完成对 Neo4j 的操作，操作过程如下。</p>

<p>首先 安装 Py2neo。Py2neo 的安装过程非常简单，在命令行通过下面命令即可安装成功。</p>

<pre>
<code>pip install py2neo
</code></pre>

<p>安装好之后，我们来看一下简单的图关系构建，看下面代码：</p>

<pre>
<code>from py2neo.data import Node, Relationship
a = Node("Person", name="Alice")
b = Node("Person", name="Bob")
ab = Relationship(a, "KNOWS", b)
</code></pre>

<p>第一行代码，首先引入 Node 和 Relationship 对象，紧接着，创建 a 和 b 节点对象，最后一行匹配 a 和 b 之间的工作雇佣关系。接着来看看 ab 对象的内容是什么：</p>

<pre>
<code>print(ab)
</code></pre>

<p>通过 print 打印出 ab 的内容：</p>

<pre>
<code>(Alice)-[:KNOWS {}]-&gt;(Bob)
</code></pre>

<p>通过这样，就完成了 Alice 和 Bob 之间的工作关系，如果有多组关系将构建成 Person 之间的一个关系网。</p>

<p>了解更多 Py2neo 的使用方法，建议查看官方文档。</p>

<h3>用 Neo4j 构建一个简单的农业知识图谱</h3>

<p>我们来看一个基于开源语料的简单农业知识图谱，由于过程比较繁杂，数据和知识图谱数据预处理过程这里不再赘述，下面，我们重点看基于 Neo4j 来创建知识图谱的过程。</p>

<p>整个过程主要包含以下步骤：</p>

<ul>
	<li>环境准备</li>
	<li>语料准备</li>
	<li>语料加载</li>
	<li>知识图谱查询展示</li>
</ul>

<h4>Neo4j 环境准备。</h4>

<p>根据上面对 Neo4j 环境的介绍，这里默认你已经搭建好 Neo4j 的环境，并能正常访问，如果没有环境，请自行搭建好 Neo4j 的可用环境。</p>

<h4>数据语料介绍。</h4>

<p>本次提供的语料是已经处理好的数据，包含6个 csv 文件，文件内容和描述如下。</p>

<ul>
	<li>attributes.csv：文件大小 2M，内容是通过互动百科页面得到的部分实体的属性，包含字段：Entity、AttributeName、Attribute，分别表示实体、属性名称、属性值。文件前5行结构如下：</li>
</ul>

<pre>
<code>Entity,AttributeName,Attribute
密度板,别名,纤维板
葡萄蔓枯病,主要为害部位,枝蔓
坎德拉,性别,男
坎德拉,国籍,法国
坎德拉,场上位置,后卫
</code></pre>

<ul>
	<li><code>hudong_pedia.csv</code>：文件大小 94.6M，内容是已经爬好的农业实体的百科页面的结构化数据，包含字段：title、url、image、openTypeList、detail、baseInfoKeyList、baseInfoValueList，分别表示名称、百科 URL 地址、图片、分类类型、详情、关键字、依据来源。文件前2行结构如下：</li>
</ul>

<pre>
<code>"title","url","image","openTypeList","detail","baseInfoKeyList","baseInfoValueList"
"菊糖","http://www.baike.com/wiki/菊糖","http://a0.att.hudong.com/72/85/20200000013920144736851207227_s.jpg","健康科学##分子生物学##化学品##有机物##科学##自然科学##药品##药学名词##药物中文名称列表","[药理作用] 诊断试剂 人体内不含菊糖，静注后，不被机体分解、结合、利用和破坏，经肾小球滤过，通过测定血中和尿中的菊糖含量，可以准确计算肾小球的滤过率。菊糖广泛存在于植物组织中,约有3.6万种植物中含有菊糖,尤其是菊芋、菊苣块根中含有丰富的菊糖[6,8]。菊芋(Jerusalem artichoke)又名洋姜,多年生草本植物,在我国栽种广泛,其适应性广、耐贫瘠、产量高、易种植,一般亩产菊芋块茎为2 000～4 000 kg,菊芋块茎除水分外,还含有15%～20%的菊糖,是加工生产菊糖及其制品的良好原料。","中文名：","菊糖"
"密度板","http://www.baike.com/wiki/密度板","http://a0.att.hudong.com/64/31/20200000013920144728317993941_s.jpg","居家##巧克力包装##应用科学##建筑材料##珠宝盒##礼品盒##科学##糖果盒##红酒盒##装修##装饰材料##隔断##首饰盒","密度板（英文：Medium Density Fiberboard (MDF)）也称纤维板，是以木质纤维或其他植物纤维为原料，施加脲醛树脂或其他适用的胶粘剂制成的人造板材。按其密度的不同，分为高密度板、中密度板、低密度板。密度板由于质软耐冲击，也容易再加工，在国外是制作家私的一种良好材料，但由于国家关于高密度板的标准比国际标准低数倍，所以，密度板在中国的使用质量还有待提高。","中文名：##全称：##别名：##主要材料：##分类：##优点：","密度板##中密度板纤维板##纤维板##以木质纤维或其他植物纤维##高密度板、中密度板、低密度板##表面光滑平整、材质细密性能稳定"
</code></pre>

<ul>
	<li>
	<p><code>hudong_pedia2.csv</code>：文件大小 41M，内容结构和&nbsp;<code>hudong_pedia.csv</code>&nbsp;文件保持一致，只是增加数据量，作为&nbsp;<code>hudong_pedia.csv</code>&nbsp;数据的补充。</p>
	</li>
	<li>
	<p><code>new_node.csv</code>：文件大小 2.28M，内容是节点名称和标签，包含字段：title、lable，分别表示节点名称、标签，文件前5行结构如下：</p>
	</li>
</ul>

<pre>
<code>title,lable
药物治疗,newNode
膳食纤维,newNode
Boven Merwede,newNode
亚美尼亚苏维埃百科全书,newNode
</code></pre>

<ul>
	<li><code>wikidata_relation.csv</code>：文件大小 1.83M，内容是实体和关系，包含字段 HudongItem1、relation、HudongItem2，分别表示实体1、关系、实体2，文件前5行结构如下：</li>
</ul>

<pre>
<code>HudongItem1,relation,HudongItem2
菊糖,instance of,化合物
菊糖,instance of,多糖
瓦尔,instance of,河流
菊糖,subclass of,食物
瓦尔,origin of the watercourse,莱茵河
</code></pre>

<ul>
	<li><code>wikidata_relation2.csv</code>：大小 7.18M，内容结构和<code>wikidata_relation.csv</code>&nbsp;一致，作为&nbsp;<code>wikidata_relation.csv</code>&nbsp;数据的补充。</li>
</ul>

<h4>语料加载。</h4>

<p>语料加载，利用 Neo4j 的&nbsp;<code>LOAD CSV WITH HEADERS FROM...</code>&nbsp;功能进行加载，具体操作过程如下。</p>

<p>首先，依次执行以下命令：</p>

<pre>
<code>// 将hudong_pedia.csv 导入
LOAD CSV WITH HEADERS  FROM "file:///hudong_pedia.csv" AS line  
CREATE (p:HudongItem{title:line.title,image:line.image,detail:line.detail,url:line.url,openTypeList:line.openTypeList,baseInfoKeyList:line.baseInfoKeyList,baseInfoValueList:line.baseInfoValueList})
</code></pre>

<p>执行成功之后，控制台显示成功：</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/160bd4e0-a1ca-11e8-aa70-d319955d5dbe" /></p>

<p>上面这张图，表示数据加载成功，并显示加载的数据条数和耗费的时间。</p>

<pre>
<code>// 新增了hudong_pedia2.csv
LOAD CSV WITH HEADERS  FROM "file:///hudong_pedia2.csv" AS line  
CREATE (p:HudongItem{title:line.title,image:line.image,detail:line.detail,url:line.url,openTypeList:line.openTypeList,baseInfoKeyList:line.baseInfoKeyList,baseInfoValueList:line.baseInfoValueList})

// 创建索引
CREATE CONSTRAINT ON (c:HudongItem)
ASSERT c.title IS UNIQUE
</code></pre>

<p>以上命令的意思是，将&nbsp;<code>hudong_pedia.csv</code>&nbsp;和&nbsp;<code>hudong_pedia2.csv</code>&nbsp;导入 Neo4j 作为结点，然后对 titile 属性添加 UNIQUE（唯一约束/索引）。</p>

<p><strong>注意：</strong>&nbsp;如果导入的时候出现 Neo4j JVM 内存溢出错误，可以在导入前，先把 Neo4j 下的&nbsp;<code>conf/neo4j.conf</code>&nbsp;中的&nbsp;<code>dbms.memory.heap.initial_size</code>&nbsp;和<code>dbms.memory.heap.max_size</code>&nbsp;调大点。导入完成后再把值改回去即可。</p>

<p>下面继续执行数据导入命令：</p>

<pre>
<code>// 导入新的节点
LOAD CSV WITH HEADERS FROM "file:///new_node.csv" AS line
CREATE (:NewNode { title: line.title })

//添加索引
CREATE CONSTRAINT ON (c:NewNode)
ASSERT c.title IS UNIQUE

//导入hudongItem和新加入节点之间的关系
LOAD CSV  WITH HEADERS FROM "file:///wikidata_relation2.csv" AS line
MATCH (entity1:HudongItem{title:line.HudongItem}) , (entity2:NewNode{title:line.NewNode})
CREATE (entity1)-[:RELATION { type: line.relation }]-&gt;(entity2)

LOAD CSV  WITH HEADERS FROM "file:///wikidata_relation.csv" AS line
MATCH (entity1:HudongItem{title:line.HudongItem1}) , (entity2:HudongItem{title:line.HudongItem2})
CREATE (entity1)-[:RELATION { type: line.relation }]-&gt;(entity2)
</code></pre>

<p>执行完这些命令后，我们导入&nbsp;<code>new_node.csv</code>&nbsp;新节点，并对 titile 属性添加 UNIQUE（唯一约束/索引），导入&nbsp;<code>wikidata_relation.csv</code>&nbsp;和<code>wikidata_relation2.csv</code>，并给节点之间创建关系。</p>

<p>紧接着，继续导入实体属性，并创建实体之间的关系：</p>

<pre>
<code>LOAD CSV WITH HEADERS FROM "file:///attributes.csv" AS line
MATCH (entity1:HudongItem{title:line.Entity}), (entity2:HudongItem{title:line.Attribute})
CREATE (entity1)-[:RELATION { type: line.AttributeName }]-&gt;(entity2);

LOAD CSV WITH HEADERS FROM "file:///attributes.csv" AS line
MATCH (entity1:HudongItem{title:line.Entity}), (entity2:NewNode{title:line.Attribute})
CREATE (entity1)-[:RELATION { type: line.AttributeName }]-&gt;(entity2);

LOAD CSV WITH HEADERS FROM "file:///attributes.csv" AS line
MATCH (entity1:NewNode{title:line.Entity}), (entity2:NewNode{title:line.Attribute})
CREATE (entity1)-[:RELATION { type: line.AttributeName }]-&gt;(entity2);

LOAD CSV WITH HEADERS FROM "file:///attributes.csv" AS line
MATCH (entity1:NewNode{title:line.Entity}), (entity2:HudongItem{title:line.Attribute})
CREATE (entity1)-[:RELATION { type: line.AttributeName }]-&gt;(entity2)  
</code></pre>

<p>这里注意，建索引的时候带了 label，因此只有使用 label 时才会使用索引，这里我们的实体有两个 label，所以一共做&nbsp;<code>2*2=4</code>&nbsp;次。当然也可以建立全局索引，即对于不同的 label 使用同一个索引。</p>

<p>以上过程，我们就完成了语料加载，并创建了实体之间的关系和属性匹配，下面我们来看看 Neo4j 图谱关系展示。</p>

<h4>知识图谱查询展示</h4>

<p>最后通过 cypher 语句查询来看看农业图谱展示。</p>

<p>首先，展示 HudongItem 实体，执行如下命令：</p>

<pre>
<code>MATCH (n:HudongItem) RETURN n LIMIT 25
</code></pre>

<p>对 HudongItem 实体进行查询，返回结果的25条数据，结果如下图：</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/2608f910-a1cd-11e8-a041-3fe38238552b" /></p>

<p>接着，展示 NewNode 实体，执行如下命令：</p>

<pre>
<code>MATCH (n:NewNode) RETURN n LIMIT 25
</code></pre>

<p>对 NewNode 实体进行查询，返回结果的25条数据，结果如下图：</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/2ee13980-a1cd-11e8-aeb1-374a1fc4569b" /></p>

<p>之后，展示 RELATION 直接的关系，执行如下命令：</p>

<pre>
<code>MATCH p=()-[r:RELATION]-&gt;() RETURN p LIMIT 25
</code></pre>

<p>展示实体属性关系，结果如下图：</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/36cbbe90-a1cd-11e8-aa70-d319955d5dbe" /></p>

<h3>总结</h3>

<p>本节内容到此结束，回顾下整篇文章，主要讲了以下内容：</p>

<ol>
	<li>解释了 Neo4j 被用来做知识图谱的原因；</li>
	<li>Neo4j 的简单安装以及在 Neo4j 浏览器中创建节点和关系；</li>
	<li>Neo4j 的 Python 接口操作及使用；</li>
	<li>从五个方面讲解了如何使用 Neo4j 构建一个简单的农业知识图谱。</li>
</ol>

<p>最后，强调一句，知识图谱未来会通过自然语言处理技术和搜索技术结合应用会越来越广，工业界所出的地位也会越来越重要。</p>

<p><strong>参考文献及推荐阅读</strong></p>

<ol>
	<li><a href="https://neo4j.com/download/other-releases/#releases">Neo4j 官网</a></li>
	<li><a href="https://neo4j.com/docs/operations-manual/3.2/">The Neo4j Developer Manual</a></li>
	<li><a href="https://www.w3cschool.cn/neo4j/">Neo4j 教程</a></li>
	<li><a href="https://www.jianshu.com/p/a2497a33390f">Py2neo&mdash;&mdash;Neo4j &amp; Python 的配合使用</a></li>
</ol>

<p><a id="第21课：中文自然语言处理的应用、现状和未来" name="第21课：中文自然语言处理的应用、现状和未来"></a>第21课：中文自然语言处理的应用、现状和未来</p>

<p>自然语言理解和自然语言生成是自然语言处理的两大内核，机器翻译是自然语言理解方面最早的研究工作。自然语言处理的主要任务是：研究表示语言能力和语言应用的模型，建立和实现计算框架并提出相应的方法不断地完善模型，根据这样的语言模型设计有效地实现自然语言通信的计算机系统，并研讨关于系统的评测技术，最终实现用自然语言与计算机进行通信。目前，具有一定自然语言处理能力的典型应用包括计算机信息检索系统、多语种翻译系统等。</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/610b88d0-9fab-11e8-9539-994b9a0319a0" /></p>

<p>微软创始人比尔&middot;盖茨曾经表示，&ldquo;语言理解是人工智能领域皇冠上的明珠&rdquo;。</p>

<p>语言是逻辑思维和交流的工具，宇宙万物中，只有人类才具有这种高级功能。要实现人与计算机间采用自然语言通信，必须使计算机同时具备自然语言理解和自然语言生成两大功能。</p>

<p>因此，NLP 作为人工智能的一个子领域，其主要目的就包括两个方面：自然语言理解，让计算机理解自然语言文本的意义；自然语言生成，让计算机能以自然语言文本来表达给定的意图、思想等。自然语言是人类智慧的结晶，自然语言处理是人工智能中最为困难的问题之一，而对自然语言处理的研究也是充满魅力和挑战的。</p>

<h3>NLP 领域发展现状如何？</h3>

<p>近年来，自然语言处理处于快速发展阶段。各种词表、语义语法词典、语料库等数据资源的日益丰富，词语切分、词性标注、句法分析等技术的快速进步，各种新理论、新方法、新模型的出现推动了自然语言处理研究的繁荣。互联网与移动互联网和世界经济社会一体化的潮流对自然语言处理技术的迫切需求，为自然语言处理研究发展提供了强大的市场动力。</p>

<p>我国直到上世纪80年代中期才开始较大规模和较系统的自然语言处理研究，尽管较国际水平尚有较大差距，但已经有了比较稳定的研究内容，包括语料库、知识库等数据资源建设，词语切分、句法分析等基础技术，以及信息检索、机器翻译等应用技术。</p>

<p>当前国内外出现了一批基于 NLP 技术的应用系统，例如 IBM 的 Watson 在电视问答节目中战胜人类冠军；苹果公司的 Siri 个人助理被大众广为测试；谷歌、微软、百度等公司纷纷发布个人智能助理；科大讯飞牵头研发高考机器人&hellip;&hellip;但相比于性能趋于饱和的计算机视觉和语音识别技术，自然语言处理因技术难度太大、应用场景太复杂，研究成果还未达到足够的高度。</p>

<h3>自然语言处理中句子级分析技术</h3>

<p>目前，自然语言处理的对象有词、句子、篇章和段落、文本等，但是大多归根到底在句子的处理上，自然语言处理中的自然语言句子级分析技术，可以大致分为词法分析、句法分析、语义分析三个层面。</p>

<p>第一层面的词法分析包括汉语分词和词性标注两部分。和大部分西方语言不同，汉语书面语词语之间没有明显的空格标记，文本中的句子以字串的形式出现。因此汉语自然语言处理的首要工作就是要将输入的字串切分为单独的词语，然后在此基础上进行其他更高级的分析，这一步骤称为分词。</p>

<p>除了分词，词性标注也通常认为是词法分析的一部分。给定一个切好词的句子，词性标注的目的是为每一个词赋予一个类别，这个类别称为词性标记，比如，名词（Noun）、动词（Verb）、形容词（Adjective）等。一般来说，属于相同词性的词，在句法中承担类似的角色。</p>

<p>第二个层面的句法分析是对输入的文本句子进行分析以得到句子的句法结构的处理过程。对句法结构进行分析，一方面是语言理解的自身需求，句法分析是语言理解的重要一环，另一方面也为其它自然语言处理任务提供支持。例如句法驱动的统计机器翻译需要对源语言或目标语言（或者同时两种语言）进行句法分析；语义分析通常以句法分析的输出结果作为输入以便获得更多的指示信息。</p>

<p>根据句法结构表示形式的不同，最常见的句法分析任务可以分为以下三种：</p>

<ol>
	<li>短语结构句法分析，该任务也被称作成分句法分析，作用是识别出句子中的短语结构以及短语之间的层次句法关系；</li>
	<li>依存句法分析，作用是识别句子中词汇与词汇之间的相互依存关系；</li>
	<li>深层文法句法分析，即利用深层文法，例如词汇化树邻接文法、词汇功能文法、组合范畴文法等，对句子进行深层的句法以及语义分析。</li>
</ol>

<p>上述几种句法分析任务比较而言，依存句法分析属于浅层句法分析。其实现过程相对简单，比较适合在多语言环境下的应用，但是依存句法分析所能提供的信息也相对较少。深层文法句法分析可以提供丰富的句法和语义信息，但是采用的文法相对复杂，分析器的运行复杂度也较高，这使得深层句法分析当前不适合处理大规模数据。短语结构句法分析介于依存句法分析和深层文法句法分析之间。</p>

<p>第三个层面是语义分析。语义分析的最终目的是理解句子表达的真实语义。但是，语义应该采用什么表示形式一直困扰着研究者们，至今这个问题也没有一个统一的答案。</p>

<p>语义角色标注是目前比较成熟的浅层语义分析技术。基于逻辑表达的语义分析也得到学术界的长期关注。出于机器学习模型复杂度、效率的考虑，自然语言处理系统通常采用级联的方式，即分词、词性标注、句法分析、语义分析分别训练模型。实际使用时，给定输入句子，逐一使用各个模块进行分析，最终得到所有结果。</p>

<h3>深度学习背景下的自然语言处理</h3>

<p>近年来，随着研究工作的深入，研究者们开始从传统机器学习转向深度学习。2006年开始，有人利用深层神经网络在大规模无标注语料上无监督的为每个词学到了一个分布式表示，形式上把每个单词表示成一个固定维数的向量，当作词的底层特征。在此特征基础上，完成了词性标注、命名实体识别和语义角色标注等多个任务，后来有人利用递归神经网络完成了句法分析、情感分析和句子表示等多个任务，这也为语言表示提供了新的思路。</p>

<p>面向自然语言处理的深度学习研究工作，目前尚处于起步阶段，尽管已有的深度学习算法模型如循环神经网络、递归神经网络和卷积神经网络等已经有较为显著的应用，但还没有重大突破。围绕适合自然语言处理领域的深度学习模型构建等研究应该有着非常广阔的空间。</p>

<p>在当前已有的深度学习模型研究中，难点是在模型构建过程中参数的优化调整方面。主要有深度网络层数、正则化问题及网络学习速率等，可能的解决方案比如有采用多核机提升网络训练速度，针对不同应用场合，选择合适的优化算法等。</p>

<h3>自然语言处理未来的研究方向</h3>

<p>纵观自然语言处理技术研究发展的态势和现状，以下研究方向或问题将可能成为自然语言处理未来研究必须攻克的堡垒：</p>

<p><img alt="enter image description here" src="https://images.gitbook.cn/0f2c36e0-9fab-11e8-9539-994b9a0319a0" /></p>

<ol>
	<li>
	<p>词法和句法分析方面：包括多粒度分词、新词发现、词性标注等；</p>
	</li>
	<li>
	<p>语义分析方面：包括词义消歧、非规范文本的语义分析。其中，非规范划化文本主要指社交平台上比较口语化、弱规范甚至不规范的短文本，因其数据量巨大和实时性而具有研究和应用价值，被广泛用于舆情监控、情感分析和突发事件发现等任务；</p>
	</li>
	<li>
	<p>语言认知模型方面：比如使用深度神经网络处理自然语言，建立更有效、可解释的语言计算模型，例如，词嵌入的发现。还有目前词的表示是通过大量的语料库学习得到的，如何通过基于少量样本来发现新词、低频词也急需探索；</p>
	</li>
	<li>
	<p>知识图谱方面：如何构建能够融合符号逻辑和表示学习的大规模高精度的知识图谱；</p>
	</li>
	<li>
	<p>文本分类与聚类方面：通过有监督、半监督和无监督学习，能够准确进行分类和聚类。当下大多数语料都是没有标签的，未来在无监督或者半监督方面更有需求；</p>
	</li>
	<li>
	<p>信息抽取方面：对于多源异构信息，如何准确进行关系、事件的抽取等。信息抽取主要从面向开放域的可扩展信息抽取技术、自学习与自适应和自演化的信息抽取系统以及面向多源异构数据的信息融合技术方向发展；</p>
	</li>
	<li>
	<p>情感分析方面：包括基于上下文感知的情感分析、跨领域跨语言情感分析、基于深度学习的端到端情感分析、情感解释、反讽分析、立场分析等；</p>
	</li>
	<li>
	<p>自动文摘方面：如何表达要点信息？如何评估信息单元的重要性？这些都要随着语义分析、篇章理解、深度学习等技术快速发展；</p>
	</li>
	<li>
	<p>信息检索方面：包括意图搜索、语义搜索等，都将有可能出现在各种场景的垂直领域，将以知识化推理为检索运行方式，以自然语言多媒体交互为手段的智能化搜索与推荐技术；</p>
	</li>
	<li>
	<p>自动问答方面：包括深度推理问答、多轮问答等各种形式的自动问答系统；</p>
	</li>
	<li>
	<p>机器翻译方面：包括面向小数据的机器翻译、非规范文本的机器翻译和篇章级机器翻译等。</p>
	</li>
</ol>

<h3>总结</h3>

<p>本文，从 NLP 的概念出发，首先指出了自然语言处理的两大内核：自然语言理解和自然语言生成；然后简单介绍了国内外 NLP 研究发展现状；紧接着重点介绍了最常用、应用最广的自然语言处理中句子级分析技术，最后在深度学习背景下，指出了自然语言处理未来可能遇到的挑战和重点研究方向，为后期的学习提供指导和帮助。</p>

<h3>关于本课程结束寄语</h3>

<p>首先，非常感谢 GitChat 给我们提供这样一个学习平台，非常感谢编辑老师的辛苦指导，非常感谢各位同学能够报名参加本课程。</p>

<p>近年来，深度学习正在逐渐的填平领域鸿沟，继在图像和语音领域取得的巨大成功后，深度学习在同属人类认知范畴的自然语言处理方面也在不断取得更好的效果。</p>

<p>在《中文自然语言处理入门实战》达人课中，相信各位同学通过一些小数据量的&ldquo;简易版&rdquo;实例，已经体会到了中文自然语言处理的精妙，并完成了中文自然语言处理从0到1的过程。但如何更好地把这些技术应用在工业生产中，不仅需要过硬的技术更要求熟练掌握相关核心算法的原理，做到知其然并知其所以然。</p>

<p>因此，我计划在《中文自然语言处理入门实战》课程的基础上，推出《中文自然语言处理核心算法精要剖析》课程，本课程共包含28节，包含了中文自然语言处理核心算法精要，有针对性的面向想要深入学习中文自然语言处理和想要从事 NLP 相关工作的人，重点学习这些核心算法的原理，注重理论与实战结合，助力在中文自然语言处理上理解的更深、做的更好。</p>

<p>对《中文自然语言处理核心算法精要剖析》感兴趣的同学，请继续关注我，课程后期将尽快上线。</p>

<p><strong>参考以及推荐阅读</strong></p>

<ul>
	<li><a href="http://36kr.com/p/5096134.html">今日头条李航：深度学习 NLP 的现有优势与未来挑战</a></li>
</ul>
				    </section>
					</article>
				</main>
            </div>
        </div>
    </section>
    <a href="#top" id="goTop" ><i class="fa fa-angle-up fa-3x"></i></a>
	<script src="../js/jquery.min.js"></script>
    <script type="text/javascript" src="../js/datas/detailAngularJs.js"></script>
    <script src="../plugins/scrolltopcontrol.js"></script>
</body>
</html>